% This file was created with JabRef 2.10.
% Encoding: UTF8


@Article{-2014-p319-334,
  Title                    = {Accident modelling and analysis in process industries},
  Author                   = {Ali Al-shanini and Arshad Ahmad and Faisal Khan},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {319 - 334},
  Volume                   = {32},

  Abstract                 = {Abstract Accident modelling is a methodology used to relate the causes and effects of events that lead to accidents. This modelling effectively seeks to answer two main questions: (i) Why does an accident occur, and (ii) How does it occur. This paper presents a review of accident models that have been developed for the chemical process industry with in-depth analyses of a class of models known as dynamic sequential accident models (DSAMs). \{DSAMs\} are sequential models with a systematic procedure to utilise precursor data to estimate the posterior risk profile quantitatively. \{DSAM\} also offers updates on the failure probabilities of accident barriers and the prediction of future end states. Following a close scrutiny of these methodologies, several limitations are noted and discussed, and based on these insights, future work is suggested to enhance and improve this category of models further. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2014.09.016},
  File                     = {Published version:-2014-p319-334.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Accident modelling},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {过程控制的事故建木以及分析},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423014001594},
  Zone                     = {3Q}
}

@InProceedings{-2011-p1335-1340,
  Title                    = {Research of risk-based security assessment of power system},
  Author                   = {Chen Li-jie and Li Jing and Cai Jing},
  Booktitle                = {Electric Utility Deregulation and Restructuring and Power Technologies (DRPT), 2011 4th International Conference on},
  Year                     = {2011},
  Pages                    = {1335--1340},

  Doi                      = {10.1109/DRPT.2011.5994103},
  File                     = {Published version:-2011-p1335-1340.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {电力系统的基于风险的安全评估，没感觉，会议水文章},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5994103},
  Zone                     = {-}
}

@Article{-2013-p465-470,
  Title                    = {Methods for Using Intrusion Logs to Establish Criminal Evidence Against Intruders },
  Author                   = {Nehinbe Joshua ojo and Nehibe Johnson Ige},
  Journal                  = {Procedia Computer Science },
  Year                     = {2013},
  Note                     = {The 4th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN-2013) and the 3rd International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH) },
  Number                   = {0},
  Pages                    = {465 - 470},
  Volume                   = {21},

  Abstract                 = {Abstract The admissible rate of criminal evidence against intruders has continued to generate classical arguments because the reports extracted from intrusion lo gs are often disputed in many courts of law. Besides, forensic experts still spend excessive resources to prepare reports for litigation before intruders can be charged. Thus, we propose Forenlog Analyzer to lessen the aforementioned problems. The pattern of attacks in an intrusion log is partitioned into sixty subgroups according to the values held in the timestamp of the evidence and the overall uncertainty of the pattern is subsequently computed. Evaluation illustrates that neither the internal attributes nor the external attributes of attacks are sufficient to litigate intruders in courts of laws in all cases. The results further demonstrate that forensic analysts should not just destroy, include or ignore supportive evidence on the basis of their sizes without determine their inherent uncertainty.},
  Doi                      = {http://dx.doi.org/10.1016/j.procs.2013.09.062},
  File                     = {Published version:-2013-p465-470.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-0509},
  Keywords                 = {Intruder},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877050913008557},
  Zone                     = {-}
}

@InProceedings{-2011-p428-433,
  Title                    = {Industry-University-Research Knowledge Innovation Alliance Risk Assessment Based on BP Neural Network Algorithm},
  Author                   = {Xiao Ling-nuo and Shi Jian-feng and Sun Yu-zhong and Yang Mei-rong},
  Booktitle                = {Information Management, Innovation Management and Industrial Engineering (ICIII), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {Nov},
  Pages                    = {428-433},
  Volume                   = {3},

  Abstract                 = {This paper analyzes risk causes of Industry-university-research knowledge innovation alliance from the perspective of knowledge innovation, it takes knowledge sharing insufficiency, knowledge spillovers, insufficient knowledge transformation and upgrade as the origin of risk, Management risk and interest risk are inner risk, market risk and technology risk are extraversion risk. Learn from previous research achievements, the risk assessment indexes are set. According to the risk characteristics of research, through the theoretical analysis, BP neural network is took as a risk evaluation method, MATLAB software is used to survey data processing, and the risk degree evaluation model is preliminary formatted, at last, an empirical demonstration is took to prove it.},
  Doi                      = {10.1109/ICIII.2011.384},
  File                     = {-2011-p428-433.pdf:-2011-p428-433.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {backpropagation;educational institutions;innovation management;knowledge management;mathematics computing;risk management;BP neural network algorithm;Matlab software;industry-university-research knowledge innovation alliance;knowledge sharing;knowledge spillovers;knowledge transformation;risk assessment;Companies;Educational institutions;Indexes;Personnel;Risk management;Technological innovation;Training;industry-university-research;knowledge innovation alliance;neural network;risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6116898&isnumber=6114683},
  Zone                     = {-}
}

@Article{-2014-p860-864,
  Title                    = {Prediction of prognosis to lamivudine in patients with spontaneous reactivation of hepatitis B virus-related acute-on-chronic liver failure: Using virologic response at week 4 },
  Author                   = {Yang Xiang-hui and Xiao Lang and Zhang Yan and Zhang Li and Shi Xiao-feng and Ren Hong},
  Journal                  = {European Journal of Internal Medicine},
  Year                     = {2014},
  Number                   = {9},
  Pages                    = {860 - 864},
  Volume                   = {25},

  Abstract                 = {AbstractBackground/aims Current results had demonstrated lamivudine (LAM) contributed to improve liver function and short-term prognosis in patients with hepatitis B virus-related acute-on-chronic liver failure (ACLF), but data concerning the outcome of long-term prognosis are limited. Our objective was to explore the prediction value of early viral response for prognosis and \{LAM\} resistance in \{ACLF\} patients with lamivudine treatment within 96&#xa0;weeks. Methods One hundred and forty consecutive subjects were recruited, 76 patients were treated with \{LAM\} and supportive treatment (LAM group) and 64 patients only received supportive treatment (non-NAs group). All the patients were followed up until death or 96&#xa0;weeks. The primary end point was overall survival rate at 96&#xa0;weeks, as well as the relationship between the virologic response at weeks 4 or 12 and prognosis and resistance at 96&#xa0;weeks. Results At 96&#xa0;weeks, the cumulative survival was higher in the \{LAM\} group than that in the non-NA group (43/76 (56.58%) vs 9/64 (14.06%), respectively, p&#xa0;=&#xa0;0.000). The survival rate of patients achieved complete viral response (CVR) at week 4 was higher than that of those with partial virologic response (PVR) during the 96-week follow-up (27/29 [93.10%] vs 16/45 [35.56%], p&#xa0;=&#xa0;0.000). In \{CVR\} patients, there was a significant improvement in model for end-stage liver failure (MELD) scores compared to PVR. Logistic recurrence indicated that both 4-week \{CVR\} and \{MELD\} scores were an independent predictor of the 96-week survival. Twelve patients developed \{LAM\} resistance (22.22%); all of them came from the \{PVR\} at 4&#xa0;weeks. Conclusion \{LAM\} can significantly improve the long-term survival rate, and 4&#xa0;weeks \{CVR\} can predict the long-term clinical outcome and LAM-resistant in patients with HBV-related ACLF. },
  Doi                      = {http://dx.doi.org/10.1016/j.ejim.2014.10.007},
  File                     = {Published version:-2014-p860-864.pdf:PDF},
  Impactfactor             = {2.049},
  ISSN                     = {0953-6205},
  Keywords                 = {Hepatitis B virus},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，不相关，医学方面的},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0953620514002891},
  Zone                     = {3Q}
}

@InProceedings{Aagedal-2002-p51-62,
  Title                    = {Model-based risk assessment to improve enterprise security},
  Author                   = {Aagedal, J.O. and den Braber, F. and Dimitrakos, T. and Gran, B.A. and Raptis, D. and Stolen, K.},
  Booktitle                = {Enterprise Distributed Object Computing Conference, 2002. EDOC '02. Proceedings. Sixth International},
  Year                     = {2002},
  Pages                    = {51-62},

  Abstract                 = {The main objective of the CORAS project is to provide methods and tools for precise, unambiguous, and efficient risk assessment of security critical systems. To this end, we advocate a model-based approach to risk assessment, and define the required models for this. Whereas traditional risk assessment is performed without any formal description of the target of evaluation or results of the risk assessment, CORAS aims to provide a well defined set of models well suited to (1) describe the target of assessment at the right level of abstraction, (2) as a medium for communication between different groups of stakeholders involved in a risk assessment, and (3) to document risk assessment results and the assumptions on which these results depend. We propose models for each step in a risk assessment process and report results of use.},
  Doi                      = {10.1109/EDOC.2002.1137696},
  File                     = {Published version:Aagedal-2002-p51-62.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {business data processing;distributed object management;risk management;security of data;CORAS project;Reference Model for Open Distributed Processing;distributed object systems;enterprise security;model-based risk assessment;research and development project;security critical systems;Failure analysis;Informatics;Laboratories;Performance evaluation;Research and development;Risk analysis;Risk management;Security;Telecommunications;Unified modeling language},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1137696&isnumber=25384},
  Zone                     = {-}
}

@InProceedings{Abadeer-2003-p17-22,
  Title                    = {Behavior of NBTI under AC dynamic circuit conditions},
  Author                   = {Abadeer, W. and Ellis, W.},
  Booktitle                = {Reliability Physics Symposium Proceedings, 2003. 41st Annual. 2003 IEEE International},
  Year                     = {2003},
  Month                    = {March},
  Pages                    = {17-22},

  Abstract                 = {The NBTI mechanism was investigated for a 5 nm gate oxide technology under AC dynamic circuit conditions in the frequency range of 2 MHz to 20 MHz and device "ON" duty factor in the range of 30% to 70%. The increase in the magnitude of threshold voltage, and the decrease in device current under AC operation are generally lower than that of DC operation by 3× or higher. The time power function dependency of the degradation at sufficient stress times is lower than that of DC operation. These results indicate that more emphasis needs to be placed on AC behavior of NBTI for scaled CMOS technologies.},
  Doi                      = {10.1109/RELPHY.2003.1197714},
  File                     = {Published version:Abadeer-2003-p17-22.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {CMOS integrated circuits;MOSFET;integrated circuit reliability;semiconductor device models;thermal stresses;0.25 micron;2 to 20 MHz;5 nm;5 nm gate oxide technology;AC dynamic circuit conditions;NBTI mechanism;ON duty factor;Si:B;circuit reliability;degradation;device current;negative bias temperature instability;physical model;polysilicon:B gated PFETs;scaled CMOS technologies;threshold voltage;time power function dependency;CMOS technology;Circuits;Degradation;Frequency;Microelectronics;Niobium compounds;Predictive models;Stress;Threshold voltage;Titanium compounds},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1197714&isnumber=26927},
  Zone                     = {-}
}

@Article{Abdolhamid-2011-p575-593,
  Title                    = {Domino effect in process-industry accidents – An inventory of past events and identification of some patterns},
  Author                   = {Bahman Abdolhamidzadeh and Tasneem Abbasi and D. Rashtchian and S.A. Abbasi},
  Journal                  = {Journal of Loss Prevention in the Process Industries},
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {575 - 593},
  Volume                   = {24},

  Abstract                 = {The paper presents an inventory, perhaps the most comprehensive till date, of the major process-industry accidents involving ‘domino effect’. The inventory includes, among other relevant information, the sequence of accidents that had occurred in each domino episode. The information has been analyzed to identify several patterns which may be useful in further work on understanding domino effect and reducing the probability of its occurrence in future. A concept of ‘local domino effect’ has been introduced. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2010.06.013},
  File                     = {Published version:Abdolhamid-2011-p575-593.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Domino effect},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.10.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095042301000080X},
  Zone                     = {3Q}
}

@Article{Abdolhamid-2010-p416-426,
  Title                    = {A new method for assessing domino effect in chemical process industry },
  Author                   = {Bahman Abdolhamidzadeh and Tasneem Abbasi and D. Rashtchian and S.A. Abbasi},
  Journal                  = {Journal of Hazardous Materials},
  Year                     = {2010},
  Number                   = {1–3},
  Pages                    = {416 - 426},
  Volume                   = {182},

  Abstract                 = {A new methodology is presented with which the likely impact of accident in one process unit of an industry on other process units can be forecast and assessed. The methodology is based on Monte Carlo Simulation and overcomes the limitations of analytical methods, used hitherto, which were inherently limited in their ability to handle the uncertainty and the complexity associated with domino effect phenomena. The methodology has been validated and its applicability has been demonstrated with two case studies. },
  Doi                      = {http://dx.doi.org/10.1016/j.jhazmat.2010.06.049},
  File                     = {Published version:Abdolhamid-2010-p416-426.pdf:PDF},
  Impactfactor             = {3.925},
  ISSN                     = {0304-3894},
  Keywords                 = {Accidents},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304389410007892},
  Zone                     = {1Q}
}

@InProceedings{Abercrombi-2011-p1-10,
  Title                    = {Validating Cyber Security Requirements: A Case Study},
  Author                   = {Abercrombie, R.K. and Sheldon, F.T. and Mili, A},
  Booktitle                = {System Sciences (HICSS), 2011 44th Hawaii International Conference on},
  Year                     = {2011},
  Month                    = {Jan},
  Pages                    = {1-10},

  Doi                      = {10.1109/HICSS.2011.480},
  File                     = {Published version:Abercrombi-2011-p1-10.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1530-1605},
  Keywords                 = {formal verification;security of data;cyber security requirement validation;security attributes;security policy;system security;system vulnerabilities;Availability;Certification;Computer security;Mobile communication;Vehicles},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5718517&isnumber=5718420},
  Zone                     = {-}
}

@Article{Abimbola-2014-p74-85,
  Title                    = {Dynamic safety risk analysis of offshore drilling },
  Author                   = {Majeed Abimbola and Faisal Khan and Nima Khakzad},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {74 - 85},
  Volume                   = {30},

  Abstract                 = {Abstract The exploration and production of oil and gas involve the drilling of wells using either one or a combination of three drilling techniques based on drilling fluid density: conventional overbalanced drilling, managed pressure drilling and underbalanced drilling. The conventional overbalanced drilling involves drilling of wells with mud which exerts higher hydrostatic bottom-hole pressure than the formation pore pressure. Unlike the conventional overbalanced drilling, underbalanced drilling involves designing the hydrostatic pressure of the drilling fluid to be lower than the pore pressure of the formation being drilled. During circulation, the equivalent circulating density is used to determine the bottom-hole pressure conditions. Due to lower hydrostatic pressure, underbalanced drilling portends higher safety risk than its alternatives of conventional overbalanced drilling and managed pressure drilling. The safety risk includes frequent kicks from the well and subsequent blowout with potential threat to human, equipments and the environment. Safety assessment and efficient control of well is critical to ensure a safe drilling operation. Traditionally, safety assessment is done using static failure probabilities of drilling components which failed to represent a specific case. However, in this present study, a dynamic safety assessment approach for is presented. This approach is based on Bow-tie analysis and real time barriers failure probability assessment of offshore drilling operations involving subsurface Blowout Preventer. The Bow-tie model is used to represent the potential accident scenarios, their causes and the associated consequences. Real time predictive models for the failure probabilities of key barriers are developed and used in conducting dynamic risk assessment of the drilling operations. Using real time observed data, potential accident probabilities and associated risks are updated and used for safety assessment. This methodology can be integrated into a real time risk monitoring device for field application during drilling operations. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2014.05.002},
  File                     = {Published version:Abimbola-2014-p74-85.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Dynamic risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，跟我做的很像，重点看，故障树，Bow-tie模型，没有仿真

case study},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423014000667},
  Zone                     = {3Q}
}

@InProceedings{Adamov-2011-p169-172,
  Title                    = {A security model of individual cyberspace},
  Author                   = {Adamov, A and Hahanov, V.},
  Booktitle                = {Design Test Symposium (EWDTS), 2011 9th East-West},
  Year                     = {2011},
  Month                    = {Sept},
  Pages                    = {169-172},

  Doi                      = {10.1109/EWDTS.2011.6116597},
  File                     = {Published version:Adamov-2011-p169-172.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {cloud computing;formal specification;security of data;ICS security model;cloud computing;contemporary security threats;formal model;individual cyberspace;real world objects;security model;virtual environment;Analytical models;Cloud computing;Computational modeling;Cyberspace;Integrated circuit modeling;MySpace;Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6116597&isnumber=6116407},
  Zone                     = {-}
}

@Article{Ahmadineja-2011-p2221-2240,
  Title                    = {A hybrid model for correlating alerts of known and unknown attack scenarios and updating attack graphs },
  Author                   = {Seyed Hossein Ahmadinejad and Saeed Jalili and Mahdi Abadi},
  Journal                  = {Computer Networks },
  Year                     = {2011},
  Number                   = {9},
  Pages                    = {2221 - 2240},
  Volume                   = {55},

  Abstract                 = {Managing and analyzing a huge number of low-level alerts is very difficult and exhausting for network administrators. Alert correlation methods have been proposed to decrease the number of alerts and make them more intelligible. Proposed methods for alert correlation are different in terms of their performance, accuracy and adaptivity. We present a new hybrid model not only to correlate alerts as accurately and efficiently as possible but also to be able to boost the model in the course of time. The model presented in this paper consists of two parts: (1) an attack graph-based method to correlate alerts raised for known attacks and hypothesize missed alerts and (2) a similarity-based method to correlate alerts raised for unknown attacks which can not be correlated using the first part and also to update the attack graph. These two parts cooperate with each other such that if the first part could not correlate a new alert, the second part is applied. We propose two different methods for these two parts. In order to update the attack graph, we present a technique (using the similarity-based method in the second part of the model) which is actually the most salient feature of our model: capability of hypothesizing missed exploits and discovering defects in pre and post conditions of known exploits in attack graphs. We also propose an additional method named alerts bisimulation for compressing graphs of correlated alerts. The results of experiments on \{DARPA2000\} clearly show the model can accurately correlate alerts. Also the ability of updating attack graphs is illustrated using an experiment. },
  Doi                      = {http://dx.doi.org/10.1016/j.comnet.2011.03.005},
  File                     = {Published version:Ahmadineja-2011-p2221-2240.pdf:PDF},
  Impactfactor             = {1.231},
  ISSN                     = {1389-1286},
  Keywords                 = {Alert correlation},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1389128611000983},
  Zone                     = {3Q}
}

@Article{Ahmed-2011-p1738-1749,
  Title                    = {SLA-based complementary approach for network intrusion detection },
  Author                   = {Abdulghani Ali Ahmed and Aman Jantan and Tat-Chee Wan},
  Journal                  = {Computer Communications },
  Year                     = {2011},
  Number                   = {14},
  Pages                    = {1738 - 1749},
  Volume                   = {34},

  Abstract                 = {Enhancing the intrusion detection system is essential to maintain user confidence in network services security. However, the threat of intruders on Internet services is prevalent. This paper proposes a distributed edge-to-edge complementary approach for intrusion detection in a DiffServ/MPLS domain. The QoS metrics are inspected at the edges routers to determine anomalous behavior in the network traffic. Consumed ratios of one-way delay variation (OWDV) and packet loss are computed to monitor service level agreement (SLA) violations. The bandwidth ratio is measured to differentiate abnormal from normal traffic as well as to detect multiple intrusions launched simultaneously. We employed \{SLA\} as a comparison scale to infer the deviation between the users consumed ratios and the predefined ratios in the SLA. Service violation occurs and intrusion may be launched when the predefined ratios are exceeded. The complementary services of DiffServ and \{MPLS\} techniques guarantee accurate measurements, whereas the complementary measurements of active and passive techniques immunize network performance against scalability limitation. Simulation results indicate that the proposed approach is capable of monitoring \{SLA\} violations and can filter out traffic of intruders who breach \{SLA\} without disturbing the normal traffic of legitimate users. },
  Doi                      = {http://dx.doi.org/10.1016/j.comcom.2011.03.013},
  File                     = {Published version:Ahmed-2011-p1738-1749.pdf:PDF},
  Impactfactor             = {1.079},
  ISSN                     = {0140-3664},
  Keywords                 = {\{SLA\}},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0140366411001204},
  Zone                     = {3Q}
}

@Article{Ahmed-2013-p777-785,
  Title                    = {Probabilistic Risk Assessment Using Dynamic Flowgraph Methodology for Copper Chloride CANDU-SCWR Hydrogen Production },
  Author                   = {Fayyaz Ahmed},
  Journal                  = {Procedia Computer Science },
  Year                     = {2013},
  Note                     = {The 4th International Conference on Ambient Systems, Networks and Technologies (ANT 2013), the 3rd International Conference on Sustainable Energy Information Technology (SEIT-2013) },
  Number                   = {0},
  Pages                    = {777 - 785},
  Volume                   = {19},

  Abstract                 = {Copper chloride cycle is a proven efficient method to separate hydrogen from water. Super Critical Water Reactor (SCWR) is the focus of generation \{IV\} reactor design for optimization of the nuclear energy. This research has modeled an integration of copper chloride cycle hydrogen and oxygen units with the proposed \{SCWR\} Primary Heat Transport (PHT) cycle, to accommodate the load variation and to compensate \{SCWR\} excessive heat produced during off peak hours. Copper chloride cycle is regenerative and can meet up to fifty percent of its heat requirements; the research investigates the efficiency and safety of a switch over between self-generated and external heat mechanism. The integration is analyzed for its reliability by using the conventional fault tree method and state of the art dynamic flowgraph methodology. Dynamic flowgraph methodology dymonda software analysis produced probable alternates, depicting the probabilistic risk assessment for the integration model. },
  Doi                      = {http://dx.doi.org/10.1016/j.procs.2013.06.102},
  File                     = {Published version:Ahmed-2013-p777-785.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-0509},
  Keywords                 = {\{SCWR\}},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，化学方面的，也用到了故障树},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877050913007102},
  Zone                     = {-}
}

@InProceedings{Ahmed-2013-p871-878,
  Title                    = {Smart Grid Wireless Network Security Requirements Analysis},
  Author                   = {Ahmed, K.A and Aung, Z. and Svetinovic, D.},
  Booktitle                = {Green Computing and Communications (GreenCom), 2013 IEEE and Internet of Things (iThings/CPSCom), IEEE International Conference on and IEEE Cyber, Physical and Social Computing},
  Year                     = {2013},
  Month                    = {Aug},
  Pages                    = {871-878},

  Doi                      = {10.1109/GreenCom-iThings-CPSCom.2013.153},
  File                     = {Published version:Ahmed-2013-p871-878.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {power system economics;power system reliability;radio networks;smart power grids;telecommunication security;GPS;GSM;ICT;cyber security;global positioning system;global system for mobile communication;information and communication technology;intelligent electronic devices;power grids;security requirements engineering process;smart grid wireless network security;smart grids;Communication system security;Energy management;Home appliances;Power grids;Security;Wireless networks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6682167&isnumber=6682013},
  Zone                     = {-}
}

@InProceedings{Ahmed-2011-p1-10,
  Title                    = {New and improved safety and risk assessment model for petrol fuel station},
  Author                   = {Ahmed, M.M. and Kutty, S.R.M. and Shariff, A.M. and Khamidi, M.F.},
  Booktitle                = {National Postgraduate Conference (NPC), 2011},
  Year                     = {2011},
  Month                    = {Sept},
  Pages                    = {1-10},

  Abstract                 = {Measurement of risk levels posed by hazardous activities on organizations assets, entities and processes are of highly significance. This can be done with risk assessment process. Risk assessment is the method to evaluate the risk arising from many activities, during the process. Many risk assessment criteria are using in many organizations to calculate the risk. Three (03) risk assessment criteria were selected to determine the risk associated with hazardous activities. 1267 activities were analyzed by using each risk assessment method. It was noticed that hazardous activities ranking in these (03) three criteria were different from one another. Fuel stations data was used for analysis. Risk variations are an indicator that there exists a significant gap among these criteria. By using fuel station data new risk and safety assessment criterion developed. Analysis of activities was carried out. Results obtained highlighted that fatalities and major accidents not only be considered to improve safety records but the reduction of incidents and near misses should be of vital importance. Near miss, accidents, incidents and miscellaneous cases were analyzed. Severity index calculated and then it multiplied with frequency to determine the risk level. Based upon this risk factor management should decide the areas of further improvement. Risk assessment is vitally important for discussion in HSE committee meetings. During analysis it was highly felt that one (01) major cause for occurrences of accidents, incidents, near misses and miscellaneous cases during working is the use of incompatible risk assessment criterion. In this study a new and improved risk and safety and risk assessment model developed and proposed for further implementation.},
  Doi                      = {10.1109/NatPC.2011.6136378},
  File                     = {Published version:Ahmed-2011-p1-10.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {accident prevention;fuel processing industries;hazards;risk management;safety;HSE committee;accidents;hazardous activities;organizations assets;petrol fuel station;risk assessment model;risk levels measurement;safety assessment model;Accidents;Companies;Fuels;Hazards;Injuries;Risk management;Risk assessment;assets;frequency;gaps;hazards;miscellaneous;root cause;severity},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6136378&isnumber=6136247},
  Zone                     = {-}
}

@Article{Ahmed-2008-p226-240,
  Title                    = {XML-Based Data Model and Architecture for a Knowledge-Based Grid-Enabled Problem-Solving Environment for High-Throughput Biological Imaging},
  Author                   = {Ahmed, W.M. and Lenz, D. and Jia Liu and Robinson, J.Paul and Ghafoor, A.},
  Journal                  = {Information Technology in Biomedicine, IEEE Transactions on},
  Year                     = {2008},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {226-240},
  Volume                   = {12},

  Doi                      = {10.1109/TITB.2007.904153},
  File                     = {Published version:Ahmed-2008-p226-240.pdf:PDF},
  Impactfactor             = {1.978},
  ISSN                     = {1089-7771},
  Keywords                 = {XML;biological techniques;biology computing;grid computing;knowledge acquisition;knowledge based systems;spatiotemporal phenomena;XML-based data model;automated knowledge extraction;cellular imaging markup language;grid infrastructure;high-level knowledge management;high-throughput biological imaging;knowledge-based grid-enabled problem-solving environment;microscopic imaging;reasoning services;spatiotemporal event composition;spatiotemporal knowledge extraction;Grid computing;High-throughput biological imaging;grid computing;high-throughput biological imaging;knowledge extraction;knowledge extraction (KE);spatio-temporal modeling;spatiotemporal modeling;Artificial Intelligence;Database Management Systems;Humans;Image Interpretation, Computer-Assisted;Information Dissemination;Information Storage and Retrieval;Internet;Models, Theoretical;Pattern Recognition, Automated;Programming Languages;Radiology Information Systems;United States},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4358901&isnumber=4463817},
  Zone                     = {2Q}
}

@InProceedings{Aitken-2010-p1-6,
  Title                    = {A case for dynamic risk assessment in NEC systems of systems},
  Author                   = {Aitken, J.M. and Alexander, R. and Kelly, T.},
  Booktitle                = {System of Systems Engineering (SoSE), 2010 5th International Conference on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {1-6},

  Abstract                 = {The level of complexity in Systems of Systems is increasing as more complex functionality emerges from the interaction of individual components. As networks become more complex it becomes more difficult for an individual to identify potential safety risks. We know, from previous accidents, that poor understanding of networks can be dangerous. In this paper, we demonstrate the potential value of incorporating a process to identify risks in a deployed network, focusing on factors concerned with the interaction of this process with a user, and the potential for new hazards.},
  Doi                      = {10.1109/SYSOSE.2010.5544114},
  File                     = {Published version:Aitken-2010-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {military computing;risk management;safety systems;user interfaces;NEC systems-of-systems;dynamic risk assessment;network enabled capability;Accidents;Aerodynamics;Aerospace safety;Bandwidth;Computer science;Delay;Hazards;National electric code;Risk management;Systems engineering and theory;Risk Assessment;Safety;Systems of Systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5544114&isnumber=5543954},
  Zone                     = {-}
}

@Article{Aiyou-2014-p204-212,
  Title                    = {City Fire Risk Analysis based on Coupling Fault Tree Method and Triangle Fuzzy Theory },
  Author                   = {Wu Aiyou and Shi Shiliang and Li Runqiu and Tang Deming and Tang Xiafang},
  Journal                  = {Procedia Engineering },
  Year                     = {2014},
  Note                     = {2014 International Symposium on Safety Science and Technology },
  Number                   = {0},
  Pages                    = {204 - 212},
  Volume                   = {84},

  Abstract                 = {Abstract City fire risk analysis is a very important basis for scientifically and effectively planning and deploying urban fire protection and formulating its related regulations. It also can guarantee and facilitate the coordinated development of city's economic construction and social economy. Combining the city's development characteristic of China and the fire control safety management state, the thesis based on fire control safety engineering and systematic safety engineering theory and combine, has set up the fire risk fault tree of city area. Applying the method of the fuzzy important degree sorted the basic incident of the fault tree and did a fuzzy graduation on the factors of caused fire. The order and grade of fuzzy probability has shown that the main factors of limiting fire risk are the city area overall arrangement, the moved fire control strength and the personnel quality, which is in conformity with the main reason of the city fire of our country taken place over the years. For this reason, in order to limit the city fire risk fundamentally, we must manage the above three important factors strictly, which has a very positive significance for city fire safety management of China. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2014.10.427},
  File                     = {Published version:Aiyou-2014-p204-212.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {City fire risk},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705814017482},
  Zone                     = {-}
}

@InProceedings{AlBaalbak-2013-p1-4,
  Title                    = {Autonomic Critical Infrastructure Protection ({AC}{IP}) system},
  Author                   = {Al Baalbaki, B. and Al-Nashif, Y. and Hariri, S. and Kelly, D.},
  Booktitle                = {Computer Systems and Applications (AICCSA), 2013 ACS International Conference on},
  Year                     = {2013},
  Pages                    = {1--4},

  Doi                      = {10.1109/AICCSA.2013.6616494},
  File                     = {Published version:AlBaalbak-2013-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6616494},
  Zone                     = {-}
}

@Article{Alam-2002-p42-48,
  Title                    = {A future of function or failure? [CMOS gate oxide scaling]},
  Author                   = {Alam, M. and Weir, B. and Silverman, A.},
  Journal                  = {Circuits and Devices Magazine, IEEE},
  Year                     = {2002},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {42-48},
  Volume                   = {18},

  Abstract                 = {Transistors are scaled in each successive technology generation to increase circuit speed and to improve packing density. However, as the devices get smaller and the gate oxides thinner, ensuring their reliability becomes increasingly difficult. The simple question is: based on the current reliability specifications, will 99.99% of the ICs produced today with given technology remain functional for at least ten years into the future? This is a question that device engineers, circuit designers, and system architects must grapple with as we move into the unknown territory of sub-0.1 μm CMOS technology.},
  Doi                      = {10.1109/101.994857},
  File                     = {Published version:Alam-2002-p42-48.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {8755-3996},
  Keywords                 = {CMOS integrated circuits;Weibull distribution;dielectric thin films;failure analysis;integrated circuit reliability;leakage currents;semiconductor device breakdown;0.1 micron;CMOS technology;ICs;MOSFET;Weibull distribution;accelerated testing;device scaling;gate dielectric;gate oxides;high-k dielectrics;leakage current;oxide failure;oxide thickness reduction;reliability specifications;voltage scaling;Atomic measurements;Bridge circuits;Circuits and systems;Degradation;History;Insulation;MOSFETs;Semiconductor device reliability;Voltage;Weibull distribution},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=994857&isnumber=21453},
  Zone                     = {-}
}

@Article{Alberts-2003-p-,
  Title                    = {Introduction to the OCTAVE Approach},
  Author                   = {Alberts, Christopher and Dorofee, Audrey and Stevens, James and Woody, Carol},
  Journal                  = {Pittsburgh, PA, Carnegie Mellon University},
  Year                     = {2003},

  File                     = {Published version:Alberts-2003-p-.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.itgovernanceusa.com/files/Octave.pdf},
  Zone                     = {-}
}

@Article{Alcaraz-2013-p1091-1104,
  Title                    = {Security of industrial sensor network-based remote substations in the context of the Internet of Things },
  Author                   = {Cristina Alcaraz and Rodrigo Roman and Pablo Najera and Javier Lopez},
  Journal                  = {Ad Hoc Networks },
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {1091 - 1104},
  Volume                   = {11},

  Abstract                 = {The main objective of remote substations is to provide the central system with sensitive information from critical infrastructures, such as generation, distribution or transmission power systems. Wireless sensor networks have been recently applied in this particular context due to their attractive services and inherent benefits, such as simplicity, reliability and cost savings. However, as the number of control and data acquisition systems that use the Internet infrastructure to connect to substations increases, it is necessary to consider what connectivity model the sensor infrastructure should follow: either completely isolated from the Internet or integrated with it as part of the Internet of Things paradigm. This paper therefore addresses this question by providing a thorough analysis of both security requirements and infrastructural requirements corresponding to all those TCP/IP integration strategies that can be applicable to networks with constrained computational resources. },
  Doi                      = {http://dx.doi.org/10.1016/j.adhoc.2012.12.001},
  File                     = {Published version:Alcaraz-2013-p1091-1104.pdf:PDF},
  Impactfactor             = {1.456},
  ISSN                     = {1570-8705},
  Keywords                 = {The Internet},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1570870512002156},
  Zone                     = {3Q}
}

@InProceedings{Alipour-2012-p612-617,
  Title                    = {Bayesian belief based tactic analysis of attack events in broadcast soccer video},
  Author                   = {Alipour, S. and Oskouie, P. and Eftekhari-Moghadam, A.},
  Booktitle                = {Informatics, Electronics Vision (ICIEV), 2012 International Conference on},
  Year                     = {2012},
  Month                    = {May},
  Pages                    = {612-617},

  Doi                      = {10.1109/ICIEV.2012.6317514},
  File                     = {Published version:Alipour-2012-p612-617.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {belief networks;broadcasting;data mining;feature extraction;image classification;sport;video signal processing;2010 UEFA championship games;Bayesian belief based tactic analysis;Bayesian belief network;attack situations extraction;audiovisual features extraction;broadcast soccer video;defense situations extraction;goal situations identification;heuristic rules;match sequences extraction;player policies;post-processing level;semantic mining;team strength detection;team weaknesses detection;Analytical models;Robustness;audio-visual features;goal detection;play and break;soccer semantic analysis;tactical analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6317514&isnumber=6317321},
  Zone                     = {-}
}

@Article{Almaraz-2012-p119-131,
  Title                    = {Fatigue endurance and crack propagation under rotating bending fatigue tests on aluminum alloy \{AISI\} 6063-T5 with controlled corrosion attack },
  Author                   = {Gonzalo M. Domínguez Almaraz and Jorge L. Ávila Ambriz and Erasmo Cadenas Calderón},
  Journal                  = {Engineering Fracture Mechanics },
  Year                     = {2012},
  Number                   = {0},
  Pages                    = {119 - 131},
  Volume                   = {93},

  Abstract                 = {Rotating bending fatigue tests were carried out on the aluminum alloy 6063-T5 for pre-corroded and non corroded specimens. Special attention was devoted to fatigue endurance reduction induced by controlled surface corrosion on testing specimens. Corrosion attack was obtained by immersion of specimens in an acid solution for: two, four and six minutes in order to induce three degrees of surface corrosion. The corrosion agent was a solution of hydrochloric acid with a pH close to 0.8 and solution concentration of 38%. Rotating bending fatigue tests at the frequency of 75&#xa0;Hz, room temperature and without environmental humidity control were carried out on the pre-corroded and non corroded specimens in order to investigate the corrosion effect on the fatigue endurance and the stress intensity factor range threshold. Additionally, numerical crack propagation investigation was carried out using a 2D model for testing specimen and the Displacements Correlation Method (DCM). A Kitagawa–Takahashi diagram for this aluminum alloy is developed in the last section of this work; it was obtained with experimental results. Finally, conclusions are listed concerning rotating bending fatigue tests, crack propagation and fatigue-corrosion behavior of this material. },
  Doi                      = {http://dx.doi.org/10.1016/j.engfracmech.2012.06.012},
  File                     = {Published version:Almaraz-2012-p119-131.pdf:PDF},
  Impactfactor             = {1.413},
  ISSN                     = {0013-7944},
  Keywords                 = {Aluminum alloy},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {~，没用},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0013794412002664},
  Zone                     = {3Q}
}

@InProceedings{Al-Shorafa-2013-p666-673,
  Title                    = {Security in software engineering requirement},
  Author                   = {Al-Shorafat, W.S.},
  Booktitle                = {Internet Technology and Secured Transactions (ICITST), 2013 8th International Conference for},
  Year                     = {2013},
  Month                    = {Dec},
  Pages                    = {666-673},

  Doi                      = {10.1109/ICITST.2013.6750287},
  File                     = {Published version:Al-Shorafa-2013-p666-673.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {security of data;software engineering;classification safety-related requirement;quality factors;quality model;robustness security requirement;security architectural components;security constraints;security ramifications;security-related requirements taxonomy;security-significant requirements;software engineering requirement;Inspection;Logic gates;Personnel;Planning;Security;Viruses (medical);Weapons;Requirement;Seure;secure recuirement},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6750287&isnumber=6750143},
  Zone                     = {-}
}

@Article{Altwaijry-2012-p1-6,
  Title                    = {Bayesian based intrusion detection system },
  Author                   = {Hesham Altwaijry and Saeed Algarny},
  Journal                  = {Journal of King Saud University - Computer and Information Sciences },
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {1 - 6},
  Volume                   = {24},

  Abstract                 = {In this paper an intrusion detection system is developed using Bayesian probability. The system developed is a naive Bayesian classifier that is used to identify possible intrusions. The system is trained a priori using a subset of the \{KDD\} dataset. The trained classifier is then tested using a larger subset of \{KDD\} dataset. The Bayesian classifier was able to detect intrusion with a superior detection rate. },
  Doi                      = {http://dx.doi.org/10.1016/j.jksuci.2011.10.001},
  File                     = {Published version:Altwaijry-2012-p1-6.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1319-1578},
  Keywords                 = {Intrusion detection system (IDS)},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，可以检测出已知攻击的类型},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1319157811000292},
  Zone                     = {-}
}

@InProceedings{Ambrose-2014-p267-273,
  Title                    = {SDG2KPN: System Dependency Graph to function-level KPN generation of legacy code for MPSoCs},
  Author                   = {Ambrose, J.A and Peddersen, J. and Parameswaran, S. and Labios, A and Yachide, Y.},
  Booktitle                = {Design Automation Conference (ASP-DAC), 2014 19th Asia and South Pacific},
  Year                     = {2014},
  Month                    = {Jan},
  Pages                    = {267-273},

  Abstract                 = {The Multiprocessor System-on-Chip (MPSoC) paradigm as a viable implementation platform for parallel processing has expanded to encompass embedded devices. The ability to execute code in parallel gives MPSoCs the potential to achieve high performance with low power consumption. In order for sequential legacy code to take advantage of the MPSoC design paradigm, it must first be partitioned into data flow graphs (such as Kahn Process Networks - KPNs) to ensure the data elements can be correctly passed between the separate processing elements that operate on them. Existing techniques are inadequate for use in complex legacy code. This paper proposes SDG2KPN, a System Dependency Graph to KPN conversion methodology targeting the conversion of legacy code. By creating KPNs at the granularity of the function-/procedure-level, SDG2KPN is the first of its kind to support shared and global variables as well as many more program patterns/application types. We also provide a design flow which allows the creation of MPSoC systems utilizing the produced KPNs. We demonstrate the applicability of our approach by retargeting several sequential applications to the Tensilica MPSoC framework. Our system parallelized AES, an application of 950 lines, in 4.8 seconds, while H.264, of 57896 lines, took 164.9 seconds to parallelize.},
  Doi                      = {10.1109/ASPDAC.2014.6742901},
  File                     = {Published version:Ambrose-2014-p267-273.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data flow graphs;low-power electronics;multiprocessing systems;parallel processing;system-on-chip;H.264;Kahn process networks;MPSoC design paradigm;SDG2KPN;Tensilica MPSoC framework;data flow graphs;embedded devices;function-level KPN generation;function-procedure-level;low power consumption;multiprocessor system-on-chip paradigm;parallel processing;sequential legacy code;system dependency graph to KPN conversion methodology;time 164.9 s;time 4.8 s;Australia;Generators;Input variables;Manuals;Parallel processing;Program processors;Syntactics},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6742901&isnumber=6742831},
  Zone                     = {-}
}

@Article{Amin-2013-p19-24,
  Title                    = {In quest of benchmarking security risks to cyber-physical systems},
  Author                   = {Amin, S. and Schwartz, G.A. and Hussain, A.},
  Journal                  = {Network, IEEE},
  Year                     = {2013},

  Month                    = {January},
  Number                   = {1},
  Pages                    = {19-24},
  Volume                   = {27},

  __markedentry            = {[Qiqi:5]},
  Abstract                 = {We present a generic yet practical framework for assessing security risks to cyber-physical systems (CPSs). Our framework can be used to benchmark security risks when information is less than perfect, and interdependencies of physical and computational components may result in correlated failures. Such environments are prone to externalities, and can cause huge societal losses. We focus on the risks that arise from interdependent reliability failures (faults) and security failures (attacks). We advocate that a sound assessment of these risks requires explicit modeling of the effects of both technology-based defenses and institutions necessary for supporting them. Thus, we consider technology-based security defenses grounded in information security tools and fault-tolerant control in conjunction with institutional structures. Our game-theoretic approach to estimating security risks facilitates more effective defenses, especially against correlated failures.},
  Doi                      = {10.1109/MNET.2013.6423187},
  File                     = {Published version:Amin-2013-p19-24.pdf:PDF},
  Impactfactor             = {2.853},
  ISSN                     = {0890-8044},
  Keywords                 = {failure analysis;fault tolerance;game theory;reliability;risk management;security of data;CPS;benchmarking security risk assessment;cyber-physical systems;fault-tolerant control;game-theoretic approach;information security tools;interdependent reliability failures;security failures;technology-based security defenses;Benchmark testing;Computer crime;Comupter security;Investments;Network security;Reliability;Risk management;Supervisory control},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，重点看，没试验，没Case},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6423187&isnumber=6423180},
  Zone                     = {2Q}
}

@Article{Ammar-2001-p171-183,
  Title                    = {Risk assessment of software-system specifications},
  Author                   = {Ammar, H.H. and Nikzadeh, T. and Bechta Dugan, J.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {171-183},
  Volume                   = {50},

  Abstract                 = {This paper presents a methodology and an example of risk assessment of functional-requirement specifications for complex real-time software systems. A heuristic risk-assessment technique based on CPN (colored Petri-net) models is presented. This technique is used to classify software functional-requirement specification components according to their relative importance in terms of such factors as severity and complexity. A dynamic complexity measure, based on concurrence in the functional requirements, is introduced. This technique is applied on the Earth Operation Commanding Center (EOC COMMANDING), a large component of the NASA Earth Observing System (EOS) project. Two specification models of the system are considered. Results of applying this technique to both CPN models are presented. The risk assessment methodology in this paper suggests the following conclusions: (i) risk assessment at the functional-requirement specification phase can be used to classify functional requirements in terms of their complexity and severity; (ii) dynamic complexity metrics and the concurrence metric can be important in assessing the risk factors based on the complexity of functional specifications; (iii) the concurrence complexity metric is an important aspect of dynamic complexity; and (iv) CPN models can be used to build an executable specification of the system, which helps the analyst not only to acquire deep understanding of the system but also to study the dynamic behavior of the system by simulating the model},
  Doi                      = {10.1109/24.963125},
  File                     = {Published version:Ammar-2001-p171-183.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Petri nets;aerospace computing;formal specification;risk management;software reliability;Earth Operation Commanding Center;NASA Earth Observing System;colored Petri-net models;complex real-time software systems;concurrence complexity metric;dynamic complexity;dynamic complexity measure;dynamic complexity metrics;functional requirements classification;functional requirements concurrence;functional-requirement specifications;heuristic risk factor;heuristic risk-assessment technique;risk assessment;software functional-requirement specification components;software-system specifications;specification models;Control systems;Earth Observing System;Failure analysis;Functional analysis;NASA;Real time systems;Risk analysis;Risk management;Software architecture;Space vehicles},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，没有仿真和实验},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=963125&isnumber=20792},
  Zone                     = {2Q}
}

@Article{Ammar-2001-p171-183a,
  Title                    = {Risk assessment of software-system specifications},
  Author                   = {Ammar, H.H. and Nikzadeh, T. and Bechta Dugan, J.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {171-183},
  Volume                   = {50},

  Abstract                 = {This paper presents a methodology and an example of risk assessment of functional-requirement specifications for complex real-time software systems. A heuristic risk-assessment technique based on CPN (colored Petri-net) models is presented. This technique is used to classify software functional-requirement specification components according to their relative importance in terms of such factors as severity and complexity. A dynamic complexity measure, based on concurrence in the functional requirements, is introduced. This technique is applied on the Earth Operation Commanding Center (EOC COMMANDING), a large component of the NASA Earth Observing System (EOS) project. Two specification models of the system are considered. Results of applying this technique to both CPN models are presented. The risk assessment methodology in this paper suggests the following conclusions: (i) risk assessment at the functional-requirement specification phase can be used to classify functional requirements in terms of their complexity and severity; (ii) dynamic complexity metrics and the concurrence metric can be important in assessing the risk factors based on the complexity of functional specifications; (iii) the concurrence complexity metric is an important aspect of dynamic complexity; and (iv) CPN models can be used to build an executable specification of the system, which helps the analyst not only to acquire deep understanding of the system but also to study the dynamic behavior of the system by simulating the model},
  Doi                      = {10.1109/24.963125},
  File                     = {Published version:Ammar-2001-p171-183a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Petri nets;aerospace computing;formal specification;risk management;software reliability;Earth Operation Commanding Center;NASA Earth Observing System;colored Petri-net models;complex real-time software systems;concurrence complexity metric;dynamic complexity;dynamic complexity measure;dynamic complexity metrics;functional requirements classification;functional requirements concurrence;functional-requirement specifications;heuristic risk factor;heuristic risk-assessment technique;risk assessment;software functional-requirement specification components;software-system specifications;specification models;Control systems;Earth Observing System;Failure analysis;Functional analysis;NASA;Real time systems;Risk analysis;Risk management;Software architecture;Space vehicles},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {这个Background是相关概念的介绍},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=963125&isnumber=20792},
  Zone                     = {2Q}
}

@Article{Amundson-2014-p320-325,
  Title                    = {Life-cycle Risk Modeling: Alternate Methods Using Bayesian Belief Networks },
  Author                   = {Joseph Amundson and Adam Brown and Matthias Grabowski and Fazleena Badurdeen},
  Journal                  = {Procedia \{CIRP\} },
  Year                     = {2014},
  Note                     = {Variety Management in Manufacturing Proceedings of the 47th \{CIRP\} Conference on Manufacturing Systems },
  Number                   = {0},
  Pages                    = {320 - 325},
  Volume                   = {17},

  Abstract                 = {Abstract In recent years natural and man-made disasters have highlighted the need for robust supply chain risk management (SCRM) in manufacturing firms from a life-cycle perspective (pre-manufacturing, manufacturing, use, post-use stages). Bayesian Belief Networks (BBN) provide a means to probabilistically represent risk interdependencies and to proactively identify and manage any existing vulnerabilities. In this work, the \{BBN\} method is implemented for a product in the aerospace industry. Risk network maps are developed to identify interdependencies and describe the potential risk propagation behavior during each life-cycle phase and from one phase to another. Due to limited number of respondents and lack of certainty with respect to the post-use phase, enhanced methods of risk likelihood assessment are necessary specifically for the post-use phase assessment. In this paper two alternate techniques are compared for risk modeling using \{BBN\} in such situations: Boolean nodes and numeric simulation nodes. Results show that numeric nodes provide a more thorough explanation of the interconnections of the risk items modeled. Further enhancement using an approach that combines both \{BBN\} and System Dynamics (SD) for \{SCRM\} is discussed and possible variations for linking variables between \{SD\} and \{BBN\} are also presented. },
  Doi                      = {http://dx.doi.org/10.1016/j.procir.2014.02.029},
  File                     = {Published version:Amundson-2014-p320-325.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2212-8271},
  Keywords                 = {Supply Chain Risk},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，Bayesian 大写},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2212827114002881},
  Zone                     = {-}
}

@Article{Ando-2000-p358-363,
  Title                    = {Models for air quality management and assessment},
  Author                   = {Ando, B. and Baglio, Salvatore and Graziani, S. and Pitrone, N.},
  Journal                  = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {358-363},
  Volume                   = {30},

  Abstract                 = {Models which could be useful for estimating air pollutant concentrations as a function of the emission distribution and the attendant meteorological conditions have been investigated. The integration of this model into an emission control scheme for the control of the pollution sources may represent a very useful approach to air quality management and assessment. A novel black box approach for air pollution modeling has been proposed. The main target of this approach is the prediction, on the basis of meteorological forecasts, of the air pollution concentration as a function of the expected causes.},
  Doi                      = {10.1109/5326.885117},
  File                     = {Published version:Ando-2000-p358-363.pdf:PDF},
  Impactfactor             = {2.548},
  ISSN                     = {1094-6977},
  Keywords                 = {air pollution control;air pollution measurement;neural nets;optimal control;air pollutant concentrations;air pollution modeling;air quality assessment;air quality management;black box approach;emission control scheme;emission distribution;meteorological conditions;neural nets;risk management;Air pollution;Carbon dioxide;Environmentally friendly manufacturing techniques;Humans;Industrial pollution;Meteorology;Quality management;Risk management;Vegetation;Weather forecasting},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=885117&isnumber=19148},
  Zone                     = {2Q}
}

@Article{Aneziris-2010-p321-329,
  Title                    = {Assessment of occupational risks in an aluminium processing industry },
  Author                   = {O.N. Aneziris and I.A. Papazoglou and O. Doudakmani},
  Journal                  = {International Journal of Industrial Ergonomics },
  Year                     = {2010},
  Number                   = {3},
  Pages                    = {321 - 329},
  Volume                   = {40},

  Abstract                 = {This paper presents the quantification of occupational risk in an aluminum plant producing profiles, located in Northern Greece. Risk assessment is based on the Workgroup Occupational Risk Model (WORM) project, developed in the Netherlands. This model can assess occupational risk at hazard level, activity level, job level and overall company risk. Twenty six job positions have been identified for this plant, such as operators of press extruders, forklift operators, crane operators, painters, and various other workers across the process units. All risk profiles of workers have been quantified and jobs have been ranked according to their risk. Operators at the entrance of the painting unit have the highest fatality risk (3.25 × 10−5/year), followed by the workers at the storage area (2.18 × 10−5/year) and the workers performing sandblasting of dies (1.91 × 10−5/year). Occupational risk has also been assessed for all plant units and the overall company. Storage area has the higher expected number of fatalities (2.8 × 10−4/year) followed by the surface treatment area (2.54 × 10−4/year) and the extrusion unit (1.7 × 10−3/year). },
  Doi                      = {http://dx.doi.org/10.1016/j.ergon.2010.01.005},
  File                     = {Published version:Aneziris-2010-p321-329.pdf:PDF},
  Impactfactor             = {1.208},
  ISSN                     = {0169-8141},
  Keywords                 = {Occupational risk},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {铝工业中的职业风险评估，好像不是很相关},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0169814110000065},
  Zone                     = {3Q}
}

@Article{Aneziris-2010-p321-329a,
  Title                    = {Assessment of occupational risks in an aluminium processing industry },
  Author                   = {O.N. Aneziris and I.A. Papazoglou and O. Doudakmani},
  Journal                  = {International Journal of Industrial Ergonomics},
  Year                     = {2010},
  Number                   = {3},
  Pages                    = {321 - 329},
  Volume                   = {40},

  Abstract                 = {This paper presents the quantification of occupational risk in an aluminum plant producing profiles, located in Northern Greece. Risk assessment is based on the Workgroup Occupational Risk Model (WORM) project, developed in the Netherlands. This model can assess occupational risk at hazard level, activity level, job level and overall company risk. Twenty six job positions have been identified for this plant, such as operators of press extruders, forklift operators, crane operators, painters, and various other workers across the process units. All risk profiles of workers have been quantified and jobs have been ranked according to their risk. Operators at the entrance of the painting unit have the highest fatality risk (3.25 × 10−5/year), followed by the workers at the storage area (2.18 × 10−5/year) and the workers performing sandblasting of dies (1.91 × 10−5/year). Occupational risk has also been assessed for all plant units and the overall company. Storage area has the higher expected number of fatalities (2.8 × 10−4/year) followed by the surface treatment area (2.54 × 10−4/year) and the extrusion unit (1.7 × 10−3/year). },
  Doi                      = {http://dx.doi.org/10.1016/j.ergon.2010.01.005},
  File                     = {Published version:Aneziris-2010-p321-329a.pdf:PDF},
  Impactfactor             = {1.208},
  ISSN                     = {0169-8141},
  Keywords                 = {Occupational risk},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0169814110000065},
  Zone                     = {3Q}
}

@Article{Aqlan-2014-p39-48,
  Title                    = {Integrating lean principles and fuzzy bow-tie analysis for risk assessment in chemical industry },
  Author                   = {Faisal Aqlan and Ebrahim Mustafa Ali},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {39 - 48},
  Volume                   = {29},

  Abstract                 = {Abstract In this research, a framework combining lean manufacturing principles and fuzzy bow-tie analyses is used to assess process risks in chemical industry. Lean manufacturing tools and techniques are widely used for eliminating wastes in manufacturing environments. The five principles of lean (identify value, map the value stream, create flow, establish pull, and seek perfection) are utilized in the risk assessment process. Lean tools such as Fishbone Diagram, and Failure Mode and Effect Analysis (FMEA) are used for risk analysis and mitigation. Lean principles and tools are combined with bow-tie analysis for effective risk assessment process. The uncertainty inherent with the risks is handled using fuzzy logic principles. A case study from a chemical process industry is provided. Main risks and risk factors are identified and analyzed by the risk management team. Fuzzy estimates are obtained for the risk factors and bow-tie analysis is used to calculate the aggregated risk probability and impact. The risks are prioritized using risk priority matrix and mitigation strategies are selected based on FMEA. Results showed that the proposed framework can effectively improve the risk management process in the chemical industry. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2014.01.006},
  File                     = {Published version:Aqlan-2014-p39-48.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {化工工业的风险评估，里面有时间的原因分析，故障树分析，有时间看看},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423014000229},
  Zone                     = {3Q}
}

@Article{Aqlan-2014-p39-48a,
  Title                    = {Integrating lean principles and fuzzy bow-tie analysis for risk assessment in chemical industry },
  Author                   = {Faisal Aqlan and Ebrahim Mustafa Ali},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {39 - 48},
  Volume                   = {29},

  Abstract                 = {Abstract In this research, a framework combining lean manufacturing principles and fuzzy bow-tie analyses is used to assess process risks in chemical industry. Lean manufacturing tools and techniques are widely used for eliminating wastes in manufacturing environments. The five principles of lean (identify value, map the value stream, create flow, establish pull, and seek perfection) are utilized in the risk assessment process. Lean tools such as Fishbone Diagram, and Failure Mode and Effect Analysis (FMEA) are used for risk analysis and mitigation. Lean principles and tools are combined with bow-tie analysis for effective risk assessment process. The uncertainty inherent with the risks is handled using fuzzy logic principles. A case study from a chemical process industry is provided. Main risks and risk factors are identified and analyzed by the risk management team. Fuzzy estimates are obtained for the risk factors and bow-tie analysis is used to calculate the aggregated risk probability and impact. The risks are prioritized using risk priority matrix and mitigation strategies are selected based on FMEA. Results showed that the proposed framework can effectively improve the risk management process in the chemical industry. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2014.01.006},
  File                     = {Published version:Aqlan-2014-p39-48a.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423014000229},
  Zone                     = {3Q}
}

@Article{Aqlan-2014-p39-48b,
  Title                    = {Integrating lean principles and fuzzy bow-tie analysis for risk assessment in chemical industry },
  Author                   = {Faisal Aqlan and Ebrahim Mustafa Ali},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {39 - 48},
  Volume                   = {29},

  Abstract                 = {Abstract In this research, a framework combining lean manufacturing principles and fuzzy bow-tie analyses is used to assess process risks in chemical industry. Lean manufacturing tools and techniques are widely used for eliminating wastes in manufacturing environments. The five principles of lean (identify value, map the value stream, create flow, establish pull, and seek perfection) are utilized in the risk assessment process. Lean tools such as Fishbone Diagram, and Failure Mode and Effect Analysis (FMEA) are used for risk analysis and mitigation. Lean principles and tools are combined with bow-tie analysis for effective risk assessment process. The uncertainty inherent with the risks is handled using fuzzy logic principles. A case study from a chemical process industry is provided. Main risks and risk factors are identified and analyzed by the risk management team. Fuzzy estimates are obtained for the risk factors and bow-tie analysis is used to calculate the aggregated risk probability and impact. The risks are prioritized using risk priority matrix and mitigation strategies are selected based on FMEA. Results showed that the proposed framework can effectively improve the risk management process in the chemical industry. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2014.01.006},
  File                     = {Published version:Aqlan-2014-p39-48b.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423014000229},
  Zone                     = {3Q}
}

@Article{Arnault-2005-p1374-1383,
  Title                    = {Design and properties of a new pseudorandom generator based on a filtered FCSR automaton},
  Author                   = {Arnault, F. and Berger, T.P.},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {1374-1383},
  Volume                   = {54},

  Abstract                 = {Feedback with carry shift registers (FCSR) was introduced by Goresky and Klapper in 1993. It is similar to the classical linear feedback shift registers (LFSR) used in many pseudorandom generators. The main difference is that the elementary additions are not additions modulo 2 but with propagation of carries. The main problem for the use of an FCSR automaton is the fact that the generated sequences are predictable. In order to remove this weakness of FCSR-based generators, we propose filtering the state of the FCSR with a linear function. This method is efficient since the FCSR structure is not related to a linear property. This paper presents an extensive study of FCSR automata, a security analysis of our generator (concerning linear and 2-adic cryptanalysis, algebraic attack, correlation attack, etc.), and a practical example of parameters in order to design this generator. An important point concerning this generator is the fact that it is simple and efficient, both in hardware and software implementation.},
  Doi                      = {10.1109/TC.2005.181},
  File                     = {Published version:Arnault-2005-p1374-1383.pdf:PDF},
  Impactfactor             = {1.379},
  ISSN                     = {0018-9340},
  Keywords                 = {automata theory;carry logic;cryptography;random number generation;sequences;shift registers;2-adic cryptanalysis;algebraic attack;correlation attack;feedback with carry shift registers;filtered FCSR automaton;linear feedback shift registers;linear function;periodic sequences;pseudorandom generator;secret key cryptography;security analysis;Automata;Boolean functions;Circuits;Computer architecture;Cryptography;Filtering;Hardware;Linear feedback shift registers;Nonlinear filters;Shift registers;2-adic numbers;Index Terms- Pseudorandom generator;periodic sequences;secret key cryptography.;shift register},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1514417&isnumber=32433},
  Zone                     = {3Q}
}

@Article{Asian-2014-p1139-1154,
  Title                    = {Coordination in Supply Chains With Uncertain Demand and Disruption Risks: Existence, Analysis, and Insights},
  Author                   = {Asian, S. and Xiaofeng Nie},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Sept},
  Number                   = {9},
  Pages                    = {1139-1154},
  Volume                   = {44},

  Abstract                 = {Many companies with global supply networks suffer from market volatility and supply disruptions, which adversely affect both their short and long-term profits. Although using mechanisms, such as supply contracts, is useful to mitigate uncertainty, the inherent inefficiency of decentralization is a critical issue that needs to be considered at early design stages. This paper studies a supply chain problem where a buyer receives a product from a cheap but unreliable main supplier and signs an option contract with a perfectly reliable backup supplier to share supply and demand uncertainty. To build efficiency benchmark models, we first consider a centralized problem and then explore a decentralized problem where there is only a wholesale price contract between the buyer and the backup supplier. Considering the option contract, we reconstruct optimization problems and sequentially characterize the members' reservation and production policies under a voluntary compliance regime. Subsequently, we establish a win-win coordination mechanism that maximizes system efficiency and meanwhile is desirable from both contract members' perspectives. Results reveal that the proposed mechanism leads the backup supplier to choose a lower level of production capacity than the buyer's reservation amount (i.e., an underproduction policy). We realize that the existing mismatch between the members' optimal policies is caused by the buyer's phantom ordering. This paper sheds light on the effectiveness of contract-based mitigation strategies that enable firms to ensure responsive backup capacity under demand uncertainty and supply disruptions.},
  Doi                      = {10.1109/TSMC.2014.2313121},
  File                     = {Published version:Asian-2014-p1139-1154.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Pareto optimisation;contracts;pricing;supply and demand;supply chains;Pareto improvement;backup supplier;buyer reservation amount;centralized problem;contract-based mitigation strategies;decentralized problem;global supply networks;long-term profits;market volatility;member reservation policies;optimal policies;optimization problems;option contract;perfectly reliable backup supplier;production capacity;production policies;short-term profits;supply and demand uncertainty;supply chain problem;supply disruptions;system efficiency maximization;underproduction policy;unreliable main supplier;voluntary compliance regime;whole-sale price contract;win-win coordination mechanism;Benchmark testing;Contracts;Mathematical model;Reliability;Supply chains;Uncertainty;Coordination;Pareto improvement;demand uncertainty;option contracts;supply disruptions},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6804754&isnumber=6878502},
  Zone                     = {-}
}

@InProceedings{Atmaja-2011-p1-6,
  Title                    = {Cyber security strategy for future distributed energy delivery system},
  Author                   = {Atmaja, T.D. and Fitriana, F.},
  Booktitle                = {Electrical Engineering and Informatics (ICEEI), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {July},
  Pages                    = {1-6},

  Doi                      = {10.1109/ICEEI.2011.6021635},
  File                     = {Published version:Atmaja-2011-p1-6.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2155-6822},
  Keywords                 = {cloud computing;data privacy;distributed power generation;open systems;power distribution planning;power distribution protection;power engineering computing;power system security;risk management;smart power grids;virtualisation;cloud computing;cyber security strategy;distributed energy delivery system;electricity security guideline-and-protection plan;information flow;interoperability;key management;organizational perspective;power generations;risk assessment;risk management framework;smart grid;smart privacy;transmission network;user automation;virtualization security;Computer security;Electricity;Power system reliability;Privacy;Risk management;Smart grids;cyber security;delivery system;distributed energy;risk management;strategy},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6021635&isnumber=6021499},
  Zone                     = {-}
}

@InProceedings{Auguston-2005-p2-1875,
  Title                    = {Using attributed event grammar environment models for automated test generation and software risk assessment of system-of-systems},
  Author                   = {Auguston, M. and Michael, J.B. and Man-Tak Shing and Floodeen, D.L.},
  Booktitle                = {Systems, Man and Cybernetics, 2005 IEEE International Conference on},
  Year                     = {2005},
  Month                    = {Oct},
  Pages                    = {1870-1875 Vol. 2},
  Volume                   = {2},

  Abstract                 = {This paper presents some concepts, principles, and techniques for automated testing of system-of-systems based on attributed event grammar (AEG) modeling of the system's operational environment. AEG provides a uniform approach for automatically generating, executing, and analyzing tests. Quantitative and qualitative software risk assessment can be performed based on statistics gathered during automatic test execution within the specified environment model.},
  Doi                      = {10.1109/ICSMC.2005.1571419},
  File                     = {Published version:Auguston-2005-p2-1875.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {grammars;program testing;risk management;attributed event grammar environment models;automated test generation;software risk assessment;system-of-systems;Automatic testing;Computer science;Performance evaluation;Risk management;Software performance;Software quality;Software systems;Software testing;System testing;Timing;Testing automation;software risk assessment;system-of-systems;test generation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1571419&isnumber=33255},
  Zone                     = {-}
}

@InProceedings{Axelrod-2013-p1-6,
  Title                    = {Managing the risks of cyber-physical systems},
  Author                   = {Axelrod, C.W.},
  Booktitle                = {Systems, Applications and Technology Conference (LISAT), 2013 IEEE Long Island},
  Year                     = {2013},
  Pages                    = {1--6},

  Doi                      = {10.1109/LISAT.2013.6578215},
  File                     = {Published version:Axelrod-2013-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6578215},
  Zone                     = {-}
}

@Article{Ayday-2012-p375-386,
  Title                    = {Iterative Trust and Reputation Management Using Belief Propagation},
  Author                   = {Ayday, E. and Fekri, F.},
  Journal                  = {Dependable and Secure Computing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {375-386},
  Volume                   = {9},

  Abstract                 = {In this paper, we introduce the first application of the belief propagation algorithm in the design and evaluation of trust and reputation management systems. We approach the reputation management problem as an inference problem and describe it as computing marginal likelihood distributions from complicated global functions of many variables. However, we observe that computing the marginal probability functions is computationally prohibitive for large-scale reputation systems. Therefore, we propose to utilize the belief propagation algorithm to efficiently (in linear complexity) compute these marginal probability distributions; resulting a fully iterative probabilistic and belief propagation-based approach (referred to as BP-ITRM). BP-ITRM models the reputation system on a factor graph. By using a factor graph, we obtain a qualitative representation of how the consumers (buyers) and service providers (sellers) are related on a graphical structure. Further, by using such a factor graph, the global functions factor into products of simpler local functions, each of which depends on a subset of the variables. Then, we compute the marginal probability distribution functions of the variables representing the reputation values (of the service providers) by message passing between nodes in the graph. We show that BP-ITRM is reliable in filtering out malicious/unreliable reports. We provide a detailed evaluation of BP-ITRM via analysis and computer simulations. We prove that BP-ITRM iteratively reduces the error in the reputation values of service providers due to the malicious raters with a high probability. Further, we observe that this probability drops suddenly if a particular fraction of malicious raters is exceeded, which introduces a threshold property to the scheme. Furthermore, comparison of BP-ITRM with some well-known and commonly used reputation management techniques (e.g., Averaging Scheme, Bayesian Approach, and Cluster Filtering) indicates the superiority of - he proposed scheme in terms of robustness against attacks (e.g., ballot stuffing, bad mouthing). Finally, BP-ITRM introduces a linear complexity in the number of service providers and consumers, far exceeding the efficiency of other schemes.},
  Doi                      = {10.1109/TDSC.2011.64},
  File                     = {Published version:Ayday-2012-p375-386.pdf:PDF},
  Impactfactor             = {1.059},
  ISSN                     = {1545-5971},
  Keywords                 = {belief maintenance;computational complexity;graph theory;inference mechanisms;iterative methods;maximum likelihood estimation;security of data;statistical distributions;BP-ITRM models;Bayesian approach;averaging scheme;belief propagation algorithm;cluster filtering;computer simulations;factor graph;global functions factor;graphical structure;inference problem;iterative probabilistic approach;iterative trust management system;linear complexity;malicious reports;marginal likelihood distributions;marginal probability distribution functions;reputation management system;threshold property;unreliable reports;Bayesian methods;Belief propagation;Iterative decoding;Maximum likelihood decoding;Peer to peer computing;Probability distribution;Trust and reputation management;bad mouthing;ballot stuffing;belief propagation;e-commerce.;iterative algorithms;online services},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6112781&isnumber=6167342},
  Zone                     = {3Q}
}

@Article{Aziz-2014-p77-84,
  Title                    = {Occurrence of polycyclic aromatic hydrocarbons in the Soan River, Pakistan: Insights into distribution, composition, sources and ecological risk assessment },
  Author                   = {Faiqa Aziz and Jabir Husain Syed and Riffat Naseem Malik and Athanasios Katsoyiannis and Adeel Mahmood and Jun Li and Gan Zhang and Kevin C. Jones},
  Journal                  = {Ecotoxicology and Environmental Safety },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {77 - 84},
  Volume                   = {109},

  Abstract                 = {Abstract Present study investigates the occurrence, distribution and sources of ∑17 polycyclic aromatic hydrocarbons (PAHs) in the surface water of Soan River, Pakistan. The concentrations of total \{PAHs\} ranged from 61 to 207&#xa0;ng/l. Low molecular weight (LMW) \{PAHs\} were recorded higher in concentrations (64.7 percent) as compared to high molecular weight (HMW) \{PAHs\} (35.6 percent). Principal component analysis (PCA) revealed domestic and industrial wastewater discharge, vehicular exhaust, petroleum residues and biomass combustion as the main sources for \{PAHs\} contamination. Using the \{USEPA\} toxic equivalency factor (TEF) approach: Benzo(a)Pyrene and Dibenzo(a,h)Anthracene contributed highly carcinogenic exposure equivalent. The surface water of the Soan River is found to be slightly polluted with \{PAHs\} thereby posing health risks to aquatic bodies. },
  Doi                      = {http://dx.doi.org/10.1016/j.ecoenv.2014.07.022},
  File                     = {Published version:Aziz-2014-p77-84.pdf:PDF},
  Impactfactor             = {2.203},
  ISSN                     = {0147-6513},
  Keywords                 = {Polycyclic aromatic hydrocarbons},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {烃类物质的风险评估，没仔细看},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0147651314003212},
  Zone                     = {3Q}
}

@InProceedings{Bai-2008-p550-555,
  Title                    = {A function failure analysis method for improving reliability of the product based on GO-FLOW methodology},
  Author                   = {Zhonghang Bai and Xiangdong Li and Runhua Tan and Benning Lian},
  Booktitle                = {Industrial Engineering and Engineering Management, 2008. IEEM 2008. IEEE International Conference on},
  Year                     = {2008},
  Month                    = {Dec},
  Pages                    = {550-555},

  Doi                      = {10.1109/IEEM.2008.4737929},
  File                     = {Published version:Bai-2008-p550-555.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {failure analysis;probability;product design;quality management;reliability;statistical analysis;GO-FLOW methodology;anticipatory operation failures;arithmetic mean;conceptual design;detail design process;failure probability;flow failures;function failure analysis;function structure network;function structures;product quality;product reliability;quality guarantee;standard deviation;Arithmetic;Costs;Design methodology;Failure analysis;Ontologies;Process design;Product design;Reliability;Robustness;Vocabulary;conceptual design;failure analysis;function modelling;reliability design},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {！，思路跟我的很像，要好好看看},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4737929&isnumber=4737816},
  Zone                     = {-}
}

@Article{Baird-2006-p309-318,
  Title                    = {Non-destructive measurement of the degradation of transformer insulating paper},
  Author                   = {Baird, P.J. and Herman, H. and Stevens, G.C. and Jarman, P.N.},
  Journal                  = {Dielectrics and Electrical Insulation, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {309-318},
  Volume                   = {13},

  Abstract                 = {Knowledge of the condition of power transformer winding insulation paper is fundamental to making optimum asset replacement decisions in the power industry. The ability to assess the aged condition of Kraft paper quickly and non-destructively using portable instrumentation would significantly increase the opportunities for gaining this knowledge. Insulation paper degrades over time in-service and its degree of polymerization (DP) reduces, eventually affecting its mechanical strength. At low DP levels the insulation may start to disintegrate and the risk of electrical breakdown increases. Currently-used methods of estimating DP are either approximate or destructive. The use of spectroscopy together with multivariate statistical analysis (MVSA) provides a powerful non-destructive evaluation of the condition of paper. From initial feasibility studies, we have developed a simple, portable system (TRANSPEC) using fiber-optics and broad-band spectroscopy that can measure the degree of polymerization of various aged transformer papers to a precision of approximately 30 DP units with a spatial resolution of 14 mm. The system can also measure the chemical composition and condition of the insulating mineral oil. MVSA regression models were constructed from library spectral data, and these models are used to predict the DP of other papers with parameters that fall within the range spanned by the set of calibration samples. Separating oil and moisture information from wetted paper is possible and will be reported in a separate publication. With a single TRANSPEC system, non-destructive in-situ analysis of the DP of insulating paper is possible, providing a rapid cost-effective method for transformer insulation condition assessment and monitoring, which correlates well with current destructive methods.},
  Doi                      = {10.1109/TDEI.2006.1624275},
  File                     = {Published version:Baird-2006-p309-318.pdf:PDF},
  Impactfactor             = {1.36},
  ISSN                     = {1070-9878},
  Keywords                 = {condition monitoring;costing;decision making;electric breakdown;insulation testing;nondestructive testing;paper;polymerisation;power transformer insulation;power transformer testing;regression analysis;risk analysis;spectrochemical analysis;transformer oil;transformer windings;wetting;broad-band spectroscopy;calibration;cellulose degradation;chemical composition measurement;condition assessment;condition monitoring;cost-effective method;degree of polymerization;electrical breakdown;fiber-optic spectroscopy;insulation mineral oil;kraft paper insulation;mechanical strength;multivariate statistical analysis;nondestructive testing;power industry;power transformer winding;regression model;risk analysis;wetting},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，不相干},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1624275&isnumber=34095},
  Zone                     = {3Q}
}

@Article{Bala-2015-p980-989,
  Title                    = {Intelligent failure prediction models for scientific workflows },
  Author                   = {Anju Bala and Inderveer Chana},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {980 - 989},
  Volume                   = {42},

  Abstract                 = {Abstract The ever-growing demand and heterogeneity of Cloud Computing is garnering popularity with scientific communities to utilize the services of Cloud for executing large scale scientific applications in the form of set of tasks known as Workflows. As scientific workflows stipulate a process or computation to be executed in the form of data flow and task dependencies that allow users to simply articulate multi-step computational and complex tasks. Hence, proactive fault tolerance is required for the execution of scientific workflows. To reduce the failure effect of workflow tasks on the Cloud resources during execution, task failures can be intelligently predicted by proactively analyzing the data of multiple scientific workflows using the state of the art of machine learning approaches for failure prediction. Therefore, this paper makes an effort to focus on the research problem of designing an intelligent task failure prediction models for facilitating proactive fault tolerance by predicting task failures for Scientific Workflow applications. Firstly, failure prediction models have been implemented through machine learning approaches using evaluated performance metrics and also demonstrates the maximum prediction accuracy for Naive Bayes. Then, the proposed failure models have also been validated using Pegasus and Amazon \{EC2\} by comparing actual task failures with predicted task failures. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2014.09.014},
  File                     = {Published version:Bala-2015-p980-989.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Cloud Computing},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417414005533},
  Zone                     = {3Q}
}

@Article{Baldoni-2014-p-,
  Title                    = {On-line failure prediction in safety-critical systems },
  Author                   = {Roberto Baldoni and Luca Montanari and Marco Rizzuto},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract In safety-critical systems such as Air Traffic Control system, \{SCADA\} systems, Railways Control Systems, there has been a rapid transition from monolithic systems to highly modular ones, using off-the-shelf hardware and software applications possibly developed by different manufactures. This shift increased the probability that a fault occurring in an application propagates to others with the risk of a failure of the entire safety-critical system. This calls new tools for the on-line detection of anomalous behaviors of the system, predicting thus a system failure before it happens, allowing the deployment of appropriate mitigation policies. The paper proposes a novel architecture, namely CASPER, for online failure prediction that has the distinctive features to be (i) black-box: no knowledge of applications internals and logic of the system is required (ii) non-intrusive: no status information of the components is used such as \{CPU\} or memory usage; The architecture has been implemented to predict failures in a real Air Traffic Control System. \{CASPER\} exhibits high degree of accuracy in predicting failures with low false positive rate. The experimental validation shows how operators are provided with predictions issued a few hundred of seconds before the occurrence of the failure. },
  Doi                      = {http://dx.doi.org/10.1016/j.future.2014.11.015},
  File                     = {Published version:Baldoni-2014-p-.pdf:PDF},
  Impactfactor             = {1.864},
  ISSN                     = {0167-739X},
  Keywords                 = {Failure prediction},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X14002490},
  Zone                     = {2Q}
}

@Article{Bandara-2014-p421-435,
  Title                    = {Fatigue failure predictions for steels in the very high cycle region – A review and recommendations },
  Author                   = {Chaminda S. Bandara and Sudath C. Siriwardane and Udaya I. Dissanayake and Ranjith Dissanayake},
  Journal                  = {Engineering Failure Analysis },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {421 - 435},
  Volume                   = {45},

  Abstract                 = {Abstract Very high cycle fatigue properties of various steels were studied using findings of previous research and laboratory fatigue testing. First, experimental data for more than 550 specimens covering 25 high and medium strength steels were used to investigate the relationships between the applied stress, number of failure cycles, size of defects or inclusions at fracture origins and stress intensity factors. Using the results of the investigation of these data, general conclusions were arrived at for steels as a whole. It was observed that the size of the failure origin can be predicted using strength properties of steels. Existing methods for estimating major parameters such as size of failure origins and stress intensity factors were reviewed, new methods were proposed and their accuracy was verified using experimental data. Also, the possibility of simplifying existing formulae with substitutions for the major parameters was reviewed. Employing these major parameters, new formulae for predicting fatigue strengths of both medium and high strength steels were proposed. Predictions of these proposed formulae were compared with existing well known formulae using experimental data and statistical methods highlighting the simplicity and importance of the proposed formulae. The ability of employing the proposed formulae for predicting, “fatigue strengths that are more close to the real values” as well as “fatigue strengths that are more safe and conservative” was reviewed. Secondly, fatigue properties and failure causes of medium strength – low carbon structural steels that are usually used in civil engineering structures were investigated. For this investigation, 35 smooth specimens of five steels were tested using a rotating bending fatigue tester. It was observed that fatigue failures occur up to around 107 cycles and that the failure originates from the surface. It was found that the formulae proposed are able to predict failures of these medium strengths steels. Slopes of stress life curves in the very high cycle fatigue regions were well predicted by these proposed formulae while the predictions were fairly aligned with values suggested in previous research. Finally, recommendations were given for employing suitable prediction methods considering safety and importance of components and structures. },
  Doi                      = {http://dx.doi.org/10.1016/j.engfailanal.2014.07.015},
  File                     = {Published version:Bandara-2014-p421-435.pdf:PDF},
  Impactfactor             = {0.855},
  ISSN                     = {1350-6307},
  Keywords                 = {High cycle fatigue},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1350630714002404},
  Zone                     = {3Q}
}

@InProceedings{Baoqian-2010-p318-320,
  Title                    = {Risk Assessment Based on Accident Theory in Urban Railway Transportation},
  Author                   = {Dai Baoqian and Wang Tong},
  Booktitle                = {Intelligent System Design and Engineering Application (ISDEA), 2010 International Conference on},
  Year                     = {2010},
  Pages                    = {318--320},
  Volume                   = {2},

  Doi                      = {10.1109/ISDEA.2010.374},
  File                     = {Published version:Baoqian-2010-p318-320.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5743430},
  Zone                     = {-}
}

@Article{Barbosa-2013-p150-158,
  Title                    = {Flow whitelisting in \{SCADA\} networks },
  Author                   = {Rafael Ramos Regis Barbosa and Ramin Sadre and Aiko Pras},
  Journal                  = {International Journal of Critical Infrastructure Protection },
  Year                     = {2013},
  Number                   = {3–4},
  Pages                    = {150 - 158},
  Volume                   = {6},

  Abstract                 = {Abstract Supervisory control and data acquisition (SCADA) networks are commonly deployed in large industrial facilities. Modern \{SCADA\} networks are becoming more vulnerable to cyber attacks due to the common use of standard communications protocols and increased interconnections with corporate networks and the Internet. This paper describes an approach for improving the security of \{SCADA\} networks using flow whitelisting. A flow whitelist describes legitimate traffic based on four properties of network packets: client address, server address, server-side port and transport protocol. The proposed approach incorporates a learning phase in which a flow whitelist is learned by capturing network traffic over a period of time and aggregating it into flows. After the learning phase is complete, any non-whitelisted connection observed generates an alarm. The evaluation of the approach focuses on two important whitelist characteristics: size and stability. The applicability of the approach is demonstrated using real-world traffic traces captured at two water treatment plants and at an electric-gas utility. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijcip.2013.08.003},
  File                     = {Published version:Barbosa-2013-p150-158.pdf:PDF},
  Impactfactor             = {0.63},
  ISSN                     = {1874-5482},
  Keywords                 = {\{SCADA\} systems},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1874548213000437},
  Zone                     = {4Q}
}

@Article{Baron-1999-p87-100,
  Title                    = {Designing risk-management strategies for critical engineering systems},
  Author                   = {Baron, M.M. and Pat\'{e}-Cornell, M.E.},
  Journal                  = {Engineering Management, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {87-100},
  Volume                   = {46},

  Abstract                 = {Effective management of the tradeoff between productivity and safety is a challenge in many industries that operate critical engineering systems such as nuclear power plants or offshore oil platforms. The objective of this paper is to link risk-management strategies to a system's safety and productivity over its lifetime. These strategies involve decisions that affect the physical system both directly and indirectly though the performance of the personnel that design, construct, or operate it. The problem is thus to link the different components of such risk-management strategies to human and system performance. In this paper, we present the basis of a decision support framework for the design and assessment of different risk-management strategies in risk-critical systems. First we discuss the inherent difficulty in balancing productivity and safety in the short and the long term and the different components of a risk-management strategy. We present a model involving both production failures and catastrophic failures as a function of strategic alternatives. This model is based on a probabilistic and dynamic risk analysis of a system, linking different aspects of risk-management strategies to specific characteristics of the physical system. We show how this model, coupled with explicit value judgments, can be used to design optimal strategies, e.g., to balance initial costs, long-term operations and maintenance costs, and the potential costs of catastrophic failures. To illustrate the concepts we use the case of the maintenance of a corporate airplane},
  Doi                      = {10.1109/17.740040},
  File                     = {Published version:Baron-1999-p87-100.pdf:PDF},
  Impactfactor             = {0.893},
  ISSN                     = {0018-9391},
  Keywords                 = {aircraft;maintenance engineering;probability;risk management;catastrophic failures;corporate airplane;critical engineering systems;decision support framework;dynamic risk analysis;human error;initial costs balancing;long-term operations;maintenance costs;personnel performance;potential costs;probabilistic risk analysis;production failures;productivity;reliability;risk-critical systems;risk-management strategies design;safety;value tradeoff;Cost function;Design engineering;Energy management;Engineering management;Power engineering and energy;Power generation;Power system management;Product safety;Productivity;Systems engineering and theory},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {C，风险管理， 权衡多种成本进行策略决策，重点看一下},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=740040&isnumber=15979},
  Zone                     = {4Q}
}

@Article{Baron-1999-p87-100a,
  Title                    = {Designing risk-management strategies for critical engineering systems},
  Author                   = {Baron, M.M. and Pate?-Cornell, M.E.},
  Journal                  = {Engineering Management, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {87-100},
  Volume                   = {46},

  Abstract                 = {Effective management of the tradeoff between productivity and safety is a challenge in many industries that operate critical engineering systems such as nuclear power plants or offshore oil platforms. The objective of this paper is to link risk-management strategies to a system's safety and productivity over its lifetime. These strategies involve decisions that affect the physical system both directly and indirectly though the performance of the personnel that design, construct, or operate it. The problem is thus to link the different components of such risk-management strategies to human and system performance. In this paper, we present the basis of a decision support framework for the design and assessment of different risk-management strategies in risk-critical systems. First we discuss the inherent difficulty in balancing productivity and safety in the short and the long term and the different components of a risk-management strategy. We present a model involving both production failures and catastrophic failures as a function of strategic alternatives. This model is based on a probabilistic and dynamic risk analysis of a system, linking different aspects of risk-management strategies to specific characteristics of the physical system. We show how this model, coupled with explicit value judgments, can be used to design optimal strategies, e.g., to balance initial costs, long-term operations and maintenance costs, and the potential costs of catastrophic failures. To illustrate the concepts we use the case of the maintenance of a corporate airplane},
  Doi                      = {10.1109/17.740040},
  File                     = {Published version:Baron-1999-p87-100a.pdf:PDF},
  Impactfactor             = {0.893},
  ISSN                     = {0018-9391},
  Keywords                 = {aircraft;maintenance engineering;probability;risk management;catastrophic failures;corporate airplane;critical engineering systems;decision support framework;dynamic risk analysis;human error;initial costs balancing;long-term operations;maintenance costs;personnel performance;potential costs;probabilistic risk analysis;production failures;productivity;reliability;risk-critical systems;risk-management strategies design;safety;value tradeoff;Cost function;Design engineering;Energy management;Engineering management;Power engineering and energy;Power generation;Power system management;Product safety;Productivity;Systems engineering and theory},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {C，好像有状态控制，没有仿真和实验},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=740040&isnumber=15979},
  Zone                     = {4Q}
}

@Article{Baron-1999-p87-100b,
  Title                    = {Designing risk-management strategies for critical engineering systems},
  Author                   = {Baron, M.M. and Pate?-Cornell, M.E.},
  Journal                  = {Engineering Management, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {87-100},
  Volume                   = {46},

  Abstract                 = {Effective management of the tradeoff between productivity and safety is a challenge in many industries that operate critical engineering systems such as nuclear power plants or offshore oil platforms. The objective of this paper is to link risk-management strategies to a system's safety and productivity over its lifetime. These strategies involve decisions that affect the physical system both directly and indirectly though the performance of the personnel that design, construct, or operate it. The problem is thus to link the different components of such risk-management strategies to human and system performance. In this paper, we present the basis of a decision support framework for the design and assessment of different risk-management strategies in risk-critical systems. First we discuss the inherent difficulty in balancing productivity and safety in the short and the long term and the different components of a risk-management strategy. We present a model involving both production failures and catastrophic failures as a function of strategic alternatives. This model is based on a probabilistic and dynamic risk analysis of a system, linking different aspects of risk-management strategies to specific characteristics of the physical system. We show how this model, coupled with explicit value judgments, can be used to design optimal strategies, e.g., to balance initial costs, long-term operations and maintenance costs, and the potential costs of catastrophic failures. To illustrate the concepts we use the case of the maintenance of a corporate airplane},
  Doi                      = {10.1109/17.740040},
  File                     = {Published version:Baron-1999-p87-100b.pdf:PDF},
  Impactfactor             = {0.893},
  ISSN                     = {0018-9391},
  Keywords                 = {aircraft;maintenance engineering;probability;risk management;catastrophic failures;corporate airplane;critical engineering systems;decision support framework;dynamic risk analysis;human error;initial costs balancing;long-term operations;maintenance costs;personnel performance;potential costs;probabilistic risk analysis;production failures;productivity;reliability;risk-critical systems;risk-management strategies design;safety;value tradeoff;Cost function;Design engineering;Energy management;Engineering management;Power engineering and energy;Power generation;Power system management;Product safety;Productivity;Systems engineering and theory},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {C},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=740040&isnumber=15979},
  Zone                     = {4Q}
}

@Article{Barrault-2012-p285-287,
  Title                    = {Risk Management of Major Industrial Complex Sites: Key Success Factors – \{OCP\} Example },
  Author                   = {Stephane Barrault and Herve Marzal and Francis Chevalier},
  Journal                  = {Procedia Engineering },
  Year                     = {2012},
  Note                     = {\{SYMPHOS\} 2011 - 1st International Symposium on Innovation andTechnology in the Phosphate Industry },
  Number                   = {0},
  Pages                    = {285 - 287},
  Volume                   = {46},

  Abstract                 = {Large industrial complex sites are exposed to risks that can affect safety of employees and surrounding populations, challenge their public image and also impact their shareholding. They have to establish an organisation and engage a risk management approach which can allow them to take reasonable decisions and engage adapted actions. This approach goes beyond the simple respect of applicable regulations. Mining and phosphate processing in mining sites and chemical complex sites are obviously confronted with this need to control the various risks associated with accidents at workplace, but also those which are considered as major risks for the environment generated by hazardous materials used in the process. The objective of this intervention is to present the active approach adopted by \{OCP\} to map the various risks at its various sites, and to evaluate them and define an organization and an improvement actions plan to ensure management of these risks. Key success factors of effective risk management have been identified as the design of processes and equipment (safety in design) taking into account the safety behavior of the \{OCP\} personnel, the establishment of an organization that provides change tracking and control, the knowledge of the hazardous zones to the off-site populations in order to ensure the land-use planning around the sites, and the definition of emergency plans. This approach is part of a process of on-going improvement of safety control introduced by a large international group, with the objective to be recognized as a leader in safety and to develop a reliable and sustainable safety culture. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2012.09.474},
  File                     = {Published version:Barrault-2012-p285-287.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {Industrial complex sites – \{OCP\} – Risk management – Key success factors – Safety overview -Risk study – Emergency plans – \{SMS\} analysis },
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705812045377},
  Zone                     = {-}
}

@Article{Bartlett-2002-p344-349,
  Title                    = {Choosing a heuristic for the "fault tree to binary decision diagram" conversion, using neural networks},
  Author                   = {Bartlett, L.M. and Andrews, J.D.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {344-349},
  Volume                   = {51},

  Abstract                 = {Fault-tree analysis is commonly used for risk assessment of industrial systems. Several computer packages are available to carry out the analysis. Despite its common usage there are associated limitations of the technique in terms of accuracy and efficiency when dealing with large fault-tree structures. The most recent approach to aid the analysis of the fault-tree diagram is the BDD (binary decision diagram). To use the BDD, the fault-tree structure needs to be converted into the BDD format. Converting the fault tree is relatively straightforward but requires that the basic events of the tree be ordered. This ordering is critical to the resulting size of the BDD, and ultimately affects the qualitative and quantitative performance and benefits of this technique. Several heuristic approaches were developed to produce an optimal ordering permutation for a specific tree. These heuristic approaches do not always yield a minimal BDD structure for all trees. There is no single heuristic that guarantees a minimal BDD for any fault-tree structure. This paper looks at a selection approach using a neural network to choose the best heuristic from a set of alternatives that will yield the smallest BDD and promote an efficient analysis. The set of possible selection choices are 6 alternative heuristics, and the prediction capacity produced was a 70% chance of the neural network choosing the best ordering heuristic from the set of 6 for the test set of given fault trees},
  Doi                      = {10.1109/TR.2002.802892},
  File                     = {Published version:Bartlett-2002-p344-349.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {binary decision diagrams;fault trees;neural nets;risk management;binary decision diagram;computer packages;fault tree to binary decision diagram conversion;fault-tree analysis;fault-tree diagram;fault-tree structure;fault-tree structures;industrial systems;neural network;optimal ordering permutation;prediction capacity;risk assessment;variable ordering heuristics;Binary decision diagrams;Boolean functions;Data structures;Fault trees;Multilayer perceptrons;Neural networks;Packaging;Risk analysis;Risk management;Testing},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1028407&isnumber=22091},
  Zone                     = {2Q}
}

@InProceedings{Basharat-2013-p176-181,
  Title                    = {Risks factors identification and assessment in virtual projects of software industry: A survey study},
  Author                   = {Basharat, I. and Nafees, T. and Abbas, M.},
  Booktitle                = {Science and Information Conference (SAI), 2013},
  Year                     = {2013},
  Month                    = {Oct},
  Pages                    = {176-181},

  Abstract                 = {The technological advanced global environment has originated multi-cultural virtual teams. It is quite novel conception to practitioners. The growth of virtual teams, virtual work groups, and virtual companies has been expedited with the arrival of internet technology that provides low-cost telecommunication and emails facilities. Different companies in all over the world and in Pakistan are rapidly adopting the concept of virtual teams also. The system by which these virtual teams interact and work together to achieve a specific goal in a predetermined time span, is called virtual project management. Managing virtual teams is a multifaceted task. The literature reports different risks factors like, cultural differences, trust, time zone, communication etc. That are involved in global software development. The purpose of this research that is based on a survey is to analyze these factors in Pakistan software industry. The major risks identified by this study are; (1) communication risk, (2) schedule risk and (3) cultural risk. The significances of this generalization are used as alleviation for virtual project team manager in virtual projects. This survey study provides fundamental indication that foremost project risk is different in a virtual environment, still there is need to develop an understanding and explore the virtual project risks. Apart from the major risk factor, i.e. Communication risk, the other risk areas also needs in depth. It has been concluded that there is a great need to improve painstaking understanding of the critical risks acknowledged here interrelates with one another and in-depth study needs to be carried out on how well a project manager can forestall, accomplish and alleviate the risks.},
  File                     = {Basharat-2013-p176-181.pdf:Basharat-2013-p176-181.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {DP industry;Internet;globalisation;project management;software development management;software houses;team working;Internet technology;Pakistan software industry;communication risk;cultural differences;emails facilities;foremost project risk;global software development;low-cost telecommunication;multicultural virtual teams;project manager;risks factors assessment;risks factors identification;technological advanced global environment;virtual companies;virtual environment;virtual project management;virtual project team manager;virtual projects;virtual work groups;Companies;Cultural differences;Project management;Schedules;Software;Standards;Virtual groups;project management;risks in virtual projects;virtual projects;virtual team},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6661735&isnumber=6661709},
  Zone                     = {-}
}

@Article{Basole-2014-p109-120,
  Title                    = {Visual analysis of supply network risks: Insights from the electronics industry },
  Author                   = {Rahul C. Basole and Marcus A. Bellamy},
  Journal                  = {Decision Support Systems },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {109 - 120},
  Volume                   = {67},

  Abstract                 = {Abstract In today's complex, global supply networks it has become increasingly challenging to identify, evaluate, and mitigate risks of disruption. Traditional supply chain practices have primarily focused on dyadic risk management, rarely considering risks in the sub-tier supply network. However, this approach severely limits a decision maker's ability to understand the highly interconnected nature of systemic risks and develop corresponding mitigation strategies. Grounded in theories of supply chains as complex systems, network analysis, and risk management, we demonstrate the importance of visual decision support for supply network risk assessment. We empirically illustrate our approach with supply network visualization examples from the electronics industry. We conclude the study with implications for the design and implementation of visual supply network decision support systems and future research opportunities. },
  Doi                      = {http://dx.doi.org/10.1016/j.dss.2014.08.008},
  File                     = {Published version:Basole-2014-p109-120.pdf:PDF},
  Impactfactor             = {2.201},
  ISSN                     = {0167-9236},
  Keywords                 = {Supply networks},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {好像谈的是供应网络的风险，不是很搭边},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167923614002255},
  Zone                     = {2Q}
}

@Article{Basu-1993-p177-189,
  Title                    = {A knowledge representation model for multiuser knowledge-based systems},
  Author                   = {Basu, A.},
  Journal                  = {Knowledge and Data Engineering, IEEE Transactions on},
  Year                     = {1993},

  Month                    = {Apr},
  Number                   = {2},
  Pages                    = {177-189},
  Volume                   = {5},

  Doi                      = {10.1109/69.219729},
  File                     = {Published version:Basu-1993-p177-189.pdf:PDF},
  Impactfactor             = {1.892},
  ISSN                     = {1041-4347},
  Keywords                 = {knowledge based systems;knowledge representation;multi-access systems;MKBS;UKBs;consistency maintenance;explanation;knowledge representation model;largely autonomous user knowledge bases;multipole heterogeneous users;multiuser knowledge-based system;problem solving;rule translation;Costs;Engines;Expert systems;Indexing;Inference algorithms;Information retrieval;Knowledge acquisition;Knowledge based systems;Knowledge representation;Problem-solving},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=219729&isnumber=5754},
  Zone                     = {3Q}
}

@Article{Baudrit-2006-p593-608,
  Title                    = {Joint Propagation and Exploitation of Probabilistic and Possibilistic Information in Risk Assessment},
  Author                   = {Baudrit, C. and Dubois, D. and Guyonnet, D.},
  Journal                  = {Fuzzy Systems, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {Oct},
  Number                   = {5},
  Pages                    = {593-608},
  Volume                   = {14},

  Doi                      = {10.1109/TFUZZ.2006.876720},
  File                     = {Published version:Baudrit-2006-p593-608.pdf:PDF},
  Impactfactor             = {5.484},
  ISSN                     = {1063-6706},
  Keywords                 = {possibility theory;probability;random processes;risk management;uncertain systems;possibilistic information;probabilistic information;random variability;risk assessment;uncertainty representation;Contamination;Decision making;Geologic measurements;Humans;Measurement errors;Pollution measurement;Probability distribution;Risk management;Soil;Uncertainty;(random) fuzzy intervals;Belief functions;dependence;possibility;probability},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1707747&isnumber=36039},
  Zone                     = {1Q}
}

@Article{Bauer-2009-p706-719,
  Title                    = {Cybersecurity: Stakeholder incentives, externalities, and policy options},
  Author                   = {Bauer, Johannes M. and van Eeten, Michel J.G.},
  Journal                  = {Telecommunications Policy},
  Year                     = {2009},

  Month                    = {Nov},
  Number                   = {10-11},
  Pages                    = {706鈥?719},
  Volume                   = {33},

  Doi                      = {10.1016/j.telpol.2009.09.001},
  File                     = {Published version:Bauer-2009-p706-719.pdf:PDF},
  Impactfactor             = {1.594},
  ISSN                     = {0308-5961},
  Level                    = {B},
  Publisher                = {Elsevier BV},
  Url                      = {http://dx.doi.org/10.1016/j.telpol.2009.09.001},
  Zone                     = {3Q}
}

@InProceedings{Beckers-2013-p465-474,
  Title                    = {Structured Pattern-Based Security Requirements Elicitation for Clouds},
  Author                   = {Beckers, K. and Heisel, M. and Cote, I and Goeke, L. and Guler, S.},
  Booktitle                = {Availability, Reliability and Security (ARES), 2013 Eighth International Conference on},
  Year                     = {2013},
  Month                    = {Sept},
  Pages                    = {465-474},

  Doi                      = {10.1109/ARES.2013.61},
  File                     = {Published version:Beckers-2013-p465-474.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {cloud computing;program verification;security of data;cloud computing systems;cloud services;cloud system analysis pattern;online-banking system;security requirements patterns;structured pattern-based security requirements elicitation;validation conditions;Business;Cloud computing;Security;Servers;Unified modeling language;Virtual machining;ISO 27001;cloud computing;requirements patterns;security requirements engineering;security standards},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6657277&isnumber=6657192},
  Zone                     = {-}
}

@InProceedings{BenMahmou-2011-p1-9,
  Title                    = {A Risk Propagation Based Quantitative Assessment Methodology for Network Security - Aeronautical Network Case Study},
  Author                   = {Ben Mahmoud, M.S. and Larrieu, N. and Pirovano, A.},
  Booktitle                = {Network and Information Systems Security (SAR-SSI), 2011 Conference on},
  Year                     = {2011},
  Month                    = {May},
  Pages                    = {1-9},

  Abstract                 = {Recently, risk assessment has been considered as an essential technique in evaluating the security of network information systems. Many proposals have been made in this area in order to provide new approaches to allow administrators and engineers to analyze the impact of any attack that could target their systems. Nevertheless, there is a lack of quantitative techniques and methods which take into account the inherent characteristics of a network such as interconnection between nodes. This paper presents an original risk assessment approach based on risk propagation and network node correlation to provide relevant and accurate results. Each parameter involved in the risk assessment process is quantified then the overall approach is described in detail. At the end of the paper, the network security assessment methodology is applied to a satellite-based system architecture we designed for an industrial project entitled FAST (Fiber-like Aircraft Satellite Telecommunications). This project is co-funded by the Aerospace Valley pole and the French government (Direction Generale de la Competitivite, de l'Industrie et des Services - DGCIS, Fonds Unique Interministeriel - FUI) and aims to provide bi-directional satellite communication services on commercial aircraft worldwide.},
  Doi                      = {10.1109/SAR-SSI.2011.5931372},
  File                     = {Published version:BenMahmou-2011-p1-9.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer network security;information systems;risk management;satellite communication;FAST;aeronautical network case study;bidirectional satellite communication service;fiber-like aircraft satellite telecommunication;network information systems;network security;quantitative assessment methodology;risk assessment approach;risk propagation;Databases;ISO standards;Information systems;Mathematical model;Risk management;Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5931372&isnumber=5931356},
  Zone                     = {-}
}

@Article{Ben-Haim-2008-p5-12,
  Title                    = {Improved Upper Bounds on the Reliability Function of the Gaussian Channel},
  Author                   = {Ben-Haim, Y. and Litsyn, S.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2008},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {5-12},
  Volume                   = {54},

  Abstract                 = {A new lower bound on the distance distribution of spherical codes is derived. This yields two new upper bounds on the reliability function of the Gaussian channel. These bounds outperform previously known bounds, and imply a new range of rates for which the exact value of the reliability function is known.},
  Doi                      = {10.1109/TIT.2007.911215},
  File                     = {Published version:Ben-Haim-2008-p5-12.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {Gaussian channels;codes;reliability;Gaussian channel;codes;reliability function;upper bounds;Communication channels;Error probability;Gaussian channels;Gaussian noise;H infinity control;Helium;Information theory;Maximum likelihood decoding;Signal to noise ratio;Upper bound;Distance distribution;Gaussian channel;reliability function;spherical codes},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4418460&isnumber=4418455},
  Zone                     = {2Q}
}

@Article{Beretta-2015-p252-264,
  Title                    = {From atmospheric corrosive attack to crack propagation for \{A1N\} railway axles steel under fatigue: Damage process and detection },
  Author                   = {Stefano Beretta and Antonietta Lo Conte and John Rudlin and Dorothee Panggabean},
  Journal                  = {Engineering Failure Analysis },
  Year                     = {2015},
  Note                     = {Failure analysis of structure components undergone stress corrosion, fatigue and neutron irradiation },
  Number                   = {0},
  Pages                    = {252 - 264},
  Volume                   = {47, Part B},

  Abstract                 = {Abstract In this paper a detailed investigation about the evolution of the corrosion fatigue process of this material is presented. The corrosion pits appear in the early stage of the corrosion fatigue life and a preferential site of pit nucleation has been observed at the ferrite–ferrite grain boundary. At the bottom of these primary pits, the formation, due to electrochemical action, of a secondary pit that triggers for the pit-to-crack transition has been observed. The growth of micro-cracks has also been analyzed and it is characterized by the development of a surface pattern of coalescing cracks. An original optical detection method has been proposed to inspect the exposed surfaces of the corroded railway axles and to detect the in situ crack pattern associated to the corrosion–fatigue damage. It is shown that the optical detection method is able to identify the damage observed and it has been successfully applied in field observations. },
  Doi                      = {http://dx.doi.org/10.1016/j.engfailanal.2014.07.026},
  File                     = {Published version:Beretta-2015-p252-264.pdf:PDF},
  Impactfactor             = {0.855},
  ISSN                     = {1350-6307},
  Keywords                 = {\{A1N\} steel},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1350630714002544},
  Zone                     = {3Q}
}

@Article{Berges-2014-p1887-1890a,
  Title                    = {Risk and reliability assessment about a manufacturing issue in a power \{MOSFET\} for automotive applications },
  Author                   = {C. Bergès and P. Soufflet and A. Jadrani},
  Journal                  = {Microelectronics Reliability },
  Year                     = {2014},
  Note                     = {SI: \{ESREF\} 2014 },
  Number                   = {9–10},
  Pages                    = {1887 - 1890},
  Volume                   = {54},

  Abstract                 = {Abstract In the automotive semiconductor industry, risk assessments are requested by customers on quality incidents that happen in the assembly line or in field. More rarely, in a die business context, such a study is requested about a defect observed during the optical inspection performed by the customers on the known good dice after assembly. This article deals with the case of pin holes in the top metal surface of a \{MOSFET\} component. The risk assessment is addressing detection, occurrence and severity of the defect: this implies process and failure analysis. Also, reliability has been carried out, by completing accelerated and typical stress tests. },
  Doi                      = {http://dx.doi.org/10.1016/j.microrel.2014.07.022},
  File                     = {Published version:Berges-2014-p1887-1890a.pdf:PDF},
  Impactfactor             = {1.137},
  ISSN                     = {0026-2714},
  Keywords                 = {Automotive semiconductor industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0026271414002182},
  Zone                     = {3Q}
}

@Article{Bernatik-2004-p271-278,
  Title                    = {Loss prevention in heavy industry: risk assessment of large gasholders },
  Author                   = {A. Bernatik and M. Libisova},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2004},
  Number                   = {4},
  Pages                    = {271 - 278},
  Volume                   = {17},

  Abstract                 = {The article summarises results of quantitative risk assessment for the operation of six large gasholders in the area of great industrial agglomeration in the Czech Republic. Gasholders for storing combustion gases contain significant amounts of dangerous substances, especially of carbon monoxide and hydrogen. That is why safety reports on these gasholders were prepared in the framework of implementation of the \{SEVESO\} \{II\} Directive. Although gasholders have gradually been shut down in the world, they still bring the heavy industry in the area of the city of Ostrava a considerable financial saving. Chosen industrial plants actually combust cheap waste gases, i.e. coking, converter and blast-furnace gases instead of natural gas. As a consequence, the gasholders are not expected to be closed in the near future either. With regard to the age (the oldest gasholder is more than 60 years old) and a high population density in the area concerned, it was necessary to do a detailed risk analysis. Many methods, from simple screening and indexing methods (Selection Method from Purple Book \{CPR\} 18E, IAEA-TECDOC-727 method, method based on the Dow’s Fire and Explosion Index), through the systematic \{HAZOP\} method, modelling of the dispersion of toxic gas releases up to the determination of risk probability and societal acceptability were employed for the assessment of risks of the gasholders. The goal of the article is to present results of the assessment of risks connected with the operation of the large gasholders, to furnish information about possible operational problems and to verify the applicability of recognised methods of risk analysis for these specific serious sources of risks.},
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2004.04.004},
  File                     = {Published version:Bernatik-2004-p271-278.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Gasholder},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {故障树、事件树},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095042300400021X},
  Zone                     = {3Q}
}

@Article{Bernatik-2004-p271-278a,
  Title                    = {Loss prevention in heavy industry: risk assessment of large gasholders },
  Author                   = {A. Bernatik and M. Libisova},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2004},
  Number                   = {4},
  Pages                    = {271 - 278},
  Volume                   = {17},

  Abstract                 = {The article summarises results of quantitative risk assessment for the operation of six large gasholders in the area of great industrial agglomeration in the Czech Republic. Gasholders for storing combustion gases contain significant amounts of dangerous substances, especially of carbon monoxide and hydrogen. That is why safety reports on these gasholders were prepared in the framework of implementation of the \{SEVESO\} \{II\} Directive. Although gasholders have gradually been shut down in the world, they still bring the heavy industry in the area of the city of Ostrava a considerable financial saving. Chosen industrial plants actually combust cheap waste gases, i.e. coking, converter and blast-furnace gases instead of natural gas. As a consequence, the gasholders are not expected to be closed in the near future either. With regard to the age (the oldest gasholder is more than 60 years old) and a high population density in the area concerned, it was necessary to do a detailed risk analysis. Many methods, from simple screening and indexing methods (Selection Method from Purple Book \{CPR\} 18E, IAEA-TECDOC-727 method, method based on the Dow’s Fire and Explosion Index), through the systematic \{HAZOP\} method, modelling of the dispersion of toxic gas releases up to the determination of risk probability and societal acceptability were employed for the assessment of risks of the gasholders. The goal of the article is to present results of the assessment of risks connected with the operation of the large gasholders, to furnish information about possible operational problems and to verify the applicability of recognised methods of risk analysis for these specific serious sources of risks. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2004.04.004},
  File                     = {Published version:Bernatik-2004-p271-278a.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Gasholder},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095042300400021X},
  Zone                     = {3Q}
}

@Article{Beroggi-2000-p32-44,
  Title                    = {Multi-expert operational risk management},
  Author                   = {Beroggi, G.E.G. and Waliace, W.A.},
  Journal                  = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {32-44},
  Volume                   = {30},

  Abstract                 = {Operational risk management is the process of monitoring, evaluating, and changing courses of actions with potential detrimental consequences in real time. In this paper, we extend the decision models proposed in the literature for individual risk managers to account for situations where multiple risk managers are involved. For this purpose, two dynamic and adaptive preference aggregation models for cardinal and ordinal assessments are proposed and discussed. The mechanical aspects of the models are then validated using field data collected from experienced operational risk managers in an individual-expert setting. Sensitivity analysis indicates that the models have enough flexibility to be adapted to account for behavioral considerations. The paper closes with a research agenda},
  Doi                      = {10.1109/5326.827452},
  File                     = {Published version:Beroggi-2000-p32-44.pdf:PDF},
  Impactfactor             = {2.548},
  ISSN                     = {1094-6977},
  Keywords                 = {decision support systems;management of change;risk management;sensitivity analysis;adaptive preference aggregation models;behavioral considerations;cardinal assessments;course of action changing;course of action evaluation;course of action monitoring;decision models;dynamic preference aggregation models;individual-expert setting;multi-expert operational risk management;ordinal assessments;potential detrimental consequences;sensitivity analysis;Communication system control;Communications technology;Computer network management;Crisis management;Disaster management;Large-scale systems;Logistics;Monitoring;Risk management;Technology management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，好像也没有仿真},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=827452&isnumber=17956},
  Zone                     = {2Q}
}

@Article{Beroggi-1994-p1450-1457,
  Title                    = {Operational risk management: a new paradigm for decision making},
  Author                   = {Beroggi, G.E.G. and Wallace, W.A.},
  Journal                  = {Systems, Man and Cybernetics, IEEE Transactions on},
  Year                     = {1994},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {1450-1457},
  Volume                   = {24},

  Abstract                 = {The need for more effective ways to manage the risk and safety of technological systems has been widely recognized and accepted by government and industry. Traditionally, risk analysis has been considered as part of the process of planning a technological system and addressed the risk inherent in its day-to-day operations. However, risk must also be considered when responding to episodic events whose uniqueness requires taking actions that are variants of, or different from planned operational procedures. The purpose of this paper is to present a new paradigm for real-time risk analysis that capitalizes upon the advances in computer power, human-machine interfaces, and communication technology. A new process for risk assessment and an appropriate reasoning algorithm for choice has been developed which supports the human operator in analyzing risks and making decisions in real-time during unexpected disruptions in the operations of large-scale systems. The process recognizes that although advances in technology may automate many tasks, humans will always be an integral part of managing large-scale systems. The practical realism of the new approach of operational risk management is illustrated by two examples, hazardous material transportation and emergency management. The first example is discussed within the context of a prototype decision support system for interactive real-time risk management},
  Doi                      = {10.1109/21.310528},
  File                     = {Published version:Beroggi-1994-p1450-1457.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0018-9472},
  Keywords                 = {decision support systems;inference mechanisms;large-scale systems;real-time systems;risk management;communication technology;computer power;decision making;emergency management;episodic events;government;hazardous material transportation;human-machine interfaces;industry;interactive real-time risk management;large-scale systems;operational risk management;prototype decision support system;real-time risk analysis;reasoning algorithm;risk assessment;safety;technological systems;unexpected disruptions;Decision making;Disaster management;Government;Humans;Large-scale systems;Real time systems;Risk analysis;Risk management;Safety;Technology management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，有点老},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=310528&isnumber=7524},
  Zone                     = {-}
}

@Article{Bertolotti-2007-p387-397,
  Title                    = {Improving the security of industrial networks by means of formal verification },
  Author                   = {Ivan Cibrario Bertolotti and Luca Durante and Paolo Maggi and Riccardo Sisto and Adriano Valenzano},
  Journal                  = {Computer Standards \& Interfaces },
  Year                     = {2007},
  Number                   = {3},
  Pages                    = {387 - 397},
  Volume                   = {29},

  Abstract                 = {Computer networks are exposed to serious security threats that can even have catastrophic consequences from both the points of view of economy and safety if such networks control critical infrastructures, such as for example industrial plants. Security must then be considered as a fundamental issue starting from the earlier phases of the design of a system, and suitable techniques and tools should be adopted to satisfy the security-related requirements. The focus of this paper is on how formal methods can help in analysing the standard cryptographic protocols used to implement security-critical services such as authentication and secret keys distribution in critical environments. The analysis of the 802.11 shared key authentication protocol by S3A, a fully automatic software tool that is based on a formal approach, is illustrated as a case study, which also highlights the peculiarities of analysing protocols based on wireless channels. },
  Doi                      = {http://dx.doi.org/10.1016/j.csi.2006.06.002},
  File                     = {Published version:Bertolotti-2007-p387-397.pdf:PDF},
  Impactfactor             = {0.978},
  ISSN                     = {0920-5489},
  Keywords                 = {Protocol verification},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0920548906000766},
  Zone                     = {4Q}
}

@Article{Betensky-2003-p7-8,
  Title                    = {Commentary: failure-rate functions for doubly-truncated random variables},
  Author                   = {Betensky, R.A. and Martin, E.C.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {7-8},
  Volume                   = {52},

  Doi                      = {10.1109/TR.2002.807241},
  File                     = {Published version:Betensky-2003-p7-8.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {brain;failure analysis;maximum likelihood estimation;probability;tumours;brain tumor progression;chemotherapy;doubly-truncated random variables;failure-rate functions;failure-time random variable distribution;generalized failure-rate functions;malignant brain tumor;nonparametric maximum likelihood estimator;probability density functions;subject-specific truncating intervals;Cancer;Distribution functions;Hazards;Maximum likelihood estimation;Neoplasms;Plugs;Probability density function;Public healthcare;Random variables},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1179788&isnumber=26500},
  Zone                     = {2Q}
}

@Article{Betensky-2003-p7-8a,
  Title                    = {Commentary: failure-rate functions for doubly-truncated random variables},
  Author                   = {Betensky, R.A. and Martin, E.C.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {7-8},
  Volume                   = {52},

  Abstract                 = {Navarro and Ruiz (see ibid., vol.45, p.685-90, 1996) express the nonparametric maximum likelihood estimator (NPMLE) of the distribution of a failure-time random variable as a function of the NPMLE of generalized failure-rate functions. These generalized failure-rate functions are equal to the probability density functions of a doubly-truncated failure-time random variable at the endpoints of the truncating interval. Readers can infer from this paper that this simple estimator can be applied to a doubly-truncated sample of failure times. This commentary explains why that estimator does not apply to the general setting in which the observed failure times are doubly-truncated with subject-specific truncating intervals. A doubly-truncated sample of times to brain tumor progression illustrates the deviation of that estimator from the NPMLE for these data.},
  Doi                      = {10.1109/TR.2002.807241},
  File                     = {Published version:Betensky-2003-p7-8a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {brain;failure analysis;maximum likelihood estimation;probability;tumours;brain tumor progression;chemotherapy;doubly-truncated random variables;failure-rate functions;failure-time random variable distribution;generalized failure-rate functions;malignant brain tumor;nonparametric maximum likelihood estimator;probability density functions;subject-specific truncating intervals;Cancer;Distribution functions;Hazards;Maximum likelihood estimation;Neoplasms;Plugs;Probability density function;Public healthcare;Random variables},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1179788&isnumber=26500},
  Zone                     = {2Q}
}

@Article{Bi-2012-p1112-1118,
  Title                    = {Dynamic risk assessment of oil spill scenario for Three Gorges Reservoir in China based on numerical simulation },
  Author                   = {Haipu Bi and Hu Si},
  Journal                  = {Safety Science },
  Year                     = {2012},
  Note                     = {First International Symposium on Mine Safety Science and Engineering 2011 },
  Number                   = {4},
  Pages                    = {1112 - 1118},
  Volume                   = {50},

  Abstract                 = {A novel method for dynamic risk assessment of oil spill accidents based on numerical simulation was presented in this paper. The dynamic risk assessment model was developed consisting of a comprehensive list of caused consequences like environmental damage, asset loss, health impact and social effect as well as emergency actions preventing these losses. Contributing events in the Mater Logic Diagram (MLD) of the dynamic risk assessment model were valued based on the simulated evolvement of oil contaminants under spill scenario on quasi-static fluid, which was obtained by coupling an oil spill model with hydrodynamic module in Jialing river of Three Gorges Reservoir in China after the impoundment of the reservoir to 175&#xa0;m water level in 2010. Calculated result of dynamic risk as grade \{IV\} indicated that the assessed oil spill was not as catastrophic as we thought because of the slow transport of oil fractions on water surface and absence of dispersed oil in water body due to the quasi-static fluid, very gentle wind and effective emergency actions, as well as by the reason that rare agriculture or industrial crops exist in spill adjacent area. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2011.11.012},
  File                     = {Published version:Bi-2012-p1112-1118.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，这篇文章思路几乎跟我一样，风险分解+AHP},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S092575351100302X},
  Zone                     = {3Q}
}

@InProceedings{Bian-2009-p4452-4457,
  Title                    = {Simulation model and Fault Tree Analysis for AUV},
  Author                   = {Xinqian Bian and Chunhui Mou and Zheping Yan and Jian Xu},
  Booktitle                = {Mechatronics and Automation, 2009. ICMA 2009. International Conference on},
  Year                     = {2009},
  Month                    = {Aug},
  Pages                    = {4452-4457},

  Abstract                 = {In this paper, the traditional method of fault tree analysis is briefly introduced. AUV, whose fault tree was built, did not work normally as top event. Using Monte-Carlo random sampling method, we establish the simulation model of the AUV, set up the Fault Tree Analysis simulation MATLAB program and engage in its fault tree quantitative analysis. The life distribution function of the AUV system is obtained through the digital simulation of the FT and the other elements attained are the basic unit importance and the mode importance of the basic components and are working out the Mean Time Between Failure. The specific process of the fault tree's digital simulation is researched. And in this process, we can see the contribution of each part to the reliability of the whole system, which is a great help to study the reliability of the AUV system.},
  Doi                      = {10.1109/ICMA.2009.5246716},
  File                     = {Published version:Bian-2009-p4452-4457.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Monte Carlo methods;fault trees;remotely operated vehicles;sampling methods;underwater vehicles;AUV system;MATLAB program;Monte-Carlo random sampling;digital simulation;fault tree analysis;fault tree quantitative analysis;life distribution function;simulation model;system reliability;Analytical models;Automation;Availability;Computational modeling;Digital simulation;Fault trees;Maintenance;Mathematical model;Safety;Uncertainty;Autonomous Underwater Vehicle;Digital Simulation;Fault Tree;Reliability},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5246716&isnumber=5244759},
  Zone                     = {-}
}

@Article{Bielikova-2014-p48-56,
  Title                    = {Prediction Techniques of Agriculture Enterprises Failure },
  Author                   = {Tatiana Bieliková and Tatiana Bányiová and Andrea Piterková},
  Journal                  = {Procedia Economics and Finance },
  Year                     = {2014},
  Note                     = {17th International Conference Enterprise and Competitive Environment 2014 },
  Number                   = {0},
  Pages                    = {48 - 56},
  Volume                   = {12},

  Abstract                 = {Abstract Early diagnosis of potential corporate failure is very important due to the uncertainty of the current business environment and global competition. The main purpose of this paper is to make comparison of the prediction abilities among different techniques to become the best prediction model, which will classify the agricultural enterprises to prosperous and unprosperous. This paper examines three classification techniques, namely discriminant analysis, logistic regression and decision trees. It is implemented by using financial data for Slovak enterprises from the agriculture sector. The selection of appropriate economic and financial ratios is based on the relevant literature and refers to the key ratios of bankruptcy models specified for agriculture enterprises. The article also offers several potential areas for the further analysis. },
  Doi                      = {http://dx.doi.org/10.1016/S2212-5671(14)00319-0},
  File                     = {Published version:Bielikova-2014-p48-56.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2212-5671},
  Keywords                 = {Agriculture enterprises},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2212567114003190},
  Zone                     = {-}
}

@Article{Billinton-2004-p1759-1765,
  Title                    = {Historic performance-based distribution system risk assessment},
  Author                   = {Billinton, R. and Zhaoming Pan},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {1759-1765},
  Volume                   = {19},

  Abstract                 = {Regulatory authorities are increasingly adopting performance-based regulation (PBR) in the deregulated electricity industry. A PBR regime is intended to provide distribution utilities with incentives for economic efficiency gains. In order to discourage distribution utilities from sacrificing service reliability while pursuing economic incentives, historic utility performance is utilized as the specified service reliability standard in some places. A reward/penalty structure based on the historic reliability record could be integrated into a PBR plan. Historic reliability data are therefore, extremely important for distribution system risk assessment and remedial work in this new regime. This paper presents actual reliability data taken from the Canadian Electricity Association (CEA) Service Continuity Reports. The financial risk analyzes associated with the historic reliability data are conducted by incorporating reliability index probability distributions in imposed reward/penalty policies. The major cause contributions to the service continuity indices utilizing the CEA cause code categories are analyzed and illustrated. This work should prove useful for those utilities facing the emerging application of PBR.},
  Doi                      = {10.1109/TPWRD.2004.832404},
  File                     = {Published version:Billinton-2004-p1759-1765.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {electricity supply industry deregulation;power distribution economics;power distribution reliability;probability;risk management;deregulated electricity industry;distribution system risk assessment;distribution utilities;performance-based regulation;reliability index probability distributions;service reliability;Bioreactors;Electricity supply industry deregulation;Monitoring;Power generation;Power generation economics;Power system reliability;Probability distribution;Protocols;Risk analysis;Risk management;Distribution system;historic performance;performance-based regulation;probability distribution;risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，不是很相关},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1339344&isnumber=29530},
  Zone                     = {3Q}
}

@Article{Bobbio-2001-p249-260,
  Title                    = {Improving the analysis of dependable systems by mapping fault trees into Bayesian networks },
  Author                   = {A. Bobbio and L. Portinale and M. Minichino and E. Ciancamerla},
  Journal                  = {Reliability Engineering \& System Safety},
  Year                     = {2001},
  Number                   = {3},
  Pages                    = {249 - 260},
  Volume                   = {71},

  Abstract                 = {Bayesian Networks (BN) provide a robust probabilistic method of reasoning under uncertainty. They have been successfully applied in a variety of real-world tasks but they have received little attention in the area of dependability. The present paper is aimed at exploring the capabilities of the \{BN\} formalism in the analysis of dependable systems. To this end, the paper compares \{BN\} with one of the most popular techniques for dependability analysis of large, safety critical systems, namely Fault Trees (FT). The paper shows that any \{FT\} can be directly mapped into a \{BN\} and that basic inference techniques on the latter may be used to obtain classical parameters computed from the former (i.e. reliability of the Top Event or of any sub-system, criticality of components, etc). Moreover, by using BN, some additional power can be obtained, both at the modeling and at the analysis level. At the modeling level, several restrictive assumptions implicit in the \{FT\} methodology can be removed and various kinds of dependencies among components can be accommodated. At the analysis level, a general diagnostic analysis can be performed. The comparison of the two methodologies is carried out by means of a running example, taken from the literature, that consists of a redundant multiprocessor system. },
  Doi                      = {http://dx.doi.org/10.1016/S0951-8320(00)00077-6},
  File                     = {Published version:Bobbio-2001-p249-260.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Dependable systems},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，},
  Timestamp                = {2015.01.17},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832000000776},
  Zone                     = {2Q}
}

@Article{Bogumil-1982-p24-28,
  Title                    = {Limitations of Probabilistic Risk Assessment},
  Author                   = {Bogumil, R.J.},
  Journal                  = {Technology and Society Magazine, IEEE},
  Year                     = {1982},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {24-28},
  Volume                   = {1},

  Doi                      = {10.1109/MTAS.1982.5009713},
  File                     = {Published version:Bogumil-1982-p24-28.pdf:PDF},
  Impactfactor             = {0.542},
  ISSN                     = {0278-0097},
  Keywords                 = {Biomedical engineering;Economic forecasting;Mathematical model;Predictive models;Reliability engineering;Risk analysis;Risk management},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5009713&isnumber=5009697},
  Zone                     = {4Q}
}

@Article{Bogumil-1982-p24-28a,
  Title                    = {Limitations of Probabilistic Risk Assessment},
  Author                   = {Bogumil, R.J.},
  Journal                  = {Technology and Society Magazine, IEEE},
  Year                     = {1982},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {24-28},
  Volume                   = {1},

  Abstract                 = {Probabilistic risk assessment attempts to predict the frequency of occurrence of hypothetical events by the development and analysis of a mathematical-logic model of the generative physical system. The discipline involves an amalgam of probability theory and engineering, as in reliability analysis, combined with a concern for psycho-social, medical and economic issues, due to the public-policy aspect of many applications. The literature in the field has the superficial appearance of well reasoned, dispassionate analysis extrapolated to its logical implications. However, critical examination reveals unresolved social and theoretical issues of fundamental importance. These matters are discussed from both a mathematical and a socioeconomic perspective. From consideration of socioeconomic dynamics, it is argued that, notwithstanding the great difficulty of the problem and serious limitations of current risk assessment procedures, advanced technological civilization requires some form of prospective risk/benefit analysis.},
  Doi                      = {10.1109/MTAS.1982.5009713},
  File                     = {Published version:Bogumil-1982-p24-28a.pdf:PDF},
  Impactfactor             = {0.542},
  ISSN                     = {0278-0097},
  Keywords                 = {Biomedical engineering;Economic forecasting;Mathematical model;Predictive models;Reliability engineering;Risk analysis;Risk management},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {~，太老了},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5009713&isnumber=5009697},
  Zone                     = {4Q}
}

@Article{Bolognesi-2014-p345-355,
  Title                    = {Genotoxic risk in rubber manufacturing industry: A systematic review },
  Author                   = {Claudia Bolognesi and Angelo Moretto},
  Journal                  = {Toxicology Letters },
  Year                     = {2014},
  Note                     = {Environmental contaminants and target organ toxicities },
  Number                   = {2},
  Pages                    = {345 - 355},
  Volume                   = {230},

  Abstract                 = {Abstract A large body of evidence from epidemiological studies among workers employed in the rubber manufacturing industry has indicated a significant excess cancer risk in a variety of sites. The International Agency for Research on Cancer has recently classified the “Occupational exposures in the rubber-manufacturing industry” as carcinogenic to humans (Group 1). A genotoxic mechanism for the increased cancer risk was suggested on the basis of the evidence from the scientific literature. Exposure assessment studies have shown that workers in the rubber manufacturing industry may be exposed to different airborne carcinogenic and/or genotoxic chemicals, such as certain aromatic amines, polycyclic aromatic hydrocarbons, N-nitrosamines, although the available information does not allow to establish a causal association of cancer or genotoxic risk with particular substances/classes of chemicals or specific jobs. The aim of this paper is to critically evaluate, by conducting a systematic review, the available biomonitoring studies using genotoxicity biomarkers in rubber manufacturing industry. This systematic review suggests that a genotoxic hazard may still be present in certain rubber manufacturing industries. A quantitative risk assessment needs further studies addressing the different, processes and chemicals in the rubber manufacturing industries. },
  Doi                      = {http://dx.doi.org/10.1016/j.toxlet.2013.11.013},
  File                     = {Published version:Bolognesi-2014-p345-355.pdf:PDF},
  Impactfactor             = {3.145},
  ISSN                     = {0378-4274},
  Keywords                 = {Rubber manufacturing industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {分析工业对癌症产生的风险，不靠谱},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0378427413014197},
  Zone                     = {3Q}
}

@Article{Bonafede-2007-p22-28,
  Title                    = {Bayesian Networks for enterprise risk assessment },
  Author                   = {C.E. Bonafede and P. Giudici},
  Journal                  = {Physica A: Statistical Mechanics and its Applications },
  Year                     = {2007},
  Note                     = {Applications of Physics in Financial Analysis Proceedings of the 5th International Conference 'Applications of Physics in Financial Analysis' (APFA 5), Torino, Italy, 29 June 2006 -- 1 July 2006 5th International Conference 'Applications of Physics in Financial Analysis' },
  Number                   = {1},
  Pages                    = {22 - 28},
  Volume                   = {382},

  Abstract                 = {According to different typologies of activity and priority, risks can assume diverse meanings and it can be assessed in different ways. Risk, in general, is measured in terms of a probability combination of an event (frequency) and its consequence (impact). To estimate the frequency and the impact (severity) historical data or expert opinions (either qualitative or quantitative data) are used. Moreover, qualitative data must be converted in numerical values or bounds to be used in the model. In the case of enterprise risk assessment the considered risks are, for instance, strategic, operational, legal and of image, which many times are difficult to be quantified. So in most cases only expert data, gathered by scorecard approaches, are available for risk analysis. The Bayesian Networks (BNs) are a useful tool to integrate different information and in particular to study the risk's joint distribution by using data collected from experts. In this paper we want to show a possible approach for building a \{BN\} in the particular case in which only prior probabilities of node states and marginal correlations between nodes are available, and when the variables have only two states. },
  Doi                      = {http://dx.doi.org/10.1016/j.physa.2007.02.065},
  File                     = {Published version:Bonafede-2007-p22-28.pdf:PDF},
  Impactfactor             = {1.676},
  ISSN                     = {0378-4371},
  Keywords                 = {Bayesian Networks},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S037843710700132X},
  Zone                     = {3Q}
}

@Article{Bontempi-2004-p533-542,
  Title                    = {Semiphysical modelling architecture for dynamic assessment of power components loading capability},
  Author                   = {Bontempi, G. and Vaccaro, A. and Villacci, D.},
  Journal                  = {Generation, Transmission and Distribution, IEE Proceedings-},
  Year                     = {2004},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {533-542},
  Volume                   = {151},

  Abstract                 = {Dynamic loading of power components in a deregulated electricity market requires reliable models that are able to predict the thermal behaviour when the load exceeds a particular value. The thermal stress of the components is known to be the most critical factor to the assessment of network load capability. Predicting the evolution of the thermal stress during overload conditions is essential to estimate the loss of insulation life and to evaluate the consequent risks of both technical and economical nature. The paper discusses an innovative grey-box architecture for integrating physical knowledge modelling (also know as white-box) with machine learning techniques (also known as black-box) in dynamic load capability assessment of power components. To evaluate the effectiveness of the proposed solution, a specific case study concerning a system of medium-voltage power cables is presented.},
  Doi                      = {10.1049/ip-gtd:20040537},
  File                     = {Published version:Bontempi-2004-p533-542.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1350-2360},
  Keywords                 = {load management;power cables;power markets;thermal stresses;black-box;deregulated electricity market;dynamic load capability assessment;dynamic loading;grey-box architecture;insulation loss;knowledge modelling;machine learning techniques;medium-voltage power cables;power components loading capability;semiphysical modelling architecture;thermal stress;white-box},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，会议，有 Case Study，仿真的结果都是些电力相关的物理量},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1331018&isnumber=29389},
  Zone                     = {-}
}

@Article{Bontempi-2004-p533-542a,
  Title                    = {Semiphysical modelling architecture for dynamic assessment of power components loading capability},
  Author                   = {Bontempi, G. and Vaccaro, A. and Villacci, D.},
  Journal                  = {Generation, Transmission and Distribution, IEE Proceedings-},
  Year                     = {2004},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {533-542},
  Volume                   = {151},

  Abstract                 = {Dynamic loading of power components in a deregulated electricity market requires reliable models that are able to predict the thermal behaviour when the load exceeds a particular value. The thermal stress of the components is known to be the most critical factor to the assessment of network load capability. Predicting the evolution of the thermal stress during overload conditions is essential to estimate the loss of insulation life and to evaluate the consequent risks of both technical and economical nature. The paper discusses an innovative grey-box architecture for integrating physical knowledge modelling (also know as white-box) with machine learning techniques (also known as black-box) in dynamic load capability assessment of power components. To evaluate the effectiveness of the proposed solution, a specific case study concerning a system of medium-voltage power cables is presented.},
  Doi                      = {10.1049/ip-gtd:20040537},
  File                     = {Published version:Bontempi-2004-p533-542a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1350-2360},
  Keywords                 = {load management;power cables;power markets;thermal stresses;black-box;deregulated electricity market;dynamic load capability assessment;dynamic loading;grey-box architecture;insulation loss;knowledge modelling;machine learning techniques;medium-voltage power cables;power components loading capability;semiphysical modelling architecture;thermal stress;white-box},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1331018&isnumber=29389},
  Zone                     = {-}
}

@Article{Bouejla-2014-p222-230,
  Title                    = {A Bayesian network to manage risks of maritime piracy against offshore oil fields },
  Author                   = {Amal Bouejla and Xavier Chaze and Franck Guarnieri and Aldo Napoli},
  Journal                  = {Safety Science },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {222 - 230},
  Volume                   = {68},

  Abstract                 = {Abstract In recent years, pirate attacks against shipping and oil field installations have become more frequent and more serious. This article proposes an innovative solution to the problem of offshore piracy from the perspective of the entire processing chain: from the detection of a potential threat to the implementation of a response. The response to an attack must take into account multiple variables: the characteristics of the threat and the potential target, existing protection tools, environmental constraints, etc. The potential of Bayesian networks is used to manage this large number of parameters and identify appropriate counter-measures. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2014.04.010},
  File                     = {Published version:Bouejla-2014-p222-230.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Oil platforms},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，Bayesian 大写},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753514000952},
  Zone                     = {3Q}
}

@Article{Bouwknegt-2015-p50-58,
  Title                    = {Quantitative farm-to-fork risk assessment model for norovirus and hepatitis A virus in European leafy green vegetable and berry fruit supply chains },
  Author                   = {Martijn Bouwknegt and Katharina Verhaelen and Artur Rzeżutka and Iwona Kozyra and Leena Maunula and Carl-Henrik von Bonsdorff and Apostolos Vantarakis and Petros Kokkinos and Tamas Petrovic and Sava Lazic and Ivo Pavlik and Petra Vasickova and Kris A. Willems and Arie H. Havelaar and Saskia A. Rutjes and Ana Maria de Roda Husman},
  Journal                  = {International Journal of Food Microbiology },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {50 - 58},
  Volume                   = {198},

  Abstract                 = {Abstract Fresh produce that is contaminated with viruses may lead to infection and viral gastroenteritis or hepatitis when consumed raw. It is thus important to reduce virus numbers on these foods. Prevention of virus contamination in fresh produce production and processing may be more effective than treatment, as sufficient virus removal or inactivation by post-harvest treatment requires high doses that may adversely affect food quality. To date knowledge of the contribution of various potential contamination routes is lacking. A risk assessment model was developed for human norovirus, hepatitis A virus and human adenovirus in raspberry and salad vegetable supply chains to quantify contributions of potential contamination sources to the contamination of produce at retail. These models were used to estimate public health risks. Model parameterization was based on monitoring data from European supply chains and literature data. No human pathogenic viruses were found in the soft fruit supply chains; human adenovirus (hAdV) was detected, which was additionally monitored as an indicator of fecal pollution to assess the contribution of potential contamination points. Estimated risks per serving of lettuce based on the models were 3&#xa0;×&#xa0;10−&#xa0;4 (6&#xa0;×&#xa0;10−&#xa0;6–5&#xa0;×&#xa0;10−&#xa0;3) for NoV infection and 3&#xa0;×&#xa0;10−&#xa0;8 (7&#xa0;×&#xa0;10−&#xa0;10–3&#xa0;×&#xa0;10−&#xa0;6) for hepatitis A jaundice. The contribution to virus contamination of hand-contact was larger as compared with the contribution of irrigation, the conveyor belt or the water used for produce rinsing. In conclusion, viral contamination in the lettuce and soft fruit supply chains occurred and estimated health risks were generally low. Nevertheless, the 97.5% upper limit for the estimated NoV contamination of lettuce suggested that infection risks up to 50% per serving might occur. Our study suggests that attention to full compliance for hand hygiene will improve fresh produce safety related to virus risks most as compared to the other examined sources, given the monitoring results. This effect will be further aided by compliance with other hygiene and water quality regulations in production and processing facilities. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijfoodmicro.2014.12.013},
  File                     = {Published version:Bouwknegt-2015-p50-58.pdf:PDF},
  Impactfactor             = {3.425},
  ISSN                     = {0168-1605},
  Keywords                 = {\{QMRA\}},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0168160514006187},
  Zone                     = {2Q}
}

@Article{Bowerman-1990-p158-161,
  Title                    = {Calculation of the Poisson cumulative distribution function [reliability applications]},
  Author                   = {Bowerman, P.N. and Nolty, R.G. and Scheuer, E.M.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1990},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {158-161},
  Volume                   = {39},

  Abstract                 = {A method for calculating the Poisson CDF (cumulative distribution function) is presented. The method avoids computer underflow and overflow during the process. The computer program uses this technique to calculate the Poisson CDF for arbitrary inputs. An algorithm that determines the Poisson parameter required to yield a specified value of the CDF is presented},
  Doi                      = {10.1109/24.55875},
  File                     = {Published version:Bowerman-1990-p158-161.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {C listings;probability;reliability theory;statistical analysis;C programming language;CDF;Poisson cumulative distribution function;computer program;overflow;reliability;underflow;Application software;Computer languages;Distribution functions;Laboratories;Probability;Propulsion;Random variables;Reliability engineering;Reliability theory;Statistical distributions},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=55875&isnumber=2019},
  Zone                     = {2Q}
}

@Article{Bowerman-1990-p162-166,
  Title                    = {Calculation of the binomial survivor function [reliability applications]},
  Author                   = {Bowerman, P.N. and Scheuer, E.M.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1990},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {162-166},
  Volume                   = {39},

  Abstract                 = {A method is presented for calculating the binomial SF (cumulative binomial distribution), binfc(k;p,n), especially for a large n, beyond the range of existing tables, where conventional computer programs fail because of underflow and overflow, and Gaussian or Poisson approximations yield insufficient accuracy for the purpose at hand. This method is used to calculate and sum the individual binomial terms while using multiplication factors to avoid underflow; the factors are then divided out of the partial sum whenever it has the potential to overflow. A computer program uses this technique to calculate the binomial SF for arbitrary inputs of k, p, and n. Two other algorithms are presented to determine the value of p needed to yield a specified SF for given values of k and n and calculate the value where p=SF for a given k and n. Reliability applications of each algorithm/program are given, e.g. the value of p needed to achieve a stated k-out-of-n :G system reliability and the value of p for which k -out-of-n:G system reliability equals p},
  Doi                      = {10.1109/24.55876},
  File                     = {Published version:Bowerman-1990-p162-166.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {C listings;probability;reliability theory;statistical analysis;C programming language;binomial survivor function;computer programs;cumulative binomial distribution;k-out-of-n:G system;multiplication factors;overflow;reliability applications;underflow;Application software;Distributed computing;Gaussian approximation;Laboratories;Probability;Propulsion;Reliability engineering;Reliability theory;Statistical distributions;Statistics},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=55876&isnumber=2019},
  Zone                     = {2Q}
}

@Article{Boyce-2007-p386-397,
  Title                    = {Modeling Drug Mechanism Knowledge Using Evidence and Truth Maintenance},
  Author                   = {Boyce, R.D. and Collins, C. and Horn, J. and Kalet, I.},
  Journal                  = {Information Technology in Biomedicine, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {386-397},
  Volume                   = {11},

  Doi                      = {10.1109/TITB.2007.890842},
  File                     = {Published version:Boyce-2007-p386-397.pdf:PDF},
  Impactfactor             = {1.978},
  ISSN                     = {1089-7771},
  Keywords                 = {biochemistry;drugs;knowledge based systems;medical computing;patient care;truth maintenance;DDI predictions;DIKB;Drug Interaction Knowledge-base;drug mechanism knowledge;drug property;drug-drug interaction;knowledge-based system;metabolic inhibition;metabolic mechanisms;patient safety;potential informatics solutions;truth maintenance;Clinical trials;Databases;Drugs;Informatics;Knowledge based systems;Libraries;Mechanical factors;Protection;Prototypes;Safety;Drug interactions;Truth Maintenance Systems;drug mechanisms;knowledge representation;Artificial Intelligence;Databases, Factual;Drug Interactions;Evidence-Based Medicine;Logistic Models;Medical Informatics;Natural Language Processing},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4267677&isnumber=4267676},
  Zone                     = {2Q}
}

@Article{Briggs-2008-p19-24,
  Title                    = {Risk Assessment: Perchlorate as a National Security Threat},
  Author                   = {Briggs, C.},
  Journal                  = {Technology and Society Magazine, IEEE},
  Year                     = {2008},

  Month                    = {Fall},
  Number                   = {3},
  Pages                    = {19-24},
  Volume                   = {27},

  __markedentry            = {[Qiqi:5]},
  Abstract                 = {Sources of drinking water have been contaminated by military-related activity for decades, but such environmental pollution has not been recognized as a security threat in its own right. The case of perchlorate in the United States demonstrates the reflexive nature of environmental security risks, as well as the inconsistent burdens of proof required for responses to uncertain threats. A known endocrine disruptor, perchlorate blocks the uptake of iodide in the thyroid, and poses a differential risk to women, children, and other at-risk groups. Despite a conclusion by the U.S. National Research Council (NRC) that known studies of perchlorate indicate no harm at low levels of ingestion (translated roughly to 27 parts per billion in water), newer research indicates a measure of concern for some tens of millions of residents in the southwestern U.S. In this article, we examine the decision-making process that translated risk assessment studies to regulatory levels, and explore how endocrine disrupting chemicals pose a methodological dilemma for chemical risk assessment policy.},
  Doi                      = {10.1109/MTS.2008.929002},
  File                     = {Published version:Briggs-2008-p19-24.pdf:PDF},
  Impactfactor             = {0.542},
  ISSN                     = {0278-0097},
  Keywords                 = {health hazards;risk management;water pollution;US National Research Council;United States;chemical risk assessment policy;differential risk;drinking water source contamination;endocrine disrupting chemical;environmental pollution;environmental security risk;iodide uptake;military-related activity;national security threat;perchlorate;regulatory level;thyroid;Chemical technology;Contamination;Endocrine system;Europe;Law;National security;Pollution;Risk management;Terrorism;Weapons},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {A，杂志，无仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4623823&isnumber=4623811},
  Zone                     = {4Q}
}

@Article{Buritica-2015-p233-241,
  Title                    = {Consequence-based framework for electric power providers using Bayesian belief network },
  Author                   = {Jessica A. Buriticá and Solomon Tesfamariam},
  Journal                  = {International Journal of Electrical Power \& Energy Systems },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {233 - 241},
  Volume                   = {64},

  Abstract                 = {Abstract Novel consequence-based framework for electric power providers is proposed. This framework includes six performance objectives, such as reputation, health and safety, environmental, financial, reliability, and system conditions. The six performance objectives are quantified with the consideration of 41 key performance indicators (KPIs). The framework is illustrated with a case study of 10 Canadian power utilities. Furthermore, a sensitivity analysis is undertaken to identify importance of the \{KPIs\} on the decision framework. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijepes.2014.07.034},
  File                     = {Published version:Buritica-2015-p233-241.pdf:PDF},
  Impactfactor             = {3.432},
  ISSN                     = {0142-0615},
  Keywords                 = {Consequence-based framework},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Bayesian 大写},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0142061514004669},
  Zone                     = {2Q}
}

@InProceedings{Burnap-2014-p608-613,
  Title                    = {Towards Real-Time Probabilistic Risk Assessment by Sensing Disruptive Events from Streamed News Feeds},
  Author                   = {Burnap, P. and Rana, O. and Pauran, N. and Bowen, P.},
  Booktitle                = {Complex, Intelligent and Software Intensive Systems (CISIS), 2014 Eighth International Conference on},
  Year                     = {2014},
  Month                    = {July},
  Pages                    = {608-613},

  Abstract                 = {Risk management has become an important concern over recent years and understanding how risk models could be developed based on the availability of real time (streaming) data has become a challenge. As the volume and velocity of event data (from news media, for instance) continues to grow, we investigate how such data can be used to inform the development of dynamic risk models. A Bayesian Belief Network based approach is adopted in this work, which is able to make use of priors derived from a variety of different news sources (based on data available in RSS feeds).},
  Doi                      = {10.1109/CISIS.2014.87},
  File                     = {Published version:Burnap-2014-p608-613.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Bayes methods;belief networks;data handling;electronic publishing;risk management;Bayesian belief network based approach;RSS feeds;disruptive event sensing;dynamic risk model;event data velocity;event data volume;news media;real time streaming data;real-time probabilistic risk assessment;risk management;streamed news feeds;Bayes methods;Computational modeling;Data models;Feeds;Media;Meteorology;Roads;dynamic data based modelling;risk modelling;streaming data and event analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6915582&isnumber=6915447},
  Zone                     = {-}
}

@InProceedings{Cardenas-2011-p355-366,
  Title                    = {Attacks Against Process Control Systems: Risk Assessment, Detection, and Response},
  Author                   = {C\'{a}rdenas, Alvaro A. and Amin, Saurabh and Lin, Zong-Syun and Huang, Yu-Lun and Huang, Chi-Yen and Sastry, Shankar},
  Booktitle                = {Proceedings of the 6th ACM Symposium on Information, Computer and Communications Security},
  Year                     = {2011},

  Address                  = {New York, NY, USA},
  Pages                    = {355--366},
  Publisher                = {ACM},
  Series                   = {ASIACCS '11},

  Acmid                    = {1966959},
  Doi                      = {10.1145/1966913.1966959},
  File                     = {Published version:Cardenas-2011-p355-366.pdf:PDF},
  Impactfactor             = {-},
  ISBN                     = {978-1-4503-0564-8},
  Keywords                 = {IDS, SCADA, control systems, critical infrastructure protection, cyber-physical systems, security},
  Level                    = {-},
  Location                 = {Hong Kong, China},
  Numpages                 = {12},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://doi.acm.org/10.1145/1966913.1966959},
  Zone                     = {-}
}

@InProceedings{Cammarota-2013-p1-4,
  Title                    = {Quantitative measurement of industrial risks in distribution networks},
  Author                   = {Cammarota, A. and De Giovanni, N. and Cudemo, M. and Noferi, G.L.},
  Booktitle                = {Electricity Distribution (CIRED 2013), 22nd International Conference and Exhibition on},
  Year                     = {2013},
  Pages                    = {1--4},

  Doi                      = {10.1049/cp.2013.0806},
  File                     = {Published version:Cammarota-2013-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6683409},
  Zone                     = {-}
}

@Article{Canfield-1970-p13-16,
  Title                    = {A Bayesian Approach to Reliability Estimation Using a Loss Function},
  Author                   = {Canfield, R.V.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1970},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {13-16},
  Volume                   = {R-19},

  Abstract                 = {A Bayesian estimate of reliability for the exponential case is developed which utilizes the basic notion of loss in estimation theory. Since the loss associated with overestimation is usually greater than the loss associated with underestimation of reliability, the loss function can be a useful tool. The loss function and prior distribution of reliability presented are sufficiently flexible to be compatible with many situations in which reliability estimates are require. When no prior information is at hand and a symmetric loss is used, the resulting estimate is seen to be the minimum variance unbiased estimate of reliability. This agreement gives some credibility to the precision of the estimation approach.},
  Doi                      = {10.1109/TR.1970.5216372},
  File                     = {Published version:Canfield-1970-p13-16.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Bayesian methods;Decision theory;Equations;Estimation theory;History;Life testing;Redundancy;Reliability engineering;Reliability theory;State estimation},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5216372&isnumber=5216365},
  Zone                     = {2Q}
}

@Article{Cardin-2015-p1-1,
  Title                    = {Training Design and Management of Flexible Engineering Systems: An Empirical Study Using Simulation Games},
  Author                   = {Cardin, M.-A. and Yixin, J. and Yue, H.K.-. and Haidong, F.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},
  Number                   = {99},
  Pages                    = {1-1},
  Volume                   = {PP},

  Abstract                 = {This paper presents the results of an empirical study of training procedures enabling flexibility in the design and management of large-scale engineering systems. The work relies on the development and use of a simulation game environment to study decision-making dynamics under different treatment conditions. Evaluation of short-term, long-term, and in-game training is completed to assess the main and interaction effects on quantitative lifecycle performance indicators, and qualitative user impressions. Forty-six graduate engineering students participated in controlled experiments where they worked on the design and management of a flexible emergency medical services system. Results show that in-game training produces a statistically significant improvement on lifecycle performance score, while long-term flexibility training significantly reduces decision-making time. In-game training improves process satisfaction. Both short-term and in-game training lead to improved satisfaction with the results, and contribute to improved anticipated quality of the results. Correlation studies suggest that participants taking more time for decision-making may improve lifecycle performance scores. Lifecycle score improvement also increases as satisfaction with the process and anticipated quality of results increase. Experimental results show that different training procedures produce different effects on design and management decision-making for flexible engineering systems operating under uncertainty. They provide further insights to support the development and evaluation of novel training approaches useful for systems engineering practice and education.},
  Doi                      = {10.1109/TSMC.2015.2392072},
  File                     = {Published version:Cardin-2015-p1-1.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Decision making;Games;Mathematical model;Modeling;System analysis and design;Training;Uncertainty;Computer simulation;decision making;large-scale systems;risk analysis;systems analysis and design;systems engineering education},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7042319&isnumber=6376248},
  Zone                     = {-}
}

@Article{Carin-2008-p20-26,
  Title                    = {Cybersecurity Strategies: The QuERIES Methodology},
  Author                   = {Carin, L. and Cybenko, G. and Hughes, J.},
  Journal                  = {Computer},
  Year                     = {2008},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {20-26},
  Volume                   = {41},

  Abstract                 = {QuERIES offers a novel multidisciplinary approach to quantifying risk associated with security technologies resulting in investment-efficient cybersecurity strategies. R esearchers can use the QuERIES methodology to rigorously determine, for the first time, appropriate investment levels and strategies for the protection of intellectual property in complex systems. As a result, it can have a significant and immediate impact on the protection of critical IP, including weapons systems and chip designs, complex computer software, and databases containing personal and financial information. In this paper, initial testing of QuERIES in small-scale, realistic scenarios, were performed with results that suggest the methodology can significantly improve risk assessments in complex systems under attack by rational and capable adversaries. Such systems include software, hardware, and data critical to national security and industrial competitiveness. Consequently, it is believed that QuERIES has wide applicability within both the DoD and private sectors.},
  Doi                      = {10.1109/MC.2008.295},
  File                     = {Published version:Carin-2008-p20-26.pdf:PDF},
  Impactfactor             = {1.675},
  ISSN                     = {0018-9162},
  Keywords                 = {IP networks;industrial property;national security;risk management;security of data;telecommunication security;DoD sectors;QuERIES;chip designs;complex computer software;critical IP protection;cybersecurity;databases;financial information;industrial competitiveness;intellectual property;investment efficient strategies;national security;personal information;private sectors;quantitative risk evaluation;risk assessments;weapons systems;Chip scale packaging;Computer security;Databases;Intellectual property;Investments;Performance evaluation;Protection;Software;System testing;Weapons;IP protection;cybersecurity;game theory;risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4597131&isnumber=4597119},
  Zone                     = {2Q}
}

@Article{Carrasco-2009-p207-220,
  Title                    = {An ROBDD-Based Combinatorial Method for the Evaluation of Yield of Defect-Tolerant Systems-on-Chip},
  Author                   = {Carrasco, J.A. and Sune, V.},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {207-220},
  Volume                   = {17},

  Abstract                 = {In this paper, we develop a combinatorial method for the evaluation of the functional yield of defect-tolerant systems-on-chip (SoC). The method assumes that random manufacturing defects are produced according to a model in which defects cause the failure of given components of the system following a distribution common to all defects. The distribution of the number of defects is arbitrary. The yield is obtained by conditioning on the number of defects that result in the failure of some component and performing recursive computations over a reduced ordered binary decision diagram (ROBDD) representation of the fault-tree function of the system. The method has excellent error control. Numerical experiments seem to indicate that the method is efficient and, with some exceptions, allows the analysis with affordable computational resources of systems with very large numbers of components.},
  Doi                      = {10.1109/TVLSI.2008.2004479},
  File                     = {Published version:Carrasco-2009-p207-220.pdf:PDF},
  Impactfactor             = {1.218},
  ISSN                     = {1063-8210},
  Keywords                 = {binary decision diagrams;reduced order systems;system-on-chip;ROBDD;combinatorial method;defect-tolerant systems-on-chip;error control;fault-tree function;random manufacturing defects;reduced ordered binary decision diagram;yield;Combinatorial method;defect-tolerant systems-on-chip (SoC);manufacturing defects;reduced ordered binary decision diagram (ROBDD);yield},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4745812&isnumber=4749486},
  Zone                     = {3Q}
}

@Article{Castro-2013-p930-936,
  Title                    = {A Wireless Robot for Networked Laparoscopy},
  Author                   = {Castro, C.A and Alqassis, A and Smith, S. and Ketterl, T. and Yu Sun and Ross, S. and Rosemurgy, A and Savage, P.P. and Gitlin, R.D.},
  Journal                  = {Biomedical Engineering, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {930-936},
  Volume                   = {60},

  Doi                      = {10.1109/TBME.2012.2232926},
  File                     = {Published version:Castro-2013-p930-936.pdf:PDF},
  Impactfactor             = {2.348},
  ISSN                     = {0018-9294},
  Keywords                 = {biomedical communication;biomedical imaging;image sensors;man-machine systems;medical computing;medical robotics;surgery;user interfaces;video cameras;wireless channels;LESS procedures;Miniature Anchored Robotic Videoscope for Expedited Laparoscopy;abdominal cavity view;digital zoom;full hemisphere field of view;independent wireless control;inexpensive in vivo wireless camera module;laparoscope image quality;laparoscope operation;laparoscopic endoscopic single-site method;light source cabling;master control module;minimal invasive abdominal surgery;multiple CM system;multiple MARVEL CM;multiwavelength illumination control system;near zero latency video wireless communications link;networked laparoscopy;power cabling;semiautonomous wireless control;semiautonomous wireless networked robotic cyberphysical devices;state-of-the-art laparoscopes;surgical instruments interference;surgical-tool bottleneck experiences;trocar port occupation;video cabling;wireless MARVEL;wireless adjustable camera focus;wireless body area networks;wireless control pan camera platform;wireless control tilt camera platform;wireless human-machine interface;wireless robots;Cameras;In vivo;Lenses;Light emitting diodes;Solid modeling;Wireless communication;Wireless sensor networks;In vivo wireless networking;minimally invasive surgery (MIS);robotic videoscope;Abdomen;Animals;Equipment Design;Humans;Laparoscopy;Models, Theoretical;Phantoms, Imaging;Robotics;Surgery, Computer-Assisted;Swine;Video Recording;Wireless Technology},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.04},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=622912&isnumber=13557},
  Zone                     = {3Q}
}

@Article{Ch-2006-p189-198,
  Title                    = {An extended model for optimal burn-in procedures},
  Author                   = {Ji Hwan Cha},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {189-198},
  Volume                   = {55},

  Abstract                 = {In this paper, the problem of determining optimal burn-in time is considered under the general failure model. There are two types of failure in the general failure model. One is Type I failure (minor failure) which can be removed by a minimal repair, and the other is Type II failure (catastrophic failure) which can be removed only by a complete repair. In the researches on optimal burn-in, the assumption of a bathtub shaped failure rate function is commonly adopted. In this paper, upper bounds for optimal burn-in times are obtained under a more general assumption on the shape of the failure rate function, which includes the bathtub shaped failure rate function as a special case.},
  Doi                      = {10.1109/TR.2006.874921},
  File                     = {Published version:Ch-2006-p189-198.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {failure analysis;maintenance engineering;Type I failure;Type II failure;bathtub shaped failure rate function;catastrophic failure;extended model;minor failure;optimal burn-in procedure;repairing;upper bound;Cost function;Distribution functions;Helium;Performance evaluation;Probability density function;Random variables;Shape;System performance;Time measurement;Upper bound;Bathtub-shaped failure rate function;complete repair;eventually increasing failure rate function;general failure model;minimal repair;optimal burn-in;upper bound},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Function 表示函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1638403&isnumber=34350},
  Zone                     = {2Q}
}

@Article{Chan-2014-p1-1,
  Title                    = {Quick Response Healthcare Apparel Supply Chains: Value of RFID and Coordination},
  Author                   = {Chan, H.-L. and Choi, T.-M. and Hui, C.-L. and Ng, S.-F.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},
  Number                   = {99},
  Pages                    = {1-1},
  Volume                   = {PP},

  Abstract                 = {Radio frequency identification (RFID) is an important tool for enhancing the performance of inventory management. Motivated by our real-world observations in local hospitals, we study the case where a {hospital,} which is using a bar-coding system, considers switching to an RFID system because the RFID technology can potentially enhance the hospital's inventory management under the quick response {system} (QRS). In order to examine the value of RFID, we develop a formal analytical Bayesian model for the information updating process. We derive the expected value of information of the RFID system, and reveal the conditions in which the RFID system outperforms the bar-coding system. In addition, we evaluate the impacts of the QRS toward the expected profit and level of risk of the hospital, the supplier, and the whole supply chain (SC) system. We further propose two policies to help achieve SC coordination. Numerical analyses are reported and important insights are generated.},
  Doi                      = {10.1109/TSMC.2014.2371891},
  File                     = {Published version:Chan-2014-p1-1.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Bayes methods;Contracts;Hospitals;Inventory management;Organizations;Radiofrequency identification;Quick response system (QRS);SC systems;radio frequency identification (RFID) system;supply chain (SC) management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6979266&isnumber=6376248},
  Zone                     = {-}
}

@Article{Chan-2011-p492-500,
  Title                    = {Risk-Based Assessment of Financial Losses Due to Voltage Sag},
  Author                   = {Jhan Yhee Chan and Milanovic, J.V. and Delahunty, A.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {492-500},
  Volume                   = {26},

  Abstract                 = {This paper investigates the factors that influence the outcome of financial-loss analysis in voltage sag studies. Focusing on industrial processes, this paper considers the effects of voltage sag profile estimation, plant-load profile modelling, and process cycle modelling on the financial risk associated with process failure. Based on voltage sag monitoring records, sag profile is probabilistically modeled for the estimation of future sag characteristic. Variations in financial risk due to varying process activities in different stages of a process (process cycle) and load profile are also investigated. The effects of the individual factors are analyzed through Monte Carlo simulation. The stochastic net present value method is introduced as a financial analysis tool.},
  Doi                      = {10.1109/TPWRD.2009.2037426},
  File                     = {Published version:Chan-2011-p492-500.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {Monte Carlo methods;financial management;power supply quality;probability;risk management;stochastic processes;Monte Carlo simulation;financial loss analysis;financial risk;industrial processes;plant-load profile modelling;process cycle modelling;process failure;risk-based assessment;stochastic net present value method;voltage sag monitoring records;voltage sag profile estimation;Industrial plants;Load modeling;Monitoring;Power quality;Probabilistic logic;Sensitivity;Voltage fluctuations;Financial losses;net present value (NPV);probability;risk analysis;stochastic;voltage sag},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5738431&isnumber=5738430},
  Zone                     = {3Q}
}

@Article{Chan-2009-p2405-2414,
  Title                    = {Generic Failure-Risk Assessment of Industrial Processes due to Voltage Sags},
  Author                   = {Jhan Yhee Chan and Milanovic?, J.V. and Delahunty, A.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {2405-2414},
  Volume                   = {24},

  Abstract                 = {This paper proposes methodologies for modeling and assessment of equipment and industrial processes susceptibility to voltage sags and short interruptions. Voltage sag severity indices are proposed that translate physical disturbance events into expected equipment behavior, allowing fast identification of possible process failure. Based on these indices, probabilistic and fuzzy-based methodologies are developed to provide realistic equipment representation and to incorporate flexibility for customized process modeling. Extensive simulations showed that the proposed models and methodologies yield greater accuracy in assessments compared to conventional approaches.},
  Doi                      = {10.1109/TPWRD.2009.2014493},
  File                     = {Published version:Chan-2009-p2405-2414.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {failure analysis;fuzzy set theory;industrial power systems;power supply quality;power system faults;probability;risk management;customized process modeling;equipment susceptibility;fuzzy-based methodologies;generic failure-risk assessment;industrial processes;physical disturbance event;probabilistic methodologies;voltage sag;Equipment susceptibility;failure-risk assessment;industrial process susceptibility;power quality (PQ);voltage sag severity indices},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4914788&isnumber=5235723},
  Zone                     = {3Q}
}

@Article{Chan-1968-p199-201,
  Title                    = {A Generalized Reliability Function for Systems of Parallel Components},
  Author                   = {Chan, Wah-Chun},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1968},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {199-201},
  Volume                   = {R-17},

  Abstract                 = {In systems of parallel components, the system reliability function Rp(t) is usually defined as the probability that not all the parallel components fail in a time interval t, given that all the components are operating at the beginning of the interval. This definition implies that if there is one component which operates throughout the whole interval in question, then the system reliability is perfect. Consider the system S which always requires M > 1 components to do its job. It is obvious that the system is not reliable if there are only k, 1 ?? k < M, components working in the time interval t. The conventional reliability function Rp(t) is then insufficient for studying the reliability of the system S. A generalized reliability function Rr,n(t) is presented in this paper, and it is shown that the conventional reliability function Rp(t) is a special case of the generalized reliability function Rr,n(t). The practical application of this generalized reliability function is also discussed.},
  Doi                      = {10.1109/TR.1968.5216946},
  File                     = {Published version:Chan-1968-p199-201.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Electric breakdown;Job design;Manufacturing;Redundancy;Reliability theory},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5216946&isnumber=5216940},
  Zone                     = {2Q}
}

@InProceedings{Changming-2011-p4084-4087,
  Title                    = {Multi-objective decision and risk assessment techniques for reservoir operation},
  Author                   = {Ji Changming and Pei Zheyi and Zhang Yanke and Zhan JinYan and Zhao Bikui},
  Booktitle                = {Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC), 2011 2nd International Conference on},
  Year                     = {2011},
  Pages                    = {4084--4087},

  Doi                      = {10.1109/AIMSEC.2011.6009926},
  File                     = {Published version:Changming-2011-p4084-4087.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6009926},
  Zone                     = {-}
}

@InProceedings{Chaoyang-2010-p959-962,
  Title                    = {Research on the fault diagnosis of dual-redundancy BLDC motor},
  Author                   = {Fu Chaoyang and Liu Jinglin and Chang Weiwei and Zhao Xiaopeng},
  Booktitle                = {Electrical Machines and Systems (ICEMS), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {Oct},
  Pages                    = {959-962},

  Abstract                 = {In order to improve the reliability of the system, a dual-redundancy high-voltage brushless DC motor based on 270V is designed. Methods of motor fault detection and diagnosis are studied. The fault signal is analyzed by Fourier transform. For the Fourier transform, a fault detection using wavelet transform method is proposed. The current is determined to the fault detection signal based on the motor fault tree. The coifS is selected as the wavelet basis function. Through the analysis of motor failures, the characteristics of the winding open circuit, winding short circuit, audion short circuit, audion open circuit, a phase with Hall for high and low are obtained by the coifS wavelet function. The fault eigenvectors are obtained by the layer 2 decomposition coefficients. Based on the characteristics, the wavelet neural network is selected. Multiple eigenvectors are collected by the wavelet transform. Winding short circuit and open circuit are research objects. The fault diagnosis model is established based on the BP neural network. The results showed that the two models can accurately identify the fault.},
  File                     = {Published version:Chaoyang-2010-p959-962.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Fourier transforms;backpropagation;brushless DC motors;eigenvalues and eigenfunctions;electric machine analysis computing;fault location;fault trees;machine windings;neural nets;redundancy;wavelet transforms;BP neural network;Fourier transform;audion open circuit;audion short circuit;dual-redundancy BLDC motor;dual-redundancy high-voltage brushless DC motor;fault diagnosis;fault eigenvector;motor failure analysis;motor fault detection;motor fault tree;voltage 270 V;wavelet basis function;wavelet neural network;wavelet transform;winding open circuit;winding short circuit;Artificial neural networks;Brushless DC motors;Training;Wavelet analysis;Wavelet transforms},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5664000&isnumber=5662269},
  Zone                     = {-}
}

@Article{Chee-2014-p7026-7034,
  Title                    = {Multiply Constant-Weight Codes and the Reliability of Loop Physically Unclonable Functions},
  Author                   = {Yeow Meng Chee and Cherif, Z. and Danger, J.-L. and Guilley, S. and Han Mao Kiah and Jon-Lark Kim and Sole, P. and Xiande Zhang},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {7026-7034},
  Volume                   = {60},

  Abstract                 = {We introduce the class of multiply constant-weight codes to improve the reliability of certain physically unclonable function response, and extend classical coding methods to construct multiply constant-weight codes from known (q) -ary and constant-weight codes. We derive analogs of Johnson bounds and give constructions showing these bounds to be asymptotically tight up to a constant factor under certain conditions. We also examine the rates of multiply constant-weight codes and demonstrate that these rates are the same as those of constant-weight codes of corresponding parameters.},
  Doi                      = {10.1109/TIT.2014.2359207},
  File                     = {Published version:Chee-2014-p7026-7034.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {codes;telecommunication network reliability;(q)-ary codes;Johnson bounds;coding methods;loop physically unclonable function reliability improvement;multiply constant-weight codes;physically unclonable function response;Binary codes;Delay lines;Delays;Educational institutions;Manganese;Reliability;Systematics;Constant-weight codes;constant-weight codes;doubly constant-weight codes;multiply constant-weight codes;physically unclonable functions},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6912009&isnumber=6926896},
  Zone                     = {2Q}
}

@Article{Chelton-1991-p269-288,
  Title                    = {Chemical hazards in the semiconductor industry},
  Author                   = {Chelton, C.F. and Glowatz, M. and Mosovsky, J.A.},
  Journal                  = {Education, IEEE Transactions on},
  Year                     = {1991},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {269-288},
  Volume                   = {34},

  Abstract                 = {Chemical hazards in the semiconductor industry are reviewed with regard to occupational health and safety concerns. Exposure potentials are summarized for each step in the development and processing of semiconductor devices. The hazards associated with replacement substitutes for III-V compounds are also presented. Risk assessment of the hazards involved in these processes requires that redundant control technology be employed. Controls in semiconductor research and development are reviewed with emphasis on those processes and equipment most often involved in losses in the semiconductor industry. These include limiting orifices on compressed gas cylinders, a ventilated gas cabinet for storage of compressed gases, dedicated exhaust risers, exhaust flow sensors, multipoint gas monitoring systems, interlocked devices for exhaust and power failure, smoke and fire detection, and fire suppression},
  Doi                      = {10.1109/13.85086},
  File                     = {Published version:Chelton-1991-p269-288.pdf:PDF},
  Impactfactor             = {0.95},
  ISSN                     = {0018-9359},
  Keywords                 = {electronics industry;health hazards;chemical hazards;compressed gas cylinders;dedicated exhaust risers;exhaust failure;exhaust flow sensors;fire detection;fire suppression;multipoint gas monitoring systems;power failure;risk assessment;semiconductor industry;smoke detection;ventilated gas cabinet;Chemical hazards;Electronics industry;Fires;Gases;III-V semiconductor materials;Occupational health;Orifices;Research and development;Risk management;Semiconductor devices},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {C，半导体工业中的化学污染，不相关},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=85086&isnumber=2785},
  Zone                     = {4Q}
}

@Article{Cheminod-2013-p277-293,
  Title                    = {Review of Security Issues in Industrial Networks},
  Author                   = {Cheminod, M. and Durante, L. and Valenzano, A.},
  Journal                  = {Industrial Informatics, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {277-293},
  Volume                   = {9},

  Abstract                 = {Although awareness is constantly rising, that industrial computer networks (in a very broad sense) can be exposed to serious cyber threats, many people still think that the same countermeasures, developed to protect general-purpose computer networks, can be effectively adopted also in those situations where a physical system is managed/controlled through some distributed Information and Communication Technology (ICT) infrastructure. Unfortunately, this is not the case, as several examples of successful attacks carried out in the last decade, and more frequently in the very recent past, have dramatically shown. Experts in this area know very well that often the peculiarities of industrial networks prevent the adoption of classical approaches to their security and, in particular, of those popular solutions that are mainly based on a detect and patch philosophy. This paper is a contribution, from the security point of view, to the assessment of the current situation of a wide class of industrial distributed computing systems. In particular, the analysis presented in this paper takes into account the process of ensuring a satisfactory degree of security for a distributed industrial system, with respect to some key elements such as the system characteristics, the current state of the art of standardization and the adoption of suitable controls (countermeasures) that can help in lowering the security risks below a predefined, acceptable threshold.},
  Doi                      = {10.1109/TII.2012.2198666},
  File                     = {Published version:Cheminod-2013-p277-293.pdf:PDF},
  Impactfactor             = {3.381},
  ISSN                     = {1551-3203},
  Keywords                 = {computer network security;production engineering computing;risk management;Information and Communication Technology;cyber threat;detect-and-patch philosophy;distributed ICT infrastructure;general-purpose computer network protection;industrial computer network;industrial distributed computing system;security countermeasure;security issues;security risk;Availability;Guidelines;NIST;Process control;Risk management;Security;Servers;Industrial networks;information security;network security;risk assessment;security analysis and monitoring;security countermeasures},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6198335&isnumber=6387656},
  Zone                     = {2Q}
}

@Article{Cheminod-2013-p277-293a,
  Title                    = {Review of Security Issues in Industrial Networks},
  Author                   = {Cheminod, M. and Durante, L. and Valenzano, A.},
  Journal                  = {Industrial Informatics, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {277-293},
  Volume                   = {9},

  Abstract                 = {Although awareness is constantly rising, that industrial computer networks (in a very broad sense) can be exposed to serious cyber threats, many people still think that the same countermeasures, developed to protect general-purpose computer networks, can be effectively adopted also in those situations where a physical system is managed/controlled through some distributed Information and Communication Technology (ICT) infrastructure. Unfortunately, this is not the case, as several examples of successful attacks carried out in the last decade, and more frequently in the very recent past, have dramatically shown. Experts in this area know very well that often the peculiarities of industrial networks prevent the adoption of classical approaches to their security and, in particular, of those popular solutions that are mainly based on a detect and patch philosophy. This paper is a contribution, from the security point of view, to the assessment of the current situation of a wide class of industrial distributed computing systems. In particular, the analysis presented in this paper takes into account the process of ensuring a satisfactory degree of security for a distributed industrial system, with respect to some key elements such as the system characteristics, the current state of the art of standardization and the adoption of suitable controls (countermeasures) that can help in lowering the security risks below a predefined, acceptable threshold.},
  Doi                      = {10.1109/TII.2012.2198666},
  File                     = {Published version:Cheminod-2013-p277-293a.pdf:PDF},
  Impactfactor             = {3.381},
  ISSN                     = {1551-3203},
  Keywords                 = {computer network security;production engineering computing;risk management;Information and Communication Technology;cyber threat;detect-and-patch philosophy;distributed ICT infrastructure;general-purpose computer network protection;industrial computer network;industrial distributed computing system;security countermeasure;security issues;security risk;Availability;Guidelines;NIST;Process control;Risk management;Security;Servers;Industrial networks;information security;network security;risk assessment;security analysis and monitoring;security countermeasures},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6198335&isnumber=6387656},
  Zone                     = {2Q}
}

@InProceedings{Cheminod-2012-p247-265,
  Title                    = {System configuration check against security policies in industrial networks},
  Author                   = {Cheminod, M. and Durante, L. and Valenzano, A},
  Booktitle                = {Industrial Embedded Systems (SIES), 2012 7th IEEE International Symposium on},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {247-265},

  Doi                      = {10.1109/SIES.2012.6356591},
  File                     = {Published version:Cheminod-2012-p247-265.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer network management;computer network security;digital communication;embedded systems;production engineering computing;Stuxnet worm;awareness;cyber-security requirement;cyber-threat vulnerability;device configuration;digital communication network;distributed industrial automation system;embedded device security;industrial network;industrial system;networked embedded system;security capability;security policy;service configuration;system configuration check;Access control;Actuators;Embedded systems;IP networks;Protocols},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6356591&isnumber=6356563},
  Zone                     = {-}
}

@Article{Chen-2015-p22-35,
  Title                    = {Securing DV-Hop localization against wormhole attacks in wireless sensor networks },
  Author                   = {Honglong Chen and Wei Lou and Zhi Wang and Junfeng Wu and Zhibo Wang and Aihua Xia},
  Journal                  = {Pervasive and Mobile Computing },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {22 - 35},
  Volume                   = {16, Part A},

  Abstract                 = {Abstract Node localization becomes an important issue in the wireless sensor network as its wide applications in environment monitoring, emergency rescue and battlefield surveillance, etc. Basically, the DV-Hop localization scheme can work well with the assistance of beacon nodes that have the capability of self-positioning. However, if the network is invaded by a wormhole attack, the attacker can tunnel the packets via the wormhole link to severely disrupt the DV-Hop localization process. The distance-vector propagation phase during the DV-Hop localization can even aggravate the positioning error, compared to the localization schemes without wormhole attacks. In this paper, we analyze the impacts of wormhole attack on the DV-Hop localization scheme, based on which we propose a label-based DV-Hop secure localization scheme to defend against the wormhole attack. We further theoretically prove the correctness of the proposed scheme. Simulation results illustrate the effectiveness of the proposed label-based DV-Hop secure localization scheme. },
  Doi                      = {http://dx.doi.org/10.1016/j.pmcj.2014.01.007},
  File                     = {Published version:Chen-2015-p22-35.pdf:PDF},
  Impactfactor             = {1.629},
  ISSN                     = {1574-1192},
  Keywords                 = {DV-Hop localization},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1574119214000248},
  Zone                     = {3Q}
}

@Article{Chen-2013-p180-195,
  Title                    = {A hybrid \{ANFIS\} model for business failure prediction utilizing particle swarm optimization and subtractive clustering},
  Author                   = {Mu-Yen Chen},
  Journal                  = {Information Sciences},
  Year                     = {2013},
  Note                     = {Online Fuzzy Machine Learning and Data Mining },
  Number                   = {0},
  Pages                    = {180 - 195},
  Volume                   = {220},

  Abstract                 = {In recent years, newly-developed data mining and machine learning techniques have been applied to various fields to build intelligent information systems. However, few of these approaches offer online support or are able to flexibly adapt to large and complex financial datasets. Therefore, the present research adopts particle swarm optimization (PSO) techniques to obtain appropriate parameter settings for subtractive clustering (SC) and integrates the adaptive-network-based fuzzy inference system (ANFIS) model to construct a model for predicting business failures. Experiments were conducted based on an initial sample of 160 electronics companies listed on the Taiwan Stock Exchange Corporation (TSEC). Experimental results show that the proposed model is superior to other models, providing a lower mean absolute percentage error (MAPE) and root mean squared error (RMSE). The proposed one-order momentum method is able to learn quickly through one-pass training and provides high-accuracy short-term predictions, while the proposed two-order momentum provides high-accuracy long-term predictions from large financial datasets. Therefore, the proposed approach fulfills some important characteristics of the proposed model: the one-order momentum method is suitable for online learning and the two-order momentum method is suitable for incremental learning. Thus, the PS-ANFIS approach could provide better results in predicting potential financial distress. },
  Doi                      = {http://dx.doi.org/10.1016/j.ins.2011.09.013},
  File                     = {Published version:Chen-2013-p180-195.pdf:PDF},
  Impactfactor             = {3.643},
  ISSN                     = {0020-0255},
  Keywords                 = {Adaptive network-based fuzzy inference system (ANFIS)},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0020025511004701},
  Zone                     = {1Q}
}

@Article{Chen-2005-p823-834,
  Title                    = {Identifying high risk N-k contingencies for online security assessment},
  Author                   = {Qiming Chen and McCalley, J.D.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {May},
  Number                   = {2},
  Pages                    = {823-834},
  Volume                   = {20},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {It is impractical to analyze all of the possible contingencies in a large-scale interconnected power network. Therefore, a standard approach is to analyze only a subset of the contingencies. The normal method of selecting this subset is via use of the so-called N-1 rule. This paper goes a step further by proposing a new method of forming a contingency list, based on substation configuration obtained from topology processing data and probability analysis of protection system failures. This method is particularly suited for online security assessment. Protection system failures assessed include stuck breakers and failure to operate. The method is described via application to a single substation, verified using the IEEE-RTS96, and illustrated using topology data from a large utilities EMS.},
  Doi                      = {10.1109/TPWRS.2005.846065},
  File                     = {Published version:Chen-2005-p823-834.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {fault trees;power engineering computing;power system interconnection;power system protection;power system security;probability;substations;IEEE-RTS96;data transmission;event tree;high risk N-k contingencies;high-order contingency;large-scale interconnected power network;online security assessment;probability analysis;protection system failures;stuck breakers;substation configuration;Circuit faults;Data security;Integrated circuit interconnections;Power system faults;Power system modeling;Power system protection;Power system security;Risk analysis;Substation protection;Topology;Contingency identification;event tree;functional group;high-order contingency;power system;probability order;rare event approximation;security assessment;topology data;transmission},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，也是用的这个 IEEE-RTS96，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1425578&isnumber=30784},
  Zone                     = {2Q}
}

@Article{Chen-2005-p823-834a,
  Title                    = {Identifying high risk N-k contingencies for online security assessment},
  Author                   = {Qiming Chen and McCalley, J.D.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {May},
  Number                   = {2},
  Pages                    = {823-834},
  Volume                   = {20},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {It is impractical to analyze all of the possible contingencies in a large-scale interconnected power network. Therefore, a standard approach is to analyze only a subset of the contingencies. The normal method of selecting this subset is via use of the so-called N-1 rule. This paper goes a step further by proposing a new method of forming a contingency list, based on substation configuration obtained from topology processing data and probability analysis of protection system failures. This method is particularly suited for online security assessment. Protection system failures assessed include stuck breakers and failure to operate. The method is described via application to a single substation, verified using the IEEE-RTS96, and illustrated using topology data from a large utilities EMS.},
  Doi                      = {10.1109/TPWRS.2005.846065},
  File                     = {Published version:Chen-2005-p823-834.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {fault trees;power engineering computing;power system interconnection;power system protection;power system security;probability;substations;IEEE-RTS96;data transmission;event tree;high risk N-k contingencies;high-order contingency;large-scale interconnected power network;online security assessment;probability analysis;protection system failures;stuck breakers;substation configuration;Circuit faults;Data security;Integrated circuit interconnections;Power system faults;Power system modeling;Power system protection;Power system security;Risk analysis;Substation protection;Topology;Contingency identification;event tree;functional group;high-order contingency;power system;probability order;rare event approximation;security assessment;topology data;transmission},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，也是用的这个 IEEE-RTS96，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1425578&isnumber=30784},
  Zone                     = {2Q}
}

@Article{Chen-2010-p2-3,
  Title                    = {Stuxnet, the real start of cyber warfare? [Editor's Note]},
  Author                   = {Chen, T.M.},
  Journal                  = {Network, IEEE},
  Year                     = {2010},

  Month                    = {November},
  Number                   = {6},
  Pages                    = {2-3},
  Volume                   = {24},

  Abstract                 = {In recent years, the press has been banging the drums about the prospects of cyber warfare between nations. It makes a good story. In the imagined scenario, national governments are gathering and training elite teams of computer hackers prepared to infiltrate or take down the computer network of an enemy nation. Every nation has the weapons ?? computers and software ?? to be a major threat without having to be a superpower. Any David can take on a Goliath, and in fact the most industrialized countries might be the most network- dependent and vulnerable.},
  Doi                      = {10.1109/MNET.2010.5634434},
  File                     = {Published version:Chen-2010-p2-3.pdf:PDF},
  Impactfactor             = {2.853},
  ISSN                     = {0890-8044},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5634434&isnumber=5634431},
  Zone                     = {2Q}
}

@Article{Chen-2014-p5972-5983,
  Title                    = {Efficient classification using parallel and scalable compressed model and its application on intrusion detection},
  Author                   = {Tieming Chen and Xu Zhang and Shichao Jin and Okhee Kim},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2014},
  Number                   = {13},
  Pages                    = {5972 - 5983},
  Volume                   = {41},

  Abstract                 = {Abstract In order to achieve high efficiency of classification in intrusion detection, a compressed model is proposed in this paper which combines horizontal compression with vertical compression. OneR is utilized as horizontal compression for attribute reduction, and affinity propagation is employed as vertical compression to select small representative exemplars from large training data. As to be able to computationally compress the larger volume of training data with scalability, MapReduce based parallelization approach is then implemented and evaluated for each step of the model compression process abovementioned, on which common but efficient classification methods can be directly used. Experimental application study on two publicly available datasets of intrusion detection, \{KDD99\} and CMDC2012, demonstrates that the classification using the compressed model proposed can effectively speed up the detection procedure at up to 184 times, most importantly at the cost of a minimal accuracy difference with less than 1% on average. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2014.04.009},
  File                     = {Published version:Chen-2014-p5972-5983.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Compressed model},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417414002048},
  Zone                     = {3Q}
}

@Article{Chen-2013-p1387-1394,
  Title                    = {Propagation modeling of active \{P2P\} worms based on ternary matrix },
  Author                   = {Ting Chen and Xiao-song Zhang and Hong-yuan Li and Dong Wang and Yue Wu},
  Journal                  = {Journal of Network and Computer Applications },
  Year                     = {2013},
  Number                   = {5},
  Pages                    = {1387 - 1394},
  Volume                   = {36},

  Abstract                 = {Propagation modeling of worms has become an attractive research field in recent years since it facilitates worm prediction, detection, analysis and prevention etc. In this work, we propose a novel ternary-matrix-based model to describe the propagation trend of active \{P2P\} worms. Compared to existing logic-matrix-based models, our model takes the time lags into consideration by introducing new states and special matrix operations. Our model is easy of derivation and deployment because it confines derivation process to pure matrix operations. Moreover, two other advantages of our model are fully explored. One is expressiveness: (1) practical \{P2P\} topology can be modeled in the topology matrix; (2) the state of any node can be identified at any time; (3) and the attack path of any node can be backtracked in linear time. Flexibility is the other merit: our model can adapt to different scenes by changing the related parameters, particularly our model is general for different kinds of time lags and \{P2P\} topologies. },
  Doi                      = {http://dx.doi.org/10.1016/j.jnca.2013.02.032},
  File                     = {Published version:Chen-2013-p1387-1394.pdf:PDF},
  Impactfactor             = {1.467},
  ISSN                     = {1084-8045},
  Keywords                 = {Active \{P2P\} worms},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1084804513000702},
  Zone                     = {3Q}
}

@Article{Chen-2009-p275-283,
  Title                    = {Design Methodology and Protection Strategy for ESD-CDM Robust Digital System Design in 90-nm and 130-nm Technologies},
  Author                   = {Tze Wee Chen and Ito, C. and Loh, W. and Wei Wang and Doddapaneni, K. and Mitra, S. and Dutton, R.W.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {275-283},
  Volume                   = {56},

  Abstract                 = {A design methodology and protection strategy for ESD charged-device-model (CDM) robust digital systems is presented using a scalable postbreakdown transistor macromodel for 90- and 130-nm technologies. The macromodel was implemented in a design tool to aid reliable chip design and used to predict function failure in three different system-on-chip design examples. Simulations agree well with failure analysis observations, verifying the validity of the macromodel. A ldquocorrect-by-constructionrdquo protection strategy for overcoming induced ESD-CDM events is also proposed. No ESD-CDM-related function failures are observed for product chips protected with this strategy.},
  Doi                      = {10.1109/TED.2008.2010586},
  File                     = {Published version:Chen-2009-p275-283.pdf:PDF},
  Impactfactor             = {2.062},
  ISSN                     = {0018-9383},
  Keywords                 = {digital integrated circuits;electrostatic discharge;integrated circuit design;microprocessor chips;system-on-chip;ESD charged-device-model;ESD-CDM robust digital system design;chip design;correct-by-construction protection strategy;design methodology;design tool;size 130 nm;size 90 nm;system-on-chip design;Circuit simulation;Design methodology;Digital systems;Electric breakdown;Electrostatic discharge;Indium tin oxide;Integrated circuit technology;Large scale integration;Protection;Robustness;Chip-level function failure prediction;ESD-charged device model (CDM);inductive coupling;postbreakdown transistor macromodel},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，摘要里面有这个表达 predict function failure，关键词里面有 Chip-level function failure prediction ，意味芯片级的功能失效预测},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4757246&isnumber=4757346},
  Zone                     = {2Q}
}

@InProceedings{Chen-2007-p78-85,
  Title                    = {Macro-Model for Post-Breakdown 90NM and 130NM Transistors and its Applications in Predicting Chip-Level Function Failure after ESD-CDM Events},
  Author                   = {Tze Wee Chen and Ito, C. and Loh, W. and Wei Wang and Mitra, S. and Dutton, R.W.},
  Booktitle                = {Reliability physics symposium, 2007. proceedings. 45th annual. ieee international},
  Year                     = {2007},
  Month                    = {April},
  Pages                    = {78-85},

  Doi                      = {10.1109/RELPHY.2007.369872},
  File                     = {Published version:Chen-2007-p78-85.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {failure analysis;integrated circuit design;semiconductor device breakdown;semiconductor device models;semiconductor device reliability;system-on-chip;transistors;130 nm;90 nm;ESD-CDM events;chip-level function failure;failure analysis;inductive coupling;oxide breakdown;post-breakdown transistors macromodel;system-on-chip design;Breakdown voltage;Circuit simulation;Coupling circuits;Degradation;Electric breakdown;Geometry;Integrated circuit modeling;Large scale integration;Solid modeling;Stress;ESD-CDM;chip-level function failure prediction;inductive coupling;post-breakdown transistor macro-model},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4227613&isnumber=4227592},
  Zone                     = {-}
}

@InProceedings{Chen-2007-p78-85a,
  Title                    = {Macro-Model for Post-Breakdown 90NM and 130NM Transistors and its Applications in Predicting Chip-Level Function Failure after ESD-CDM Events},
  Author                   = {Tze Wee Chen and Ito, C. and Loh, W. and Wei Wang and Mitra, S. and Dutton, R.W.},
  Booktitle                = {Reliability physics symposium, 2007. proceedings. 45th annual. ieee international},
  Year                     = {2007},
  Month                    = {April},
  Pages                    = {78-85},

  Abstract                 = {A post-breakdown transistor macro-model for 90nm and 130nm technologies is presented and experimentally verified. Oxide breakdown does not necessarily imply function failure. The location of breakdown within the circuit is also important. A simulation methodology implementing this macro-model is presented. This tool can be used to predict function failure for three different system-on-chip (SoC) design examples. Simulations agree well with failure analysis (FA) observations, verifying the validity of the macro-model},
  Doi                      = {10.1109/RELPHY.2007.369872},
  File                     = {Published version:Chen-2007-p78-85a.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {failure analysis;integrated circuit design;semiconductor device breakdown;semiconductor device models;semiconductor device reliability;system-on-chip;transistors;130 nm;90 nm;ESD-CDM events;chip-level function failure;failure analysis;inductive coupling;oxide breakdown;post-breakdown transistors macromodel;system-on-chip design;Breakdown voltage;Circuit simulation;Coupling circuits;Degradation;Electric breakdown;Geometry;Integrated circuit modeling;Large scale integration;Solid modeling;Stress;ESD-CDM;chip-level function failure prediction;inductive coupling;post-breakdown transistor macro-model},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，里面有 In particular, oxide breakdown does not necessarily imply function failure.},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4227613&isnumber=4227592},
  Zone                     = {-}
}

@Article{Cheng-1997-p1289-1307,
  Title                    = {Wireless intelligent ATM network and protocol design for future personal communication systems},
  Author                   = {Fang-Chen Cheng and Holtzman, J.M.},
  Journal                  = {Selected Areas in Communications, IEEE Journal on},
  Year                     = {1997},

  Month                    = {Sep},
  Number                   = {7},
  Pages                    = {1289-1307},
  Volume                   = {15},

  Doi                      = {10.1109/49.622912},
  File                     = {Published version:Cheng-1997-p1289-1307.pdf:PDF},
  Impactfactor             = {3.121},
  ISSN                     = {0733-8716},
  Keywords                 = {B-ISDN;BCH codes;asynchronous transfer mode;cellular radio;concatenated codes;convolutional codes;data communication;intelligent networks;interleaved codes;land mobile radio;multimedia communication;network interfaces;personal communication networks;protocols;subscriber loops;telecommunication control;telecommunication network routing;ATM backbone network;ATM cell relay;B-ISDN UNI;BCH code;IS-41 circuit-switching handoff;WIATM network;advanced intelligent network;air interface;basic call state model;cellular/PCS network architecture;concatenated coding;convolutional coding;hybrid concatenated error control;interleaving;low rate source codecs;multimedia service control;personal communication systems;protocol design;quality of service guarantee;routing information;wireless access network;wireless customer premises equipment;wireless customer premises network;wireless intelligent ATM;wireless resource management;Access protocols;B-ISDN;Concatenated codes;Intelligent networks;Intserv networks;Quality of service;Radio control;Relays;Wireless application protocol;Wireless networks},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.04},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=622912&isnumber=13557},
  Zone                     = {2Q}
}

@Article{Cheng-2014-p1372-1381,
  Title                    = {Application of fault tree approach for technical assessment of small-sized biogas systems in Nepal },
  Author                   = {Shikun Cheng and Zifu Li and Heinz-Peter Mang and Kalidas Neupane and Marc Wauthelet and Elisabeth-Maria Huba},
  Journal                  = {Applied Energy },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {1372 - 1381},
  Volume                   = {113},

  Abstract                 = {Abstract Biogas technology is becoming increasingly popular throughout the world, particularly in countries where governments promote domestic biogas systems. Despite its popularity, problems exist in biogas technology, such as low-quality construction, leakage of pipelines, low biogas production, and lack of maintenance. These problems should be identified and analyzed for effective implementation and efficient operation of small-sized biogas systems (SBS). This research applies the fault tree approach (FTA) to identify failures, and evaluate their effects on the operation of \{SBS\} from a technical point of view, based on a nationwide field study in Nepal. Ninety-four sets of \{SBS\} were selected and sampled in targeted areas. Five subsystems of \{SBS\} were defined, including structural components, biogas utilization equipment, piping system, biogas production, and effluent disposal system. Based on the statistical analysis of the 94 targeted cases, the fault probabilities of the five subsystems are 0.67, 0.48, 0.73, 0.26, and 0.64, respectively. The weights of five subsystems are determined by Delphi method and fault probability of \{SBS\} is calculated. The results show that faults on piping systems happen most frequently, and that proper disposal and reuse of bio-slurry are often neglected. Regardless of the social and economic benefits of SBS, implementation scenarios of \{SBS\} throughout Nepal are evaluated under a technical perspective, as follows: (1) the operational status of \{SBS\} is not optimal. (2) Based on criteria for failures, well-operation ratio is about 53% in practice. (3) Skilled masons are prerequisites for efficient functioning of SBS. (4) Maintenance plays a key role for efficient functioning. The study results prove that \{FTA\} is mostly suitable for \{SBS\} evaluation and is an effective analysis tool for technical evaluation in the field of biogas technology. },
  Doi                      = {http://dx.doi.org/10.1016/j.apenergy.2013.08.052},
  Impactfactor             = {4.781},
  ISSN                     = {0306-2619},
  Keywords                 = {Small-sized biogas system},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S030626191300696X},
  Zone                     = {2Q}
}

@InProceedings{Cheng-2008-p532-538,
  Title                    = {Network Dynamic Risk Assessment Based on the Threat Stream Analysis},
  Author                   = {Wencong Cheng and Xishan Xu and Yan Jia and Peng Zou},
  Booktitle                = {Web-Age Information Management, 2008. WAIM '08. The Ninth International Conference on},
  Year                     = {2008},
  Month                    = {July},
  Pages                    = {532-538},

  Abstract                 = {This paper considers the problem of the dynamic risk assessment for the network based on the threat stream analysis. We analyze the general approach to do the network dynamic risk assessment. A stream based cube model is built to analyze the characteristics of the threat stream. Then combining with the research about the description and analysis of the threat effect, we propose the architecture of the network dynamic risk assessment.},
  Doi                      = {10.1109/WAIM.2008.65},
  File                     = {Published version:Cheng-2008-p532-538.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer networks;data analysis;risk management;security of data;telecommunication security;cube model;network dynamic risk assessment problem;threat stream analysis;Computer networks;Data analysis;Data security;Event detection;Frequency estimation;Information analysis;Information security;Protection;Risk analysis;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4597062&isnumber=4596967},
  Zone                     = {-}
}

@InProceedings{Cheng-2012-p1106-1109,
  Title                    = {The Research on Dynamic Risk Assessment Based on Hidden Markov Models},
  Author                   = {Xiaorong Cheng and Yangdan Ni},
  Booktitle                = {Computer Science Service System (CSSS), 2012 International Conference on},
  Year                     = {2012},
  Month                    = {Aug},
  Pages                    = {1106-1109},

  Abstract                 = {In order to effectively finish the dynamic risk assessment of the electricity system, this paper will divide each attack into three distinct phases, The difficulty of attack is assessed by percent of each attack time of distinct stages take up in the total attack time to describe the attack difficulty in order to determine the status of the assets transition matrix, realising the dynamic nature of risk assessment. The real-time dynamic risk assessment methods based on Hidden Markov Model HMM has a strong adaptability and scalability, it can be effectively applied on the network, host, system, service level of risk assessment. This paper designs and implements the dynamic risk assessment examples power system, and then demonstrateds the dynamic assessment model.},
  Doi                      = {10.1109/CSSS.2012.280},
  File                     = {Published version:Cheng-2012-p1106-1109.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {hidden Markov models;matrix algebra;power engineering computing;power system security;risk management;security of data;adaptability;assets transition matrix;electricity system;hidden Markov model;power system;real-time dynamic risk assessment method;risk assessment service level;scalability;total attack time;Heuristic algorithms;Hidden Markov models;Markov processes;Power system dynamics;Risk management;Security;Vectors;Hidden Markov;data integration;dynamic risk assessment;neural networks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6394518&isnumber=6394246},
  Zone                     = {-}
}

@InProceedings{Cheshmikha-2014-p1-6,
  Title                    = {Accelerating accurate fault tree analysis using HW/SW co-design},
  Author                   = {Cheshmikhani, E. and Zarandi, H.R. and Aliee, H.},
  Booktitle                = {Reliability and Maintainability Symposium (RAMS), 2014 Annual},
  Year                     = {2014},
  Month                    = {Jan},
  Pages                    = {1-6},

  Abstract                 = {Fault tree analysis is a widespread-use method for measuring dependability parameters such as reliability and safety. Fault tree analysis is more popular than Markov modeling and Reliability Block Diagram (RBD), which need good background of mathematical equations and impose complex analysis for large systems. In this paper, accurate analysis of the fault tree is accelerated using a hardware/ software (HW/SW) co-design. This method converts a fault tree into a stochastic-based tree with logic gates and long bit streams as its inputs. This kind of tree has an exponential function and conversion of probability to a stochastic bit stream, which uses a random number generator. Moreover, based on probabilistic analysis of dynamic gate i.e., Cold Spare, an accurate model of its stochastic gate is developed. So, implementing it on a HW/SW co-design platform, speeds up the analysis as well as accuracy. The experimental results show that implementing this method on HW/SW co-design is 14 times faster than CPU implementation and 21 times more accurate than previous methods.},
  Doi                      = {10.1109/RAMS.2014.6798490},
  File                     = {Published version:Cheshmikha-2014-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {fault trees;field programmable gate arrays;hardware-software codesign;logic gates;probability;random number generation;reliability theory;Cold Spare;HW/SW codesign;exponential function;fault tree analysis;hardware /software codesign;logic gates;probability conversion;random number generator;reliability;safety;stochastic bit stream;stochastic gate;Fault trees;Hardware;Logic gates;Mathematical model;Reliability;Software;Stochastic processes;Co-design;Fault tree analysis;Field Programmable Gate Array (FPGA);Reliability;Stochastic logic},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6798490&isnumber=6798433},
  Zone                     = {-}
}

@Article{Chou-1998-p1009-1013,
  Title                    = {Evaluation of surveillance test interval from risk viewpoint},
  Author                   = {Chou, H.P. and Wang, Y.H. and Wei, Y.S. and Lin, T.J.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {Jun},
  Number                   = {3},
  Pages                    = {1009-1013},
  Volume                   = {45},

  Doi                      = {10.1109/23.682698},
  File                     = {Published version:Chou-1998-p1009-1013.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {fault trees;fission reactor accidents;fission reactor safety;fission reactor theory;nuclear power stations;surveillance;ATWS;Maanshan nuclear plant;PRA;anticipated transient without scram;common cause failure;core melt down frequencies;event tree;fault tree;human error;probabilistic risk assessment;reactor protection system;sequential test;staggered test;surveillance test interval;test induced transients;Fault trees;Inductors;Power generation;Power system modeling;Power system protection;Risk management;Sequential analysis;Surveillance;Testing;Transient analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=682698&isnumber=15001},
  Zone                     = {3Q}
}

@Article{Chou-1994-p1566-1575,
  Title                    = {Knowledge model based approach in recognition of on-line Chinese characters},
  Author                   = {Kuo-Sen Chou and Kuo-Chin Fan and Tzu-I.Fan and Chang-Keng Lin and Bor-Shenn Jeng},
  Journal                  = {Selected Areas in Communications, IEEE Journal on},
  Year                     = {1994},

  Month                    = {Dec},
  Number                   = {9},
  Pages                    = {1566-1575},
  Volume                   = {12},

  Doi                      = {10.1109/49.339925},
  File                     = {Published version:Chou-1994-p1566-1575.pdf:PDF},
  Impactfactor             = {3.121},
  ISSN                     = {0733-8716},
  Keywords                 = {image classification;image matching;knowledge based systems;optical character recognition;tree searching;OCR system;coarse classification;connected stroke Chinese characters;deviation modeling method;knowledge model based approach;model matching;online Chinese characters;radical recognition;recognition;regular writing characters;segment attribute;segment sequence;stroke-number deviations;stroke-order deviations;Character recognition;Computer science;Dynamic programming;Helium;Laboratories;Office automation;Optical character recognition software;User interfaces;Writing},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=339925&isnumber=7963},
  Zone                     = {2Q}
}

@InProceedings{Chung-2014-p410-413,
  Title                    = {Standardizations on IT risk analysis service in NGN},
  Author                   = {Heechang Chung and Sok Pal Cho and Yongseon Jang},
  Booktitle                = {Advanced Communication Technology (ICACT), 2014 16th International Conference on},
  Year                     = {2014},
  Month                    = {Feb},
  Pages                    = {410-413},

  Abstract                 = {Information technology (IT) risk analysis service is a service which is capable of identifying risk, assessing the risk, and then invoking process which can identify the proper actions which should be taken to reduce damage that could affect users or organizations subscribed to an Network. Provided that a risk situation exists, the risk analysis function performs the analysis and assessment of the risk event data with an algorithm which applies the most recent pattern according to procedures, and reports the analysis results and the proper complementary measures which, if invoked, will reduce risk.},
  Doi                      = {10.1109/ICACT.2014.6778992},
  File                     = {Published version:Chung-2014-p410-413.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data analysis;next generation networks;risk analysis;telecommunication network reliability;IT risk analysis service;NGN;information technology risk analysis service;risk event data analysis;risk event data assessment;risk identification;risk reduction;Educational institutions;Hardware;Next generation networking;Organizations;Risk analysis;Software;Standardization;IT risk analysis;Identifying risk;assessing risk;external risk;internal risk;mitigation risk},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6778992&isnumber=6778899},
  Zone                     = {-}
}

@Article{Chung-1971-p85-87,
  Title                    = {Generalized Reliability Function for Systems of Arbitrary Configurations},
  Author                   = {Chung, Who-Kee},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1971},

  Month                    = {May},
  Number                   = {2},
  Pages                    = {85-87},
  Volume                   = {R-20},

  Abstract                 = {In this note we consider a special system that always requires more than one elementary path between its input and output terminals to perform its job. Since the special system may consist of components that are not series-parallel connected, the generalized reliability function for systems of parallel components introduced by Chan is insufficient for studying the reliability of this class of systems. However, an even more generalized reliability function for the special system can be obtained by generalizing the path enumeration approach (path-tracing method). The conventional reliability function and the generalized reliability function of Chan are special cases of the one derived in this note.},
  Doi                      = {10.1109/TR.1971.5216091},
  File                     = {Published version:Chung-1971-p85-87.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Cost function;Fixtures;Logic;Marine vehicles;Monitoring;Performance evaluation;Proposals;Reliability engineering;Reliability theory;System testing},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5216091&isnumber=5216077},
  Zone                     = {2Q}
}

@Article{Clancey-1989-p9-23,
  Title                    = {Viewing knowledge bases as qualitative models},
  Author                   = {Clancey, W.J.},
  Journal                  = {IEEE Expert},
  Year                     = {1989},

  Month                    = {Summer},
  Number                   = {2},
  Pages                    = {9-23},
  Volume                   = {4},

  Doi                      = {10.1109/64.24918},
  File                     = {Published version:Clancey-1989-p9-23.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0885-9000},
  Keywords                 = {expert systems;medical diagnostic computing;artificial intelligence;diagnosis;expert systems;knowledge bases;knowledge representations;medical diagnosis;monitoring;qualitative models;reasoning;sequences of tasks;Artificial intelligence;Bayesian methods;Circuit simulation;Diagnostic expert systems;Electronic circuits;Expert systems;Knowledge representation;Mathematical model;Prototypes;Writing},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=24918&isnumber=946},
  Zone                     = {-}
}

@Article{Clinton-2013-p-,
  Title                    = {Annual Safety Performance Report},
  Author                   = {Andrew Clinton},
  Journal                  = {Annual Safety Performance Report 2001},
  Year                     = {2013},
  Volume                   = {2},

  File                     = {Clinton-2013-p-.pdf:Clinton-2013-p-.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.15},
  Url                      = {http://www.rssb.co.uk/Library/risk-analysis-and-safety-reporting/2014-07-aspr-2013-14-full-report.pdf},
  Zone                     = {-}
}

@Article{Codetta-Ra-2015-p13-24,
  Title                    = {Dynamic Bayesian Networks for Fault Detection, Identification, and Recovery in Autonomous Spacecraft},
  Author                   = {Codetta-Raiteri, D. and Portinale, L.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {13-24},
  Volume                   = {45},

  Abstract                 = {This paper describes how to exploit the modeling features and inference capabilities of dynamic Bayesian networks (DBN), in designing and implementing an innovative approach to fault detection, identification, and recovery (FDIR) for autonomous spacecrafts (e.g., a Mars rover). In particular, issues like partial observability, uncertain system evolution and system-environment interaction, as well as the prediction and mitigation of imminent failures can be naturally addressed by the proposed approach. The DBN framework can augment the modeling and analytical power of standard FDIR methodologies, while still being able to be integrated into the usual system modeling procedures (like, for instance, fault tree analysis). An FDIR cycle composed of the tasks of diagnosis (identification of the current state of the system), prognosis (identification of the future state under the current conditions), and recovery (selection of the best set of actions the autonomous system can perform, in order to avoid critical situations) is introduced and characterized through a DBN model. In particular, by considering the execution of recovery actions in response to either a current or a future abnormal situation, both reactive as well as preventive recovery can be addressed respectively. The proposed approach has been implemented in an on-board software architecture called Anomaly resolution and prognostic health management for autonomy (ARPHA), realized during the Verifim study funded by the European Space Agency and jointly performed with Thales/Alenia Italy. We report on some of the results obtained by performing a case study concerning the FDIR analysis of the power supply system of the ExoMars rover, by considering different anomalous and failure simulated scenarios; we conclude that ARPHA is able to properly detect and deal with the simulated problems.},
  Doi                      = {10.1109/TSMC.2014.2323212},
  File                     = {Published version:Codetta-Ra-2015-p13-24.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {aerospace computing;autonomous aerial vehicles;belief networks;fault diagnosis;software architecture;space vehicles;uncertain systems;ARPHA;Alenia;Anomaly resolution-and-prognostic health management for autonomy;DBN;European Space Agency;ExoMars rover;FDIR cycle;Italy;Thales;VERIFIM;autonomous spacecraft;dynamic bayesian networks;fault detection identification-and-recovery;on-board software architecture;power supply system;system-environment interaction;uncertain system evolution;usual system modeling procedures;Bayes methods;Inference algorithms;Probabilistic logic;Prognostics and health management;Sensor systems;Space vehicles;Autonomous spacecraft;dynamic Bayesian networks;fault detection identification and recovery},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {故障树转贝叶斯网，提到了一个工具 DFT to DBN Compiler},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6823144&isnumber=6983672},
  Zone                     = {-}
}

@Article{Codetta-Ra-2015-p13-24a,
  Title                    = {Dynamic Bayesian Networks for Fault Detection, Identification, and Recovery in Autonomous Spacecraft},
  Author                   = {Codetta-Raiteri, D. and Portinale, L.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {13-24},
  Volume                   = {45},

  Abstract                 = {This paper describes how to exploit the modeling features and inference capabilities of dynamic Bayesian networks (DBN), in designing and implementing an innovative approach to fault detection, identification, and recovery (FDIR) for autonomous spacecrafts (e.g., a Mars rover). In particular, issues like partial observability, uncertain system evolution and system-environment interaction, as well as the prediction and mitigation of imminent failures can be naturally addressed by the proposed approach. The DBN framework can augment the modeling and analytical power of standard FDIR methodologies, while still being able to be integrated into the usual system modeling procedures (like, for instance, fault tree analysis). An FDIR cycle composed of the tasks of diagnosis (identification of the current state of the system), prognosis (identification of the future state under the current conditions), and recovery (selection of the best set of actions the autonomous system can perform, in order to avoid critical situations) is introduced and characterized through a DBN model. In particular, by considering the execution of recovery actions in response to either a current or a future abnormal situation, both reactive as well as preventive recovery can be addressed respectively. The proposed approach has been implemented in an on-board software architecture called Anomaly resolution and prognostic health management for autonomy (ARPHA), realized during the Verifim study funded by the European Space Agency and jointly performed with Thales/Alenia Italy. We report on some of the results obtained by performing a case study concerning the FDIR analysis of the power supply system of the ExoMars rover, by considering different anomalous and failure simulated scenarios; we conclude that ARPHA is able to properly detect and deal with the simulated problems.},
  Doi                      = {10.1109/TSMC.2014.2323212},
  File                     = {Published version:Codetta-Ra-2015-p13-24a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {aerospace computing;autonomous aerial vehicles;belief networks;fault diagnosis;software architecture;space vehicles;uncertain systems;ARPHA;Alenia;Anomaly resolution-and-prognostic health management for autonomy;DBN;European Space Agency;ExoMars rover;FDIR cycle;Italy;Thales;VERIFIM;autonomous spacecraft;dynamic bayesian networks;fault detection identification-and-recovery;on-board software architecture;power supply system;system-environment interaction;uncertain system evolution;usual system modeling procedures;Bayes methods;Inference algorithms;Probabilistic logic;Prognostics and health management;Sensor systems;Space vehicles;Autonomous spacecraft;dynamic Bayesian networks;fault detection identification and recovery},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {Bayesian network 被用于故障诊断},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6823144&isnumber=6983672},
  Zone                     = {-}
}

@Article{Cox-1982-p465-468,
  Title                    = {An Analytic Method for Uncertainty Analysis of Nonlinear Output Functions, with Applications to Fault-Tree Analysis},
  Author                   = {Cox, David C.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1982},

  Month                    = {Dec},
  Number                   = {5},
  Pages                    = {465-468},
  Volume                   = {R-31},

  Abstract                 = {An analytic method for uncertainty analysis of the output of a complex model is described. It is assumed that model inputs are s-independent random variables and that model output is given as an analytic though possibly nonlinear function of the inputs. Using conditional s-expectations as a tool, a theoretical basis is developed for a method of partitioning the variance of the output among contributing causes. Such a partitioning helps the system analyst to identify the most important contributors to output uncertainty and, hence, to find effective ways of reducing that uncertainty. The method is illustrated by application to the uncertainty analysis of fault trees. There is no reason, in principle, why the method should not be applied to large computer codes where output cannot be represented as an analytic function of input. However, the evaluation of the required conditional s-expectations in such cases would be likely to involve considerable computation. It would be very desirable to extend the method to handle the case of s-correlated input variables. However, the partitioning of output variance in the presence of s-dependent inputs is difficult. The difficulties involved have not yet been resolved except in simple cases of s-dependence which can be reduced to the s-independent case by transformations of variables or other devices.},
  Doi                      = {10.1109/TR.1982.5221435},
  File                     = {Published version:Cox-1982-p465-468.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Analysis of variance;Input variables;Laboratories;Measurement standards;Polynomials;Probability distribution;Random variables;Reliability theory;Risk analysis;Uncertainty;Conditional s-expectation;Fault-tree analysis;Partitioning of variance;Uncertainty analysis},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5221435&isnumber=5221403},
  Zone                     = {2Q}
}

@Article{Dai-2001-p616-623,
  Title                    = {Annual risk assessment for overload security},
  Author                   = {Dai, Y. and McCalley, J.D. and Abi-Samra, N. and Vittal, V.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {616-623},
  Volume                   = {16},

  Abstract                 = {This paper presents an approach to assess the cumulative risk associated with overload security for the purposes of mid-term power system planning. The work is motivated by a need to refine bulk power system reliability assessment to meet the needs of the restructured industry. There are two main contributions to the work. First, the assessment is performed over a year-long trajectory of hourly operating conditions, resulting in reliability assessment of all expected conditions and not just a limited number of snapshots. Secondly, the assessment is quantified by a risk index comprised of summed products of probability and consequence. This index is a good indicator of the actual reliability level, it is decomposable, and it can be effectively integrated into economic decision making paradigms. The work is illustrated using the 1996 IEEE Reliability Test System},
  Doi                      = {10.1109/59.962405},
  File                     = {Published version:Dai-2001-p616-623.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {electricity supply industry;power system analysis computing;power system planning;power system reliability;power system security;probability;risk management;1996 IEEE Reliability Test System;annual cumulative risk assessment;bulk power system reliability assessment;computer simulation;hourly operating conditions;mid-term power system planning;overload security;probability;restructured power industry;risk index;Circuits;Decision making;Energy measurement;Loss measurement;Power generation economics;Power system economics;Power system planning;Power system reliability;Power system security;Risk management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，利用历史数据进行风险评估},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=962405&isnumber=20771},
  Zone                     = {2Q}
}

@Article{Dai-2001-p616-623a,
  Title                    = {Annual risk assessment for overload security},
  Author                   = {Dai, Y. and McCalley, J.D. and Abi-Samra, N. and Vittal, V.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {616-623},
  Volume                   = {16},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {This paper presents an approach to assess the cumulative risk associated with overload security for the purposes of mid-term power system planning. The work is motivated by a need to refine bulk power system reliability assessment to meet the needs of the restructured industry. There are two main contributions to the work. First, the assessment is performed over a year-long trajectory of hourly operating conditions, resulting in reliability assessment of all expected conditions and not just a limited number of snapshots. Secondly, the assessment is quantified by a risk index comprised of summed products of probability and consequence. This index is a good indicator of the actual reliability level, it is decomposable, and it can be effectively integrated into economic decision making paradigms. The work is illustrated using the 1996 IEEE Reliability Test System},
  Doi                      = {10.1109/59.962405},
  File                     = {Published version:Dai-2001-p616-623a.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {electricity supply industry;power system analysis computing;power system planning;power system reliability;power system security;probability;risk management;1996 IEEE Reliability Test System;annual cumulative risk assessment;bulk power system reliability assessment;computer simulation;hourly operating conditions;mid-term power system planning;overload security;probability;restructured power industry;risk index;Circuits;Decision making;Energy measurement;Loss measurement;Power generation economics;Power system economics;Power system planning;Power system reliability;Power system security;Risk management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，还是 IEEE Reliability Test System，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=962405&isnumber=20771},
  Zone                     = {2Q}
}

@Article{Dai-2001-p616-623b,
  Title                    = {Annual risk assessment for overload security},
  Author                   = {Dai, Y. and McCalley, J.D. and Abi-Samra, N. and Vittal, V.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {616-623},
  Volume                   = {16},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {This paper presents an approach to assess the cumulative risk associated with overload security for the purposes of mid-term power system planning. The work is motivated by a need to refine bulk power system reliability assessment to meet the needs of the restructured industry. There are two main contributions to the work. First, the assessment is performed over a year-long trajectory of hourly operating conditions, resulting in reliability assessment of all expected conditions and not just a limited number of snapshots. Secondly, the assessment is quantified by a risk index comprised of summed products of probability and consequence. This index is a good indicator of the actual reliability level, it is decomposable, and it can be effectively integrated into economic decision making paradigms. The work is illustrated using the 1996 IEEE Reliability Test System},
  Doi                      = {10.1109/59.962405},
  File                     = {Published version:Dai-2001-p616-623a.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {electricity supply industry;power system analysis computing;power system planning;power system reliability;power system security;probability;risk management;1996 IEEE Reliability Test System;annual cumulative risk assessment;bulk power system reliability assessment;computer simulation;hourly operating conditions;mid-term power system planning;overload security;probability;restructured power industry;risk index;Circuits;Decision making;Energy measurement;Loss measurement;Power generation economics;Power system economics;Power system planning;Power system reliability;Power system security;Risk management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，还是 IEEE Reliability Test System，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=962405&isnumber=20771},
  Zone                     = {2Q}
}

@InProceedings{Dandurand-2013-p1-16,
  Title                    = {Towards improved cyber security information sharing},
  Author                   = {Dandurand, L. and Serrano, O.S.},
  Booktitle                = {Cyber Conflict (CyCon), 2013 5th International Conference on},
  Year                     = {2013},
  Month                    = {June},
  Pages                    = {1-16},

  File                     = {Published version:Dandurand-2013-p1-16.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2325-5366},
  Keywords                 = {Web services;electronic data interchange;groupware;knowledge management;open systems;outsourcing;security of data;CDXI;Cyber security data exchange and collaboration infrastructure;Cyber security information sharing;Web service;burden sharing collaboration;centralized repository;communication system security;data quality;distribution list;information system security;interoperable standard;knowledge management tool;outsourcing;Automation;Collaboration;Computer security;Information management;Organizations;Standards organizations;Cyber security;automation;collaboration;data sharing;knowledge management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6568369&isnumber=6568361},
  Zone                     = {-}
}

@Article{Das-2013-p1966-1977,
  Title                    = {Security Analysis of Industrial Test Compression Schemes},
  Author                   = {Das, A. and Ege, B. and Ghosh, S. and Batina, L. and Verbauwhede, I.},
  Journal                  = {Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {1966-1977},
  Volume                   = {32},

  Abstract                 = {Test compression is widely used for reducing test time and cost of a very large scale integration circuit. It is also claimed to provide security against scan-based side-channel attacks. This paper pursues the legitimacy of this claim and presents scan attack vulnerabilities of test compression schemes used in commercial electronic design automation tools. A publicly available advanced encryption standard design is used and test compression structures provided by Synopsys, Cadence, and Mentor Graphics design for testability tools are inserted into the design. Experimental results of the differential scan attacks employed in this paper suggest that tools using X-masking and X-tolerance are vulnerable and leak information about the secret key. Differential scan attacks on these schemes have been demonstrated to have a best case success rate of 94.22% and 74.94%, respectively, for a random scan design. On the other hand, time compaction seems to be the strongest choice with the best case success rate of 3.55%. In addition, similar attacks are also performed on existing scan attack countermeasures proposed in the literature, thus experimentally evaluating their practical security. Finally, a suitable countermeasure is proposed and compared to the previously proposed countermeasures.},
  Doi                      = {10.1109/TCAD.2013.2274619},
  File                     = {Published version:Das-2013-p1966-1977.pdf:PDF},
  Impactfactor             = {1.093},
  ISSN                     = {0278-0070},
  Keywords                 = {VLSI;cryptography;data privacy;design for testability;electronic design automation;integrated circuit design;integrated circuit testing;Cadence design for testability tool;Mentor Graphics design for testability tool;Synopsys design for testability tool;X-masking tool;X-tolerance tool;advanced encryption standard design;commercial electronic design automation tool;cost reduction;industrial test compression scheme;random scan design;scan-based side-channel attack countermeasure;secret key information;security analysis;test time reduction;very large scale integration circuit;Design automation;Design for testability;Encryption;Test data compression;Very large scale integration;Adaptive scan;countermeasures;embedded deterministic test;on-product multiple input signature register (OPMISR);scan attack;security;test compression},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6663238&isnumber=6663223},
  Zone                     = {3Q}
}

@Article{Davies-2002-p203-230,
  Title                    = {Risk assessment in the \{UK\} nuclear power industry },
  Author                   = {L.P. Davies},
  Journal                  = {Safety Science },
  Year                     = {2002},
  Number                   = {1–4},
  Pages                    = {203 - 230},
  Volume                   = {40},

  Abstract                 = {The international workshop on “Promotion of Technical Harmonisation on Risk-Based Decision Making” investigated the use of risk-based decision making across a range of industries and countries throughout the world. This paper presents the contribution to the workshop covering risk assessment in the \{UK\} nuclear power industry. The format is a response to a set of questions prepared by the organisers of the workshop covering the use and performance of risk assessment, evaluation of the acceptability of the risk and the use of risk assessment in the decision making process. The possible need for a standard for risk based decision making is addressed and it is concluded that the benefits may be limited for the \{UK\} nuclear power industry and that there are many practical difficulties in introducing a harmonised standard across industries and countries. },
  Doi                      = {http://dx.doi.org/10.1016/S0925-7535(01)00037-6},
  File                     = {Published version:Davies-2002-p203-230.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {第二章有关于风险评估的一些问答，还有一些风险评估的基础},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753501000376},
  Zone                     = {3Q}
}

@Article{Davies-2002-p203-230a,
  Title                    = {Risk assessment in the \{UK\} nuclear power industry },
  Author                   = {L.P. Davies},
  Journal                  = {Safety Science },
  Year                     = {2002},
  Number                   = {1–4},
  Pages                    = {203 - 230},
  Volume                   = {40},

  Abstract                 = {The international workshop on “Promotion of Technical Harmonisation on Risk-Based Decision Making” investigated the use of risk-based decision making across a range of industries and countries throughout the world. This paper presents the contribution to the workshop covering risk assessment in the \{UK\} nuclear power industry. The format is a response to a set of questions prepared by the organisers of the workshop covering the use and performance of risk assessment, evaluation of the acceptability of the risk and the use of risk assessment in the decision making process. The possible need for a standard for risk based decision making is addressed and it is concluded that the benefits may be limited for the \{UK\} nuclear power industry and that there are many practical difficulties in introducing a harmonised standard across industries and countries. },
  Doi                      = {http://dx.doi.org/10.1016/S0925-7535(01)00037-6},
  File                     = {Published version:Davies-2002-p203-230a.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753501000376},
  Zone                     = {3Q}
}

@Article{Davis-2011-p353-375,
  Title                    = {Data preprocessing for anomaly based network intrusion detection: A review },
  Author                   = {Jonathan J. Davis and Andrew J. Clark},
  Journal                  = {Computers \& Security },
  Year                     = {2011},
  Number                   = {6–7},
  Pages                    = {353 - 375},
  Volume                   = {30},

  Abstract                 = {Data preprocessing is widely recognized as an important stage in anomaly detection. This paper reviews the data preprocessing techniques used by anomaly-based network intrusion detection systems (NIDS), concentrating on which aspects of the network traffic are analyzed, and what feature construction and selection methods have been used. Motivation for the paper comes from the large impact data preprocessing has on the accuracy and capability of anomaly-based NIDS. The review finds that many \{NIDS\} limit their view of network traffic to the TCP/IP packet headers. Time-based statistics can be derived from these headers to detect network scans, network worm behavior, and denial of service attacks. A number of other \{NIDS\} perform deeper inspection of request packets to detect attacks against network services and network applications. More recent approaches analyze full service responses to detect attacks targeting clients. The review covers a wide range of NIDS, highlighting which classes of attack are detectable by each of these approaches. Data preprocessing is found to predominantly rely on expert domain knowledge for identifying the most relevant parts of network traffic and for constructing the initial candidate set of traffic features. On the other hand, automated methods have been widely used for feature extraction to reduce data dimensionality, and feature selection to find the most relevant subset of features from this candidate set. The review shows a trend toward deeper packet inspection to construct more relevant features through targeted content parsing. These context sensitive features are required to detect current attacks. },
  Doi                      = {http://dx.doi.org/10.1016/j.cose.2011.05.008},
  File                     = {Published version:Davis-2011-p353-375.pdf:PDF},
  Impactfactor             = {1.158},
  ISSN                     = {0167-4048},
  Keywords                 = {Data preprocessing},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167404811000691},
  Zone                     = {3Q}
}

@Article{De-2009-p413-425,
  Title                    = {An Epidemic Theoretic Framework for Vulnerability Analysis of Broadcast Protocols in Wireless Sensor Networks},
  Author                   = {De, P. and Yonghe Liu and Das, S.K.},
  Journal                  = {Mobile Computing, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {413-425},
  Volume                   = {8},

  Abstract                 = {While multi-hop broadcast protocols, such as Trickle, Deluge and MNP, have gained tremendous popularity as a means for fast and convenient propagation of data/code in large scale wireless sensor networks, they can, unfortunately, serve as potential platforms for virus spreading if the security is breached. To understand the vulnerability of such protocols and design defense mechanisms against piggy-backed virus attacks, it is critical to investigate the propagation process of these protocols in terms of their speed and reachability. In this paper, we propose a general framework based on the principles of epidemic theory, for vulnerability analysis of current broadcast protocols in wireless sensor networks. In particular, we develop a common mathematical model for the propagation that incorporates important parameters derived from the communication patterns of the protocol under test. Based on this model, we analyze the propagation rate and the extent of spread of a malware over typical broadcast protocols proposed in the literature. The overall result is an approximate but convenient tool to characterize a broadcast protocol in terms of its vulnerability to malware propagation. We have also performed extensive simulations which have validated our model.},
  Doi                      = {10.1109/TMC.2008.115},
  File                     = {Published version:De-2009-p413-425.pdf:PDF},
  Impactfactor             = {2.395},
  ISSN                     = {1536-1233},
  Keywords                 = {broadcast channels;computer viruses;routing protocols;wireless sensor networks;epidemic theory;malware propagation;mathematical model;multihop broadcast protocols;piggy-backed virus attacks;vulnerability analysis;wireless sensor networks;Distributed networks;Network Protocols;Wireless},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4591395&isnumber=4757248},
  Zone                     = {2Q}
}

@Other{Degang-2013-p1-1,
  Title                    = {Attribute reduction for heterogeneous data based on combination of classical and fuzzy rough set models},
  Abstract                 = {Attribute reduction with rough sets aims to delete superfluous condition attributes from a decision system by considering the inconsistence between condition attributes and the decision labels. However, heterogeneous condition attributes including symbolic and real-valued ones always coexist for most decision systems and different types of attributes induce different kinds of granular structures. The existing rough set models don’t have explicit mechanism to address different kinds of granular structures reasonably and effectively. In this paper we aim to perform attribute reduction for decision systems with symbolic and real-valued condition attributes by composing classical rough set and fuzzy rough set models. We first define a discernibility relation for every symbolic and real-valued condition attribute to characterize its discernible ability related to the decision labels. With these discernibility relations we can develop a dependency function to measure the inconsistence between heterogeneous condition attributes and decision labels, and attribute reduction aims to keep this dependency function with a small perturbation. The proposed attribute reduction deals with heterogeneous condition attributes from the viewpoint of discernible ability and can consider the mutual effects between two types of attributes without preprocessing into single-typed ones. Algorithm to find reducts is developed and experiments are performed to demonstrate the proposed idea is effective.},
  Author                   = {Degang, C. and Yang, Y.},
  Doi                      = {10.1109/TFUZZ.2013.2291570},
  File                     = {Published version:Degang-2013-p1-1.pdf:PDF},
  Impactfactor             = {5.484},
  ISSN                     = {1063-6706},
  Journal                  = {Fuzzy Systems, IEEE Transactions on},
  Keywords                 = {Approximation algorithms;Approximation methods;Data mining;Educational institutions;Feature extraction;Heuristic algorithms;Rough sets;Rough set;attribute reduction;dependency function;fuzzy rough sets;heterogeneous data},
  Level                    = {A},
  Number                   = {99},
  Owner                    = {Qiqi},
  Pages                    = {1-1},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6670764&isnumber=4358784},
  Volume                   = {PP},
  Year                     = {2013},
  Zone                     = {1Q}
}

@Article{Deng-2014-p209-219,
  Title                    = {Web System Upgrading with Transaction Failure and Strategic Customers},
  Author                   = {Xiaoxue Deng and Zhong Liu and Jinxing Xie and Huachun Xiong},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {209-219},
  Volume                   = {44},

  Abstract                 = {This paper considers pricing and Web system upgrading problems for an online retailer facing a group of strategic customers. Due to various Web site issues, there is a possibility of transaction failure in the process of customer online purchasing. Strategic customers will anticipate this possibility and make purchasing decisions based on their belief on transaction success probability (TSP). First, we prescribe a threshold policy for customer purchasing: The customer will buy the product if his valuation for this product is above a threshold and will not otherwise. The threshold increases as TSP decreases, customer transaction cost increases, or customers become more risk averse. Second, we derive the optimal price of each period and identify the optimal policy for Web system upgrading: There exists a threshold for each period such that the online retailers should upgrade their Web system to the state of art (i.e., achieve highest available TSP) only if current TSP is below the threshold and should not upgrade otherwise. The threshold (total discounted profit) increases as customer transaction cost decreases, customer valuations for the product become higher, or customers become more (less) risk averse. Third, we find that the online retailer tends to price higher if it ignores customer strategic behavior. The cost of ignoring customer strategic behavior is substantial. The profit-loss rate of ignoring customer strategic behavior increases as customer transaction cost increases, customer valuations for the product become lower, or customers become more risk averse.},
  Doi                      = {10.1109/TSMC.2012.2237027},
  File                     = {Published version:Deng-2014-p209-219.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Internet;Web sites;consumer behaviour;pricing;probability;purchasing;retail data processing;transaction processing;TSP;Web site;Web system upgrading problems;customer online purchasing process;customer transaction cost;online retailer;optimal price;pricing problems;product valuation;purchasing decision making;risk aversed customers;strategic customer behavior;threshold policy;total discounted profit;transaction failure;transaction success probability;Cost accounting;Cybernetics;Hazards;Internet;Pricing;Software;Timing;Online retailing;strategic customers;technology adoption;transaction failure},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6461424&isnumber=6710259},
  Zone                     = {-}
}

@Article{Deng-1990-p295-310,
  Title                    = {A G-net model for knowledge representation and reasoning},
  Author                   = {Yi Deng and Shi-Kuo Chang},
  Journal                  = {Knowledge and Data Engineering, IEEE Transactions on},
  Year                     = {1990},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {295-310},
  Volume                   = {2},

  Doi                      = {10.1109/69.60793},
  File                     = {Published version:Deng-1990-p295-310.pdf:PDF},
  Impactfactor             = {1.892},
  ISSN                     = {1041-4347},
  Keywords                 = {Petri nets;knowledge representation;software engineering;G-net model;dynamic control knowledge;dynamic knowledge;information systems prototyping;inheritance reasoning;knowledge hierarchy;knowledge objects;knowledge representation;reasoning;recognition reasoning;static semantic knowledge;Computer science;Information systems;Knowledge representation;Petri nets;Prototypes},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=60793&isnumber=2214},
  Zone                     = {3Q}
}

@Article{Derakhshan-2013-p315-325,
  Title                    = {Profit-based unit commitment with security constraints and fair allocation of cost saving in industrial microgrids},
  Author                   = {Derakhshandeh, S.Y. and Golshan, M.E.H. and Masoum, M.A.S.},
  Journal                  = {Science, Measurement Technology, IET},
  Year                     = {2013},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {315-325},
  Volume                   = {7},

  Abstract                 = {Unit commitment (UC) in industrial microgrids (IMGs) is an important issue which has not received much attention in the literature. IMGs consist of many factories with distributed energy resources (DERs) and electric loads that heavily rely on combined heat and power systems. This study illustrates that UC in an IMG is not solely a profit-based UC or a security constrained UC (SCUC) problem. Therefore a new profit-based SCUC (PB-SCUC) formulation is proposed for IMGs that includes both microgrid (MG) security and factories constraints while utilising the generators waste heat to fulfill thermal requirements. The MG controller will solve the PB-SCUC problem to minimise the final production cost through maximisation of the profit by selling electricity to the upstream network. As each DER and boiler may have an individual owner, an approach based on game theory is also implemented to fairly allocate cost saving among factories participating in the generation process. Detailed simulation results are presented for an IMG consisting of 12 factories and analysed to show the effectiveness of the proposed approaches.},
  Doi                      = {10.1049/iet-smt.2013.0044},
  File                     = {Published version:Derakhshan-2013-p315-325.pdf:PDF},
  Impactfactor             = {1},
  ISSN                     = {1751-8822},
  Keywords                 = {distributed power generation;game theory;industrial power systems;power system economics;power system measurement;power system simulation;profitability;cost saving;distributed energy resources;electric loads;fair allocation;game theory;industrial microgrids;profit-based unit commitment;security constrained UC problem;security constraints},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6650285&isnumber=6650282},
  Zone                     = {4Q}
}

@Article{Devabhaktu-2003-p1822-1833,
  Title                    = {Advanced microwave modeling framework exploiting automatic model generation, knowledge neural networks, and space mapping},
  Author                   = {Devabhaktuni, V.K. and Chattaraj, B. and Yagoub, M.C.E. and Qi-Jun Zhang},
  Journal                  = {Microwave Theory and Techniques, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1822-1833},
  Volume                   = {51},

  Doi                      = {10.1109/TMTT.2003.814318},
  File                     = {Published version:Devabhaktu-2003-p1822-1833.pdf:PDF},
  Impactfactor             = {2.229},
  ISSN                     = {0018-9480},
  Keywords                 = {circuit CAD;circuit optimisation;knowledge based systems;microwave circuits;neural nets;printed circuit design;CAD;KAMG technique;adaptive sampling;automatic model generation;coarse data generators;fine data generators;high-frequency components;knowledge neural networks;knowledge-based automatic model generation;microwave modeling framework;microwave neural models;multilayer printed circuit boards;multiple sources;reinforced neural-network learning;space mapping;High power microwave generation;MOSFET circuits;Microwave generation;Microwave theory and techniques;Neural networks;Nonhomogeneous media;Power generation;Printed circuits;Sampling methods;Training data},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1209269&isnumber=27218},
  Zone                     = {2Q}
}

@Article{Dhillon-1988-p199-208,
  Title                    = {Chemical-system reliability: a review},
  Author                   = {Dhillon, B.S. and Rayapati, S.N.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1988},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {199-208},
  Volume                   = {37},

  Abstract                 = {Selected published literature on chemical-system reliability is reviewed. The literature is classified into several categories. These categories include: reliability prediction and analysis; reliability/maintenance/safety; risk analysis, control and assessment; fault diagnosis and propagation; failure analysis and prevention; failure data and analysis; reliability optimization; human reliability; application of reliability engineering to process plant industries; availability analysis; and life-cycle cost and economy. The literature is collected mainly from major journals and conference proceedings. An extensive list of references is presented},
  Doi                      = {10.1109/24.3741},
  File                     = {Published version:Dhillon-1988-p199-208.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {chemical industry;failure analysis;maintenance engineering;reliability;reviews;safety;availability analysis;chemical-system reliability;control;economy;failure analysis;failure prevention;fault diagnosis;human reliability;life-cycle cost;maintenance;process plant industries;reliability optimization;reliability prediction;review;risk analysis;safety;Chemicals;Data analysis;Failure analysis;Fault diagnosis;Humans;Industrial control;Maintenance;Reliability engineering;Risk analysis;Safety},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=3741&isnumber=209},
  Zone                     = {2Q}
}

@Article{Diaz-Lopez-2014-p-,
  Title                    = {Dynamic counter-measures for risk-based access control systems: An evolutive approach },
  Author                   = {Daniel Díaz-López and Ginés Dólera-Tormo and Félix Gómez-Mármol and Gregorio Martínez-Pérez},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract Risk-based access control systems are a new element in access control categories, incorporating risk analysis as part of the inputs to consider when taking an authorization decision. A risk analysis over a resource leads generally to temporal allocation of the resource in a risk level (e.g. high, medium, low). Ideally, for each risk level and kind of resource, the access control system should take an authorization decision (expressed like a permit or deny) and the system administrator should also trigger specific counter-measures to protect resources according to their risk level. In a small access control system with few resources it is possible for an administrator to follow the risk level changes and react promptly with counter-measures; but in medium/large access control systems it is almost unfeasible to react in a customized way to thousands of risk level emergencies asking for attention. In this paper we propose the adoption of dynamic counter-measures (which can be integrated within access control policies) changing along time to face variations in the risk level of every resource, bringing two main benefits, namely: (i) a suitable resource protection according to the risk level (not under or over estimated) and (ii) an access control system granting/denying access depending on the fulfillment of a set of security controls applicable in an authorization access request. To define the most appropriate set of counter-measures applicable for a specific situation we define a method based on genetic algorithms, which allows to find a solution in a reasonable time frame satisfying different required conditions. Finally, the conducted experiments show the applicability of our proposal in a real scenario. },
  Doi                      = {http://dx.doi.org/10.1016/j.future.2014.10.012},
  File                     = {Published version:Diaz-Lopez-2014-p-.pdf:PDF},
  Impactfactor             = {1.864},
  ISSN                     = {0167-739X},
  Keywords                 = {\{ISO\} 27001},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X14002052},
  Zone                     = {2Q}
}

@Article{Dimic-2014-p955-960,
  Title                    = {Failure Prediction of Gas and Oil Drilling Rig Pipelines with Axial Defects },
  Author                   = {I. Dimić and B. Medjo and M. Rakin and M. Arsić and Ž. Šarkoćević and A. Sedmak},
  Journal                  = {Procedia Materials Science },
  Year                     = {2014},
  Note                     = {20th European Conference on Fracture },
  Number                   = {0},
  Pages                    = {955 - 960},
  Volume                   = {3},

  Abstract                 = {Abstract Working conditions of casing pipes in drilling rigs can significantly influence the initiation and development of damage in the material, and therefore also the safe service of the entire system. In this work, an integrity assessment of a steel pipe with initial defect (machined surface crack) is presented. The position of this defect is on the external surface; unlike transport pipes, where internal surface is often endangered due to the contact with the fluid, casing pipes are often exposed to damages at the external surface. Analyzed crack is in axial direction, bearing in mind that this type of defect most significantly decreases the load carrying capacity of the cylindrical pressurized components. A pipe segment exposed to internal pressure is analyzed experimentally and numerically, using the finite element method. The experimental setup included the tracking of crack mouth opening displacement (CMOD) values, as well as J integral, which is determined by application of direct measurement. Criteria for pipe failure are determined on finite element models of the pipe; fracture initiation and plastic collapse are considered as failure mechanisms. The size of the crack is varied in the finite element models; several 3D models with different crack sizes (length and depth) are evaluated. Dependence of maximum internal pressure on the defect size is obtained. 2D plane strain models are also examined, with an aim to determine the applicability limits of this simplified approach. Based on the obtained results, integrity assessment criteria for the analyzed geometries are discussed. },
  Doi                      = {http://dx.doi.org/10.1016/j.mspro.2014.06.155},
  File                     = {Published version:Dimic-2014-p955-960.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2211-8128},
  Keywords                 = {Casing pipe},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2211812814001564},
  Zone                     = {-}
}

@Article{Dimopoulos-2012-p2219-2226,
  Title                    = {Proposal for Probabilistic Risk Assessment in Grounding Systems and Its Application to Transmission Substations},
  Author                   = {Dimopoulos, A. and Griffiths, H. and Harid, N. and Haddad, A. and Ainsley, A. and Guo, D. and Mpofu, G.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {2219-2226},
  Volume                   = {27},

  Abstract                 = {An approach to probabilistic risk assessment of electrical system grounding is proposed. The method uses all significant factors that affect the risk of electrocution at substations and takes into account their probabilistic nature. The approach implements an accurate statistical description of IEC479-1 fibrillation and body impedance data, and it uses detailed computer simulations of the modeled grounding system to provide safety voltage distributions that take into account the individual's presence at a site as a random variable. Variation in the power system fault level is accounted for, and extensive data of actual system fault clearance time are included. It is proposed that the probabilistic risk assessment is utilized as a second stage of the grounding system assessment when the first-stage deterministic analysis requires expensive or impractical mitigation. Implementation of the second stage probabilistic risk assessment yields a measure of individual risk. This is then benchmarked against industry-accepted “as low as reasonably practicable” values to determine whether investment in mitigation is required. To illustrate the applicability of the proposed approach, the probabilistic risk assessment is applied to a practical case study of a transmission substation.},
  Doi                      = {10.1109/TPWRD.2012.2204440},
  File                     = {Published version:Dimopoulos-2012-p2219-2226.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {earthing;electrical safety;power distribution faults;power distribution protection;power system management;power transmission faults;power transmission protection;probability;random processes;risk management;statistical analysis;substation protection;IEC479-1 fibrillation statistical description;body impedance data;computer simulation;electrical grounding system;electrocution risk;fault clearance time;first-stage deterministic analysis;power system fault;random variable;safety voltage distribution;second stage probabilistic risk assessment;transmission substation;Electrical safety;Grounding;Impedance;Probabilistic logic;Risk management;Substations;Electrical safety;grounding;risk assessment;substations},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，好像是静态风险评估},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6301791&isnumber=6301804},
  Zone                     = {3Q}
}

@InProceedings{Ding-2013-p4752-4756,
  Title                    = {The research and design of visual fault tree modeling analysis},
  Author                   = {Xiaolei Ding and Qiusheng Wang and Haibin Yuan and Haiwen Yuan and Mu Ling},
  Booktitle                = {Control and Decision Conference (CCDC), 2013 25th Chinese},
  Year                     = {2013},
  Month                    = {May},
  Pages                    = {4752-4756},

  Abstract                 = {According to the situation that creating fault tree manually is inefficient, error-prone, existing software of visual fault tree modeling is not perfect, not easy to transplant, research visual modeling and analysis of the fault tree. First, this paper discusses the visual building method combines dynamic contribution with stratified achievements, fault event editing functions, separation of fault tree with failure information method, then discusses the fault qualitative analysis in visual modeling process, final design the visual fault tree modeling platform, realize the rapid modeling and visual analysis of the fault tree, and presents an application examples to illustrate. The actual instructions that the system can meet the needs of visual fault tree modeling.},
  Doi                      = {10.1109/CCDC.2013.6561793},
  File                     = {Published version:Ding-2013-p4752-4756.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {fault trees;software reliability;dynamic contribution;failure information method;fault event editing functions;fault qualitative analysis;stratified achievements;visual building method;visual fault tree modeling analysis;Analytical models;Automation;Educational institutions;Electronic mail;Fault trees;Safety;Visualization;Fault Tree;Fault Tree Analysis;Visualization Modeling},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6561793&isnumber=6560882},
  Zone                     = {-}
}

@Article{Ding-2015-p1-1,
  Title                    = {A Transaction and QoS-Aware Service Selection Approach Based on Genetic Algorithm},
  Author                   = {Ding, Z. and Liu, J. and Sun, Y. and Jiang, C. and Zhou, M.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},
  Number                   = {99},
  Pages                    = {1-1},
  Volume                   = {PP},

  Abstract                 = {As there are various risks of failure in its execution, a composite web service (CWS) requires a transactional mechanism to guarantee its reliable execution. Though the existing service selection methods have considered that its transactional properties may affect its quality of service (QoS) such as its execution time, some of these methods can just give the locally optimal transactional CWS while others can give globally optimal CWS only under a given fixed transactional workflow. This paper addresses the issue of selecting and composing web services via a genetic algorithm (GA) and gives a transaction and QoS-aware selection approach. First, it introduces transactional properties of a single web service and CWS and the transactional rules used to compose them. Next, it conducts the performance analysis of basic workflow patterns such as sequential, parallel, selectable, and loop patterns and develops an algorithm to compute the execution time of a complex CWS. Then, it presents a GA-based approach, which takes into account the execution time, price, transactional property, stability, and penalty-factor, to achieve globally optimal service selection. Finally, this paper reports experimental results that compare the proposed approach with the exhaustive search algorithm, transactional-QoS-driven selection algorithm, and transactional service selection algorithm. The experimental results show that the proposed algorithm is efficient and effective and can give a globally optimal transactional CWS.},
  Doi                      = {10.1109/TSMC.2015.2396001},
  File                     = {Published version:Ding-2015-p1-1.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Automata;Genetic algorithms;Optimization;Performance analysis;Performance evaluation;Quality of service;Web services;Discrete event systems;genetic algorithm (GA);global optimization;quality of service (QoS);web service composition;web service selection;workflow analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7047222&isnumber=6376248},
  Zone                     = {-}
}

@Article{Dissanayak-2011-p1302-1308,
  Title                    = {Risk-Based Dynamic Security Assessment},
  Author                   = {Dissanayaka, A. and Annakkage, U.D. and Jayasekara, B. and Bagen, B.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {1302-1308},
  Volume                   = {26},

  Abstract                 = {This paper presents a linearized technique to determine a risk-based index for dynamic security. The method is an extension to an existing technique in which the risk of steady state security is calculated using the mean and variance of load uncertainty. The proposed method is applied to calculate the risk indices for the IEEE New England 39-bus test system. The results obtained from the proposed method are validated against those estimated by Monte Carlo simulation. Both approaches produce virtually the same results for small load deviations.},
  Doi                      = {10.1109/TPWRS.2010.2089809},
  File                     = {Published version:Dissanayak-2011-p1302-1308.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {IEEE standards;Monte Carlo methods;power system security;risk management;IEEE New England 39-bus test system;Monte Carlo simulation;load uncertainty;risk-based dynamic security assessment;risk-based index;steady state security;Power system dynamics;Power system stability;Security;Stability criteria;Transient analysis;Dynamic security assessment;probabilistic method;risk;severity;transient stability margin},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Monte Carlo simulation},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5654625&isnumber=5958775},
  Zone                     = {2Q}
}

@Article{Dissanayak-2011-p1302-1308a,
  Title                    = {Risk-Based Dynamic Security Assessment},
  Author                   = {Dissanayaka, A. and Annakkage, U.D. and Jayasekara, B. and Bagen, B.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {1302-1308},
  Volume                   = {26},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {This paper presents a linearized technique to determine a risk-based index for dynamic security. The method is an extension to an existing technique in which the risk of steady state security is calculated using the mean and variance of load uncertainty. The proposed method is applied to calculate the risk indices for the IEEE New England 39-bus test system. The results obtained from the proposed method are validated against those estimated by Monte Carlo simulation. Both approaches produce virtually the same results for small load deviations.},
  Doi                      = {10.1109/TPWRS.2010.2089809},
  File                     = {Published version:Dissanayak-2011-p1302-1308a.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {IEEE standards;Monte Carlo methods;power system security;risk management;IEEE New England 39-bus test system;Monte Carlo simulation;load uncertainty;risk-based dynamic security assessment;risk-based index;steady state security;Power system dynamics;Power system stability;Security;Stability criteria;Transient analysis;Dynamic security assessment;probabilistic method;risk;severity;transient stability margin},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，IEEE New England 39-bus test system，指标系统负载， 和蒙特卡洛方法比较占用资源少，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5654625&isnumber=5958775},
  Zone                     = {2Q}
}

@Article{Dissanayak-2011-p1302-1308b,
  Title                    = {Risk-Based Dynamic Security Assessment},
  Author                   = {Dissanayaka, A. and Annakkage, U.D. and Jayasekara, B. and Bagen, B.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {1302-1308},
  Volume                   = {26},

  Abstract                 = {This paper presents a linearized technique to determine a risk-based index for dynamic security. The method is an extension to an existing technique in which the risk of steady state security is calculated using the mean and variance of load uncertainty. The proposed method is applied to calculate the risk indices for the IEEE New England 39-bus test system. The results obtained from the proposed method are validated against those estimated by Monte Carlo simulation. Both approaches produce virtually the same results for small load deviations.},
  Doi                      = {10.1109/TPWRS.2010.2089809},
  File                     = {Published version:Dissanayak-2011-p1302-1308b.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {IEEE standards;Monte Carlo methods;power system security;risk management;IEEE New England 39-bus test system;Monte Carlo simulation;load uncertainty;risk-based dynamic security assessment;risk-based index;steady state security;Power system dynamics;Power system stability;Security;Stability criteria;Transient analysis;Dynamic security assessment;probabilistic method;risk;severity;transient stability margin},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {这个Background就是研究现状了},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5654625&isnumber=5958775},
  Zone                     = {2Q}
}

@Article{Dolk-1984-p619-628,
  Title                    = {Knowledge Representation for Model Management Systems},
  Author                   = {Dolk, Daniel R. and Konsynski, B.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {1984},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {619-628},
  Volume                   = {SE-10},

  Doi                      = {10.1109/TSE.1984.5010291},
  File                     = {Published version:Dolk-1984-p619-628.pdf:PDF},
  Impactfactor             = {2.588},
  ISSN                     = {0098-5589},
  Keywords                 = {Database systems;Decision making;Decision support systems;Knowledge management;Knowledge representation;Mathematical model;Predictive models;Resource management;Software tools;Vehicles;Abstraction;database management system (DBMS);decision support system;frame;frame-system;knowledge management;knowledge representation (KR);model management systems},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5010291&isnumber=5010286},
  Zone                     = {1Q}
}

@InProceedings{Dongliang-2011-p1086-1093,
  Title                    = {The simulation of risk-based adequacy control for the smart grid},
  Author                   = {Xie Dongliang and Xue Yusheng and Zhaoyang Dong and Xue Feng},
  Booktitle                = {Electric Utility Deregulation and Restructuring and Power Technologies (DRPT), 2011 4th International Conference on},
  Year                     = {2011},
  Month                    = {July},
  Pages                    = {1086-1093},

  Abstract                 = {The large-scale integration of Renewable Energy Sources (RES) generation, such as wind and solar power, is very helpful for reducing emissions of green-house gases. However, this is at the cost of demanding more reserve capacity of different response time, especially that of the spinning reserve, due to various kinds of uncertainty introduced by the energy sources. A brief introduction of the essential factors relevant to the adequacy control and the used dynamic simulation models is given firstly. Based on the models, the comparison between the risk-based control method and an existing method is made to show the importance of risk quantification to the adequacy analysis and control.},
  Doi                      = {10.1109/DRPT.2011.5994057},
  File                     = {Published version:Dongliang-2011-p1086-1093.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {air pollution control;distributed power generation;large scale integration;renewable energy sources;risk analysis;smart power grids;dynamic simulation models;greenhouse gas reduction;large scale integration;renewable energy sources;risk based adequacy control;risk quantification;smart grid;Algorithm design and analysis;Analytical models;Biological system modeling;Optimization;Power system dynamics;Probabilistic logic;Schedules;adequacy control;large-scale RES integration;reserve management;risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5994057&isnumber=5993852},
  Zone                     = {-}
}

@Article{Drumheller-1999-p627-636,
  Title                    = {Estimating matched filter amplitude probability functions by failure rate analysis},
  Author                   = {Drumheller, D.M.},
  Journal                  = {Aerospace and Electronic Systems, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {Apr},
  Number                   = {2},
  Pages                    = {627-636},
  Volume                   = {35},

  Doi                      = {10.1109/7.766943},
  File                     = {Published version:Drumheller-1999-p627-636.pdf:PDF},
  Impactfactor             = {1.299},
  ISSN                     = {0018-9251},
  Keywords                 = {curve fitting;filtering theory;least squares approximations;matched filters;probability;radar clutter;radar detection;radar theory;reliability theory;sonar detection;amplitude probability functions estimation;closed-form probability functions;closed-form radar clutter model;constrained optimisation;cumulative distribution function;curve fitting;echolocation systems;failure rate analysis;least squares problem;matched filter output;model survival functions;performance analysis;probability density function;radar clutter data;reliability theory;sea surface clutter;sonar clutter data;Amplitude estimation;Clutter;Curve fitting;Distribution functions;Failure analysis;Lighting;Matched filters;Performance analysis;Probability density function;Reliability theory},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=766943&isnumber=16623},
  Zone                     = {2Q}
}

@Article{Drumheller-1999-p627-636a,
  Title                    = {Estimating matched filter amplitude probability functions by failure rate analysis},
  Author                   = {Drumheller, D.M.},
  Journal                  = {Aerospace and Electronic Systems, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {Apr},
  Number                   = {2},
  Pages                    = {627-636},
  Volume                   = {35},

  Abstract                 = {Performance analysis of echolocation systems requires knowledge of the probability density function (pdf) or cumulative distribution function (cdf) of a matched filter output. A method is presented to estimate these and other probability functions from data by estimating the failure rate function, a function employed in reliability theory. The method can also be used to derive approximations to closed-form probability functions. The method is demonstrated using experimental sonar and radar clutter data and a closed-form radar clutter model},
  Doi                      = {10.1109/7.766943},
  File                     = {Published version:Drumheller-1999-p627-636a.pdf:PDF},
  Impactfactor             = {1.299},
  ISSN                     = {0018-9251},
  Keywords                 = {curve fitting;filtering theory;least squares approximations;matched filters;probability;radar clutter;radar detection;radar theory;reliability theory;sonar detection;amplitude probability functions estimation;closed-form probability functions;closed-form radar clutter model;constrained optimisation;cumulative distribution function;curve fitting;echolocation systems;failure rate analysis;least squares problem;matched filter output;model survival functions;performance analysis;probability density function;radar clutter data;reliability theory;sea surface clutter;sonar clutter data;Amplitude estimation;Clutter;Curve fitting;Distribution functions;Failure analysis;Lighting;Matched filters;Performance analysis;Probability density function;Reliability theory},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Function 表示函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=766943&isnumber=16623},
  Zone                     = {2Q}
}

@InProceedings{Du-2009-p141-149,
  Title                    = {An Analysis for Understanding Software Security Requirement Methodologies},
  Author                   = {Jing Du and Ye Yang and Qing Wang},
  Booktitle                = {Secure Software Integration and Reliability Improvement, 2009. SSIRI 2009. Third IEEE International Conference on},
  Year                     = {2009},
  Pages                    = {141--149},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/SSIRI.2009.14},
  File                     = {Published version:Du-2009-p141-149.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {reviews;security;software engineering;process-oriented method;reuse-based method;review;software development;software quality;software security requirement;software-intensive systems;Authentication;Costs;Face detection;Failure analysis;Fault trees;Programming;Safety;Security;Software quality;Software systems;Literature Review and Analysis;Requirement Methodology;Security Requirement;Software},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5325381&isnumber=5325011},
  Zone                     = {-}
}

@Article{Du-1973-p235-240,
  Title                    = {Multiple Fault Detection in Combinational Circuits: Algorithms and Computational Results},
  Author                   = {Du, Min-Wen and Weiss, C.Dennis},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1973},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {235-240},
  Volume                   = {C-22},

  Abstract                 = {A new approach is developed for finding multiple fault detection tests under quite arbitrary fault models. Computational results are reported and discussed.},
  Doi                      = {10.1109/T-C.1973.223700},
  File                     = {Published version:Du-1973-p235-240.pdf:PDF},
  Impactfactor             = {1.379},
  ISSN                     = {0018-9340},
  Keywords                 = {Combinational logic networks, computational experiments, fault detection, fault tree, function verification, multiple faults.;Circuit faults;Circuit testing;Combinational circuits;Computer networks;Computer science;Electrical fault detection;Fault detection;Fault trees;Intelligent networks;Logic;Combinational logic networks, computational experiments, fault detection, fault tree, function verification, multiple faults.},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1672293&isnumber=35074},
  Zone                     = {3Q}
}

@Article{Duan-2014-p53-58,
  Title                    = {Security risk assessment using fast probabilistic power flow considering static power-frequency characteristics of power systems },
  Author                   = {Yao Duan and Buhan Zhang},
  Journal                  = {International Journal of Electrical Power \& Energy Systems },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {53 - 58},
  Volume                   = {60},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {Abstract Large scale blackouts in the world have aroused the study of security risk assessment (SRA) urgently. \{SRA\} difficulties brought about by uncertainties can be solved by probabilistic power flow (PPF). Conventional methods usually focus on the probabilistic density function (PDF) and the cumulative distribution function (CDF) of node voltages and branch flows only. A \{SRA\} scheme of power system using fast \{PPF\} is proposed in this paper. The scheme took static power-frequency characteristics (SPFCs) into account, and fast decoupled power flow (FDPF) was used to solve PPF. Besides node voltage and branch flows, the scheme can obtain the \{PDF\} and \{CDF\} of frequency. The computing speed of the proposed scheme considering wind power multi-scenarios is enhanced compared to conventional method. \{SRA\} indices are introduced to evaluate the power system quantitatively. The examples on the \{IEEE\} RTS-24 system demonstrate the feasibility, rapidity and validity of the proposed scheme. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijepes.2014.02.030},
  File                     = {Published version:Duan-2014-p53-58.pdf:PDF},
  Impactfactor             = {3.432},
  ISSN                     = {0142-0615},
  Keywords                 = {Security risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A， IEEE RTS-24 system，仿真},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0142061514000933},
  Zone                     = {2Q}
}

@InProceedings{Dullaert-2010-p1-4,
  Title                    = {A comparison between the SEM method and the Slepian expansion for modeling the frequency dependency of an antenna transfer function},
  Author                   = {Dullaert, W. and Rogier, H.},
  Booktitle                = {Antennas and Propagation Society International Symposium (APSURSI), 2010 IEEE},
  Year                     = {2010},
  Month                    = {July},
  Pages                    = {1-4},

  Abstract                 = {In order to account for this frequency dependency, various antenna models, which can be roughly grouped into two categories, have been proposed. First, the singularity expansion method (SEM) models the antenna using a pole/residue technique. Second, the Slepian expansion expands the frequency dependency into Discrete Prolate Spheroidal Sequences (DPSSs) or Prolate Spheroidal Wave Functions (PSWFs).},
  Doi                      = {10.1109/APS.2010.5561035},
  File                     = {Published version:Dullaert-2010-p1-4.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1522-3965},
  Keywords                 = {transfer functions;ultra wideband antennas;wave functions;DPSS;PSWF;SEM method;SEM model;antenna transfer function;discrete prolate spheroidal sequences;frequency dependency;prolate spheroidal wave function;singularity expansion method model;Antenna measurements;Antenna radiation patterns;Frequency domain analysis;Matrix decomposition},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5561035&isnumber=5560891},
  Zone                     = {-}
}

@Article{Dun-2014-p191-201,
  Title                    = {Influences of magmatic intrusion on the macromolecular and pore structures of coal: Evidences from Raman spectroscopy and atomic force microscopy },
  Author                   = {Wu Dun and Liu Guijian and Sun Ruoyu and Chen Shancheng},
  Journal                  = {Fuel },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {191 - 201},
  Volume                   = {119},

  Abstract                 = {Abstract Magmatic intrusion into coal-bearing sequences can significantly affect the rank of the intruded coal and change the macromolecular and pore structures. Here, we use Raman microscope and Atomic force microscope (AFM) to obtain quantitative information on these transformations. Six coal samples of different ranks taken from a magmatic intrusion zone of Huainan Coalfield, China, were studied. The Raman spectra were fitted with a combination of 8 Lorentzian bands and 1 Gaussian band. We found that the macromolecular structures in coals of different ranks have significant relationships with Raman spectral parameters, particularly the band area ratios (ID1/IG, ID2/IG, ID3/IG, ID4/IG and IG/IAll). With the increase of coal rank, ID1/IG, ID2/IG, ID3/IG and ID4/IG show trends of decreasing intensity, suggesting an enhanced orientation of aromatic hydrocarbons. Observations by Atomic force microscopy indicate that the pore parameters (e.g. amount, size, shape) are quite different for different ranks of coals. The pore size of low-rank bituminous coal is much larger than high-rank anthracite and pore abundance is higher in the former. In addition, there is also good correspondence between the surface topography of coal grains and coal rank. },
  Doi                      = {http://dx.doi.org/10.1016/j.fuel.2013.11.012},
  File                     = {Published version:Dun-2014-p191-201.pdf:PDF},
  Impactfactor             = {3.357},
  ISSN                     = {0016-2361},
  Keywords                 = {Magmatic intrusion},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0016236113010582},
  Zone                     = {3Q}
}

@Article{Dunglinson-1983-p150-163,
  Title                    = {Interval Reliability for Initiating and Enabling Events},
  Author                   = {Dunglinson, Colin and Lambert, Howard},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1983},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {150-163},
  Volume                   = {R-32},

  Doi                      = {10.1109/TR.1983.5221514},
  File                     = {Published version:Dunglinson-1983-p150-163.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Application software;Availability;Electrical equipment industry;Failure analysis;Fault trees;Frequency;Industrial accidents;Logic;Performance analysis;Safety;Accident prevention;Common-cause initiating event;Control-system reliability analysis;Design tradeoff;Digraph analysis;Event tree-fault tree approach;Fault-tree analysis;Fault-tree computer code;Interval reliability;Probabilistic importance;Probabilistic risk assessment;Quantitative safety analysis;Time ordering of events;Top-event occurrence frequency},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5221514&isnumber=5221502},
  Zone                     = {2Q}
}

@Article{Dunglinson-1983-p150-163a,
  Title                    = {Interval Reliability for Initiating and Enabling Events},
  Author                   = {Dunglinson, Colin and Lambert, Howard},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1983},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {150-163},
  Volume                   = {R-32},

  Abstract                 = {This paper describes generation and evaluation of logic models such as fault trees for interval reliability. Interval reliability assesses the ability of a system to operate over a specific time interval without failure. The analysis requires that the sequence of events leading to system failure be identified. Two types of events are described: 1) initiating events (cause disturbances or perturbations in system variables) that cause system failure and 2) enabling events (permit initiating events to cause system failure). Control-system failures are treated. The engineering and mathematical concepts are described in terms of a simplified example of a pressure-tank system. Later these same concepts are used in an actual industrial application in which an existing chlorine vaporizer system was modified to improve safety without compromising system availability. Computer codes that are capable of performing the calculations, and pitfalls in computing accident frequency in fault tree analysis, are discussed.},
  Doi                      = {10.1109/TR.1983.5221514},
  File                     = {Published version:Dunglinson-1983-p150-163a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Application software;Availability;Electrical equipment industry;Failure analysis;Fault trees;Frequency;Industrial accidents;Logic;Performance analysis;Safety;Accident prevention;Common-cause initiating event;Control-system reliability analysis;Design tradeoff;Digraph analysis;Event tree-fault tree approach;Fault-tree analysis;Fault-tree computer code;Interval reliability;Probabilistic importance;Probabilistic risk assessment;Quantitative safety analysis;Time ordering of events;Top-event occurrence frequency},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5221514&isnumber=5221502},
  Zone                     = {2Q}
}

@InProceedings{Duqueroie-2013-p334-341,
  Title                    = {Probability update for risk assessment of dreaded events in asymmetric warfare},
  Author                   = {Duqueroie, B. and Labreuche, C. and Pichon, F. and Museux, N.},
  Booktitle                = {Information Fusion (FUSION), 2013 16th International Conference on},
  Year                     = {2013},
  Month                    = {July},
  Pages                    = {334-341},

  Abstract                 = {This paper presents a new method to update the probability of occurrence of a dreaded event, according to environmental influencers and reports of evidence on the presence or absence of such occurrence. It is particularly useful in military contexts at the tactical level for the risk assessment of a task. Indeed, the computation done during the planning of such a task often uses unprecise values (e.g. average height of buildings in an urban area) and forecast data (e.g. weather, presence of other Blue forces in the area) to determine a probability of occurrence of a given type of attack during such a task. Furthermore, new information may have been gathered from several mainly human sources, since the beginning of the task that indicate a presence/absence of the occurrence of the attack at a specific location in the area of the task. This method is based on Multi-Criteria Decision Making technics that handles the environmental influencers, and conditional probabilities that take into account the reports of evidence, including the reliability of the sources and the obsolescence of the reports.},
  File                     = {Published version:Duqueroie-2013-p334-341.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {decision making;forecasting theory;military systems;planning;probability;reliability theory;risk management;asymmetric warfare;conditional probability;dreaded events;environmental influencers;forecast data;human sources;military contexts;multicriteria decision making technique;probability update;risk assessment;source reliability;tactical level;task planning;Aging;Bayes methods;Decision making;Force;Reliability;Risk management;Asymmetric Warfare;Information Obsolescence;Information fusion;Multi-Criteria Decision Making;Probability;Risk Assessment;Source Reliability},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6641297&isnumber=6641065},
  Zone                     = {-}
}

@Article{Durant-1988-p138-143,
  Title                    = {Data bank for probabilistic risk-assessment of nuclear-fuel reprocessing plants},
  Author                   = {Durant, W.S. and Lux, C.R. and Galloway, W.D.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1988},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {138-143},
  Volume                   = {37},

  Doi                      = {10.1109/24.3733},
  File                     = {Published version:Durant-1988-p138-143.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {fission reactor fuel preparation and reprocessing;nuclear engineering computing;safety;Savannah river plant;data bank;equipment failures;failure analysis;management of data;nuclear-fuel reprocessing plants;operating problems;probabilistic risk-assessment;Contamination;Equipment failure;Failure analysis;Fuel economy;Injuries;Laboratories;Personnel;Risk analysis;Risk management;Rivers},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=3733&isnumber=209},
  Zone                     = {2Q}
}

@Article{Dzung-2005-p1152-1177,
  Title                    = {Security for Industrial Communication Systems},
  Author                   = {Dzung, D. and Naedele, M. and von Hoff, T.P. and Crevatin, M.},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {2005},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {1152-1177},
  Volume                   = {93},

  Abstract                 = {Modern industrial communication networks are increasingly based on open protocols and platforms that are also used in the office IT and Internet environment. This reuse facilitates development and deployment of highly connected systems, but also makes the communication system vulnerable to electronic attacks. This paper gives an overview of IT security issues in industrial automation systems which are based on open communication systems. First, security objectives, electronic attack methods, and the available countermeasures for general IT systems are described. General security objectives and best practices are listed. Particularly for the TCP/IP protocol suite, a wide range of cryptography-based secure communication protocols is available. The paper describes their principles and scope of application. Next, we focus on industrial communication systems, which have a number of security-relevant characteristics distinct from the office IT systems. Confidentiality of transmitted data may not be required; however, data and user authentication, as well as access control are crucial for the mission critical and safety critical operation of the automation system. As a result, modern industrial automation systems, if they include security measures at all, emphasize various forms of access control. The paper describes the status of relevant specifications and implementations for a number of standardized automation protocols. Finally, we illustrate the application of security concepts and tools by brief case studies describing security issues in the configuration and operation of substations, plants, or for remote access.},
  Doi                      = {10.1109/JPROC.2005.849714},
  File                     = {Published version:Dzung-2005-p1152-1177.pdf:PDF},
  Impactfactor             = {6.911},
  ISSN                     = {0018-9219},
  Keywords                 = {Internet;cryptography;message authentication;telecommunication security;transport protocols;IT security;Internet;TCP-IP protocol;access control;cryptography;data authentication;electronic attacks;embedded system;industrial automation system;industrial communication network;industrial communication system;mission critical operation;open communication system;open protocols;remote access;safety critical operation;secure communication protocols;security protocol;security standard;standardized automation protocol;user authentication;Access control;Access protocols;Automation;Communication industry;Communication networks;Communication system security;Cryptographic protocols;Electrical equipment industry;Electronic countermeasures;IP networks;Cryptography;embedded systems;industrial automation;industrial communication systems;remote access;security objectives;security protocols;security standards},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1435744&isnumber=30937},
  Zone                     = {1Q}
}

@Article{Elbasiony-2013-p753-762,
  Title                    = {A hybrid network intrusion detection framework based on random forests and weighted k-means },
  Author                   = {Reda M. Elbasiony and Elsayed A. Sallam and Tarek E. Eltobely and Mahmoud M. Fahmy},
  Journal                  = {Ain Shams Engineering Journal },
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {753 - 762},
  Volume                   = {4},

  Abstract                 = {Many current \{NIDSs\} are rule-based systems, which are very difficult in encoding rules, and cannot detect novel intrusions. Therefore, a hybrid detection framework that depends on data mining classification and clustering techniques is proposed. In misuse detection, random forests classification algorithm is used to build intrusion patterns automatically from a training dataset, and then matches network connections to these intrusion patterns to detect network intrusions. In anomaly detection, the k-means clustering algorithm is used to detect novel intrusions by clustering the network connections’ data to collect the most of intrusions together in one or more clusters. In the proposed hybrid framework, the anomaly part is improved by replacing the k-means algorithm with another one called weighted k-means algorithm, moreover, it uses a proposed method in choosing the anomalous clusters by injecting known attacks into uncertain connections data. Our approaches are evaluated over the Knowledge Discovery and Data Mining (KDD’99) datasets. },
  Doi                      = {http://dx.doi.org/10.1016/j.asej.2013.01.003},
  File                     = {Published version:Elbasiony-2013-p753-762.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2090-4479},
  Keywords                 = {Computer network security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2090447913000105},
  Zone                     = {-}
}

@InProceedings{El-Hadidy-2013-p1-5,
  Title                    = {Early detection of tree faults},
  Author                   = {El-Hadidy, A.M. and Rehtanz, C.},
  Booktitle                = {Electricity Distribution (CIRED 2013), 22nd International Conference and Exhibition on},
  Year                     = {2013},
  Month                    = {June},
  Pages                    = {1-5},

  Abstract                 = {Faults due to falling trees happen frequently. At the moment a tree falls on a transmission line, the impedance path of the tree is not fully conductive. Therefore, the tree resistance at these early stages is very high and hard to detect. As time passes, the conductivity of the tree increases (less fault resistance Rf) till the point where Rf is low enough for the protection algorithms to detect. This phenomenon could happen in both High Voltage and Medium Voltage networks. This paper focuses on the application of the protection function proposed in [1] for early detection of faults; especially tree faults. Tree faults with fault impedances up to 106 Ω can be detected. This very high value of impedances corresponds to very early stages of formation of the fault. Another important feature of the function in addition to detection of High Impedance Faults (HIF) is its security against non-fault phenomena like power swing, voltage instability and load encroachment. A comparison between the proposed function and conventional percentage differential relays shows superiority of the proposed approach. Other advantages are discussed throughout the text.},
  Doi                      = {10.1049/cp.2013.0913},
  File                     = {Published version:El-Hadidy-2013-p1-5.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {electrical conductivity;fault diagnosis;power transmission lines;power transmission protection;relay protection;differential relays;early tree faults detection;fault resistance;high impedance faults;high voltage networks;impedance path;load encroachment;medium voltage networks;nonfault phenomena;power swing;protection algorithms;protection function;transmission line;voltage instability},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6683516&isnumber=6683125},
  Zone                     = {-}
}

@Article{Ericsson-2010-p1501-1507,
  Title                    = {Cyber Security and Power System Communication—Essential Parts of a Smart Grid Infrastructure},
  Author                   = {Ericsson, G.N.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {July},
  Number                   = {3},
  Pages                    = {1501-1507},
  Volume                   = {25},

  Doi                      = {10.1109/TPWRD.2010.2046654},
  File                     = {Published version:Ericsson-2010-p1501-1507.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {SCADA systems;carrier transmission on power lines;energy management systems;power engineering computing;power grids;power system security;EMS system;SCADA;broadband communications;cyber security;electricity transmission;information security domain modeling;integrated computer environments;power control systems;power system communication;smart grid infrastructure;Communication systems;IT security;SCADA;control systems;cyber security;information security;power system communication;power system control;power systems;security;smart grid;wide-area networks},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5452993&isnumber=5491350},
  Zone                     = {3Q}
}

@Article{Ericsson-2009-p1174-1181,
  Title                    = {Information Security for Electric Power Utilities (EPUs)—CIGRÉ Developments on Frameworks, Risk Assessment, and Technology},
  Author                   = {Ericsson, G.N.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {July},
  Number                   = {3},
  Pages                    = {1174-1181},
  Volume                   = {24},

  __markedentry            = {[Qiqi:5]},
  Abstract                 = {This paper deals with the important issue of proper treatment of information security for electric power utilities. It is based on the efforts of CIGRE Working Group (WG) D2.22 on ldquoTreatment of Information Security for Electric Power Utilities (EPUs)rdquo carried out between 2006 and 2008/2009. The WG produces a Technical Brochure (TB), where the purpose is to emphasize three main issues: security frameworks, risk assessment, and security technology. Here, guidance is given on different security frameworks based on an information security domain model. Also, baseline controls are treated. For risk assessment, a survey has been carried out. Only few commonalities, but several differences, have been found. Here, a methodology must be developed together with practical recommendations. For security technologies, guidance is given for deployment of different solutions, based on a logical diagram using different controls. Last, proposal on further work is given.},
  Doi                      = {10.1109/TPWRD.2008.2008470},
  File                     = {Published version:Ericsson-2009-p1174-1181.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {electricity supply industry;power engineering computing;power system control;power system security;risk management;CIGRE Working Group;cyber security;electric power utilities;information security domain model;logical diagram;power system control;risk assessment;Communication systems;ISO/IEC standard;IT security;SCADA;control systems;cyber security;information security;power system communications;power system control;power systems;risk assessment;security framework;security technology;substation automation},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，无数据，无例子，无仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4797811},
  Zone                     = {3Q}
}

@Article{Eriksson-2006-p370-379,
  Title                    = {Toward architecture-based test-vector generation for timing verification of fast parallel multipliers},
  Author                   = {Eriksson, H. and Larsson-Edefors, P. and Eckerbert, D.},
  Journal                  = {Very Large Scale Integration (VLSI) Systems, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {370-379},
  Volume                   = {14},

  Abstract                 = {Fast parallel multipliers that contain logarithmic partial-product reduction trees pose a challenge to simulation-based high-accuracy timing verification, since the reduction tree has many reconvergent signal branches. However, such a multiplier architecture also offers a clue as how to attack the test-vector generation problem. The timing-critical paths are intimately associated with long carry propagation. We introduce a multiplier test-vector generation method that has the ability to exercise such long carry propagation paths. Through extensive circuit simulation and static timing analysis, we evaluate the quality of the test vectors that result from the new method. Especially for fast multipliers with a pronounced carry propagation, the timing-critical vectors manage to stimulate a path, which has a delay that comes close to the true worst case delay. We investigate the complexity and run-time for the test-vector generation, and derive timing-critical vectors up to a factor word length of 54 bits.},
  Doi                      = {10.1109/TVLSI.2006.874297},
  File                     = {Published version:Eriksson-2006-p370-379.pdf:PDF},
  Impactfactor             = {1.218},
  ISSN                     = {1063-8210},
  Keywords                 = {circuit simulation;integrated circuit testing;multiplying circuits;parallel architectures;timing;Wallace multiplier;architecture-based test-vector generation;carry propagation;circuit simulation;fast parallel multipliers;logarithmic partial-product reduction trees;over-turned stairs multiplier;static timing analysis;timing verification;Circuit simulation;Circuit testing;Computational modeling;Computer science;Crosstalk;Design automation;Propagation delay;Runtime;Timing;Very large scale integration;Array multiplier;Dadda (DAD) multiplier;Wallace (WAL) multiplier;circuit simulation;critical path;delay;over-turned stairs (OTS) multiplier;static timing analysis (STA);test vectors;three-dimensional method (TDM) multiplier;timing verification},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1637467&isnumber=34317},
  Zone                     = {3Q}
}

@InProceedings{Ertle-2012-p1-5,
  Title                    = {Conceptual Design of a Dynamic Risk-Assessment Server for Autonomous Robots},
  Author                   = {Ertle, Philipp and Tokic, Michel and Bystricky, Tobias and Ebel, Marius and Voos, Holger and Soeffker, Dirk},
  Booktitle                = {Robotics; Proceedings of ROBOTIK 2012; 7th German Conference on},
  Year                     = {2012},
  Month                    = {May},
  Pages                    = {1-5},

  Abstract                 = {Future autonomous service robots are intended to operate in open and complex environments. This in turn implies complications ensuring safe operation. The tenor of few available investigations is the need for dynamically assessing operational risks. Furthermore, there is a new kind of hazards being implicated by the robot??s capability to manipulate the environment: Hazardous environmental object interactions. Therefore, the realization of the Dynamic Risk-Assessment approach with special scope on object-interaction risks is addressed in this paper. A server-based architecture is proposed facilitating a feasible integration into robotic systems and realization of software and hardware redundancy as well. Area of conference topics (keywords): Area 6 - Safety (service robots, safety, dynamic risk assessment, safety principle)},
  File                     = {Published version:Ertle-2012-p1-5.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Decision making;Educational institutions;Hazards;Humans;Robots;Servers},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6309516&isnumber=6309471},
  Zone                     = {-}
}

@Article{Fahrner-2012-p262-271,
  Title                    = {A monitoring strategy to detect \{CO2\} intrusion in deeper freshwater aquifers },
  Author                   = {S. Fahrner and D. Schäfer and A. Dahmke},
  Journal                  = {International Journal of Greenhouse Gas Control },
  Year                     = {2012},
  Number                   = {0},
  Pages                    = {262 - 271},
  Volume                   = {9},

  Abstract                 = {The study provides guidance for the design of site-tailored monitoring protocols at \{CO2\} storage sites. Monitoring of geochemical changes induced by \{CO2\} intrusion in freshwater aquifers which are located between storage formations and close-to-surface compartments can enable the early detection of \{CO2\} leakage events and ensure the safety of freshwater resources. The suitability of the hydroelectric conductivity (EC), the total inorganic carbon (TIC) concentration, and the pH of groundwater, which have been proposed as monitoring parameters for shallow aquifers, are investigated by means of reactive scenario modeling at depths between 10 and 450&#xa0;m. Simulation results suggest that all parameters are well suited and allow distinguishing intrusion of \{CO2\} from intrusion of high-salinity water or a mixture of both. However, \{EC\} may not be well-suited for the monitoring of shallow mature clastic aquifers. The study proposes to consider the combination of geophysical and geochemical measurements in site-specific monitoring protocols. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijggc.2012.03.016},
  File                     = {Published version:Fahrner-2012-p262-271.pdf:PDF},
  Impactfactor             = {3.944},
  ISSN                     = {1750-5836},
  Keywords                 = {\{CO2\} geological storage},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~，},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1750583612000771},
  Zone                     = {2Q}
}

@Article{Fajardo-2013-p260-272,
  Title                    = {Probabilistic study of cascading failures in complex interdependent lifeline systems },
  Author                   = {Isaac Hernandez Fajardo and Leonardo Dueñas-Osorio},
  Journal                  = {Reliability Engineering \& System Safety},
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {260 - 272},
  Volume                   = {111},

  Abstract                 = {The internal complexity of lifeline systems and their standing interdependencies can operate in conjunction to amplify the negative effects of external disruptions. This paper introduces a simulation-based methodology to evaluate the joint impact of interdependence, component fragilities, and cascading failures in systemic fragility estimates. The proposed strategy uses a graph model of interdependent networks, an enhanced betweenness centrality for cascading failures approximation, and an interdependence model accounting for coupling uncertainty in the simulation of damage propagation for probabilistic performance assessment. This methodology is illustrated through its application to a realistic set of power and water networks subjected to earthquake scenarios and random failures. Test case results reveal two key insights: (1) the intensity of a perturbation influences interdependent systemic fragility by shaping the magnitudes of initial component damage and, sometimes counter-intuitively, the subsequent interdependence effects and (2) increasing local redundancy mitigates the effects of interdependence on systemic performance, but such intervention is incapable of eliminating interdependent effects completely. The previous insights provide basic guidelines for the design of systemic retrofitting policies. Additionally, the limitations of local capacity redundancy as a fragility control measure highlight the need for a critical assessment of intervention strategies in distributed infrastructure networks. Future work will assess the fragility-reduction efficiency of strategies involving informed manipulation of individual systemic topologies and the interdependence interfaces connecting them. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2012.10.012},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Lifeline Systems},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832012002153},
  Zone                     = {2Q}
}

@InProceedings{Fan-2011-p621-625,
  Title                    = {A risk assessment model for the operating system of the DTV},
  Author                   = {Kefeng Fan and Subing Zhang and Yinghua Huang and Zhen Yang},
  Booktitle                = {High Performance Computing and Simulation (HPCS), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {July},
  Pages                    = {621-625},

  Abstract                 = {The problem of secure operation system risk management is difficult to evaluate quantitatively, a quantitatively risk assessment model for the operating system is proposed in this paper. Through introducing the risk matrix, putting the information safety risk assessment as the risk evaluation model by experts matrix. Borda rule and AHP (Analytic Hierarchy Process) assess the risk assessment process, and achieve the quantitative evaluation of the secure operation system risk management, enhancing the objectivity of the operating system risk quantitative evaluation. In conclusion, the application evaluation model is validated by the examples, and the experimental results show that the model can effectively assess the operating system security risk levels.},
  Doi                      = {10.1109/HPCSim.2011.5999884},
  File                     = {Published version:Fan-2011-p621-625.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {decision making;operating systems (computers);risk management;security of data;Borda rule;DTV;analytic hierarchy process;application evaluation model;experts matrix;information safety risk assessment;operating system;risk assessment model;risk matrix;secure operation system risk management;Authentication;Digital TV;Linux;Operating systems;Risk management;Safety;AH;Borda rule;Risk Assessment;Secure operating system},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5999884&isnumber=5999789},
  Zone                     = {-}
}

@Article{Fang-2005-p201-207,
  Title                    = {Improved hybrid approach to transient stability assessment},
  Author                   = {Fang, D.Z. and David, A.K. and Kai, C. and Yunli, C.},
  Journal                  = {Generation, Transmission and Distribution, IEE Proceedings-},
  Year                     = {2005},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {201-207},
  Volume                   = {152},

  Abstract                 = {The characteristics of the minimum transient kinetic energy of post-fault trajectories are studied in order to develop a tool for dynamic security assessment during the operation mode analysis of power systems. Using available commercial software a strategy for the transient stability study is introduced to enable harmless contingencies to be screened out and the classification of high-risk and harmful contingencies to be performed. Based on these characteristics, an approach to transient stability assessment is proposed and applied to the assessment at the hybrid transient energy margin for high-risk and harmful contingencies. The significant advantage of this approach is that the identification of a critical machine group is completely unnecessary. Case studies on a small test system, the ten-generator New England test power system and a realistic system, the middle-China power network are presented to verify the effectiveness and efficiency of this method.},
  Doi                      = {10.1049/ip-gtd:20041223},
  File                     = {Published version:Fang-2005-p201-207.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1350-2360},
  Keywords                 = {power system analysis computing;power system faults;power system security;power system transient stability;dynamic security assessment;harmful contingencies;hybrid transient stability assessment;minimum transient kinetic energy;post-fault trajectories;power system analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，不相关},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1421138&isnumber=30708},
  Zone                     = {-}
}

@Article{Fang-2005-p201-207a,
  Title                    = {Improved hybrid approach to transient stability assessment},
  Author                   = {Fang, D.Z. and David, A.K. and Kai, C. and Yunli, C.},
  Journal                  = {Generation, Transmission and Distribution, IEE Proceedings-},
  Year                     = {2005},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {201-207},
  Volume                   = {152},

  Abstract                 = {The characteristics of the minimum transient kinetic energy of post-fault trajectories are studied in order to develop a tool for dynamic security assessment during the operation mode analysis of power systems. Using available commercial software a strategy for the transient stability study is introduced to enable harmless contingencies to be screened out and the classification of high-risk and harmful contingencies to be performed. Based on these characteristics, an approach to transient stability assessment is proposed and applied to the assessment at the hybrid transient energy margin for high-risk and harmful contingencies. The significant advantage of this approach is that the identification of a critical machine group is completely unnecessary. Case studies on a small test system, the ten-generator New England test power system and a realistic system, the middle-China power network are presented to verify the effectiveness and efficiency of this method.},
  Doi                      = {10.1049/ip-gtd:20041223},
  File                     = {Published version:Fang-2005-p201-207a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1350-2360},
  Keywords                 = {power system analysis computing;power system faults;power system security;power system transient stability;dynamic security assessment;harmful contingencies;hybrid transient stability assessment;minimum transient kinetic energy;post-fault trajectories;power system analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1421138&isnumber=30708},
  Zone                     = {-}
}

@InProceedings{Fanliang-2013-p1-4,
  Title                    = {Modeling and simulations of collective violent events based on criminal risk assessment model},
  Author                   = {Bu Fanliang and Dang Huisen},
  Booktitle                = {Conference Anthology, IEEE},
  Year                     = {2013},
  Month                    = {Jan},
  Pages                    = {1-4},

  Abstract                 = {This paper presents a new criminal risk assessment model to study the criminal behavior choice of individuals in crowd context. By abstracting individual's grievance, risk level and other important factors, we represent the events' growing and spreading in virtual world. Considerable simulations indicate that the appearance of police could effectively prevent “windows-breaking effect” spreading in events and the proportion of hard-core and hangers-on plays an important role in the growing and spreading of events. The results provide meaningful theoretical basis for public security organs to deal with the collective violent events.},
  Doi                      = {10.1109/ANTHOLOGY.2013.6785043},
  File                     = {Published version:Fanliang-2013-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Buildings;Computational modeling;Conferences;Context modeling;Educational institutions;Risk management;Transforms;collective violent events;risk assessment;windows-breaking effect},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6785043&isnumber=6784688},
  Zone                     = {-}
}

@Article{Fanti-2015-p485-495a,
  Title                    = {A Risk Assessment Framework for Hazmat Transportation in Highways by Colored Petri Nets},
  Author                   = {Fanti, M.P. and Iacobellis, G. and Ukovich, W.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {485-495},
  Volume                   = {45},

  Abstract                 = {The management and control of vehicles transporting hazardous materials (hazmat) on congested highways has attracted growing attention from researchers in recent years. This paper proposes a decision support system (DSS) for monitoring hazmat vehicles, aimed at assessing two problems: evaluating the social risk induced by hazmat vehicles traveling in highways and selecting restoration procedures after an accident involving heavy vehicles. The proposed DSS can estimate in real time and offline the risk of hazmat transportation, by taking into account the type of transported hazmat, the traffic, and the density of populations living close to the highway. Two main modules of the DSS are specified: the risk assessment module and the simulation module (SM) that allows forecasting risk in different contexts and scenarios. In particular, the SM is realized by modeling the highway network in a colored Petri net (CPN) framework. In order to show the effectiveness and the applicability of the DSS, a prototype is described and applied to a highway in the North-east of Italy.},
  Doi                      = {10.1109/TSMC.2014.2351373},
  File                     = {Published version:Fanti-2015-p485-495a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Petri nets;decision support systems;digital simulation;forecasting theory;hazardous materials;risk management;road traffic;traffic information systems;DSS;Italy;SM;colored Petri nets;decision support system;forecasting risk;hazardous materials;hazmat transportation;hazmat vehicle monitoring;highway congestion;risk assessment framework;simulation module;Accidents;Decision support systems;Hazardous materials;Real-time systems;Road transportation;Vehicles;Colored Petri nets (CPNs);decision support system (DSS);hazardous material transportation;simulation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {高速公路危险货物运输的风险评估，没有信息安全},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6893034&isnumber=7031988},
  Zone                     = {-}
}

@InProceedings{Farkhani-2006-p2778-2783,
  Title                    = {Examination and Classification of Security Requirements of Software Systems},
  Author                   = {Farkhani, T.R. and Razzazi, M.R.},
  Booktitle                = {Information and Communication Technologies, 2006. ICTTA '06. 2nd},
  Year                     = {2006},
  Pages                    = {2778-2783},
  Volume                   = {2},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/ICTTA.2006.1684851},
  File                     = {Published version:Farkhani-2006-p2778-2783.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {security of data;systems analysis;requirement engineering;security requirement classification;software systems;Application software;Computer crime;Costs;Information security;Programming;Quality assurance;Software quality;Software systems;Software testing;System testing},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1684851&isnumber=35471},
  Zone                     = {-}
}

@InProceedings{Fei-2014-p616-619,
  Title                    = {Risk Assessment Model of Network Intrusion in Electronic Commerce Area},
  Author                   = {Gao Fei and He Ren-Yu},
  Booktitle                = {Intelligent Computation Technology and Automation (ICICTA), 2014 7th International Conference on},
  Year                     = {2014},
  Month                    = {Oct},
  Pages                    = {616-619},

  Abstract                 = {In the electronic commerce area, the network intrusion happens frequently, and the risk assessment model of network intrusion for the electronic commerce is researched. Currently, the complexity of electronic commerce network intrusion is increasing day by day, and all kinds of potential relevance factors are becoming more and more complicated. The traditional methods access the security of network intrusion quantitatively, because the quantitative standard is fuzzy, the risk assessment is not accurate. An improved risk assessment model of electronic commerce network intrusion is proposed based on rough set grey prediction method. According to rough set theory, all factors that will influence the risk of electronic commerce network intrusion are taken into consideration. The two-dimensional decision table is established. The errors and redundant data in the decision table are removed. The data attributes are taken with simple processing, redundant attribute values are deleted. The intrusion risk knowledge representation system in the network is obtained. The cross operation is taken for the equivalence relation family in the system. According to the obtained results, the network intrusion risk related data sets are established. Rough set grey prediction model is extracted. Risk matrix of the historical data in network intrusion is obtained, the rough set grey prediction model is established, and the network intrusion risk assessment of electronic commerce is completed. Simulation results show that the improved method can be applied in the network intrusion risk assessment of electronic commerce, and the risk of network intrusion can be evaluated accurately, the security of electronic commerce can be ensured.},
  Doi                      = {10.1109/ICICTA.2014.153},
  File                     = {Published version:Fei-2014-p616-619.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer network security;decision tables;electronic commerce;fuzzy set theory;grey systems;risk management;rough set theory;data attributes;electronic commerce area;electronic commerce network intrusion security;equivalence relation family;network intrusion risk assessment;redundant attribute values;redundant data;risk matrix;rough set grey prediction method;rough set theory,all;two-dimensional decision table;Data models;Electronic commerce;Mathematical model;Predictive models;Risk management;Security;Simulation;electronic commerce;network intrusion;risk assessment;rough set Class number: TP 391},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7003616&isnumber=7003462},
  Zone                     = {-}
}

@InProceedings{Feinstein-2003-p1-314,
  Title                    = {Statistical approaches to DDoS attack detection and response},
  Author                   = {Feinstein, L. and Schnackenberg, D. and Balupari, R. and Kindred, D.},
  Booktitle                = {DARPA Information Survivability Conference and Exposition, 2003. Proceedings},
  Year                     = {2003},
  Month                    = {April},
  Pages                    = {303-314 vol.1},
  Volume                   = {1},

  Abstract                 = {The nature of the threats posed by distributed denial of service (DDoS) attacks on large networks, such as the Internet, demands effective detection and response methods. These methods must be deployed not only at the edge but also at the core of the network This paper presents methods to identify DDoS attacks by computing entropy and frequency-sorted distributions of selected packet attributes. The DDoS attacks show anomalies in the characteristics of the selected packet attributes. The detection accuracy and performance are analyzed using live traffic traces from a variety of network environments ranging from points in the core of the Internet to those inside an edge network The results indicate that these methods can be effective against current attacks and suggest directions for improving detection of more stealthy attacks. We also describe our detection-response prototype and how the detectors can be extended to make effective response decisions.},
  Doi                      = {10.1109/DISCEX.2003.1194894},
  File                     = {Published version:Feinstein-2003-p1-314.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Internet;computer crime;entropy;performance evaluation;statistical distributions;telecommunication security;DDoS;Internet core;anomalies;attack detection;attack response;detection accuracy;distributed denial of service;edge network;entropy;frequency-sorted distributions;live traffic traces;performance;selected packet attributes;statistical approaches;stealthy attacks;Computer crime;Computer networks;Distributed computing;Entropy;Frequency;IP networks;Performance analysis;Prototypes;Telecommunication traffic;Web and internet services},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1194894&isnumber=26875},
  Zone                     = {-}
}

@Article{Feng-2015-p87-99,
  Title                    = {Modeling and analysis of passive worm propagation in the \{P2P\} file-sharing network },
  Author                   = {Chao-sheng Feng and Jun Yang and Zhi-guang Qin and Ding Yuan and Hong-rong Cheng},
  Journal                  = {Simulation Modelling Practice and Theory },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {87 - 99},
  Volume                   = {51},

  Abstract                 = {Abstract A number of worms, named \{P2P\} (peer-to-peer) passive worms, have recently surfaced, which propagate in \{P2P\} file-sharing networks and have posed heavy threats to these networks. In contrast to the majority of Internet worms, it is by exploiting users’ legitimate activities instead of vulnerabilities of networks in which \{P2P\} passive worms propagate. This feature evidently slows down their propagation, which results in them not attracting an adequate amount of attention in literature. Meanwhile, this feature visibly increases the difficulty of detecting them, which makes it very possible for them to become epidemic. In this paper, we propose an analytical model for \{P2P\} passive worm propagation by adopting epidemiological approaches so as to identify their behaviors and predict the tendency of their propagation accurately. Compared with a few existing models, dynamic characteristics of \{P2P\} networks are taken into account. Based on this proposed model, the sufficient condition for the global stability of the worm free equilibrium is derived by applying epidemiological theories. Large scale simulation experiments have validated both the proposed model and the condition. },
  Doi                      = {http://dx.doi.org/10.1016/j.simpat.2014.11.004},
  File                     = {Published version:Feng-2015-p87-99.pdf:PDF},
  Impactfactor             = {1.159},
  ISSN                     = {1569-190X},
  Keywords                 = {Passive worms},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1569190X1400166X},
  Zone                     = {4Q}
}

@Article{Feng-2011-p4332-4340,
  Title                    = {An information systems security risk assessment model under uncertain environment },
  Author                   = {Nan Feng and Minqiang Li},
  Journal                  = {Applied Soft Computing },
  Year                     = {2011},
  Note                     = {Soft Computing for Information System Security },
  Number                   = {7},
  Pages                    = {4332 - 4340},
  Volume                   = {11},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {Given there is a great deal of uncertainty in the process of information systems security (ISS) risk assessment, the handling of uncertainty is of great significance for the effectiveness of risk assessment. In this paper, we propose an \{ISS\} risk assessment model based on the improved evidence theory. Firstly, we establish the \{ISS\} index system and quantify index weights, based on which the evidential diagram is constructed. To deal with the uncertain evidence found in the \{ISS\} risk assessment, this model provides a new way to define the basic belief assignment in fuzzy measure. Moreover, the model also provides a method of testing the evidential consistency, which can reduce the uncertainty derived from the conflicts of evidence. Finally, the model is further demonstrated and validated via a case study, in which sensitivity analysis is employed to validate the reliability of the proposed model.},
  Doi                      = {http://dx.doi.org/10.1016/j.asoc.2010.06.005},
  File                     = {Published version:Feng-2011-p4332-4340.pdf:PDF},
  Impactfactor             = {2.14},
  ISSN                     = {1568-4946},
  Keywords                 = {Information systems security},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {A，介绍了风险评估模型，通过一个Case验证这个模型，对象为 company's information systems，Case

这个作者请了安全公司的专家为他们的模糊系统打分},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1568494610001419},
  Zone                     = {3Q}
}

@Article{Feng-2011-p4332-4340a,
  Title                    = {An information systems security risk assessment model under uncertain environment },
  Author                   = {Nan Feng and Minqiang Li},
  Journal                  = {Applied Soft Computing },
  Year                     = {2011},
  Note                     = {Soft Computing for Information System Security },
  Number                   = {7},
  Pages                    = {4332 - 4340},
  Volume                   = {11},

  Abstract                 = {Given there is a great deal of uncertainty in the process of information systems security (ISS) risk assessment, the handling of uncertainty is of great significance for the effectiveness of risk assessment. In this paper, we propose an \{ISS\} risk assessment model based on the improved evidence theory. Firstly, we establish the \{ISS\} index system and quantify index weights, based on which the evidential diagram is constructed. To deal with the uncertain evidence found in the \{ISS\} risk assessment, this model provides a new way to define the basic belief assignment in fuzzy measure. Moreover, the model also provides a method of testing the evidential consistency, which can reduce the uncertainty derived from the conflicts of evidence. Finally, the model is further demonstrated and validated via a case study, in which sensitivity analysis is employed to validate the reliability of the proposed model. },
  Doi                      = {http://dx.doi.org/10.1016/j.asoc.2010.06.005},
  File                     = {Feng-2011-p4332-4340a.pdf:Feng-2011-p4332-4340a.pdf:PDF},
  Impactfactor             = {2.14},
  ISSN                     = {1568-4946},
  Keywords                 = {Information systems security},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1568494610001419},
  Zone                     = {3Q}
}

@Article{Fenton-2014-p21-26,
  Title                    = {Decision Support Software for Probabilistic Risk Assessment Using Bayesian Networks},
  Author                   = {Fenton, N. and Neil, M.},
  Journal                  = {Software, IEEE},
  Year                     = {2014},

  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {21-26},
  Volume                   = {31},

  Doi                      = {10.1109/MS.2014.32},
  File                     = {Published version:Fenton-2014-p21-26.pdf:PDF},
  Impactfactor             = {1.616},
  ISSN                     = {0740-7459},
  Keywords                 = {belief networks;decision support systems;risk management;Agena;Bayesian decision networks;decision support software;probabilistic risk assessment;Adaptation models;Bayes methods;Complexity theory;Decision support systems;Graphical user interfaces;Software development;AgenaRisk;Bayesian networks;impact},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6774326&isnumber=6774316},
  Zone                     = {3Q}
}

@Article{Fenton-2014-p21-26a,
  Title                    = {Decision Support Software for Probabilistic Risk Assessment Using Bayesian Networks},
  Author                   = {Fenton, N. and Neil, M.},
  Journal                  = {Software, IEEE},
  Year                     = {2014},

  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {21-26},
  Volume                   = {31},

  Doi                      = {10.1109/MS.2014.32},
  File                     = {Published version:Fenton-2014-p21-26a.pdf:PDF},
  Impactfactor             = {1.616},
  ISSN                     = {0740-7459},
  Keywords                 = {belief networks;decision support systems;risk management;Agena;Bayesian decision networks;decision support software;probabilistic risk assessment;Adaptation models;Bayes methods;Complexity theory;Decision support systems;Graphical user interfaces;Software development;AgenaRisk;Bayesian networks;impact},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6774326&isnumber=6774316},
  Zone                     = {3Q}
}

@Article{Fernandez--2004-p164-171,
  Title                    = {Towards cooperative frameworks for modeling and integrating biological processes knowledge},
  Author                   = {Fernandez-Breis, J.T. and Martinez-Bejar, R. and Vivancos-Vicente, P.J. and Valencia-Garcia, R. and Garci?a-Sa?nchez, F.},
  Journal                  = {NanoBioscience, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {164-171},
  Volume                   = {3},

  Doi                      = {10.1109/TNB.2004.833685},
  File                     = {Published version:Fernandez--2004-p164-171.pdf:PDF},
  Impactfactor             = {1.286},
  ISSN                     = {1536-1241},
  Keywords                 = {genetics;knowledge acquisition;knowledge representation;molecular biophysics;physiological models;biological processes knowledge;biological system representation;biology ontologies;data organization;genomic data;knowledge model;knowledge representation;Bioinformatics;Biological information theory;Biological processes;Biological system modeling;Biological systems;Computational biology;Genomics;Humans;Knowledge acquisition;Ontologies;Artificial Intelligence;Cell Physiology;Computational Biology;Database Management Systems;Databases, Factual;Information Storage and Retrieval;Models, Biological;Proteome;Systems Biology;Systems Integration},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1331341&isnumber=29398},
  Zone                     = {4Q}
}

@Article{Ferreyra-2005-p494-500,
  Title                    = {Failure map functions and accelerated mean time to failure tests: new approaches for improving the reliability estimation in systems exposed to single event upsets},
  Author                   = {Ferreyra, P.A. and Marques, C.A. and Ferreyra, R.T. and Gaspar, J.P.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {494-500},
  Volume                   = {52},

  Doi                      = {10.1109/TNS.2005.845883},
  File                     = {Published version:Ferreyra-2005-p494-500.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {fault tolerance;fission reactor safety;nuclear power stations;physics computing;reliability theory;space research;statistical distributions;system recovery;accelerated mean time;accelerated process;computer reliability;failure map functions;failure tests;fault tolerance;histogram;nuclear power plants;probability distribution function;radiation environment;random variable;reliability estimation;single event upset injection;space;statistical properties;system reliability;system time failure;Acceleration;Aerodynamics;Histograms;Life estimation;Probability distribution;Random variables;Registers;Single event transient;Single event upset;System testing;Fault tolerance;radiation environment;single event upset (SEU) injection;single event upsets (SEUs) computer reliability},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1417196&isnumber=30669},
  Zone                     = {3Q}
}

@Article{Ferreyra-2005-p494-500a,
  Title                    = {Failure map functions and accelerated mean time to failure tests: new approaches for improving the reliability estimation in systems exposed to single event upsets},
  Author                   = {Ferreyra, P.A. and Marques, C.A. and Ferreyra, R.T. and Gaspar, J.P.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {494-500},
  Volume                   = {52},

  Abstract                 = {The application cross section "σAP" is a parameter used to characterize the systems single event upset (SEU) vulnerability, but does not give, in a direct way, the system's reliability. Reliability prediction of a system exposed to SEU can be improved with the knowledge of the system's time to failure (TTF) probability distribution function. This work presents two new methods suitable to provide this information. The first one is based on the construction of a function named the failure map function (FMF). FMF contains the information needed to calculate all the TTF statistical properties by means of numerical procedures. For the cases where the FMF function is difficult to obtain, a second method is presented which consist of injecting SEUs at a rate several orders of magnitude higher than the real rate. A histogram of the TTF random variable for the accelerated process is obtained. The system's reliability can then be derived by means of statistical and numerical procedures.},
  Doi                      = {10.1109/TNS.2005.845883},
  File                     = {Published version:Ferreyra-2005-p494-500a.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {fault tolerance;fission reactor safety;nuclear power stations;physics computing;reliability theory;space research;statistical distributions;system recovery;accelerated mean time;accelerated process;computer reliability;failure map functions;failure tests;fault tolerance;histogram;nuclear power plants;probability distribution function;radiation environment;random variable;reliability estimation;single event upset injection;space;statistical properties;system reliability;system time failure;Acceleration;Aerodynamics;Histograms;Life estimation;Probability distribution;Random variables;Registers;Single event transient;Single event upset;System testing;Fault tolerance;radiation environment;single event upset (SEU) injection;single event upsets (SEUs) computer reliability},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，Function 表示函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1417196&isnumber=30669},
  Zone                     = {3Q}
}

@Article{Ferreyra-2005-p494-500b,
  Title                    = {Failure map functions and accelerated mean time to failure tests: new approaches for improving the reliability estimation in systems exposed to single event upsets},
  Author                   = {Ferreyra, P.A. and Marques, C.A. and Ferreyra, R.T. and Gaspar, J.P.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {494-500},
  Volume                   = {52},

  Abstract                 = {The application cross section "σAP" is a parameter used to characterize the systems single event upset (SEU) vulnerability, but does not give, in a direct way, the system's reliability. Reliability prediction of a system exposed to SEU can be improved with the knowledge of the system's time to failure (TTF) probability distribution function. This work presents two new methods suitable to provide this information. The first one is based on the construction of a function named the failure map function (FMF). FMF contains the information needed to calculate all the TTF statistical properties by means of numerical procedures. For the cases where the FMF function is difficult to obtain, a second method is presented which consist of injecting SEUs at a rate several orders of magnitude higher than the real rate. A histogram of the TTF random variable for the accelerated process is obtained. The system's reliability can then be derived by means of statistical and numerical procedures.},
  Doi                      = {10.1109/TNS.2005.845883},
  File                     = {Published version:Ferreyra-2005-p494-500b.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {fault tolerance;fission reactor safety;nuclear power stations;physics computing;reliability theory;space research;statistical distributions;system recovery;accelerated mean time;accelerated process;computer reliability;failure map functions;failure tests;fault tolerance;histogram;nuclear power plants;probability distribution function;radiation environment;random variable;reliability estimation;single event upset injection;space;statistical properties;system reliability;system time failure;Acceleration;Aerodynamics;Histograms;Life estimation;Probability distribution;Random variables;Registers;Single event transient;Single event upset;System testing;Fault tolerance;radiation environment;single event upset (SEU) injection;single event upsets (SEUs) computer reliability},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B， failure-detection algorithm 失效检测算法},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1417196&isnumber=30669},
  Zone                     = {3Q}
}

@Article{Filipe-2014-p-,
  Title                    = {Galectin-3 and heart failure: Prognosis, prediction \&amp; clinical utility },
  Author                   = {Mando D. Filipe and Wouter C. Meijers and A. Rogier van der Velde and Rudolf A. de Boer},
  Journal                  = {Clinica Chimica Acta },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract Heart failure (HF) is a leading cause of mortality in the western world. Despite advances in the treatment of HF, like the use of angiotensin-converting enzyme (ACE) inhibitors, β-blockers, angiotensin receptor blockers (ARBs), mineralocorticoid receptor antagonists (MRAs), and implantable cardiac defibrillators, prognosis of \{HF\} patients remains poor. For clinicians dealing with \{HF\} patients, risk prediction in both acute, chronic, and new onset \{HF\} remains a challenge. Biomarkers might help in risk stratification and may guide the proper use of limited resources and therapy. Galectin-3 is an emerging biomarker which has been linked to tissue fibrosis, a hallmark in cardiac remodeling and HF. Galectin-3 can reliably be measured in the circulation, and several recent studies have shown the prognostic value of galectin-3 in acute and chronic HF, and its potential utility in the general population. The purpose of this review was to summarize the literature and explore the potential role of galectin-3 as a biomarker in HF. },
  Doi                      = {http://dx.doi.org/10.1016/j.cca.2014.10.009},
  File                     = {Published version:Filipe-2014-p-.pdf:PDF},
  Impactfactor             = {2.85},
  ISSN                     = {0009-8981},
  Keywords                 = {Galectin-3},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0009898114004409},
  Zone                     = {2Q}
}

@Article{Filippini-2015-p157-169,
  Title                    = {I \textcircledR ML: An Infrastructure Resilience-Oriented Modeling Language},
  Author                   = {Filippini, R. and Silva, A.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {157-169},
  Volume                   = {45},

  Abstract                 = {Critical infrastructure (CI) modeling and analysis is a very challenging research topic. One of the most pressing issues is to find an effective representation for addressing the system vulnerabilities caused by interdependencies, which, if exploited, could result in nontrivial accident scenarios. Until now, this question has been tackled for different sector-specific infrastructures (electricity grid, telecommunications networks, supply chains, etc.), and very few generalizable analysis tools have been developed. However, all CI share some features that can be leveraged in order to build a common modeling framework. This paper identifies these common features, which it exploits to develop a modeling language: the infrastructure resilience-oriented modeling language (I?ML). I?ML is designed to facilitate the analysis of operational interdependencies among the infrastructure components and overall resilience, i.e., the ability of the infrastructure to withstand and recover under off-nominal (anomalous) conditions. A number of examples are used to illustrate the modeling concepts and highlight the analytical capability of I?ML.},
  Doi                      = {10.1109/TSMC.2014.2343751},
  File                     = {Published version:Filippini-2015-p157-169.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {program diagnostics;specification languages;CI;I?ML;critical infrastructure modeling;generalizable analysis tools;infrastructure resilience-oriented modeling language;off-nominal conditions;operational interdependencies;sector-specific infrastructures;Accidents;Analytical models;Computational modeling;Cybernetics;Resilience;Unified modeling language;Critical infrastructures;interdependencies;resilience;risk;system of systems (SoS)},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6875911&isnumber=6983672},
  Zone                     = {-}
}

@InProceedings{Finke-2012-p499-504,
  Title                    = {Industrial field service network performance measurement Performance dimensions and indicators for service providers},
  Author                   = {Finke, G.R. and Hertz, P. and Schonsleben, P.},
  Booktitle                = {Service Systems and Service Management (ICSSSM), 2012 9th International Conference on},
  Year                     = {2012},
  Pages                    = {499--504},

  Doi                      = {10.1109/ICSSSM.2012.6252287},
  File                     = {Published version:Finke-2012-p499-504.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6252287},
  Zone                     = {-}
}

@InProceedings{Fletcher-2011-p106-113,
  Title                    = {Security Requirements Analysis, Specification, Prioritization and Policy Development in Cyber-Physical Systems},
  Author                   = {Fletcher, K.K. and Liu, X.F.},
  Booktitle                = {Secure Software Integration Reliability Improvement Companion (SSIRI-C), 2011 5th International Conference on},
  Year                     = {2011},
  Month                    = {June},
  Pages                    = {106-113},

  Doi                      = {10.1109/SSIRI-C.2011.25},
  File                     = {Published version:Fletcher-2011-p106-113.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {formal specification;health care;medical computing;object-oriented programming;security of data;systems analysis;ubiquitous computing;Ayushman;Pervasive Health Monitoring System;cyber-physical system;hardware components;hardware-software interaction;malactivity;mission-critical process;mitigation options;policy development;prevention options;relative priority analysis;security requirement prioritization;security requirement specification;security requirements analysis;security threat analysis;software components;swim lane diagram;Analytical models;Monitoring;Object oriented modeling;Object recognition;Security;Software;Unified modeling language;CPS security requirements;CPS security requirements prioritization;cyber-physical systems (CPS);hardware-software interaction;high order object oriented modeling technique},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6004511&isnumber=6004287},
  Zone                     = {-}
}

@Article{Floyd-2008-p25-31,
  Title                    = {Escaping arc danger},
  Author                   = {Floyd, H.L., II},
  Journal                  = {Industry Applications Magazine, IEEE},
  Year                     = {2008},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {25-31},
  Volume                   = {14},

  Abstract                 = {The paper presents the arc flash assessment in petrochemical and chemical industry. This article provides the IAS Cement Industry Committee information and references to help apply the expertise in risk and safety management experience for the specific hazard of electrical arc flash. The information and references provided include the following: (1) injury frequency and prediction methods (2) cost information to help in building a business case to enable fact-based management decision on priority and resource allocation (3) comparison of several implementation options, including a recommendation for the best long-term strategy for minimizing both injuries and costs (4) considerations to help assure effective arc hazard analysis (5) examples of control measures that can be implemented based on the results from an arc hazard assessment (6) sources for more information. The author's intent is to aid in accelerating the implementation of technology and work practices proven effective through the technical forums sponsored by the PCIC of the IAS. The harmonization of workplace electrical safety standards biased toward optimizing the integration of inherently safer technology, improved safe work practices, and state-of-the-art safety management systems will improve the electrical safety of workers in all industries.},
  Doi                      = {10.1109/MIAS.2008.918504},
  File                     = {Published version:Floyd-2008-p25-31.pdf:PDF},
  Impactfactor             = {0.483},
  ISSN                     = {1077-2618},
  Keywords                 = {arcs (electric);electrical safety;occupational safety;arc flash hazard assessment;chemical industry;electrical safety;petrochemical industry;Cement industry;Chemical industry;Costs;Electrical safety;Hazards;Injuries;Petrochemicals;Radio spectrum management;Resource management;Risk management},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {~，不相干},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4489923&isnumber=4489909},
  Zone                     = {4Q}
}

@Article{Fovino-2012-p3943-3950,
  Title                    = {Critical State-Based Filtering System for Securing SCADA Network Protocols},
  Author                   = {Fovino, I.N. and Coletta, A. and Carcano, A. and Masera, M.},
  Journal                  = {Industrial Electronics, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {3943-3950},
  Volume                   = {59},

  Abstract                 = {The security of System Control and Data Acquisition (SCADA) systems is one of the most pressing subjects in industrial systems, particularly for those installations actively using the public network in order to provide new features and services. In this paper, we present an innovative approach to the design of filtering systems based on the state analysis of the system being monitored. The aim is to detect attacks composed of a set of “SCADA” commands that, while licit when considered in isolation on a single-packet basis, can disrupt the correct behavior of the system when executed in particular operating states. The proposed firewall detects these complex attacks thanks to an internal representation of the controlled SCADA system. Furthermore, we detail the design of the architecture of the firewall for systems that use the ModBus and DNP3 protocols, and the implementation of a prototype, providing experimental comparative results that confirm the validity of the proposed approach.},
  Doi                      = {10.1109/TIE.2011.2181132},
  File                     = {Published version:Fovino-2012-p3943-3950.pdf:PDF},
  Impactfactor             = {5.165},
  ISSN                     = {0278-0046},
  Keywords                 = {SCADA systems;authorisation;computer network security;field buses;transport protocols;DNP3 protocol;ModBus protocol;SCADA network protocol security;critical state-based filtering system;firewall architecture design;industrial system;public network;single packet basis;state analysis;system control and data acquisition system;IP networks;Measurement;Monitoring;Power generation;Protocols;SCADA systems;Security;Critical state analysis;SCADA systems;cyber security;firewall},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6111289&isnumber=6191366},
  Zone                     = {1Q}
}

@Article{Freedman-1991-p347-358,
  Title                    = {A knowledge-based methodology for tuning analytical models},
  Author                   = {Freedman, R.S. and Stuzin, G.J.},
  Journal                  = {Systems, Man and Cybernetics, IEEE Transactions on},
  Year                     = {1991},

  Month                    = {Mar},
  Number                   = {2},
  Pages                    = {347-358},
  Volume                   = {21},

  Doi                      = {10.1109/21.87083},
  File                     = {Published version:Freedman-1991-p347-358.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0018-9472},
  Keywords                 = {computer aided analysis;knowledge based systems;logic programming;analytical model tuning;knowledge-based methodology;logic programming system;macroeconometric forecasting model;Analytical models;Decision making;Economic forecasting;Finance;History;Humans;Logic programming;Macroeconomics;Predictive models;Statistical analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=87083&isnumber=2835},
  Zone                     = {-}
}

@InProceedings{Frigault-2008-p698-703,
  Title                    = {Measuring Network Security Using Bayesian Network-Based Attack Graphs},
  Author                   = {Frigault, M. and Lingyu Wang},
  Booktitle                = {Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International},
  Year                     = {2008},
  Month                    = {July},
  Pages                    = {698-703},

  Abstract                 = {Given the increasing dependence of our societies on information systems, the overall security of these systems should be measured and improved. Existing work generally focuses on measuring individual vulnerabilities instead of measuring their combined effects. Recent research has explored the application of attack graphs and probabilistic security metrics to address this challenge. However, such work usually assumes metrics of individual vulnerabilities to be independently distributed and combines them in an arbitrary manner. They cannot address more realistic cases, such as exploiting one vulnerability makes another vulnerability easier to exploit. In this paper, we propose to model probability metrics based on attack graphs as a special Bayesian Network. This approach provides a sound theoretical foundation to such metrics. It can also provide the capabilities of using conditional probabilities to address the general cases of interdependency between vulnerabilities.},
  Doi                      = {10.1109/COMPSAC.2008.88},
  File                     = {Published version:Frigault-2008-p698-703.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0730-3157},
  Keywords                 = {belief networks;security of data;Bayesian network-based attack graphs;network security;probabilistic security metrics;Application software;Bayesian methods;Computer applications;Computer networks;Computer security;Information security;Information systems;Societies;Software measurement;Systems engineering and theory;Attack;Bayesian;Graph;Measuring;Network;Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4591650&isnumber=4591503},
  Zone                     = {-}
}

@InProceedings{Frigault-2008-p698-703a,
  Title                    = {Measuring Network Security Using Bayesian Network-Based Attack Graphs},
  Author                   = {Frigault, M. and Lingyu Wang},
  Booktitle                = {Computer Software and Applications, 2008. COMPSAC '08. 32nd Annual IEEE International},
  Year                     = {2008},
  Month                    = {July},
  Pages                    = {698-703},

  Doi                      = {10.1109/COMPSAC.2008.88},
  File                     = {Published version:Frigault-2008-p698-703a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0730-3157},
  Keywords                 = {belief networks;security of data;Bayesian network-based attack graphs;network security;probabilistic security metrics;Application software;Bayesian methods;Computer applications;Computer networks;Computer security;Information security;Information systems;Societies;Software measurement;Systems engineering and theory;Attack;Bayesian;Graph;Measuring;Network;Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4591650&isnumber=4591503},
  Zone                     = {-}
}

@InProceedings{Fronczak-1997-p259-259,
  Title                    = {A top-down approach to high-consequence fault analysis for software systems},
  Author                   = {Fronczak, E.},
  Booktitle                = {Software Reliability Engineering, 1997. Proceedings., The Eighth International Symposium on},
  Year                     = {1997},
  Month                    = {Nov},
  Pages                    = {259-},

  Abstract                 = {Summary form only given, as follows. Even if software code is fault-free, hardware failures can alter values in memory, possibly where the code itself is stored, causing a computer system to reach an unacceptable state. Microprocessor systems are used to perform many safety and security functions where a design goal is to eliminate single-point failures such as these. One design approach is to use multiple processors, compare the outputs, and assume a failure has occurred if the outputs don't agree. In systems where the design is constrained to a single processor, however, analytical methods are needed to identify potential single-point failures at the bit level so that an effective fault-tolerant strategy can be employed. This paper describes a top-down methodology, based upon fault tree analysis, that has been used to identify potential high-consequence faults in microprocessor-based systems. The key to making the fault tree analysis tractable is to effectively incorporate appropriate design features such as software path control and checksums so that complicated branches of the fault tree can be terminated early. The analysis uses simplified software flow diagrams depicting relevant code elements. Pertinent sections of machine language are then examined to identify suspect hardware. A comparison of this methodology with approaches based upon failure modes and effects analysis (FMEA) is made. The methodology is demonstrated through a simple example. Use of fault trees to show that software code is free of safety or security faults is also demonstrated},
  Doi                      = {10.1109/ISSRE.1997.630873},
  File                     = {Published version:Fronczak-1997-p259-259.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {fault trees;flowcharting;safety-critical software;software fault tolerance;system recovery;analytical methods;bit level;checksums;complicated branch termination;design features;failure modes and effect analysis;fault tree analysis;fault-free software code;fault-tolerant strategy;hardware failures;high-consequence fault analysis;machine language;memory values;microprocessor systems;safety functions;security functions;single-point failures;software flow diagrams;software path control;software systems;suspect hardware identification;top-down approach;tractability;unacceptable system state;Failure analysis;Fault diagnosis;Fault tolerant systems;Fault trees;Hardware;Laboratories;Microprocessors;Security;Software safety;Software systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=630873&isnumber=13710},
  Zone                     = {-}
}

@InProceedings{Fu-2010-p154-159,
  Title                    = {A Dynamic Risk Assessment Framework Using Principle Component Analysis with Projection Pursuit in Ad Hoc Networks},
  Author                   = {Cai Fu and Jihang Ye and Li Zhang and Yunhe Zhang and Han LanSheng},
  Booktitle                = {Ubiquitous Intelligence Computing and 7th International Conference on Autonomic Trusted Computing (UIC/ATC), 2010 7th International Conference on},
  Year                     = {2010},
  Month                    = {Oct},
  Pages                    = {154-159},

  Abstract                 = {Conventional risk assessments in Ad-Hoc Networks always require sample satisfy specific distribution with large quantity and establish models through subjective judgment, these methods lack general applicability, objectivity and credibility. Moreover, some models only focus on single time point evaluation and failed to thoroughly reveal dynamic behavioral character. To solve these problems and make correct assessment, we propose RAPCA-PP model on the basis of Projection Pursuit theory to conduct both risk assessment and attribute analysis over dynamic time sequence. RAPCA-PP is completely data-driven and can be applied in small sample, incomplete data and no-prior experience. Compared with Grey Relations Projection, it boasts both better accuracy and higher discrimination. Since RAPCA-PP makes evaluation along time axis, it can reflect MANET's node series behavioral features. Experiments demonstrate that assessment with eliminated attributes can also correctly reflect each node's performance and be utilized for IDS realization. RAPCA-PP proved to be suitable for real MANET working scenarios.},
  Doi                      = {10.1109/UIC-ATC.2010.42},
  File                     = {Published version:Fu-2010-p154-159.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {ad hoc networks;principal component analysis;telecommunication security;ad-hoc networks;dynamic risk assessment;network security;principle component analysis;projection pursuit;Analytical models;Data models;Indexes;Mobile ad hoc networks;Mobile computing;Risk management;Ad-Hoc Network Security;Principle Component Analysis;Projection Pursuit;RAGA;Risk Assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5667211&isnumber=5667097},
  Zone                     = {-}
}

@InProceedings{Fu-2011-p124-128,
  Title                    = {The information security risk assessment based on AHP and fuzzy comprehensive evaluation},
  Author                   = {Sha Fu and Hangjun Zhou},
  Booktitle                = {Communication Software and Networks (ICCSN), 2011 IEEE 3rd International Conference on},
  Year                     = {2011},
  Month                    = {May},
  Pages                    = {124-128},

  Doi                      = {10.1109/ICCSN.2011.6014018},
  File                     = {Published version:Fu-2011-p124-128.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer network security;decision making;educational institutions;fuzzy set theory;information systems;risk management;AHP;analytic hierarchy process;fuzzy comprehensive evaluation;information security risk assessment;information system security;qualitative indicators;risk management;university network system;Artificial intelligence;Bismuth;TV;Analytic Hierarchy Process (AHP);fuzzy comprehensive evaluation;fuzzy relation matrix;information security;risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6014018&isnumber=6013532},
  Zone                     = {-}
}

@Article{Fujii-2006-p3310-3313,
  Title                    = {Concavity of the auxiliary function appearing in quantum reliability function},
  Author                   = {Fujii, J.I. and Nakamoto, R. and Yanagi, K.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2006},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {3310-3313},
  Volume                   = {52},

  Abstract                 = {Reliability functions characterize the asymptotic behavior of the error probability for transmission of data on a channel. Holevo introduced the quantum channel, and gave an expression for a random-coding lower bound involving an auxiliary function. Holevo, Ogawa, and Nagaoka conjectured that this auxiliary function is concave. Here we give a proof of this conjecture.},
  Doi                      = {10.1109/TIT.2006.876248},
  File                     = {Published version:Fujii-2006-p3310-3313.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {channel coding;error statistics;quantum communication;random codes;reliability theory;auxiliary function concavity;channel coding;data transmission;error probability;quantum reliability function;random-coding;Art;Australia;Capacity planning;Channel coding;Cities and towns;Error probability;Information theory;Probability distribution;Quantum mechanics;Reliability theory;Quantum information theory;quantum reliability function;random coding exponent},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1650375&isnumber=34609},
  Zone                     = {2Q}
}

@Article{Furdek-2010-p1000-1009,
  Title                    = {Attack-Aware Wavelength Assignment for Localization of In-band Crosstalk Attack Propagation},
  Author                   = {Furdek, M. and Skorin-Kapov, N. and Grbac, M.},
  Journal                  = {Optical Communications and Networking, IEEE/OSA Journal of},
  Year                     = {2010},

  Month                    = {November},
  Number                   = {11},
  Pages                    = {1000-1009},
  Volume                   = {2},

  Abstract                 = {The high data rates employed by wavelength division multiplexing transparent optical networks make them most suitable for today's growing network traffic demands. However, their transparency imposes several vulnerabilities in network security, enabling malicious signals to propagate from the source to other parts of the network without losing their attacking capabilities. Furthermore, detecting, locating the source, and localizing the spreading of such physical-layer attacks is more difficult since monitoring must be performed in the optical domain. While most failure and attack management approaches focus on network recovery after a fault or an attack has already occurred, we suggest a novel safety strategy, proposing a prevention-oriented method to aid attack localization and source identification in the planning phase. In this paper, we propose attack-aware wavelength assignment that minimizes the worst-case potential propagation of in-band crosstalk jamming attacks. We define a new objective criterion for the wavelength assignment (WA) problem, called the propagating crosstalk attack radius (P-CAR), and develop heuristic algorithms aimed at minimizing both the P-CAR and the number of wavelengths used. Our aim is to achieve better protection, but without the need for extra resources. We compare our algorithms with existing WA approaches, illustrating their benefits with respect to transparent optical networks' security, as well as the associated wavelength utilization.},
  Doi                      = {10.1364/JOCN.2.001000},
  File                     = {Published version:Furdek-2010-p1000-1009.pdf:PDF},
  Impactfactor             = {1.433},
  ISSN                     = {1943-0620},
  Keywords                 = {jamming;light propagation;optical communication;optical crosstalk;optical fibre networks;telecommunication security;wavelength assignment;wavelength division multiplexing;attack management;attack-aware wavelength assignment;failure management;heuristic algorithms;inband crosstalk attack propagation;inband crosstalk jamming attacks;malicious signal propagation;network recovery;network security;physical-layer attacks;prevention-oriented method;transparent optical networks;wavelength division multiplexing;Crosstalk;Jamming;Optical switches;Topology;Upper bound;Wavelength assignment;Bin packing;Graph coloring;In-band crosstalk;Physical-layer attacks;Transparent optical networks;Wavelength assignment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5614032&isnumber=5670333},
  Zone                     = {3Q}
}

@InProceedings{Gueven-2011-p663-670,
  Title                    = {A Real-Time Risk Assessment and Mitigation Engine Based on Dynamic Context},
  Author                   = {G\"{u}ven, S. and Barbu, C.M.},
  Booktitle                = {Services Computing (SCC), 2011 IEEE International Conference on},
  Year                     = {2011},
  Month                    = {July},
  Pages                    = {663-670},

  Abstract                 = {Today, the risk of a service related change is typically assessed at change record creation time by a Change Requester either manually or through answering a fixed set of questions. Assessing the risk of a change, thus, relies heavily on one person's opinion. Further, in the questionnaire method, a fixed set of questions implies that the change context is assumed to be the same regardless of the type of change being raised, whereas in practice, no two changes are truly identical. Such deficiencies of the standard practice to assess risk of a change may, thus, result in inaccurate assessment, which can lead to unmitigated risks, ultimately materializing as failures. We present a novel Risk Engine, which takes into account a rich, dynamic change context to calculate and mitigate the risk of a service related change in real-time with increased accuracy and reliability. We describe the recipe for creating the Risk Engine along with several user studies we have conducted to justify our design choices at each step. Our initial pilot results reveal that it is possible to drive the number of change failures down significantly using our novel Risk Engine.},
  Doi                      = {10.1109/SCC.2011.74},
  File                     = {Published version:Gueven-2011-p663-670.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {DP industry;management of change;risk management;dynamic change context;mitigation engine;real-time risk assessment;risk engine;service related change;Context;Documentation;Engines;Manuals;Real time systems;Risk management;Systematics;Change Management;Dynamic Context;Risk Assesment;Risk Management;Risk Mitigation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6009320&isnumber=6009236},
  Zone                     = {-}
}

@Article{Gaeta-2014-p1482-1494,
  Title                    = {Exploiting Rateless Codes and Belief Propagation to Infer Identity of Polluters in MANET},
  Author                   = {Gaeta, R. and Grangetto, M. and Loti, R.},
  Journal                  = {Mobile Computing, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {1482-1494},
  Volume                   = {13},

  Abstract                 = {In this paper, we consider a scenario where nodes in a MANET disseminate data chunks using rateless codes. Any node is able to successfully decode any chunk by collecting enough coded blocks from several other nodes without any coordination. We consider the problem of identifying malicious nodes that launch a pollution attack by deliberately modifying the payload of coded blocks before transmitting. It follows that the original chunk can only be obtained if there are no malicious nodes among the chunk providers. In this paper we propose SIEVE, a fully distributed technique to infer the identity of malicious nodes. A node creates what we termed a check whenever a chunk is decoded; a check is a pair composed of the set of other nodes that provided coded blocks used to decode the chunk (the chunk uploaders) and a flag indicating whether the chunk is corrupted or not. SIEVE exploits rateless codes to detect chunk integrity and belief propagation to infer the identity of malicious nodes. In particular, every node autonomously constructs its own bipartite graph (a.k.a. factor graph in the literature) whose vertexes are checks and nodes, respectively. Then, it periodically runs the belief propagation algorithm on its factor graph to infer the probability of other nodes being malicious. We show by running detailed simulations using ns-3 that SIEVE is very accurate and robust under several attack scenarios and deceiving actions. We discuss how the topological properties of the factor graph impacts SIEVE performance and show that nodes speed in the MANET plays a role on the identification accuracy. Furthermore, an interesting trade-off between coding efficiency and SIEVE accuracy, completeness, and reactivity is discovered. We also show that SIEVE is efficient requiring low computational, memory, and communication resources.},
  Doi                      = {10.1109/TMC.2013.137},
  File                     = {Published version:Gaeta-2014-p1482-1494.pdf:PDF},
  Impactfactor             = {2.395},
  ISSN                     = {1536-1233},
  Keywords                 = {codes;mobile ad hoc networks;telecommunication security;MANET;SIEVE;belief propagation algorithm;bipartite graph;chunk uploaders;communication resources;data chunks;factor graph;malicious nodes;memory resources;ns-3;polluters;rateless codes;Accuracy;Belief propagation;Decoding;Equations;Mobile ad hoc networks;Peer-to-peer computing;Protocols;Algorithm/protocol design and analysis;Distributed applications;Error control codes;MANET;Mobile Computing;belief propagation;malicious node identification;pollution attack;rateless codes;statistical inference},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6636310&isnumber=6845393},
  Zone                     = {2Q}
}

@Article{Gain-2015-p120-131,
  Title                    = {A dynamic assessment of water scarcity risk in the Lower Brahmaputra River Basin: An integrated approach },
  Author                   = {Animesh K. Gain and Carlo Giupponi},
  Journal                  = {Ecological Indicators },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {120 - 131},
  Volume                   = {48},

  Abstract                 = {Abstract Many international river basins are likely to experience increasing water scarcity over the coming decades. This water scarcity is not rooted only in the limitation of resources, i.e. the shortage in the availability of freshwater relative to water demand, but also on social factors (e.g. flawed water planning and management approaches, institutional incapability to provide water services, unsustainable economic policies). Therefore, the assessment of water scarcity risks is not limited to the assessment of physical water supply and demand, but it requires also consideration of several socio-economic factors. In this study, we provide a comprehensive dynamic assessment of water scarcity risks for the Lower Brahmaputra river basin, a region where the hydrological impact of climate change is expected to be particularly strong and population pressure is high. The basin area of Brahmaputra River lies among four different countries: China, India, Bangladesh and Bhutan. For water scarcity assessment, we propose a novel integration of different approaches: (i) the assessment of water scarcity risk, considering complex social-ecological system; (ii) the analysis of dynamic behaviour of the system; (iii) exploration of participatory approach in which limited number of stakeholders identify the most relevant issues with reference to water scarcity risks and provide their preferences for the aggregation of risk assessment indicators. Results show that water scarcity risk is expected to slightly increase and to fluctuate remarkably as a function of the hazard signal. Social indicators show trends that can at least partially compensate the increasing trend of the drought index. The results of this study are intended to be used for contributing to planned adaptation of water resources systems, in Lower Brahmaputra River Basin. },
  Doi                      = {http://dx.doi.org/10.1016/j.ecolind.2014.07.034},
  File                     = {Published version:Gain-2015-p120-131.pdf:PDF},
  Impactfactor             = {2.89},
  ISSN                     = {1470-160X},
  Keywords                 = {Risk},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，不是很相关},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1470160X14003434},
  Zone                     = {3Q}
}

@Article{Gal-2013-p59-80,
  Title                    = {Segregation of magmatic fluids and their potential in the mobilization of platinum-group elements in the South Kawishiwi Intrusion, Duluth Complex, Minnesota — Evidence from petrography, apatite geochemistry and coexisting fluid and melt inclusions },
  Author                   = {Benedek Gál and Ferenc Molnár and Tibor Guzmics and Aberra Mogessie and Csaba Szabó and Dean M. Peterson},
  Journal                  = {Ore Geology Reviews },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {59 - 80},
  Volume                   = {54},

  Abstract                 = {Abstract Pegmatitic and other felsic rock pockets and dike-like intrusions are abundant in the South Kawishiwi Intrusion of the Duluth Complex, including the basal, Cu–Ni–PGE mineralized units. These occurrences are found as pockets, pods or as veins and contain abundant accessory apatite and quartz. Quartz hosts primary fluid inclusions as well as silicate melt inclusions. Combined microthermometry and Raman spectroscopy helped to determine the bulk composition of primary fluid inclusions that are CO2-rich (95&#xa0;mol%) and contain small amounts of \{H2O\} (4.5&#xa0;mol%), \{CH4\} (0.4&#xa0;mol%) and trace N2, respectively. This combined technique also made it possible to measure total homogenization temperatures of the inclusions (Thtot&#xa0;=&#xa0;~&#xa0;225&#xa0;±&#xa0;10&#xa0;°C), otherwise not detectable during microthermometry. Silicate melt inclusions have been quenched to produce homogeneous glasses corresponding to the original melt. Composition of the entrapped melt is granitoid, peraluminous and is very poor in mafic components. We interpret the melt as a product of partial melting of the footwall rocks due to the contact effect of the South Kawishiwi Intrusion. The presence of \{CO2\} in the vapor bubbles of the quenched melt inclusions and petrographic evidence suggest that the fluid and melt inclusion assemblages are coeval. The composition of the fluid and melt phase implies that the fluid originates from the mafic magma of the South Kawishiwi Intrusion and the fluid and melt phases coexisted as a heterogeneous melt–fluid system until entrapment of the inclusions. Coexistence of primary fluid and melt inclusions makes it possible to calculate a minimum entrapment pressure (~&#xa0;1.7&#xa0;kbar) and thus estimate formation depth (~&#xa0;5.8&#xa0;km) for the inclusions. Chlorine is suggested to behave compatibly in the silicate melt phase in the fluid–melt system represented by the inclusions, indicated by the high (up to 0.3%) Cl-concentrations of the silicate melt and CO2-rich nature of the fluid. Apatite halogen-contents provide further details on the behavior of Cl. Apatite in pegmatitic pockets often has elevated Cl-concentrations compared to troctolitic rocks, suggesting enrichment of Cl with progressive crystallization. Systematic trends of Cl-loss at some differentiated melt pockets suggest that in some places Cl exsolved into a fluid phase and migrated away from its source. The segregation of Cl from the melt is probably inhibited by the presence of CO2-rich fluids until the last stages of crystallization, increasing the potential for the development of late-stage saline brines. Platinum-group minerals are often present in microcracks in silicate minerals, in late-stage differentiated sulfide veinlets and in association with chlorapatite, indicating the potential role of Cl-bearing fluids in the final distribution of PGEs. },
  Doi                      = {http://dx.doi.org/10.1016/j.oregeorev.2013.03.001},
  File                     = {Published version:Gal-2013-p59-80.pdf:PDF},
  Impactfactor             = {2.417},
  ISSN                     = {0169-1368},
  Keywords                 = {Duluth Complex},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0169136813000553},
  Zone                     = {2Q}
}

@Article{Galloway-2013-p860-880,
  Title                    = {Introduction to Industrial Control Networks},
  Author                   = {Galloway, B. and Hancke, G.P.},
  Journal                  = {Communications Surveys Tutorials, IEEE},
  Year                     = {2013},

  Month                    = {Second},
  Number                   = {2},
  Pages                    = {860-880},
  Volume                   = {15},

  Abstract                 = {An industrial control network is a system of interconnected equipment used to monitor and control physical equipment in industrial environments. These networks differ quite significantly from traditional enterprise networks due to the specific requirements of their operation. Despite the functional differences between industrial and enterprise networks, a growing integration between the two has been observed. The technology in use in industrial networks is also beginning to display a greater reliance on Ethernet and web standards, especially at higher levels of the network architecture. This has resulted in a situation where engineers involved in the design and maintenance of control networks must be familiar with both traditional enterprise concerns, such as network security, as well as traditional industrial concerns such as determinism and response time. This paper highlights some of the differences between enterprise and industrial networks, presents a brief history of industrial networking, gives a high level explanation of some operations specific to industrial networks, provides an overview of the popular protocols in use and describes current research topics. The purpose of this paper is to serve as an introduction to industrial control networks, aimed specifically at those who have had minimal exposure to the field, but have some familiarity with conventional computer networks.},
  Doi                      = {10.1109/SURV.2012.071812.00124},
  File                     = {Published version:Galloway-2013-p860-880.pdf:PDF},
  Impactfactor             = {4.818},
  ISSN                     = {1553-877X},
  Keywords                 = {Internet;computerised monitoring;condition monitoring;controller area networks;field buses;industrial control;interconnected systems;protocols;Ethernet;Web standards;enterprise networks;industrial control network;industrial environments;industrial networking;interconnected equipment;physical equipment control;physical equipment monitoring;protocols;Control systems;Monitoring;Process control;SCADA systems;Software developmnet;control;fieldbus;industrial;networks},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6248648&isnumber=6512259},
  Zone                     = {1Q}
}

@Article{Gandhi-2014-p-,
  Title                    = {Characterization and Prediction of Adverse Events from Intensive Chronic Heart Failure Management and Effect on Quality of Life: Results from the ProBNP Outpatient Tailored Chronic Heart Failure Therapy (PROTECT) Study },
  Author                   = {Parul U. Gandhi and Jackie Szymonifka and Shweta R. Motiwala and Arianna M. Belcher and James L. Januzzi Jr. and Hanna K. Gaggin},
  Journal                  = {Journal of Cardiac Failure },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {AbstractBackground Serious adverse events (SAE) from heart failure (HF) therapy are frequent; however, techniques to identify at-risk patients are inadequate. Furthermore, the relationship between SAE, quality of life (QOL), and cardiac structure are unknown. Methods and Results 151 symptomatic patients with systolic \{HF\} were followed for a mean of 10 months. In this post hoc analysis, treatment-related \{SAE\} included acute renal failure, dizziness, hypo/hyperkalemia, hypotension, or syncope. At one year, 21 treatment-related \{SAE\} occurred. No difference in \{SAE\} existed between the NT-proBNP-guided arm and the standard-of-care arm (P=0.20). At baseline, patients who suffered \{SAE\} were less likely to be receiving beta-blockers (85.7% versus 97.7%; P=0.009), had worse functional class and lower chloride levels. Patients who experienced \{SAE\} had less improvement in their Minnesota Living with Heart Failure Questionnaire scores and had a trend towards reduced echocardiographic reverse remodeling over follow up. Univariable and multivariable analyses were conducted to develop a risk score for \{SAE\} prediction; patients in the highest risk quartile had the shortest time-to-first cardiovascular event (P=0.01). Conclusion NT-proBNP-guided \{HF\} care is safe. Experiencing treatment-related \{SAE\} is associated with worse \{QOL\} and potentially reduced reverse remodeling. A risk score to prospectively predict \{SAE\} in aggressive \{HF\} management was developed. Clinical Trial Registration www.clinicaltrials.gov; NCT#00351390 },
  Doi                      = {http://dx.doi.org/10.1016/j.cardfail.2014.10.008},
  File                     = {Published version:Gandhi-2014-p-.pdf:PDF},
  Impactfactor             = {3.32},
  ISSN                     = {1071-9164},
  Keywords                 = {Heart Failure},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1071916414012524},
  Zone                     = {3Q}
}

@InProceedings{Gang-2011-p157-160,
  Title                    = {Design of interactive electronic technical manual for the thermohydraulic measuring instruments of nuclear power plant},
  Author                   = {Zhou Gang and Ge Shenqi and Peng Wei},
  Booktitle                = {Electronic Measurement Instruments (ICEMI), 2011 10th International Conference on},
  Year                     = {2011},
  Month                    = {Aug},
  Pages                    = {157-160},
  Volume                   = {2},

  Abstract                 = {The interactive electronic technical manual of thermohydraulic measuring instruments of nuclear power plant(NPP) was designed to improve the capacity of the fault detection and maintenance technical support by integrating the technique of visual, fault tree analysis, fault recognition, database management and fuzzy query. The general method and procedure for fault detection of thermohydraulic measuring instruments are provided, and the fault mechanism of thermohydraulic measuring instruments is included in this manual. The schematic circuit diagram of thermohydraulic measuring instruments and its frequent faulty phenomena and faulty part are also included. The approach of fault detection and maintenance that is most important in the fault detection and maintenance process is presented in visual and interactive way. The manual is of functions of teaching, fuzzy inquire, add data, noting and print. The interface of man-machine is favorable. The results reveal that employing the designed manual can improve the capacity of fault detection and maintenance technical support of thermohydraulic measuring instruments of nuclear power plant.},
  Doi                      = {10.1109/ICEMI.2011.6037787},
  File                     = {Published version:Gang-2011-p157-160.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {database management systems;fault trees;instruments;nuclear power stations;technical support services;database management;fault detection;fault recognition;fault tree analysis;fuzzy query;interactive electronic technical manual;maintenance technical support;nuclear power plant;schematic circuit diagram;thermohydraulic measuring instruments;Circuit faults;Fault detection;Instruments;Maintenance engineering;Manuals;Temperature measurement;Visualization;fault detection;interactive electronic technical manual;maintenance;nuclear power plant;thermohydraulic measuring instrument},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6037787&isnumber=6037752},
  Zone                     = {-}
}

@Article{Garber-2012-p17-19,
  Title                    = {News Briefs},
  Author                   = {Garber, Lee},
  Journal                  = {Computer},
  Year                     = {2012},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {17-19},
  Volume                   = {45},

  Abstract                 = {Topics include a vulnerability that could let hackers gain control of prison computer systems, an optical device that could enable chips to process data faster, a US agency approving key elements for using white spaces in wireless communications, development of real-time complex-event-processing technology, an electronic device that helps rural Indian small-business owners keep better records, and eyeglasses that use technology to change their prescription on demand.},
  Doi                      = {10.1109/MC.2012.60},
  File                     = {Published version:Garber-2012-p17-19.pdf:PDF},
  Impactfactor             = {1.675},
  ISSN                     = {0018-9162},
  Keywords                 = {PixelOptics;Spectrum Bridge;broadband transmitters;complex event processing;databases;electronic focusing eyewear;emPower;industrial-control system security;optical diodes;white spaces},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6155649&isnumber=6155638},
  Zone                     = {2Q}
}

@Article{Garcez-2014-p624-632,
  Title                    = {Multidimensional Risk Assessment of Manhole Events as a Decision Tool for Ranking the Vaults of an Underground Electricity Distribution System},
  Author                   = {Garcez, T.V. and De Almeida, A.T.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {624-632},
  Volume                   = {29},

  Abstract                 = {Even though an underground electricity distribution system is safer than an overhead system, several accidents have occurred in them. Assessing the risk of hundreds or even thousands of underground vaults is a hard task. Furthermore, given the large variability in external and internal environments and, hence, there being a wide range of possible consequences when an accident occurs, an approach to risk assessment under a multidimensional view is required. Moreover, in terms of decision making, the aggregation of the decision maker's preferences in modeling, by multiple-criteria decision-making methods, is more complete, comprehensive, and, in particular, includes considering the decision maker's desires. Therefore, this study puts forward a multidimensional assessment of the risks from underground vaults by generating a decision tool, which ranks the vaults in a risk hierarchy. Multiattribute utility theory was used to achieve this ranking. An application was generated to demonstrate the applicability of the model, under the following aspects of consequences: those that are human, financial, and operational; and disruptions to local vehicular traffic. The use of information arising from analysis of the differences between risks enabled the decision maker to make an in-depth analysis of the range of possibilities over which alternatives may be chosen in order to implement preventive actions.},
  Doi                      = {10.1109/TPWRD.2013.2273083},
  File                     = {Published version:Garcez-2014-p624-632.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {decision making;risk management;underground distribution systems;utility theory;decision tool;manhole events;multiattribute utility theory;multidimensional risk assessment;multiplecriteria decision-making methods;underground electricity distribution system;underground vaults;Accidents;Companies;Electricity;Explosions;Injuries;Risk management;Accidents;arc discharges;decision making;manhole events;multiattribute utility theory (MAUT);multidimensional risk;risk analysis;safety;uncertainty;underground power distribution lines},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6571256&isnumber=6776443},
  Zone                     = {3Q}
}

@InProceedings{Gaska-2013-p1-19,
  Title                    = {Assessing dual use embedded security for IMA},
  Author                   = {Gaska, T.},
  Booktitle                = {Digital Avionics Systems Conference (DASC), 2013 IEEE/AIAA 32nd},
  Year                     = {2013},
  Month                    = {Oct},
  Pages                    = {4A2-1-4A2-19},

  Doi                      = {10.1109/DASC.2013.6712576},
  File                     = {Published version:Gaska-2013-p1-19.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2155-7195},
  Keywords                 = {avionics;embedded systems;security of data;AutoSAR project;CPI;E-safety vehicle intrusion protected application project;EVITA project;FACE;GIG IA;IA;IMAs designs;IP;PRESERVE project;SoC;SoS;TM;TP;automotive open systems architecture project;automotive security;commercial industry levels;critical program information protection;cyber security;cyber warfare;dual use embedded security assessment;embedded system infrastructure;embedded systems;enhanced information assurance;future airborne capability environment;global information grid;global supply chain;integrated security initiatives;integrated system-on-chip hardware components;intellectual property;interconnected software based devices;next generation COTS open standards;next generation automobile architecture initiatives;next generation systems-of-systems;preparing secure vehicle-to-X communication system project;security extensions;security threat complexity;transistor chips;trusted manufacturing infrastructure;trusted processing;unmanned integrated modular avionics design;Aerospace electronics;Automobiles;Automotive engineering;Computer architecture;Next generation networking;Security;Software},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6712576&isnumber=6712497},
  Zone                     = {-}
}

@Article{Gasteiger-2012-p2178-2187,
  Title                    = {Automatic Detection and Visualization of Qualitative Hemodynamic Characteristics in Cerebral Aneurysms},
  Author                   = {Gasteiger, R. and Lehmann, D.J. and van Pelt, R. and Janiga, G. and Beuing, O. and Vilanova, A. and Theisel, H. and Preim, B.},
  Journal                  = {Visualization and Computer Graphics, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {2178-2187},
  Volume                   = {18},

  Abstract                 = {Cerebral aneurysms are a pathological vessel dilatation that bear a high risk of rupture. For the understanding and evaluation of the risk of rupture, the analysis of hemodynamic information plays an important role. Besides quantitative hemodynamic information, also qualitative flow characteristics, e.g., the inflow jet and impingement zone are correlated with the risk of rupture. However, the assessment of these two characteristics is currently based on an interactive visual investigation of the flow field, obtained by computational fluid dynamics (CFD) or blood flow measurements. We present an automatic and robust detection as well as an expressive visualization of these characteristics. The detection can be used to support a comparison, e.g., of simulation results reflecting different treatment options. Our approach utilizes local streamline properties to formalize the inflow jet and impingement zone. We extract a characteristic seeding curve on the ostium, on which an inflow jet boundary contour is constructed. Based on this boundary contour we identify the impingement zone. Furthermore, we present several visualization techniques to depict both characteristics expressively. Thereby, we consider accuracy and robustness of the extracted characteristics, minimal visual clutter and occlusions. An evaluation with six domain experts confirms that our approach detects both hemodynamic characteristics reasonably.},
  Doi                      = {10.1109/TVCG.2012.202},
  File                     = {Published version:Gasteiger-2012-p2178-2187.pdf:PDF},
  Impactfactor             = {1.898},
  ISSN                     = {1077-2626},
  Keywords                 = {computational fluid dynamics;data visualisation;haemodynamics;medical image processing;object detection;CFD;automatic detection;automatic visualization;blood flow measurements;cerebral aneurysms;computational fluid dynamics;impingement zone;inflow jet boundary contour;minimal visual clutter;occlusions;pathological vessel dilatation;qualitative flow characteristics;qualitative hemodynamic characteristics;quantitative hemodynamic information;risk of rupture;Aneurysm;Data visualization;Hemodynamics;Rendering (computer graphics);Surface morphology;CFD;Cerebral aneurysm;hemodynamic;visualization},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~，医疗方面的，不相关},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6327222&isnumber=6327196},
  Zone                     = {2Q}
}

@Article{Genge-2012-p1146-1161,
  Title                    = {A cyber-physical experimentation environment for the security analysis of networked industrial control systems },
  Author                   = {Béla Genge and Christos Siaterlis and Igor Nai Fovino and Marcelo Masera},
  Journal                  = {Computers \& Electrical Engineering },
  Year                     = {2012},
  Note                     = {Special issue on Recent Advances in Security and Privacy in Distributed Communications and Image processing },
  Number                   = {5},
  Pages                    = {1146 - 1161},
  Volume                   = {38},

  Abstract                 = {Although many studies address the security of Networked Industrial Control Systems (NICSs), today we still lack an efficient way to conduct scientific experiments that measure the impact of attacks against both the physical and the cyber parts of these systems. This paper presents an innovative framework for an experimentation environment that can reproduce concurrently physical and cyber systems. The proposed approach uses an emulation testbed based on Emulab to recreate cyber components and a real-time simulator, based on Simulink, to recreate physical processes. The main novelty of the proposed framework is that it provides a set of experimental capabilities that are missing from other approaches, e.g. safe experimentation with real malware, flexibility to use different physical processes. The feasibility of the approach is confirmed by the development of a fully functional prototype, while its applicability is proven through two case studies of industrial systems from the electrical and chemical domain. },
  Doi                      = {http://dx.doi.org/10.1016/j.compeleceng.2012.06.015},
  File                     = {Published version:Genge-2012-p1146-1161.pdf:PDF},
  Impactfactor             = {0.928},
  ISSN                     = {0045-7906},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0045790612001243},
  Zone                     = {4Q}
}

@Article{Ghazel-2014-p1123-1133,
  Title                    = {Two-Half-Barrier Level Crossings Versus Four-Half-Barrier Level Crossings: A Comparative Risk Analysis Study},
  Author                   = {Ghazel, M. and El-Koursi, E.-M.},
  Journal                  = {Intelligent Transportation Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {3},
  Pages                    = {1123-1133},
  Volume                   = {15},

  Abstract                 = {Safety is a key issue in railway operation. In this context, level crossings (LCs) are one of the most critical points in railway networks. In some countries, accidents at LC account for up to 50% of railway accidents. In this paper, we conduct a risk assessment comparative study involving two main types of Automatic Protection Systems (APSs), the first using a pair of half-barriers and the second with four half-barriers. So far, the choice of such LC protection systems has been exclusively done on the basis of qualitative expertise. The study we carry out here is based on some parameterizable behavioral models we have developed, which describe the global dynamics within the LC area. In contrast to existing studies on LC safety, our models take into account not only railway and road traffic but also the risk due to human factors while focusing on two major risky situations. The simulation results clearly show the potential risk with each of the investigated APSs, according to various features of the dynamics within the LC area. To the best of our knowledge, this is the first work dealing with a quantitative comparison between different types of LCs. The developed models can be easily accommodated in order to describe existing infrastructures.},
  Doi                      = {10.1109/TITS.2013.2294874},
  File                     = {Published version:Ghazel-2014-p1123-1133.pdf:PDF},
  Impactfactor             = {3.064},
  ISSN                     = {1524-9050},
  Keywords                 = {human factors;protection;railway accidents;railway safety;risk analysis;APS;LC protection system;LC safety;automatic protection system;barrier level crossings;global dynamics;human factors;parameterizable behavioral models;railway accidents;railway networks;railway operation;railway safety;risk analysis;risk assessment;Accidents;Europe;Rail transportation;Risk management;Roads;Safety;Vehicles;Level crossing (LC);modeling;railway safety;risk analysis;simulation;stochastic Petri net;train–car collision;train??car collision},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，用 CPN Tool 仿真，Petri 网，指标不知道是什么},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6718096&isnumber=6823199},
  Zone                     = {1Q}
}

@Article{Ghazel-2014-p1123-1133a,
  Title                    = {Two-Half-Barrier Level Crossings Versus Four-Half-Barrier Level Crossings: A Comparative Risk Analysis Study},
  Author                   = {Ghazel, M. and El-Koursi, E.-M.},
  Journal                  = {Intelligent Transportation Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {3},
  Pages                    = {1123-1133},
  Volume                   = {15},

  Abstract                 = {Safety is a key issue in railway operation. In this context, level crossings (LCs) are one of the most critical points in railway networks. In some countries, accidents at LC account for up to 50% of railway accidents. In this paper, we conduct a risk assessment comparative study involving two main types of Automatic Protection Systems (APSs), the first using a pair of half-barriers and the second with four half-barriers. So far, the choice of such LC protection systems has been exclusively done on the basis of qualitative expertise. The study we carry out here is based on some parameterizable behavioral models we have developed, which describe the global dynamics within the LC area. In contrast to existing studies on LC safety, our models take into account not only railway and road traffic but also the risk due to human factors while focusing on two major risky situations. The simulation results clearly show the potential risk with each of the investigated APSs, according to various features of the dynamics within the LC area. To the best of our knowledge, this is the first work dealing with a quantitative comparison between different types of LCs. The developed models can be easily accommodated in order to describe existing infrastructures.},
  Doi                      = {10.1109/TITS.2013.2294874},
  File                     = {Published version:Ghazel-2014-p1123-1133a.pdf:PDF},
  Impactfactor             = {3.064},
  ISSN                     = {1524-9050},
  Keywords                 = {human factors;protection;railway accidents;railway safety;risk analysis;APS;LC protection system;LC safety;automatic protection system;barrier level crossings;global dynamics;human factors;parameterizable behavioral models;railway accidents;railway networks;railway operation;railway safety;risk analysis;risk assessment;Accidents;Europe;Rail transportation;Risk management;Roads;Safety;Vehicles;Level crossing (LC);modeling;railway safety;risk analysis;simulation;stochastic Petri net;train–car collision;train??car collision},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6718096&isnumber=6823199},
  Zone                     = {1Q}
}

@Article{Glaber-2014-p2506-2515,
  Title                    = {Combined Visualization of Wall Thickness and Wall Shear Stress for the Evaluation of Aneurysms},
  Author                   = {Glaber, S. and Lawonn, K. and Hoffmann, T. and Skalej, M. and Preim, B.},
  Journal                  = {Visualization and Computer Graphics, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {2506-2515},
  Volume                   = {20},

  Abstract                 = {For an individual rupture risk assessment of aneurysms, the aneurysm's wall morphology and hemodynamics provide valuable information. Hemodynamic information is usually extracted via computational fluid dynamic (CFD) simulation on a previously extracted 3D aneurysm surface mesh or directly measured with 4D phase-contrast magnetic resonance imaging. In contrast, a noninvasive imaging technique that depicts the aneurysm wall in vivo is still not available. Our approach comprises an experiment, where intravascular ultrasound (IVUS) is employed to probe a dissected saccular aneurysm phantom, which we modeled from a porcine kidney artery. Then, we extracted a 3D surface mesh to gain the vessel wall thickness and hemodynamic information from a CFD simulation. Building on this, we developed a framework that depicts the inner and outer aneurysm wall with dedicated information about local thickness via distance ribbons. For both walls, a shading is adapted such that the inner wall as well as its distance to the outer wall is always perceivable. The exploration of the wall is further improved by combining it with hemodynamic information from the CFD simulation. Hence, the visual analysis comprises a brushing and linking concept for individual highlighting of pathologic areas. Also, a surface clustering is integrated to provide an automatic division of different aneurysm parts combined with a risk score depending on wall thickness and hemodynamic information. In general, our approach can be employed for vessel visualization purposes where an inner and outer wall has to be adequately represented.},
  Doi                      = {10.1109/TVCG.2014.2346406},
  File                     = {Published version:Glaber-2014-p2506-2515.pdf:PDF},
  Impactfactor             = {1.898},
  ISSN                     = {1077-2626},
  Keywords                 = {biomedical ultrasonics;computational fluid dynamics;data visualisation;feature extraction;flow simulation;haemodynamics;medical image processing;3D aneurysm surface mesh extraction;4D phase-contrast magnetic resonance imaging;CFD simulation;IVUS;aneurysm evaluation;aneurysm wall hemodynamics;aneurysm wall morphology;computational fluid dynamics;distance ribbons;hemodynamic information;intravascular ultrasound;noninvasive imaging technique;porcine kidney artery;risk score;rupture risk assessment;saccular aneurysm phantom;vessel visualization;visual analysis;wall shear stress visualization;wall thickness visualization;Aneurysms;Arteries;Biomedical image processing;Brain modeling;Hemodynamics;Risk management;Solid modeling;Three-dimensional displays;Aneurysm;Brushing and Linking;Focus + Context;IVUS;Wall Shear Stress;Wall Thickness},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~，不相关},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6877722&isnumber=6935054},
  Zone                     = {2Q}
}

@Article{Goel-2000-p381-386,
  Title                    = {Extending health considerations in subtransmission systems to include reliability worth},
  Author                   = {Goel, L. and Gupta, R.},
  Journal                  = {Generation, Transmission and Distribution, IEE Proceedings-},
  Year                     = {2000},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {381-386},
  Volume                   = {147},

  Abstract                 = {The basic concepts associated with quantitative reliability assessment of electric power systems are reasonably well established, and well accepted within the power industry. The evaluation of the costs and benefits of competing investments is now becoming a standard practice in power-system planning. To justify the use of new facilities and system modification specific reference to reliability has normally been included, and one approach that has received considerable attention is the evaluation of the societal worth (benefit) of system reliability, or conversely the costs incurred by consumers due to power supply interruptions. To make a consistent appraisal of economics and reliability, it is necessary to compare investment cost needed to attain a specified level of reliability with the reliability worth or benefits derived by the society at that level of system reliability. The wellbeing framework is extended to include the societal worth of electric service reliability in subtransmission systems. System well-being is defined in terms of three system states: (i) healthy, (ii) marginal and (iii) at risk, thus combining the deterministic and probabilistic approaches into a single framework. The determination of reliability worth indices of expected energy not supplied (EENS), expected cost of interruptions (ECOST) and interrupted energy assessment rate (IEAR) are discussed, for both the healthy and at risk states in the well-being framework. The concepts associated with extending the well-being framework to include reliability worth parameters are illustrated by application to a small reliability test system},
  Doi                      = {10.1049/ip-gtd:20000660},
  File                     = {Published version:Goel-2000-p381-386.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1350-2360},
  Keywords                 = {power transmission economics;power transmission reliability;at risk state;consumer costs;deterministic approach;economics;electric power systems;electric service reliability;expected cost of interruptions;expected energy not supplied;health considerations;healthy state;interrupted energy assessment rate;marginal state;power supply interruptions;power-system planning;probabilistic approach;quantitative reliability assessment;reliability worth;reliability worth indices;small reliability test system;societal worth;subtransmission systems;wellbeing framework},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=894401&isnumber=19354},
  Zone                     = {-}
}

@Article{Gordon-2006-p1-1,
  Title                    = {2006 CSI/FBI computer crime and security survey},
  Author                   = {Gordon, Lawrence A and Loeb, Martin P and Lucyshyn, William and Richardson, Robert},
  Journal                  = {Computer Security Journal},
  Year                     = {2006},
  Number                   = {3},
  Pages                    = {1},
  Volume                   = {22},

  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Publisher                = {Computer Security Institute},
  Timestamp                = {2014.10.11},
  Zone                     = {-}
}

@Article{Goseva-Pop-2003-p946-960,
  Title                    = {Architectural-level risk analysis using UML},
  Author                   = {Goseva-Popstojanova, K. and Hassan, A. and Guedem, A. and Abdelmoez, W. and Nassar, D.E.M. and Ammar, H. and Mili, A.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {946-960},
  Volume                   = {29},

  Abstract                 = {Risk assessment is an essential part in managing software development. Performing risk assessment during the early development phases enhances resource allocation decisions. In order to improve the software development process and the quality of software products, we need to be able to build risk analysis models based on data that can be collected early in the development process. These models will help identify the high-risk components and connectors of the product architecture, so that remedial actions may be taken in order to control and optimize the development process and improve the quality of the product. In this paper, we present a risk assessment methodology which can be used in the early phases of the software life cycle. We use the Unified Modeling Language (UML) and commercial modeling environment Rational Rose Real Time (RoseRT) to obtain UML model statistics. First, for each component and connector in software architecture, a dynamic heuristic risk factor is obtained and severity is assessed based on hazard analysis. Then, a Markov model is constructed to obtain scenarios risk factors. The risk factors of use cases and the overall system risk factor are estimated using the scenarios risk factors. Within our methodology, we also identify critical components and connectors that would require careful analysis, design, implementation, and more testing effort. The risk assessment methodology is applied on a pacemaker case study.},
  Doi                      = {10.1109/TSE.2003.1237174},
  File                     = {Published version:Goseva-Pop-2003-p946-960.pdf:PDF},
  Impactfactor             = {2.588},
  ISSN                     = {0098-5589},
  Keywords                 = {Markov processes;risk management;software architecture;specification languages;UML;Unified Modeling Language;dynamic coupling;risk assessment;severity of failure;software architecture;software development;software life cycle;Computer architecture;Connectors;Programming;Resource management;Risk analysis;Risk management;Software development management;Software quality;Statistics;Unified modeling language},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，用心脏起搏器做Case Study},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1237174&isnumber=27736},
  Zone                     = {1Q}
}

@Article{Goseva-Pop-2003-p946-960a,
  Title                    = {Architectural-level risk analysis using UML},
  Author                   = {Goseva-Popstojanova, K. and Hassan, A. and Guedem, A. and Abdelmoez, W. and Nassar, D.E.M. and Ammar, H. and Mili, A.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Oct},
  Number                   = {10},
  Pages                    = {946-960},
  Volume                   = {29},

  Abstract                 = {Risk assessment is an essential part in managing software development. Performing risk assessment during the early development phases enhances resource allocation decisions. In order to improve the software development process and the quality of software products, we need to be able to build risk analysis models based on data that can be collected early in the development process. These models will help identify the high-risk components and connectors of the product architecture, so that remedial actions may be taken in order to control and optimize the development process and improve the quality of the product. In this paper, we present a risk assessment methodology which can be used in the early phases of the software life cycle. We use the Unified Modeling Language (UML) and commercial modeling environment Rational Rose Real Time (RoseRT) to obtain UML model statistics. First, for each component and connector in software architecture, a dynamic heuristic risk factor is obtained and severity is assessed based on hazard analysis. Then, a Markov model is constructed to obtain scenarios risk factors. The risk factors of use cases and the overall system risk factor are estimated using the scenarios risk factors. Within our methodology, we also identify critical components and connectors that would require careful analysis, design, implementation, and more testing effort. The risk assessment methodology is applied on a pacemaker case study.},
  Doi                      = {10.1109/TSE.2003.1237174},
  File                     = {Published version:Goseva-Pop-2003-p946-960a.pdf:PDF},
  Impactfactor             = {2.588},
  ISSN                     = {0098-5589},
  Keywords                 = {Markov processes;risk management;software architecture;specification languages;UML;Unified Modeling Language;dynamic coupling;risk assessment;severity of failure;software architecture;software development;software life cycle;Computer architecture;Connectors;Programming;Resource management;Risk analysis;Risk management;Software development management;Software quality;Statistics;Unified modeling language},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1237174&isnumber=27736},
  Zone                     = {1Q}
}

@Article{Goulding-2012-p4933-4940,
  Title                    = {A Bayesian network model to assess the public health risk associated with wet weather sewer overflows discharging into waterways },
  Author                   = {R. Goulding and N. Jayasuriya and E. Horan},
  Journal                  = {Water Research },
  Year                     = {2012},
  Number                   = {16},
  Pages                    = {4933 - 4940},
  Volume                   = {46},

  Abstract                 = {Overflows from sanitary sewers during wet weather, which occur when the hydraulic capacity of the sewer system is exceeded, are considered a potential threat to the ecological and public health of the waterways which receive these overflows. As a result, water retailers in Australia and internationally commit significant resources to manage and abate sewer overflows. However, whilst some studies have contributed to an increased understanding of the impacts and risks associated with these events, they are relatively few in number and there still is a general lack of knowledge in this area. A Bayesian network model to assess the public health risk associated with wet weather sewer overflows is presented in this paper. The Bayesian network approach is shown to provide significant benefits in the assessment of public health risks associated with wet weather sewer overflows. In particular, the ability for the model to account for the uncertainty inherent in sewer overflow events and subsequent impacts through the use of probabilities is a valuable function. In addition, the paper highlights the benefits of the probabilistic inference function of the Bayesian network in prioritising management options to minimise public health risks associated with sewer overflows. },
  Doi                      = {http://dx.doi.org/10.1016/j.watres.2012.03.044},
  File                     = {Published version:Goulding-2012-p4933-4940.pdf:PDF},
  Impactfactor             = {4.655},
  ISSN                     = {0043-1354},
  Keywords                 = {Bayesian networks},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0043135412002187},
  Zone                     = {2Q}
}

@Article{Gowrison-2013-p921-927,
  Title                    = {Minimal complexity attack classification intrusion detection system },
  Author                   = {G. Gowrison and K. Ramar and K. Muneeswaran and T. Revathi},
  Journal                  = {Applied Soft Computing },
  Year                     = {2013},
  Number                   = {2},
  Pages                    = {921 - 927},
  Volume                   = {13},

  Abstract                 = {In general, the kind of users and the injection of network packets into the internet sectors are not under specific control. There is no clear description as to what packets can be considered normal or abnormal. If the invasions are not detected at the appropriate level, the loss to system may be some times unimaginable. Although many intrusion detection system (IDS) methods are used to detect the existing types of attacks within the network infrastructures, reducing false negative and false positives is still a major issue. In our paper an intrusion detection system is designed to classify by the incorporation of enhanced rules as learnt from the network behavior with less computational complexity of O(n). The method demonstrates the achievements of promising classification rate. The bench mark data \{KDD\} Cup99 data is used in our method. },
  Doi                      = {http://dx.doi.org/10.1016/j.asoc.2012.09.017},
  File                     = {Published version:Gowrison-2013-p921-927.pdf:PDF},
  Impactfactor             = {2.14},
  ISSN                     = {1568-4946},
  Keywords                 = {Intrusion detection system},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1568494612004371},
  Zone                     = {3Q}
}

@InCollection{Gran-2004-p311-324,
  Title                    = {An Approach for Model-Based Risk Assessment},
  Author                   = {Gran, BjørnAxel and Fredriksen, Rune and Thunem, AtoosaP.-J.},
  Booktitle                = {Computer Safety, Reliability, and Security},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2004},
  Editor                   = {Heisel, Maritta and Liggesmeyer, Peter and Wittmann, Stefan},
  Pages                    = {311-324},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {3219},

  Doi                      = {10.1007/978-3-540-30138-7_26},
  File                     = {Published version:Gran-2004-p311-324.pdf:PDF},
  Impactfactor             = {-},
  ISBN                     = {978-3-540-23176-9},
  Language                 = {English},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.16},
  Url                      = {http://dx.doi.org/10.1007/978-3-540-30138-7_26},
  Zone                     = {-}
}

@Article{Gregoriade-2013-p28-43,
  Title                    = {Black spots identification through a Bayesian Networks quantification of accident risk index },
  Author                   = {Andreas Gregoriades and Kyriacos C. Mouskos},
  Journal                  = {Transportation Research Part C: Emerging Technologies },
  Year                     = {2013},
  Note                     = {Euro Transportation: selected paper from the \{EWGT\} Meeting, Padova, September 2009 },
  Number                   = {0},
  Pages                    = {28 - 43},
  Volume                   = {28},

  Abstract                 = {Traffic accidents constitute a major problem worldwide. One of the principal causes of traffic accidents is adverse driving behavior that is inherently influenced by traffic conditions and infrastructure among other parameters. Probabilistic models for the assessment of road accidents risk usually employs machine learning using historical data of accident records. The main drawback of these approaches is limited coverage of traffic data. This study illustrates a prototype approach that escapes from this problem, and highlights the need to enhance historical accident records with traffic information for improved road safety analysis. Traffic conditions estimation is achieved through Dynamic Traffic Assignment (DTA) simulation that utilizes temporal aspects of a transportation system. Accident risk quantification is achieved through a Bayesian Networks (BNs) model learned from the method鈥檚 enriched accidents dataset. The study illustrates the integration of \{BN\} with the DTA-based simulator, Visual Interactive Systems for Transport Algorithms (VISTAs), for the assessment of accident risk index (ARI), used to identify accident black spots on road networks. },
  Doi                      = {http://dx.doi.org/10.1016/j.trc.2012.12.008},
  File                     = {Published version:Gregoriade-2013-p28-43.pdf:PDF},
  Impactfactor             = {2.006},
  ISSN                     = {0968-090X},
  Keywords                 = {Road safety},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0968090X12001544},
  Zone                     = {3Q}
}

@Article{Greiner-2013-p4-11,
  Title                    = {Graphical models and Bayesian domains in risk modelling: Application in microbiological risk assessment },
  Author                   = {Matthias Greiner and Joost Smid and Arie H. Havelaar and Christine M眉ller-Graf},
  Journal                  = {Preventive Veterinary Medicine },
  Year                     = {2013},
  Note                     = {Special Issue: Bayesian Graphical Modelling: Applications in Veterinary Epidemiology },
  Number                   = {1},
  Pages                    = {4 - 11},
  Volume                   = {110},

  Abstract                 = {Quantitative microbiological risk assessment (QMRA) models are used to reflect knowledge about complex real-world scenarios for the propagation of microbiological hazards along the feed and food chain. The aim is to provide insight into interdependencies among model parameters, typically with an interest to characterise the effect of risk mitigation measures. A particular requirement is to achieve clarity about the reliability of conclusions from the model in the presence of uncertainty. To this end, Monte Carlo (MC) simulation modelling has become a standard in so-called probabilistic risk assessment. In this paper, we elaborate on the application of Bayesian computational statistics in the context of QMRA. It is useful to explore the analogy between \{MC\} modelling and Bayesian inference (BI). This pertains in particular to the procedures for deriving prior distributions for model parameters. We illustrate using a simple example that the inability to cope with feedback among model parameters is a major limitation of \{MC\} modelling. However, \{BI\} models can be easily integrated into \{MC\} modelling to overcome this limitation. We refer a \{BI\} submodel integrated into a \{MC\} model to as a 鈥淏ayes domain鈥? We also demonstrate that an entire \{QMRA\} model can be formulated as Bayesian graphical model (BGM) and discuss the advantages of this approach. Finally, we show example graphs of MC, \{BI\} and \{BGM\} models, highlighting the similarities among the three approaches. },
  Doi                      = {http://dx.doi.org/10.1016/j.prevetmed.2013.02.008},
  File                     = {Published version:Greiner-2013-p4-11.pdf:PDF},
  Impactfactor             = {2.389},
  ISSN                     = {0167-5877},
  Keywords                 = {Bayesian graphical models},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167587713000391},
  Zone                     = {1Q}
}

@Article{Gribaudo-2015-p91-111,
  Title                    = {Exploiting Bayesian Networks for the Analysis of Combined Attack Trees },
  Author                   = {Marco Gribaudo and Mauro Iacono and Stefano Marrone},
  Journal                  = {Electronic Notes in Theoretical Computer Science },
  Year                     = {2015},
  Note                     = {Proceedings of the Seventh International Workshop on the Practical Application of Stochastic Modelling (PASM) },
  Number                   = {0},
  Pages                    = {91 - 111},
  Volume                   = {310},

  Abstract                 = {Abstract The growing need to find proper countermeasures able to protect critical infrastructures from threats has addressed the definition of quantitative methodologies for risk assessment. One of the most difficult aspects in this topic is the evaluation of the effects of attacks. Attacks Trees represent one of the most used formalisms in the modeling of attack scenarios: notwithstanding some extensions have been proposed to enrich the expressiveness of the original formalism, some effort should be spent on their analyzability. This paper defines a transformational approach that translates Attack Trees into Bayesian Networks. The proposed approach can cope with different Attack Trees extensions; moreover, it allows the quantitative evaluation of combined attacks modelled as a set of Attack Trees. },
  Doi                      = {http://dx.doi.org/10.1016/j.entcs.2014.12.014},
  File                     = {Published version:Gribaudo-2015-p91-111.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1571-0661},
  Keywords                 = {Attack Trees},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S157106611400098X},
  Zone                     = {-}
}

@Article{GUAN-2013-p60-63,
  Title                    = {Knowledge-based information security risk assessment method },
  Author                   = {Ji-zheng GUAN and Ming-tao LEI and Xiao-lu ZHU and Jian-yi LIU},
  Journal                  = {The Journal of China Universities of Posts and Telecommunications },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {60 - 63},
  Volume                   = {20, Supplement 2},

  Abstract                 = {Abstract It is an important function for managers to keep away from information security risks. With the increasing complex and scale of information systems, information system security risks may be more difficult to assess and strategies for risk reduction may be lack of objectivity. To solve this problem, this paper proposes a knowledge-based information security risk assessment method in which basic rules and specific rules are defined to match every asset, threat and vulnerability. Basic rules are defined as the rules without influence of external relationships. Specific rules are defined as the rules by user group. Performance analysis shows this method could increase efficiency and ensure accuracy of risk assessment. },
  Doi                      = {http://dx.doi.org/10.1016/S1005-8885(13)60220-4},
  File                     = {Published version:GUAN-2013-p60-63.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1005-8885},
  Keywords                 = {information systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，做了性能测试，风险值进行了对比，实验},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1005888513602204},
  Zone                     = {-}
}

@InProceedings{Guang-Bin-2012-p1-4,
  Title                    = {Dynamic risk assessment of power system catastrophic event caused by cascading failure},
  Author                   = {Xie Guang-Bin and Cui Zhen and Xiao Xian-Yong},
  Booktitle                = {Electricity Distribution (CICED), 2012 China International Conference on},
  Year                     = {2012},
  Month                    = {Sept},
  Pages                    = {1-4},

  Abstract                 = {Considering highly coupled relationship between network topology and electrical operation state in the physical properties of power grid, a new method based on uncertainty risk assessment theory and connection number theory is proposed. The proposed dynamic comprehensive assessment method takes into account of static direct risk and dynamic potential risk to work up the comprehensive risk index. Though compare self-layer comprehensive risk index with next layer comprehensive risk index the failure propagation process is divided into cumulative effects stage and cascade effects stage. Then, ranking the failure events by the comprehensive risk index is to evaluate risk of event. Simulation results from the assessment of an area grid in china shown that the proposed method is practical and it can depict the complete risk of catastrophic events in power system well.},
  Doi                      = {10.1109/CICED.2012.6508482},
  File                     = {Published version:Guang-Bin-2012-p1-4.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2161-7481},
  Keywords                 = {power grids;power system faults;power system reliability;risk management;China;cascade effect stage;cascading failure;connection number theory;cumulative effect stage;dynamic comprehensive assessment method;dynamic potential risk;dynamic risk assessment;electrical operation state;failure events;failure propagation process;network topology;next layer comprehensive risk index;power grid physical properties;power system catastrophic event;self-layer comprehensive risk index;static direct risk;uncertainty risk assessment theory;catastrophic event;degree of connection;direct risk;dynamic comprehensive assessment;potential risk},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6508482&isnumber=6508401},
  Zone                     = {-}
}

@InProceedings{Guang-Bin-2012-p1-4a,
  Title                    = {Dynamic risk assessment of power system catastrophic event caused by cascading failure},
  Author                   = {Xie Guang-Bin and Cui Zhen and Xiao Xian-Yong},
  Booktitle                = {Electricity Distribution (CICED), 2012 China International Conference on},
  Year                     = {2012},
  Month                    = {Sept},
  Pages                    = {1-4},

  Abstract                 = {Considering highly coupled relationship between network topology and electrical operation state in the physical properties of power grid, a new method based on uncertainty risk assessment theory and connection number theory is proposed. The proposed dynamic comprehensive assessment method takes into account of static direct risk and dynamic potential risk to work up the comprehensive risk index. Though compare self-layer comprehensive risk index with next layer comprehensive risk index the failure propagation process is divided into cumulative effects stage and cascade effects stage. Then, ranking the failure events by the comprehensive risk index is to evaluate risk of event. Simulation results from the assessment of an area grid in china shown that the proposed method is practical and it can depict the complete risk of catastrophic events in power system well.},
  Doi                      = {10.1109/CICED.2012.6508482},
  File                     = {Published version:Guang-Bin-2012-p1-4a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2161-7481},
  Keywords                 = {power grids;power system faults;power system reliability;risk management;China;cascade effect stage;cascading failure;connection number theory;cumulative effect stage;dynamic comprehensive assessment method;dynamic potential risk;dynamic risk assessment;electrical operation state;failure events;failure propagation process;network topology;next layer comprehensive risk index;power grid physical properties;power system catastrophic event;self-layer comprehensive risk index;static direct risk;uncertainty risk assessment theory;catastrophic event;degree of connection;direct risk;dynamic comprehensive assessment;potential risk},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6508482&isnumber=6508401},
  Zone                     = {-}
}

@InProceedings{Guo-2011-p862-867,
  Title                    = {Decomposition method for hierarchical Multidisciplinary Robust Design considering uncertainty functions and coupling variables},
  Author                   = {Jianbin Guo and Shengkui Zeng and Jiming Ma},
  Booktitle                = {Quality, Reliability, Risk, Maintenance, and Safety Engineering (ICQR2MSE), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {June},
  Pages                    = {862-867},

  Abstract                 = {Exiting decomposition methods based on Function Dependency Table (FDT), those ignore the substantive impact to optimization process brought by performing uncertainty analysis, and cannot decomposes coupling variables, are not suitable for complex Multidisciplinary Robust Design (MRD).To obtain optimal decomposition form for a MRD problem, a novel decomposition method, which can deal with uncertainty functions and coupling variables in MRD, is developed. Compared with existing decomposition methods, this method is more suitable for MRD with two advantages. First, uncertainty functions can be decomposed averagely to obtain better concurrency and less total computational cost. Second, coupling variables can be identified and decomposed appropriately. This method is verified by a gear reducer box problem. Compared with two existing decomposition solutions, this method significantly reduces the total computational cost.},
  Doi                      = {10.1109/ICQR2MSE.2011.5976744},
  File                     = {Published version:Guo-2011-p862-867.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {design;gears;coupling variables;decomposition method;function dependency table;gear reducer box problem;hierarchical multidisciplinary robust design;uncertainty functions;Computational efficiency;Concurrent computing;Couplings;Matrix decomposition;Optimization;TV;Uncertainty;decomposition;multidisciplinary design optimization;robust design;uncertainty analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5976744&isnumber=5976557},
  Zone                     = {-}
}

@InProceedings{Guo-2011-p5783-5786,
  Title                    = {Case study and environmental risk assessment of the petrochemical industry},
  Author                   = {Lijuan Guo and Peng Yuan and Yonghui Song and Jianfeng Peng and Li Wang},
  Booktitle                = {Remote Sensing, Environment and Transportation Engineering (RSETE), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {June},
  Pages                    = {5783-5786},

  Abstract                 = {The petrochemical industry is a high environmental risk industry because a large amount of toxic, hazardous, flammable and explosive substances were used in the processes of manufacture and operation. If the accidents of the leakage of chemicals, explosions and fires occurred, it would lead to release of toxic substances. The accidental releases are very dangerous to the republic health and the environmental receptor. Therefore, it is urgent to strengthen environmental risk assessment and management of petrochemical industry. Taking one petrochemical enterprise of Nanjing Chemical Industrial Park as an example, the essentials of environmental risk assessment of petrochemical project were analyzed. By identification of significant environmental risk sources, the chlorine leakage accident of storage tank was determined as the maximum creditable accident, and the method for leaking source strength calculation was given. Then the consequences of determined the maximum creditable accident was predicted by using gas diffusion model and the results of risk analyses were obtained. The results indicated that the damage distance was 1.4km if Chlorine leakage occurred under the disadvantageous weather conditions. The similar results were given using the software RMP* Comp provided by United States EPA. According to the RMP* Comp, the distance was 1.3km at the same weather condition. Risk assessment results showed that it would affect outside-factory environment if the chlorine leakage accident happened in the factory. Therefore it is required to improve the level of environmental risk management and to take necessary prevention measures. Finally risk management suggestions for petrochemical industry were put forward. Practical management strategies developed by evaluation results could reduce the probability of accidents release and prevent the occurrence of chlorine leakage more efficiently. The paper can provide a reference for people who do the related research on environme- - ntal risk assessment of petrochemical industry.},
  Doi                      = {10.1109/RSETE.2011.5965668},
  File                     = {Published version:Guo-2011-p5783-5786.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {accident prevention;environmental factors;explosions;leak detection;petrochemicals;risk analysis;Nanjing Chemical Industrial Park;RMP Comp;accidents;chemical leakage;chlorine leakage accident prevention;environmental risk assessment;explosions;fires;petrochemical industry;risk analysis;storage tank;Accidents;Chemicals;Industries;Meteorology;Petrochemicals;Risk management;environmental risk assessment;petrochemical industry;risk analysis;risk identification},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5965668&isnumber=5963913},
  Zone                     = {-}
}

@InProceedings{Gupta-2008-p965-968,
  Title                    = {Source Code Watermarking Based on Function Dependency Oriented Sequencing},
  Author                   = {Gupta, G. and Pieprzyk, J.},
  Booktitle                = {Intelligent Information Hiding and Multimedia Signal Processing, 2008. IIHMSP '08 International Conference on},
  Year                     = {2008},
  Month                    = {Aug},
  Pages                    = {965-968},

  Abstract                 = {In the current market, extensive software development is taking place and the software industry is thriving. Major software giants have stated source code theft as a major threat to revenues. By inserting an identity-establishing watermark in the source code, a company can prove it's ownership over the source code. In this paper, we propose a watermarking scheme for C/C++ source codes by exploiting the language restrictions. If a function calls another function, the latter needs to be defined in the code before the former, unless one uses function pre-declarations. We embed the watermark in the code by imposing an ordering on the mutually independent functions by introducing bogus dependency. Removal of dependency by the attacker to erase the watermark requires extensive manual intervention thereby making the attack infeasible. The scheme is also secure against subtractive and additive attacks. Using our watermarking scheme, an n-bit watermark can be embedded in a program having n independent functions. The scheme is implemented on several sample codes and performance changes are analyzed.},
  Doi                      = {10.1109/IIH-MSP.2008.188},
  File                     = {Published version:Gupta-2008-p965-968.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {C++ language;security of data;watermarking;C/C++ source codes;additive attacks;function dependency oriented sequencing;language restrictions;software development;source code watermarking;subtractive attacks;Authentication;Copyright protection;Fingerprint recognition;Internet;Object oriented modeling;Robustness;Signal processing algorithms;Software algorithms;Watermarking;Yarn;branch;copyright;fingerprinting;intellectual property;software;watermarking},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4604210&isnumber=4603987},
  Zone                     = {-}
}

@InProceedings{Haddad-2011-p1-6,
  Title                    = {Operational security assurance evaluation in open infrastructures},
  Author                   = {Haddad, S. and Dubus, S. and Hecker, A and Kanstren, T. and Marquet, B. and Savola, R.},
  Booktitle                = {Risk and Security of Internet and Systems (CRiSIS), 2011 6th International Conference on},
  Year                     = {2011},
  Month                    = {Sept},
  Pages                    = {1-6},

  Doi                      = {10.1109/CRiSIS.2011.6061831},
  File                     = {Published version:Haddad-2011-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {information systems;security of data;IT system;Telco service;continuous security assurance assessment;cyber security;open infrastructure;operational assurance assessment process;operational security assurance assessment;operational security assurance evaluation;operational security assurance requirement;security assurance cockpit;security assurance profile;standardized offline security assurance evaluation;Computer architecture;Measurement;Probes;Risk analysis;Security;Software;Standards},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6061831&isnumber=6061543},
  Zone                     = {-}
}

@Article{Haimes-1999-p315-315,
  Title                    = {Risk Modeling, Assessment, And Management},
  Author                   = {Haimes, Y.Y.},
  Journal                  = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {May},
  Number                   = {2},
  Pages                    = {315-315},
  Volume                   = {29},

  Abstract                 = {Not Available},
  Doi                      = {10.1109/TSMCC.1999.760576},
  File                     = {Published version:Haimes-1999-p315-315.pdf:PDF},
  Impactfactor             = {2.548},
  ISSN                     = {1094-6977},
  Keywords                 = {Acoustical engineering;Books;Decision making;Humans;Risk analysis;Risk management;Sections;Statistical analysis;Systems engineering and theory;Uncertainty},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=760576&isnumber=16448},
  Zone                     = {2Q}
}

@Article{Hajek-2002-p828-839,
  Title                    = {Capacity and reliability function for small peak signal constraints},
  Author                   = {Hajek, B. and Subramanian, V.G.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {Apr},
  Number                   = {4},
  Pages                    = {828-839},
  Volume                   = {48},

  Abstract                 = {The capacity and the reliability function as the peak constraint tends to zero are considered for a discrete-time memoryless channel with peak constrained inputs. Prelov and van der Meulen (1993) showed that under mild conditions the ratio of the capacity to the squared peak constraint converges to one-half the maximum eigenvalue of the Fisher information matrix and if the Fisher information matrix is nonzero, the asymptotically optimal input distribution is symmetric antipodal signaling. Under similar conditions, it is shown in the first part of the paper that the reliability function has the same asymptotic shape as the reliability function for the power-constrained infinite bandwidth white Gaussian noise channel. The second part of the paper deals with Rayleigh-fading channels. For such channels, the Fisher information matrix is zero, indicating the difficulty of transmission over such channels with small peak constrained signals. Asymptotics for the Rayleigh channel are derived and applied to obtain the asymptotics of the capacity of the Marzetta and Hochwald (1999) fading channel model for small peak constraints, and to obtain a result of the type of Medard and Gallager for wide-band fading channels},
  Doi                      = {10.1109/18.992766},
  File                     = {Published version:Hajek-2002-p828-839.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {AWGN channels;Rayleigh channels;channel capacity;discrete time systems;eigenvalues and eigenfunctions;information theory;matrix algebra;memoryless systems;spread spectrum communication;telecommunication network reliability;telecommunication signalling;Fisher information matrix;Rayleigh-fading channels;asymptotically optimal input distribution;capacity;discrete-time memoryless channel;maximum eigenvalue;power-constrained infinite bandwidth white Gaussian noise channel;radio communication;reliability function;small peak signal constraints;spread spectrum communication;squared peak constraint;symmetric antipodal signaling;wide-band fading channels;Bandwidth;Channel capacity;Eigenvalues and eigenfunctions;Extraterrestrial measurements;Fading;Memoryless systems;Rayleigh channels;Reliability theory;Shape;Symmetric matrices},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=992766&isnumber=21410},
  Zone                     = {2Q}
}

@Article{Haley-2008-p133-153,
  Title                    = {Security Requirements Engineering: A Framework for Representation and Analysis},
  Author                   = {Haley, C.B. and Laney, R. and Moffett, J.D. and Nuseibeh, B.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2008},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {133-153},
  Volume                   = {34},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/TSE.2007.70754},
  File                     = {Published version:Haley-2008-p133-153.pdf:PDF},
  Impactfactor             = {2.588},
  ISSN                     = {0098-5589},
  Keywords                 = {security of data;software engineering;air traffic control technology;security requirements engineering;software security;Requirements/Specifications;Security;Software/Software Engineering},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4359475&isnumber=4444339},
  Zone                     = {1Q}
}

@InProceedings{Hamadani-2007-p1191-1195,
  Title                    = {The effect of dependency on the MRL function of redundant systems},
  Author                   = {Hamadani, AZ. and Nasrabadi, AN.},
  Booktitle                = {Industrial Engineering and Engineering Management, 2007 IEEE International Conference on},
  Year                     = {2007},
  Month                    = {Dec},
  Pages                    = {1191-1195},

  Abstract                 = {In some circumstances active or standby redundancy is used to improve the reliability of the system while in others, series redundancy is used to improve a different utility measurement of the system. In evaluating the reliability of a system with redundant components, it is often assumed that the components are independent. However, this assumption is rarely valid in practice. In this paper the effect of adding redundancy on the mean residual life time function of the system is studied when two components are dependent and have a bivariate FGM distribution. The results are then compared with the case of two independent components.},
  Doi                      = {10.1109/IEEM.2007.4419380},
  File                     = {Published version:Hamadani-2007-p1191-1195.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {reliability theory;statistical distributions;MRL function;bivariate FGM distribution;mean residual life time function;redundant systems;utility measurement;Design engineering;Exponential distribution;Failure analysis;Independent component analysis;Mathematics;Random variables;Redundancy;Reliability engineering;System performance;Testing;FGM distribution;Redundant component;dependency;mean residual lifetime function},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4419380&isnumber=4419131},
  Zone                     = {-}
}

@InProceedings{Hamoud-2003-p2-764,
  Title                    = {Risk assessment of power systems SCADA},
  Author                   = {Hamoud, G. and Chen, R.-L. and Bradley, I.},
  Booktitle                = {Power Engineering Society General Meeting, 2003, IEEE},
  Year                     = {2003},
  Month                    = {July},
  Pages                    = {-764 Vol. 2},
  Volume                   = {2},

  Abstract                 = {SCADA systems are widely used in power systems for monitoring, operation and control purposes. Failure of the SCADA system can result in severe consequences such as customer load losses and equipment damages, etc. Evaluating these consequences at planning stage can help select the appropriate level of reliability of the SCADA systems. This paper presents a practical method for quantifying the risk associated with the failure of the SCADA systems utilized in power systems. The method first identifies the various components of risk and then evaluates each by considering overlap of the two events, failure of control by SCADA and failure of automatic operation of the power system network. The SCADA risk is calculated and expressed in terms of dollars on a station by station basis. The calculated risk can be used to rank a group of stations, to identify the importance of stations and to establish the reliability requirements for the SCADA system that has the lowest capital cost. The proposed method is applied to the Hydro One Transmission Networks System with its historical operating performance data.},
  Doi                      = {10.1109/PES.2003.1270402},
  File                     = {Published version:Hamoud-2003-p2-764.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {SCADA systems;power distribution reliability;power system control;power system measurement;power transmission reliability;risk analysis;Hydro One Transmission Networks system;SCADA systems;customer interruption cost;customer load losses;equipment damages;power distribution reliability;power system network;power systems control;power systems monitoring;power transmission reliability;risk analysis;Automatic control;Condition monitoring;Control systems;Costs;Power system control;Power system planning;Power system reliability;Power systems;Risk management;SCADA systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.21},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1270402&isnumber=28395},
  Zone                     = {-}
}

@Article{Hamoud-2003-p1587-1593,
  Title                    = {Assessment of customer supply reliability in performance-based contracts},
  Author                   = {Hamoud, G. and El-Nahas, I.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {1587-1593},
  Volume                   = {18},

  Abstract                 = {The number of performance-based contracts between customers (end-users of electricity) and transmission system providers is expected to grow when the electricity market opens up to competition and for customer choice. In these contracts, specified levels of supply reliability with regard to service continuity and power quality will be specified and rewards for honoring these performance levels may be awarded and penalties may be imposed for failing to honor them. These rewards and penalties will be clearly spelled out in the contracts. Transmission system providers may have to assess in advance the level of supply reliability to customers before entering into any of these agreements in order to minimize the financial risk associated with these contracts. This paper describes a probabilistic method for evaluating the level of supply reliability to a customer entering into a performance-based contract with a transmission provider. The evaluation process includes performance measures that reflect both reliability and quality of power supply to the customer. The proposed method can be used not only to assess various utility solutions for improving the reliability and power quality to the customer but also to link the level of supply reliability to the cost of service. An example is given to illustrate the concepts involved.},
  Doi                      = {10.1109/TPWRS.2003.818742},
  File                     = {Published version:Hamoud-2003-p1587-1593.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {contracts;power markets;power supply quality;power system reliability;probability;contracts;customer choice;customer supply reliability assessment;electricity end-users;feeder;financial risk minimisation;performance measures;performance-based contracts;power supply quality;probabilistic method;reliability improvement;supply reliability levels;transmission system providers;Contracts;Electricity supply industry;Industrial power systems;Performance evaluation;Power industry;Power measurement;Power quality;Power supplies;Power system planning;Power system reliability},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1245587&isnumber=27910},
  Zone                     = {2Q}
}

@Article{H-2000-p2117-2132,
  Title                    = {The reliability functions of the general source with fixed-length coding},
  Author                   = {Te Han},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Sep},
  Number                   = {6},
  Pages                    = {2117-2132},
  Volume                   = {46},

  Abstract                 = {The reliability function problems with fixed-length source coding for the general source are studied for all rates R. Our fundamental philosophy in doing so is to convert all of the reliability function problems to the pertinent computation problems in the large derivation-probability theory. It turns out that this kind of new methodology, which was previously developed by Han (see ibid., vol.43, p.1145-64, 1997), enables us to establish quite compact general formulas of the reliability function for general sources including all nonstationary and/or nonergodic sources with countably infinite alphabet. Such general formulas are presented from the information-spectrum point of view},
  Doi                      = {10.1109/18.868482},
  File                     = {Published version:H-2000-p2117-2132.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {decoding;error statistics;functional analysis;reliability;source coding;compact general formulas;countably infinite alphabet;decoding error probability;fixed-length source coding;general source;information-spectrum;large derivation-probability theory;nonergodic sources;nonstationary sources;reliability functions;Helium;History;Information systems;Information theory;Random variables;Reliability engineering;Reliability theory;Source coding;Sun;Tellurium},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=868482&isnumber=18804},
  Zone                     = {2Q}
}

@Article{Han-2014-p6819-6843,
  Title                    = {Reliability and Secrecy Functions of the Wiretap Channel Under Cost Constraint},
  Author                   = {Te Sun Han and Endo, H. and Sasaki, M.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {6819-6843},
  Volume                   = {60},

  Abstract                 = {The wiretap channel has been devised and studied first by Wyner, and subsequently extended to the case with nondegraded general wiretap channels by Csisza?r and Ko?rner. Focusing mainly on the stationary memoryless channel with cost constraint, we newly introduce the notion of reliability and secrecy functions as a fundamental tool to analyze and/or design the performance of an efficient wiretap channel system, including binary symmetric wiretap channels, Poisson wiretap channels, and Gaussian wiretap channels. Compact formulas for those functions are explicitly given for stationary memoryless wiretap channels. It is also demonstrated that, based on such a pair of reliability and secrecy functions, we can control the tradeoff between reliability and secrecy (usually conflicting), both with exponentially decreasing rates as block length (n) becomes large. Four ways to do so are given on the basis of rate shifting, rate exchange, concatenation, and change of cost constraint. In addition, the notion of the (delta ) secrecy capacity is defined and shown to attain the strongest secrecy standard among others. The maximized versus averaged secrecy measures is also discussed.},
  Doi                      = {10.1109/TIT.2014.2355811},
  File                     = {Published version:Han-2014-p6819-6843.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {Gaussian processes;telecommunication channels;telecommunication network reliability;telecommunication security;Gaussian wiretap channels;Poisson wiretap channels;binary symmetric wiretap channels;concatenation;cost constraint change;rate exchange;rate shifting;reliability function;secrecy function;stationary memoryless channel;wiretap channel system;Channel models;Decoding;Manganese;Measurement uncertainty;Probability distribution;Reliability theory;Gaussian wiretap channel;Poisson wiretap channel;Reliability function;binary symmetric wiretap channel;change of cost constraint;concatenation;cost constraint;rate exchange;rate shifting;secrecy function;secrecy measures;tradeoff between reliability and secrecy},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6894161&isnumber=6926896},
  Zone                     = {2Q}
}

@Article{Haenninen-2014-p305-312,
  Title                    = {Bayesian networks for maritime traffic accident prevention: Benefits and challenges },
  Author                   = {Maria Hänninen},
  Journal                  = {Accident Analysis \& Prevention },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {305 - 312},
  Volume                   = {73},

  Abstract                 = {Abstract Bayesian networks are quantitative modeling tools whose applications to the maritime traffic safety context are becoming more popular. This paper discusses the utilization of Bayesian networks in maritime safety modeling. Based on literature and the author's own experiences, the paper studies what Bayesian networks can offer to maritime accident prevention and safety modeling and discusses a few challenges in their application to this context. It is argued that the capability of representing rather complex, not necessarily causal but uncertain relationships makes Bayesian networks an attractive modeling tool for the maritime safety and accidents. Furthermore, as the maritime accident and safety data is still rather scarce and has some quality problems, the possibility to combine data with expert knowledge and the easy way of updating the model after acquiring more evidence further enhance their feasibility. However, eliciting the probabilities from the maritime experts might be challenging and the model validation can be tricky. It is concluded that with the utilization of several data sources, Bayesian updating, dynamic modeling, and hidden nodes for latent variables, Bayesian networks are rather well-suited tools for the maritime safety management and decision-making. },
  Doi                      = {http://dx.doi.org/10.1016/j.aap.2014.09.017},
  File                     = {Published version:Haenninen-2014-p305-312.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0001-4575},
  Keywords                 = {Bayesian networks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0001457514002735},
  Zone                     = {-}
}

@Article{Hare-2010-p128-139,
  Title                    = {The interdependent security problem in the defense industrial base: An agent-based model on a social network },
  Author                   = {Forrest Hare and Jonathan Goldstein},
  Journal                  = {International Journal of Critical Infrastructure Protection },
  Year                     = {2010},
  Number                   = {3–4},
  Pages                    = {128 - 139},
  Volume                   = {3},

  Abstract                 = {This paper extends the Kunreuther and Heal game-theoretic model of the interdependent security problem by applying the model to an empirically based network environment. The scale-free network is derived from the contract-based organization of the defense industry. Theft of knowledge in this industry could have an impact on both economic and national security. However, imperfect information in the \{IT\} security market and the interdependent nature of the investment decision create the potential for underinvestment or overinvestment. An agent-based modeling technique demonstrates that social network topology may influence policy measures designed to induce tipping and cascading in cyber security investments. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijcip.2010.07.001},
  File                     = {Published version:Hare-2010-p128-139.pdf:PDF},
  Impactfactor             = {0.63},
  ISSN                     = {1874-5482},
  Keywords                 = {Interdependent security},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1874548210000387},
  Zone                     = {4Q}
}

@Article{Haroutunia-2000-p2690-2697,
  Title                    = {On rate-reliability-distortion function for a robust descriptions system},
  Author                   = {Haroutunian, E.A. and Harutyunyan, A.N. and Ghazaryan, A.R.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Nov},
  Number                   = {7},
  Pages                    = {2690-2697},
  Volume                   = {46},

  Abstract                 = {A source coding problem is considered for a robust description system with one encoder and many decoders. The rate-reliability-distortion function is specified. An example is given showing the distinction in the calculation of the rate-reliability-distortion function as compared with the rate-distortion function},
  Doi                      = {10.1109/18.887883},
  File                     = {Published version:Haroutunia-2000-p2690-2697.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {reliability;source coding;decoder;encoder;rate-reliability-distortion function;robust descriptions system;source coding problem;Associate members;Decoding;Distortion measurement;Error probability;Information theory;Probability distribution;Random variables;Rate-distortion;Robustness;Source coding},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=887883&isnumber=19190},
  Zone                     = {2Q}
}

@Article{Harp-1995-p727-737,
  Title                    = {Modeling student knowledge with self-organizing feature maps},
  Author                   = {Harp, S.A. and Samad, T. and Villano, M.},
  Journal                  = {Systems, Man and Cybernetics, IEEE Transactions on},
  Year                     = {1995},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {727-737},
  Volume                   = {25},

  Doi                      = {10.1109/21.376487},
  File                     = {Published version:Harp-1995-p727-737.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0018-9472},
  Keywords                 = {intelligent tutoring systems;self-organising feature maps;user modelling;aircraft fuel management;computer aided instruction;insensitivity;intelligent tutoring system;knowledge space theory;neural networks;noise-tolerance;self-organizing feature maps;student assessment;student knowledge modelling;Aircraft;Automatic testing;Computer aided instruction;Context modeling;Educational programs;Fuels;Intelligent networks;Intelligent systems;Neural networks;Spatial databases},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=376487&isnumber=8585},
  Zone                     = {-}
}

@Article{Harutyunya-2004-p2768-2773,
  Title                    = {On properties of rate-reliability-distortion functions},
  Author                   = {Harutyunyan, A.N. and Haroutunian, E.A.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {2768-2773},
  Volume                   = {50},

  Abstract                 = {Some important properties of the rate-reliability-distortion function of discrete memoryless source (DMS) are established. For the binary source and Hamming distortion measure this function is derived and analyzed. Even that elementary case suffices to show the nonconvexity of the rate-reliability-distortion function in the reliability (error exponent) argument.},
  Doi                      = {10.1109/TIT.2004.836706},
  File                     = {Published version:Harutyunya-2004-p2768-2773.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {Hamming codes;binary codes;memoryless systems;rate distortion theory;reliability;source coding;Hamming distortion measure;binary source;discrete memoryless source;error exponent;nonconvexity;rate-reliability-distortion functions;reliability argument;Codes;Data compression;Decoding;Distortion measurement;Information theory;Notice of Violation;Random variables;Rate distortion theory;Rate-distortion;Source coding;Convexity;Hamming distance;error exponent;rate- distortion function;rate-reliability-distortion function;reliability;time- sharing argument},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1347362&isnumber=29673},
  Zone                     = {2Q}
}

@Article{Hassine-2014-p15-24,
  Title                    = {Time to failure prediction in rubber components subjected to thermal ageing: A combined approach based upon the intrinsic defect concept and the fracture mechanics },
  Author                   = {M. Ben Hassine and M. Naït-Abdelaziz and F. Zaïri and X. Colin and C. Tourcher and G. Marque},
  Journal                  = {Mechanics of Materials },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {15 - 24},
  Volume                   = {79},

  Abstract                 = {Abstract In this contribution, we attempt to derive a tool allowing the prediction of the stretch ratio at failure in rubber components subjected to thermal ageing. To achieve this goal, the main idea is to combine the fracture mechanics approach and the intrinsic defect concept. Using an accelerated ageing procedure for an Ethylene–Propylene–Diene Monomer (EPDM), it is first shown that the average molar mass of the elastically active chains (i.e. between crosslinks) can be used as the main indicator of the macromolecular network degradation. By introducing the time–temperature equivalence principle, a shift factor obeying to an Arrhenius law is derived, and master curves are built as well for the average molar mass as for the ultimate mechanical properties. Fracture mechanics tests are also achieved and the square root dependence of the fracture energy with the average molar mass is pointed out. Moreover, it is shown that the mechanical response could be approximated by the phantom network theory, which allows to relate the strain energy density function to the average molar mass. Assuming that the fracture of a smooth specimen is the consequence of a virtual intrinsic defect whose the size can be easily estimated, the stretch ratio at break can be therefore computed for any thermal ageing condition. The estimated values are found in a very nice agreement with \{EPDM\} experimental data, making this approach a useful tool when designing rubber components for moderate to high temperature environments.},
  Doi                      = {http://dx.doi.org/10.1016/j.mechmat.2014.07.015},
  File                     = {Published version:Hassine-2014-p15-24.pdf:PDF},
  Impactfactor             = {1.936},
  ISSN                     = {0167-6636},
  Keywords                 = {Rubber},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167663614001318},
  Zone                     = {3Q}
}

@InProceedings{He-2008-p249-253,
  Title                    = {A Network Security Risk Assessment Framework Based on Game Theory},
  Author                   = {Wei He and Chunhe Xia and Cheng Zhang and Yi Ji and Xinyi Ma},
  Booktitle                = {Future Generation Communication and Networking, 2008. FGCN '08. Second International Conference on},
  Year                     = {2008},
  Month                    = {Dec},
  Pages                    = {249-253},
  Volume                   = {2},

  Doi                      = {10.1109/FGCN.2008.166},
  File                     = {Published version:He-2008-p249-253.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer networks;decision theory;game theory;risk management;telecommunication security;GTADM;HRCM;attacker behavioral decision;game process;game theoretical attack-defense model;hierarchical risk computing model;network security risk assessment;Computer networks;Computer science;Computer security;Data security;Game theory;Helium;History;Information security;Risk analysis;Risk management;GTADM;Game Theory;HRCM;network security risk assessment framework},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4734216&isnumber=4734155},
  Zone                     = {-}
}

@InProceedings{He-2012-p1-6,
  Title                    = {Generic security cases for information system security in healthcare systems},
  Author                   = {He, Y. and Johnson, C.W.},
  Booktitle                = {System Safety, incorporating the Cyber Security Conference 2012, 7th IET International Conference on},
  Year                     = {2012},
  Month                    = {Oct},
  Pages                    = {1-6},

  Doi                      = {10.1049/cp.2012.1507},
  File                     = {Published version:He-2012-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data privacy;health care;medical information systems;safety-critical software;security of data;China;GSN;Shenzhen Hospital;United States VA administration;United States Veterans Affairs administration;clinician confidentiality;cyber-security policies;data breach incidents;generic security cases;goal structuring notations;graphical argumentation techniques;healthcare systems;information system security;large complex healthcare organisations;patient confidentiality;safety-critical systems;security arguments;security incident analysis;security management;security recommendations;Generic Security Case;Healthcare System;Security Incidents;System Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6464470&isnumber=6494528},
  Zone                     = {-}
}

@Article{Hearn-2001-p730-734,
  Title                    = {Electrostatic ignition hazards from flexible intermediate bulk containers (FIBCs) with materials of minimum ignition energies down to 0.12 mJ},
  Author                   = {Hearn, G.L.},
  Journal                  = {Industry Applications, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {730-734},
  Volume                   = {37},

  Abstract                 = {The use of flexible intermediate bulk containers (FIBCs), or “big bags” as they are commonly known, is becoming widespread in a variety of industries, including chemical, pharmaceutical, and foodstuffs. Typically, FIBCs, roughly cubic in shape and constructed from woven polypropylene, are used to store and transport powdered or granular material in loads of between 300-1000 kg. In many cases, FIBCs are used in the vicinity of sensitive flammable atmospheres which may arise from the presence of dispersed combustible dusts or solvent vapors. Over the years, there have been a number of serious fires and explosions (particularly during the discharging of the bag contents) which have been attributed to static electricity. As a result, various “antistatic” FIBC designs have been produced containing intrinsic features aimed at minimizing the risk of electrostatic ignition. This paper presents the results of a rigorous assessment of a variety of FIBC designs with regard to electrostatic ignition hazard in flammable atmospheres of minimum ignition energy 0.12, 0.25, and 1.5 mJ. The study involved setting up a test rig to enable the FIBCs to be filled and emptied under controlled conditions of relative humidity. During these operations, the electrostatic activity in terms of surface potentials, electric field, and electrostatic discharge energy was monitored. Incendiary discharges were quantified by using a calibrated propane/oxygen/nitrogen gas probe. The findings of this paper indicate which of the designs can be safely used in various industrial processes without risk of ignition},
  Doi                      = {10.1109/28.924752},
  File                     = {Published version:Hearn-2001-p730-734.pdf:PDF},
  Impactfactor             = {1.672},
  ISSN                     = {0093-9994},
  Keywords                 = {electric ignition;electrostatic discharge;materials handling;powders;probes;0.12 mJ;0.25 mJ;1.5 mJ;300 to 1000 kg;FIBC emptying;FIBC filling;antistatic FIBC designs;big bags;calibrated propane/oxygen/nitrogen gas probe;dispersed combustible dusts;electric field;electrostatic activity;electrostatic discharge energy;electrostatic ignition hazard;electrostatic ignition hazards;flammable atmospheres;flexible intermediate bulk containers;granular material;incendiary discharges;minimum ignition energies;powdered material;sensitive flammable atmospheres;solvent vapors;static electricity;surface potentials;woven polypropylene;Atmosphere;Chemical hazards;Chemical industry;Containers;Electrostatics;Flammability;Food industry;Ignition;Pharmaceuticals;Shape},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=924752&isnumber=19998},
  Zone                     = {3Q}
}

@Article{Hearty-2009-p124-137,
  Title                    = {Predicting Project Velocity in XP Using a Learning Dynamic Bayesian Network Model},
  Author                   = {Hearty, P. and Fenton, N. and Marquez, D. and Neil, M.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {124-137},
  Volume                   = {35},

  Abstract                 = {Bayesian networks, which can combine sparse data, prior assumptions and expert judgment into a single causal model, have already been used to build software effort prediction models. We present such a model of an extreme programming environment and show how it can learn from project data in order to make quantitative effort predictions and risk assessments without requiring any additional metrics collection program. The model's predictions are validated against a real world industrial project, with which they are in good agreement.},
  Doi                      = {10.1109/TSE.2008.76},
  File                     = {Published version:Hearty-2009-p124-137.pdf:PDF},
  Impactfactor             = {2.588},
  ISSN                     = {0098-5589},
  Keywords                 = {belief networks;project management;risk management;software metrics;XP;extreme programming;learning dynamic Bayesian network model;metrics collection program;project velocity;quantitative effort predictions;risk assessments;software development;software effort prediction models;Bayesian networks;causal models;extreme programming;risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4624275&isnumber=4771845},
  Zone                     = {1Q}
}

@Article{Heckerman-1995-p49-57,
  Title                    = {Decision-theoretic troubleshooting},
  Author                   = {Heckerman, David and Breese, John S and Rommelse, Koos},
  Journal                  = {Communications of the ACM},
  Year                     = {1995},
  Number                   = {3},
  Pages                    = {49--57},
  Volume                   = {38},

  File                     = {Published version:Heckerman-1995-p49-57.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Publisher                = {ACM},
  Timestamp                = {2015.05.08},
  Url                      = {http://dl.acm.org/ft_gateway.cfm?id=203341&ftid=31000&dwn=1&CFID=510027334&CFTOKEN=52435956},
  Zone                     = {-}
}

@Article{Heikkil-2010-p430-435,
  Title                    = {Challenges in risk management in multi-company industrial parks },
  Author                   = {Anna-Mari Heikkil and Yngve Malmén and Minna Nissil? and Helena Kortelainen},
  Journal                  = {Safety Science },
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {430 - 435},
  Volume                   = {48},

  Abstract                 = {The trend of companies focusing ever stronger on their core businesses has led to outsourcing of certain activities, e.g. maintenance, and even to selling out parts of industrial plants. This is the case also in Finland. Clusters of neighbouring companies, commonly called industrial parks, have been formed. In these multi-company parks safety and environmental responsibilities are not always clear and the risk map has changed. It has become evident that Finnish legislation has not followed this development of the industrial sector and the formation of industrial and technology parks. A Finnish project on the safety in chemical industrial parks has studied how safety and environmental issues can be best managed in multi-company chemical parks, and how the current legislation in Finland supports companies facing problems accentuated in or specific to industrial parks. The outcome outlines the identified challenges as well as the benefits of cooperation between the independent companies in industrial parks. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2009.12.008},
  File                     = {Published version:Heikkil-2010-p430-435.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Industrial park},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753509002197},
  Zone                     = {3Q}
}

@Article{Henneaux-2013-p4722-4731,
  Title                    = {Blackout Probabilistic Risk Assessment and Thermal Effects: Impacts of Changes in Generation},
  Author                   = {Henneaux, P. and Labeau, P.-E. and Maun, J.-C.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {4722-4731},
  Volume                   = {28},

  Doi                      = {10.1109/TPWRS.2013.2263851},
  File                     = {Published version:Henneaux-2013-p4722-4731.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {power generation faults;power generation reliability;risk management;wind power plants;blackout probabilistic risk assessment;blackout risk estimation;cascading failure mechanisms;cascading failures risk;cross-border flows variability;dynamic probabilistic blackout;power plants shut-down;renewable energy deregulation;renewable energy integration;thermal effects;wind farms penetration;Blackout;Monte Carlo methods;power system reliability;power system security;risk analysis},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6523180&isnumber=6627990},
  Zone                     = {2Q}
}

@Article{Henneaux-2013-p4722-4731a,
  Title                    = {Blackout Probabilistic Risk Assessment and Thermal Effects: Impacts of Changes in Generation},
  Author                   = {Henneaux, P. and Labeau, P.-E. and Maun, J.-C.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {4722-4731},
  Volume                   = {28},

  Abstract                 = {Renewable energy integration and deregulation imply that the electric grid will be operated near its limits in the future, and that the variability of cross-border flows will increase. Therefore, it is becoming more and more crucial to study the impact of these changes on the risk of cascading failures leading to blackout. We propose in this paper to emphasize important factors leading to blackouts, to review methodologies which were developed to simulate cascading failure mechanisms and to study specifically the impact of thermal effects on the risk of blackout for several changes in generation (variations in cross-border flows, wind farms penetration, shut-down of power plants). This is studied by applying to a test system the first level of a dynamic probabilistic blackout risk assessment developed previously. We show that taking into account thermal effects in cascading failures is important not only to have a good estimation of the risk of blackout in different grid configurations, but also to determine if a specific change in generation has a positive or a negative impact on the blackout risk.},
  Doi                      = {10.1109/TPWRS.2013.2263851},
  File                     = {Published version:Henneaux-2013-p4722-4731a.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {power generation faults;power generation reliability;risk management;wind power plants;blackout probabilistic risk assessment;blackout risk estimation;cascading failure mechanisms;cascading failures risk;cross-border flows variability;dynamic probabilistic blackout;power plants shut-down;renewable energy deregulation;renewable energy integration;thermal effects;wind farms penetration;Blackout;Monte Carlo methods;power system reliability;power system security;risk analysis},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，电力系统，给了一个 Case，指标是电力系统的指标},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6523180&isnumber=6627990},
  Zone                     = {2Q}
}

@Article{Henneaux-2013-p4722-4731b,
  Title                    = {Blackout Probabilistic Risk Assessment and Thermal Effects: Impacts of Changes in Generation},
  Author                   = {Henneaux, P. and Labeau, P.-E. and Maun, J.-C.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {4722-4731},
  Volume                   = {28},

  Abstract                 = {Renewable energy integration and deregulation imply that the electric grid will be operated near its limits in the future, and that the variability of cross-border flows will increase. Therefore, it is becoming more and more crucial to study the impact of these changes on the risk of cascading failures leading to blackout. We propose in this paper to emphasize important factors leading to blackouts, to review methodologies which were developed to simulate cascading failure mechanisms and to study specifically the impact of thermal effects on the risk of blackout for several changes in generation (variations in cross-border flows, wind farms penetration, shut-down of power plants). This is studied by applying to a test system the first level of a dynamic probabilistic blackout risk assessment developed previously. We show that taking into account thermal effects in cascading failures is important not only to have a good estimation of the risk of blackout in different grid configurations, but also to determine if a specific change in generation has a positive or a negative impact on the blackout risk.},
  Doi                      = {10.1109/TPWRS.2013.2263851},
  File                     = {Published version:Henneaux-2013-p4722-4731b.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {power generation faults;power generation reliability;risk management;wind power plants;blackout probabilistic risk assessment;blackout risk estimation;cascading failure mechanisms;cascading failures risk;cross-border flows variability;dynamic probabilistic blackout;power plants shut-down;renewable energy deregulation;renewable energy integration;thermal effects;wind farms penetration;Blackout;Monte Carlo methods;power system reliability;power system security;risk analysis},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6523180&isnumber=6627990},
  Zone                     = {2Q}
}

@Article{Herrero-2013-p250-261,
  Title                    = {RT-MOVICAB-IDS: Addressing real-time intrusion detection },
  Author                   = {Álvaro Herrero and Martí Navarro and Emilio Corchado and Vicente Julián},
  Journal                  = {Future Generation Computer Systems },
  Year                     = {2013},
  Note                     = {Including Special section: AIRCC-NetCoM 2009 and Special section: Clouds and Service-Oriented Architectures },
  Number                   = {1},
  Pages                    = {250 - 261},
  Volume                   = {29},

  Abstract                 = {This study presents a novel Hybrid Intelligent Intrusion Detection System (IDS) known as RT-MOVICAB-IDS that incorporates temporal control. One of its main goals is to facilitate real-time Intrusion Detection, as accurate and swift responses are crucial in this field, especially if automatic abortion mechanisms are running. The formulation of this hybrid \{IDS\} combines Artificial Neural Networks (ANN) and Case-Based Reasoning (CBR) within a Multi-Agent System (MAS) to detect intrusions in dynamic computer networks. Temporal restrictions are imposed on this IDS, in order to perform real/execution time processing and assure system response predictability. Therefore, a dynamic real-time multi-agent architecture for \{IDS\} is proposed in this study, allowing the addition of predictable agents (both reactive and deliberative). In particular, two of the deliberative agents deployed in this system incorporate temporal-bounded CBR. This upgraded \{CBR\} is based on an anytime approximation, which allows the adaptation of this Artificial Intelligence paradigm to real-time requirements. Experimental results using real data sets are presented which validate the performance of this novel hybrid IDS. },
  Doi                      = {http://dx.doi.org/10.1016/j.future.2010.12.017},
  File                     = {Published version:Herrero-2013-p250-261.pdf:PDF},
  Impactfactor             = {1.864},
  ISSN                     = {0167-739X},
  Keywords                 = {Hybrid Artificial Intelligent Systems},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167739X10002785},
  Zone                     = {2Q}
}

@InProceedings{Heussen-2012-p1-6,
  Title                    = {Understanding control function and failure from a process perspective},
  Author                   = {Heussen, K. and Lind, M.},
  Booktitle                = {Complexity in Engineering (COMPENG), 2012},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {1-6},

  Doi                      = {10.1109/CompEng.2012.6242946},
  File                     = {Published version:Heussen-2012-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {control system synthesis;fault tolerance;process control;complex process control system design;control abstractions;control failure modes;control functions;dynamical process;fault identification;fault tolerant control;formal analysis;formal methodology;mathematical model;offline analysis;online supervisory control;process behaviour;process functions;Analytical models;Biological system modeling;Cognition;Context;Control systems;Mathematical model;Process control},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6242946&isnumber=6242942},
  Zone                     = {-}
}

@Article{Hibey-1983-p621-624,
  Title                    = {Stochastic reliability functions for failure rates derived from Gauss - Markov processes (Corresp.)},
  Author                   = {Hibey, Joseph L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1983},

  Month                    = {Jul},
  Number                   = {4},
  Pages                    = {621-624},
  Volume                   = {29},

  Doi                      = {10.1109/TIT.1983.1056702},
  File                     = {Published version:Hibey-1983-p621-624.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {Failure analysis;Gaussian processes;Markov processes;Counting circuits;Density functional theory;Frequency estimation;Gaussian processes;Logic;Markov processes;Mean square error methods;Notice of Violation;Stochastic processes;Yield estimation},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1056702&isnumber=22733},
  Zone                     = {2Q}
}

@Article{Hibey-1983-p621-624a,
  Title                    = {Stochastic reliability functions for failure rates derived from Gauss - Markov processes (Corresp.)},
  Author                   = {Hibey, Joseph L.},
  Journal                  = {Information Theory, IEEE Transactions on},
  Year                     = {1983},

  Month                    = {Jul},
  Number                   = {4},
  Pages                    = {621-624},
  Volume                   = {29},

  Abstract                 = {An extension of the well-known Cameron-Martin formula can be interpreted as the expectation of a stochastic reliability function applicable in those situations where nondecreasing failure rates are desired. This follows ff the failure rate is modeled as the square of a Gauss-Markov process. We describe the methodology for the general vector case, and then specialize the results to the one-dimensional case so as to obtain an exact closed-form expression for the reliability function. Using the theory of recurrent and transient processes, we then show how the choice of a model parameter and the initial state influence reliability.},
  Doi                      = {10.1109/TIT.1983.1056702},
  File                     = {Published version:Hibey-1983-p621-624a.pdf:PDF},
  Impactfactor             = {2.621},
  ISSN                     = {0018-9448},
  Keywords                 = {Failure analysis;Gaussian processes;Markov processes;Counting circuits;Density functional theory;Frequency estimation;Gaussian processes;Logic;Markov processes;Mean square error methods;Notice of Violation;Stochastic processes;Yield estimation},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Function 表示函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1056702&isnumber=22733},
  Zone                     = {2Q}
}

@Article{Hindley-2014-p10-19,
  Title                    = {Optimisation of the link volume for weakest link failure prediction in NBG-18 nuclear graphite },
  Author                   = {Michael P. Hindley and Albert A. Groenwold and Deborah C. Blaine and Thorsten H. Becker},
  Journal                  = {Nuclear Engineering and Design },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {10 - 19},
  Volume                   = {274},

  Abstract                 = {Abstract This paper describes the process for approximating the optimal size of a link volume required for weakest link failure calculation in nuclear graphite, with NBG-18 used as an example. As part of the failure methodology, the link volume is defined in terms of two grouping criteria. The first criterion is a factor of the maximum grain size and the second criterion is a function of an equivalent stress limit. A methodology for approximating these grouping criteria is presented. The failure methodology employs finite element analysis (FEA) in order to predict the failure load, at 50% probability of failure. The average experimental failure load, as determined for 26 test geometries, is used to evaluate the accuracy of the weakest link failure calculations. The influence of the two grouping criteria on the failure load prediction is evaluated by defining an error in prediction across all test cases. Mathematical optimisation is used to find the minimum error across a range of test case failure predictions. This minimum error is shown to deliver the most accurate failure prediction across a whole range of components, although some test cases in the range predict conservative failure load. The mathematical optimisation objective function is penalised to account for non-conservative prediction of the failure load for any test case. The optimisation is repeated and a link volume found for conservative failure prediction. The failure prediction for each test case is evaluated, in detail, for the proposed link volumes. Based on the analysis, link design volumes for NBG-18 are recommended for either accurate or conservative failure prediction. },
  Doi                      = {http://dx.doi.org/10.1016/j.nucengdes.2014.04.003},
  File                     = {Published version:Hindley-2014-p10-19.pdf:PDF},
  Impactfactor             = {0.805},
  ISSN                     = {0029-5493},
  Keywords                 = {Nuclear graphite},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0029549314001897},
  Zone                     = {4Q}
}

@InProceedings{Hirata-2012-p1340-1343,
  Title                    = {Diagnosis photovoltaic failure by simple function method to acquire I–V curve of photovoltaic modules string},
  Author                   = {Hirata, Y. and Noro, S. and Aoki, T. and Miyazawa, S.},
  Booktitle                = {Photovoltaic Specialists Conference (PVSC), 2012 38th IEEE},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {001340-001343},

  Doi                      = {10.1109/PVSC.2012.6317848},
  File                     = {Published version:Hirata-2012-p1340-1343.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0160-8371},
  Keywords                 = {failure analysis;fault diagnosis;solar cells;DC power capacity;I-V characteristic;I-V curve;PV generation;PV module;diagnosis photovoltaic failure function;photovoltaic module string;power conditioner;simple function method;variable DC power;voltage dependent areas;Area measurement;Capacitance;Current measurement;Photovoltaic systems;Resistance;Silicon;Voltage measurement;current-voltage characteristics;curve fitting;failure analysis;fault detection;photovoltaic systems;power conditioning;system analysis design},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6317848&isnumber=6317554},
  Zone                     = {-}
}

@Article{Hoffmann-2014-p201-205,
  Title                    = {Small Scale Slope Failure Benchmark Test. Modelling and Prediction },
  Author                   = {C. Hoffmann and N. Meler and N.M. Pinyol and E.E. Alonso},
  Journal                  = {Procedia Earth and Planetary Science },
  Year                     = {2014},
  Note                     = {The Third Italian Workshop on Landslides: Hydrological Response of Slopes through Physical Experiments, Field Monitoring and Mathematical Modeling },
  Number                   = {0},
  Pages                    = {201 - 205},
  Volume                   = {9},

  Abstract                 = {Abstract A benchmark was designed with the aim of evaluating the capabilities of current modelling techniques and computational codes to reproduce slope failures. Laboratory tests and three instrumented small scale experiments of slopes initially unsaturated and subjected to a controlled rainfall up to failure were performed. The objective of the benchmark was to predict one of the slope failures knowing the rest of the data. The paper presents the modelling strategy and the results obtained using the finite element code Code_Bright and the Barcelona Basic Model as the constitutive model for the unsaturated soils. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeps.2014.06.021},
  File                     = {Published version:Hoffmann-2014-p201-205.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1878-5220},
  Keywords                 = {unsaturated soil},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1878522014000502},
  Zone                     = {-}
}

@InProceedings{Hofmann-1990-p84-87,
  Title                    = {A knowledge-based reliability analysis system},
  Author                   = {Hofmann, M.O. and McCullough, Claire L.},
  Booktitle                = {Systems, Man and Cybernetics, 1990. Conference Proceedings., IEEE International Conference on},
  Year                     = {1990},
  Month                    = {Nov},
  Pages                    = {84-87},

  Abstract                 = {Probabilistic reliability analysis is reviewed, and an expert system that assists a reliability engineer in performing risk analysis for complex systems is presented. The system releases the reliability engineer from the repetitive tasks of enumerating support states and modifying the primary failure equation and assists in organizing target system descriptions into an explicit model and in deriving a block diagram from it. The methodology used to derive Boolean failure equations from a description of the target system is described. System functionality is modeled in such a way that reliability analysis can be facilitated and automated as far as possible. Knowledge about objects in the application domain, i.e. components and functions, as well as knowledge about the analysis process is incorporated into the analysis system. Knowledge is represented in the form of frames and rules in the function dependency model and in the supporting agents},
  Doi                      = {10.1109/ICSMC.1990.142065},
  File                     = {Published version:Hofmann-1990-p84-87.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {engineering computing;expert systems;knowledge representation;probability;reliability;Boolean failure equations;agents;complex systems;expert system;function dependency model;knowledge based systems;knowledge representation;reliability analysis system;risk analysis;Equations;Failure analysis;Maintenance;Power generation;Power system modeling;Power system reliability;Redundancy;Reliability engineering;Risk analysis;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=142065&isnumber=3815},
  Zone                     = {-}
}

@Article{Hole-2009-p34-41,
  Title                    = {Risk Assessment of a National Security Infrastructure},
  Author                   = {Hole, K.J. and Klingsheim, A.N. and Netland, L.-H. and Espelid, Y. and Tjostheim, T. and Moen, V.},
  Journal                  = {Security Privacy, IEEE},
  Year                     = {2009},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {34-41},
  Volume                   = {7},

  __markedentry            = {[Qiqi:5]},
  Abstract                 = {In Norway, BankID is the banking industry's public-key infrastructure (PKI) of choice for authenticating Internet customers. But do BanklD's differences from standard PKIs make it a riskier choice? This assessment, based on both publicly available information and usage experiences, addresses that question.},
  Doi                      = {10.1109/MSP.2009.17},
  File                     = {Published version:Hole-2009-p34-41.pdf:PDF},
  Impactfactor             = {0.962},
  ISSN                     = {1540-7993},
  Keywords                 = {Internet;bank data processing;public key cryptography;BankID;Internet customer authentication;banking industry;national security infrastructure;public-key infrastructure;risk assessment;standard PKIs;Authentication;Certification;Content addressable storage;Digital signatures;Ice;National security;Privacy;Public key;Public key cryptography;Risk management},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {C，杂志，没仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4768652&isnumber=4768640},
  Zone                     = {4Q}
}

@Article{Hole-2010-p21-27,
  Title                    = {Toward Risk Assessment of Large-Impact and Rare Events},
  Author                   = {Hole, K.J. and Netland, L.-H.},
  Journal                  = {Security Privacy, IEEE},
  Year                     = {2010},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {21-27},
  Volume                   = {8},

  Abstract                 = {Traditional risk assessment methods underestimate the risks of large-impact, hard-to-predict, and rare events in information systems. An alternative approach extends these methods to better evaluate risks associated with black and gray swans.},
  Doi                      = {10.1109/MSP.2010.55},
  File                     = {Published version:Hole-2010-p21-27.pdf:PDF},
  Impactfactor             = {0.962},
  ISSN                     = {1540-7993},
  Keywords                 = {information systems;risk management;black swan risk evaluation;gray swan risk evaluation;information systems;risk assessment method;LHR;and rare events;black swan;gray swan;hard-to-predict;identity systems;large-impact;risk assessment},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5406500&isnumber=5470945},
  Zone                     = {4Q}
}

@Article{Holm-2015-p304-318,
  Title                    = {A Bayesian network model for likelihood estimations of acquirement of critical software vulnerabilities and exploits},
  Author                   = {Hannes Holm and Matus Korman and Mathias Ekstedt},
  Journal                  = {Information and Software Technology },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {304 - 318},
  Volume                   = {58},

  Abstract                 = {AbstractContext Software vulnerabilities in general, and software vulnerabilities with publicly available exploits in particular, are important to manage for both developers and users. This is however a difficult matter to address as time is limited and vulnerabilities are frequent. Objective This paper presents a Bayesian network based model that can be used by enterprise decision makers to estimate the likelihood that a professional penetration tester is able to obtain knowledge of critical vulnerabilities and exploits for these vulnerabilities for software under different circumstances. Method Data on the activities in the model are gathered from previous empirical studies, vulnerability databases and a survey with 58 individuals who all have been credited for the discovery of critical software vulnerabilities. Results The proposed model describes 13 states related by 17 activities, and a total of 33 different datasets. Conclusion Estimates by the model can be used to support decisions regarding what software to acquire, or what measures to invest in during software development projects. },
  Doi                      = {http://dx.doi.org/10.1016/j.infsof.2014.07.001},
  File                     = {Published version:Holm-2015-p304-318.pdf:PDF},
  Impactfactor             = {1.522},
  ISSN                     = {0950-5849},
  Keywords                 = {Cyber security},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950584914001554},
  Zone                     = {3Q}
}

@Article{Hong-2003-p528-541,
  Title                    = {A neurofuzzy network knowledge extraction and extended Gram-Schmidt algorithm for model subspace decomposition},
  Author                   = {Xia Hong and Harris, C.J.},
  Journal                  = {Fuzzy Systems, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Aug},
  Number                   = {4},
  Pages                    = {528-541},
  Volume                   = {11},

  Doi                      = {10.1109/TFUZZ.2003.814842},
  File                     = {Published version:Hong-2003-p528-541.pdf:PDF},
  Impactfactor             = {5.484},
  ISSN                     = {1063-6706},
  Keywords                 = {digital simulation;fuzzy neural nets;inference mechanisms;knowledge acquisition;matrix algebra;modelling;optimisation;parameter estimation;uncertain systems;A-optimality;Gram-Schmidt orthogonal decomposition algorithm;T-S inference mechanism;Takagi-Sugeno inference mechanism;average model output sensitivity;extended Gram-Schmidt algorithm;fuzzy rules;model matrix feature subspace;model subspace decomposition;model transparency;neurofuzzy model construction;neurofuzzy network knowledge extraction;observed finite data sets;one-to-one mapping;parameter estimation;rule-based knowledge extraction;unknown dynamical systems;weighting matrix;Data mining;Design for experiments;Energy states;Fuzzy sets;Fuzzy systems;Inference algorithms;Inference mechanisms;Matrix decomposition;Parameter estimation;Takagi-Sugeno model},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1220298&isnumber=27420},
  Zone                     = {1Q}
}

@InProceedings{Hou-2013-p1001-1004,
  Title                    = {Research and Application of Dynamic Risk Assessment Model for Tunnel Construction of Thin Layered Rock},
  Author                   = {Yao Hou},
  Booktitle                = {Digital Manufacturing and Automation (ICDMA), 2013 Fourth International Conference on},
  Year                     = {2013},
  Month                    = {June},
  Pages                    = {1001-1004},

  Abstract                 = {As a result of the risk and uncertainty of underground engineering itself, a large number of risks had effected the tunnel construction which is complex between mutual relationship, Established risk dynamic assessment system of tunnel construction on thin layered rock, from risk factors as geological conditions, construction methods, technology and management to assess and manage the risks dynamically, that could reduce the risk level from high to low effectively and to reach security, stability construction requirements. Specific case of dynamic risk management and assessment system of thin layered rock in tunnel construction could proof it validity.},
  Doi                      = {10.1109/ICDMA.2013.234},
  File                     = {Published version:Hou-2013-p1001-1004.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {geotechnical engineering;risk analysis;rocks;structural engineering;tunnels;dynamic risk assessment model;geological conditions;risk factors;risk management;risk reduction;thin layered rock;tunnel construction methods;underground engineering;Geologic measurements;Indexes;Risk management;Rocks;Temperature measurement;Tunneling;Assessment Indexes;Dynamic Risk Assessment Model;Management Indexes;Thin Layered Rock},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6598158&isnumber=6597910},
  Zone                     = {-}
}

@InProceedings{Houmb-2002-p79-91,
  Title                    = {Towards a UML profile for model-based risk assessment},
  Author                   = {Houmb, Siv Hilde and Den Braber, Folker and Lund, M Soldal and St{\o}len, Ketil},
  Booktitle                = {Critical systems development with UML-Proceedings of the UML’02 workshop},
  Year                     = {2002},
  Organization             = {Citeseer},
  Pages                    = {79--91},

  File                     = {Houmb-2002-p79-91.pdf:Houmb-2002-p79-91.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.14},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.195.6517&rep=rep1&type=pdf},
  Zone                     = {-}
}

@InProceedings{Housel-1979-p194-208,
  Title                    = {The Functional Dependency Model For Logical Database Design},
  Author                   = {Housel, B.C. and Waddle, V. and Yao, S. B.},
  Booktitle                = {Very Large Data Bases, 1979. Fifth International Conference on},
  Year                     = {1979},
  Month                    = {Oct},
  Pages                    = {194-208},

  Abstract                 = {In order to have an effective logical database design tool, a designer should have facilities for specifying his data and application requirements at a high level. These specifications should be invariant with respect to a given realization, and should not require irrelevant details. Also, one should be able to compare alternative designs at a high level in order to reduce the solution space at minimal cost. This paper presents several concepts which are aimed toward the above objectives. Specifically, we define a data model, a transaction specification language, and an approach for high level design evaluation.},
  Doi                      = {10.1109/VLDB.1979.718135},
  File                     = {Published version:Housel-1979-p194-208.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Arithmetic;Costs;Data models;Humans;Laboratories;Personnel;Process design;Specification languages;Transaction databases},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=718135&isnumber=15497},
  Zone                     = {-}
}

@Article{Hsiao-2008-p311-321,
  Title                    = {Risk Informed Design Refinement of a Power System Protection Scheme},
  Author                   = {Tsun-Yu Hsiao and Chan-Nan Lu},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2008},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {311-321},
  Volume                   = {57},

  Doi                      = {10.1109/TR.2008.920796},
  File                     = {Published version:Hsiao-2008-p311-321.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {fault trees;power system faults;power system protection;power system reliability;risk management;power system protection scheme;power system reliability;probabilistic risk-based index;risk informed design refinement;risk reduction;system operating practices;wide-area disturbances;Fault tree analysis;interval analysis;probabilistic risk assessment;remedial action scheme;sensitivity;special protection system;system protection scheme;uncertainty},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4488173&isnumber=4538180},
  Zone                     = {2Q}
}

@Article{Hsieh-2014-p268-276,
  Title                    = {In vitro measurement and dynamic modeling-based approaches for deposition risk assessment of inhaled aerosols in human respiratory system },
  Author                   = {Nan-Hung Hsieh and Chung-Min Liao},
  Journal                  = {Atmospheric Environment },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {268 - 276},
  Volume                   = {95},

  Abstract                 = {Abstract Respiratory deposition dynamics of inhaled particle are developed rapidly in recent decades. Understandings of aerosol properties are useful for predicting respiratory deposition risk. This study conducted an aerosol exposure experiment to quantify the respiratory deposition dynamics of inhaled aerosols, and to infer the deposition risk probability. The experimental aerosols included reference oil droplets and road dust particles. This study developed an aerosol dynamic model to simulate time-dependent particle concentration in exposure chamber and respiratory system. The parameters of particle loss in exposure chamber and deposition in respiratory system can be estimated by experimental measurements. The deposition risks were estimated through particle size distributions and size-dependent deposition fractions. We showed that the experimental and predicted deposition fractions were consistent with the previous in vivo, in vitro and in silico studies. We found that the generated aerosols were polydisperse that followed a lognormal distribution with geometric mean diameters of 0.52 and 0.26 μm for resuspended oil droplet and road dust, respectively. The deposition rate estimates range from 0.015 to 0.362 and 0.013 to 0.157 s−1 in particle size ranging from 0.3 to 3.0 and 0.3 to 4.0 μm for oil droplet and road dust, respectively. Result also revealed that inhaled oil droplet had higher respiratory deposition risk than road dust aerosol. Our study has major implications for the respiratory tract burden of inhaled fine particles from long-term exposure in ambient air based on our developed probabilistic risk model. },
  Doi                      = {http://dx.doi.org/10.1016/j.atmosenv.2014.06.045},
  File                     = {Published version:Hsieh-2014-p268-276.pdf:PDF},
  Impactfactor             = {3.11},
  ISSN                     = {1352-2310},
  Keywords                 = {Particulate matter},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，不相干},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1352231014004919},
  Zone                     = {2Q}
}

@InProceedings{Hu-2010-p1-5,
  Title                    = {GIS reliability analysis based trapezoid fuzzy fault tree},
  Author                   = {Shengwu Hu and Xiaosan Ge},
  Booktitle                = {Geoinformatics, 2010 18th International Conference on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {1-5},

  Abstract                 = {GIS reliability refers to the ability to complete the requirements under the specified conditions and time. In this paper, there are five elements including object, conditions of use, use of time, functions and capabilities, to evaluate GIS reliability proposed by the combination in a engineering GIS. The paper imported trapezoid fuzzy fault tree into GIS reliability analysis for the fist time. The paper discussed how GIS reliability analysis uses trapezoid fuzzy fault tree method, mainly researches two problems that are GIS trapezoid fuzzy fault tree establishing and analysis step of GIS trapezoid fuzzy fault tree; uses example to adopt trapezoid fuzzy fault tree method computing GIS reliability analysis; because the technique considers not only GIS random uncertainty but also GIS fuzzy uncertainty, the result is precise, scientific and reasonable; finally summarizes up and points out the problems to need solving.},
  Doi                      = {10.1109/GEOINFORMATICS.2010.5567634},
  File                     = {Published version:Hu-2010-p1-5.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {fault trees;fuzzy set theory;geographic information systems;GIS fuzzy uncertainty;GIS reliability analysis;trapezoid fuzzy fault tree;Fault trees;Geographic Information Systems;Logic gates;Reliability theory;Software reliability;Uncertainty;GIS;fuzzy fault tree;reliability analysis;trapezoid fuzzy fault tree},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5567634&isnumber=5567473},
  Zone                     = {-}
}

@Article{Hu-2013-p522-533,
  Title                    = {A dynamic multimedia fuzzy-stochastic integrated environmental risk assessment approach for contaminated sites management },
  Author                   = {Yan Hu and Jing-ya Wen and Xiao-li Li and Da-zhou Wang and Yu Li},
  Journal                  = {Journal of Hazardous Materials },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {522 - 533},
  Volume                   = {261},

  Abstract                 = {Abstract A dynamic multimedia fuzzy-stochastic integrated environmental risk assessment approach was developed for contaminated sites management. The contaminant concentrations were simulated by a validated interval dynamic multimedia fugacity model, and different guideline values for the same contaminant were represented as a fuzzy environmental guideline. Then, the probability of violating environmental guideline (Pv) can be determined by comparison between the modeled concentrations and the fuzzy environmental guideline, and the constructed relationship between the Pvs and environmental risk levels was used to assess the environmental risk level. The developed approach was applied to assess the integrated environmental risk at a case study site in China, simulated from 1985 to 2020. Four scenarios were analyzed, including “residential land” and “industrial land” environmental guidelines under “strict” and “loose” strictness. It was found that \{PAH\} concentrations will increase steadily over time, with soil found to be the dominant sink. Source emission in soil was the leading input and atmospheric sedimentation was the dominant transfer process. The integrated environmental risks primarily resulted from petroleum spills and coke ovens, while the soil environmental risks came from coal combustion. The developed approach offers an effective tool for quantifying variability and uncertainty in the dynamic multimedia integrated environmental risk assessment and the contaminated site management. },
  Doi                      = {http://dx.doi.org/10.1016/j.jhazmat.2013.08.009},
  File                     = {Published version:Hu-2013-p522-533.pdf:PDF},
  Impactfactor             = {3.925},
  ISSN                     = {0304-3894},
  Keywords                 = {Contaminated sites management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，环境风险评估，有参考价值},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304389413005657},
  Zone                     = {1Q}
}

@Article{Huang-1996-p225-263,
  Title                    = {Inference in belief networks: A procedural guide },
  Author                   = {Cecil Huang and Adnan Darwiche},
  Journal                  = {International Journal of Approximate Reasoning },
  Year                     = {1996},
  Number                   = {3},
  Pages                    = {225 - 263},
  Volume                   = {15},

  Abstract                 = {Belief networks are popular tools for encoding uncertainty in expert systems. These networks rely on inference algorithms to compute beliefs in the context of observed evidence. One established method for exact inference on belief networks is the probability propagation in trees of clusters (PPTC) algorithm, as developed by Lauritzen and Spiegelhalter and refined by Jensen et al. \{PPTC\} converts the belief network into a secondary structure, then computes probabilities by manipulating the secondary structure. In this document, we provide a self-contained, procedural guide to understanding and implementing PPTC. We synthesize various optimizations to \{PPTC\} that are scattered throughout the literature. We articulate undocumented “open secrets” that are vital to producing a robust and efficient implementation of PPTC. We hope that this document makes probabilistic inference more accessible and affordable to those without extensive prior exposure. },
  Doi                      = {http://dx.doi.org/10.1016/S0888-613X(96)00069-2},
  File                     = {Published version:Huang-1996-p225-263.pdf:PDF},
  Impactfactor             = {1.729},
  ISSN                     = {0888-613X},
  Keywords                 = {artificial intelligence},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.02},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0888613X96000692},
  Zone                     = {3Q}
}

@Article{Huang-2012-p1485-1493,
  Title                    = {An Efficient Probabilistic Assessment Method for Electricity Market Risk Management},
  Author                   = {Jie Huang and Yusheng Xue and Zhao Yang Dong and Kit Po Wong},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {1485-1493},
  Volume                   = {27},

  Doi                      = {10.1109/TPWRS.2012.2183900},
  File                     = {Published version:Huang-2012-p1485-1493.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {Monte Carlo methods;power markets;risk management;Australian National Electricity Market system;Monte Carlo simulation results;adaptive importance sampling;efficient probabilistic assessment method;electricity market risk management;electricity price risk management;estimation efficiency;generator bidding strategy;hybrid probabilistic assessment method;load level;market participants;outage rate;price spike probability;renewable energy output;risk control;sequential importance sampling;system load;Electricity supply industry;Estimation;Monte Carlo methods;Probabilistic logic;Proposals;Risk management;Uncertainty;Adaptive importance sampling;electricity market;price estimation;price spike probability;risk assessment;sequential importance sampling},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6159106&isnumber=6243218},
  Zone                     = {2Q}
}

@InProceedings{Huang-2008-p1354-1359,
  Title                    = {Equipment risk management assessment in the petrochemical industry},
  Author                   = {Huang, R.H. and Yang, C.L. and Kao, C.S.},
  Booktitle                = {Management of Innovation and Technology, 2008. ICMIT 2008. 4th IEEE International Conference on},
  Year                     = {2008},
  Month                    = {Sept},
  Pages                    = {1354-1359},

  Abstract                 = {This study examines quantization and quality factors to construct an assessment model for equipment risk management in the petrochemical industry. The procedure is divided into three stages: 1. setting up a preliminary indicator construct, 2. revising assessment indicators and 3. setting up assessment models. This model is based on five major dimensions, including financial performance, logistic support, service level, learning and innovation and risk control. It further develops thirteen strategy subjects and seventy-seven performance-measurement indicators. Performance assessment indicators are initially established and revised after collecting expert opinions by questionnaire. Further, Analysis Network Process (ANP) is used to calculate the weighted values of indicators at each layer and to construct assessment models with applicable and valuable references.},
  Doi                      = {10.1109/ICMIT.2008.4654568},
  File                     = {Huang-2008-p1354-1359.pdf:Huang-2008-p1354-1359.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {chemical industry;financial management;logistics;petrochemicals;petroleum industry;production equipment;analysis network process;equipment risk management assessment;financial performance;innovation;logistic support;petrochemical industry;risk control;Chemical industry;Electric breakdown;Electrical equipment industry;Industrial control;Logistics;Performance analysis;Petrochemicals;Risk analysis;Risk management;Technological innovation;Petrochemical Industry;analysis network process;equipment risk management;risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4654568&isnumber=4654323},
  Zone                     = {-}
}

@Article{Huang-2012-p1056-1066,
  Title                    = {Assessment model for equipment risk management: Petrochemical industry cases },
  Author                   = {Rong-Hwa Huang and Chang-Lin Yang and Chung-Szu Kao},
  Journal                  = {Safety Science },
  Year                     = {2012},
  Note                     = {First International Symposium on Mine Safety Science and Engineering 2011 },
  Number                   = {4},
  Pages                    = {1056 - 1066},
  Volume                   = {50},

  Abstract                 = {This study presents an assessment model that examines quantity and quality factors for equipment risk management in the petrochemical industry. The proposed model has five dimensions—financial performance, logistical support, service level, learning and innovation, and risk control. This evaluation model uses 13 strategy subjects and 78 performance-measurement indicators. Performance assessment indicators are initially established and revised based on expert opinions collected via a questionnaire. Further, the analytical network process (ANP) is utilized to calculate the weights of indicators in each layer and to construct assessment models with applicable and valuable references. To determine model practicability, this study assesses four subsidiaries of the case company. Each subsidiary has a capitalization exceeding \{TWD\} 50 billion. In addition to evaluating company performance in terms of each dimension and indicator, the proposed model provides a valuable reference for decision-making in equipment risk management. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2010.02.024},
  File                     = {Published version:Huang-2012-p1056-1066.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Petrochemical industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {看了没感觉},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753511003146},
  Zone                     = {3Q}
}

@Article{Huang-2012-p1056-1066a,
  Title                    = {Assessment model for equipment risk management: Petrochemical industry cases },
  Author                   = {Rong-Hwa Huang and Chang-Lin Yang and Chung-Szu Kao},
  Journal                  = {Safety Science },
  Year                     = {2012},
  Note                     = {First International Symposium on Mine Safety Science and Engineering 2011 },
  Number                   = {4},
  Pages                    = {1056 - 1066},
  Volume                   = {50},

  Abstract                 = {This study presents an assessment model that examines quantity and quality factors for equipment risk management in the petrochemical industry. The proposed model has five dimensions—financial performance, logistical support, service level, learning and innovation, and risk control. This evaluation model uses 13 strategy subjects and 78 performance-measurement indicators. Performance assessment indicators are initially established and revised based on expert opinions collected via a questionnaire. Further, the analytical network process (ANP) is utilized to calculate the weights of indicators in each layer and to construct assessment models with applicable and valuable references. To determine model practicability, this study assesses four subsidiaries of the case company. Each subsidiary has a capitalization exceeding \{TWD\} 50 billion. In addition to evaluating company performance in terms of each dimension and indicator, the proposed model provides a valuable reference for decision-making in equipment risk management. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2010.02.024},
  File                     = {Published version:Huang-2012-p1056-1066a.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Petrochemical industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753511003146},
  Zone                     = {3Q}
}

@Article{Huang-2008-p2064-2074,
  Title                    = {A Study on Cycle Attack by Multiaccess Interference in Multigranularity OCDM-Based Optical Networks},
  Author                   = {Huang, S. and Baba, K.-i. and Murata, M. and Kitayama, K.},
  Journal                  = {Lightwave Technology, Journal of},
  Year                     = {2008},

  Month                    = {July},
  Number                   = {14},
  Pages                    = {2064-2074},
  Volume                   = {26},

  Abstract                 = {Previously, an optical code-division multiplexing (OCDM)-based network architecture was proposed to improve the wavelength utilization and to provide finer bandwidth granularities to users. By this technology, different channels using distinct optical codes (OCs) can be multiplexed onto the same wavelength, in which an OC is considered as the basic unit in lightpath provisioning. In the ideal case, multiaccess interference (MAI) inherent to the OCDM technology is assumed to be removed completely at intermediate nodes and cannot be propagated or accumulated along the lightpath. However, since no optical-electrical (O/E) or electrical-optical (E/O) conversion is allowed in transparent OCDM-based optical networks, the MAI cannot be removed completely at intermediate nodes with current all-optical regeneration techniques. As a result, the residual MAI may be propagated and accumulated along the lightpath and affect other active lightpaths carried by the same wavelength in the network. The affected active lightpaths may build unintended cycles along which the MAI is accumulated. Furthermore, this MAI keeps increasing when the lightpaths traversed by the cycle are active, which deteriorates the lightpath signal quality. Since this deterioration may eventually result in unacceptable signal quality and service disruption, the phenomenon caused by the MAI is termed as cycle attack in this paper. The explanations of the MAI propagation mechanism and the cycle attack problem are given. A depth-first search (DFS)-based algorithm is proposed to diagnose such cycle attacks under dynamic traffic conditions. The numerical results show that our DFS-based cycle attack diagnostic algorithm enables one to detect cycle attacks effectively, and the two-way resource reservation method associated with heuristic wavelength assignment is shown to mitigate the blocking performance degradation due to cycle attacks greatly with some proper wavelength and OC configuration.},
  Doi                      = {10.1109/JLT.2008.919483},
  File                     = {Published version:Huang-2008-p2064-2074.pdf:PDF},
  Impactfactor             = {2.555},
  ISSN                     = {0733-8724},
  Keywords                 = {code division multiplexing;interference (signal);multi-access systems;optical fibre networks;optical repeaters;tree searching;wavelength assignment;MAI propagation mechanism;active lightpaths;all-optical regeneration techniques;cycle attack diagnostic algorithm;depth-first search algorithm;heuristic wavelength assignment;lightpath signal quality;multiaccess interference;multigranularity OCDM;network architecture;optical code-division multiplexing;optical codes;optical networks;resource reservation method;Code division multiplexing;Computer security;Degradation;Multiple access interference;Optical crosstalk;Optical fiber networks;Optical network units;Optical noise;Optical propagation;Wavelength assignment;Attack;cycle;depth-first search (DFS);multiaccess interference (MAI);optical code-division multiplexing (OCDM);optical cross-connect (OXC);wavelength-routed optical network (WRON)},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4610004&isnumber=4609977},
  Zone                     = {2Q}
}

@Article{Hubballi-2014-p1-17,
  Title                    = {False alarm minimization techniques in signature-based intrusion detection systems: A survey },
  Author                   = {Neminath Hubballi and Vinoth Suryanarayanan},
  Journal                  = {Computer Communications },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {1 - 17},
  Volume                   = {49},

  Abstract                 = {Abstract A network based Intrusion Detection System (IDS) gathers and analyzes network packets and report possible low level security violations to a system administrator. In a large network setup, these low level and partial reports become unmanageable to the administrator resulting in some unattended events. Further it is known that state of the art \{IDS\} generate many false alarms. There are techniques proposed in \{IDS\} literature to minimize false alarms, many of which are widely used in practice in commercial Security Information and Event Management (SIEM) tools. In this paper, we review existing false alarm minimization techniques in signature-based Network Intrusion Detection System (NIDS). We give a taxonomy of false alarm minimization techniques in signature-based \{IDS\} and present the pros and cons of each class. We also study few of the prominent commercial \{SIEM\} tools which have implemented these techniques along with their performance. Finally, we conclude with some directions to the future research. },
  Doi                      = {http://dx.doi.org/10.1016/j.comcom.2014.04.012},
  File                     = {Published version:Hubballi-2014-p1-17.pdf:PDF},
  Impactfactor             = {1.079},
  ISSN                     = {0140-3664},
  Keywords                 = {False alarms},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0140366414001480},
  Zone                     = {3Q}
}

@Article{Hudepohl-1996-p56-60,
  Title                    = {Emerald: software metrics and models on the desktop},
  Author                   = {Hudepohl, J.P. and Aud, S.J. and Khoshgoftaar, T.M. and Allen, E.B. and Mayrand, J.},
  Journal                  = {Software, IEEE},
  Year                     = {1996},

  Month                    = {Sep},
  Number                   = {5},
  Pages                    = {56-60},
  Volume                   = {13},

  Abstract                 = {As software becomes more and more sophisticated, industry has begun to place a premium on software reliability. The telecommunications industry is no exception. Consequently software reliability is a strategic business weapon in an increasingly competitive marketplace. In response to these concerns, BNR, Nortel, and Bell Canada developed the Enhanced Measurement for Early Risk Assessment of Latent Defects (Emerald), a decision support system designed to improve telecommunications software reliability. Emerald efficiently integrates software measurements, quality models, and delivery of results to the desktop of software developers. We have found that Emerald not only improves software reliability, but also facilitates the accurate correction of field problems. Our experiences developing Emerald have also taught us some valuable lessons about the implementation and adoption of this type of software tool},
  Doi                      = {10.1109/52.536459},
  File                     = {Published version:Hudepohl-1996-p56-60.pdf:PDF},
  Impactfactor             = {1.616},
  ISSN                     = {0740-7459},
  Keywords                 = {decision support systems;software metrics;software quality;software reliability;software tools;telecommunication computing;Emerald;Enhanced Measurement for Early Risk Assessment of Latent Defects;competitive marketplace;decision support system;software measurements;software metrics;software models;software quality models;software reliability;software tool;strategic business weapon;telecommunications industry;telecommunications software reliability;Communication industry;Computer industry;Decision support systems;Defense industry;Risk management;Software measurement;Software metrics;Software quality;Software reliability;Weapons},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=536459&isnumber=11391},
  Zone                     = {3Q}
}

@Article{Huh-2011-p210-218,
  Title                    = {A Novel Risk Assessment for Complex Structural Systems},
  Author                   = {Jungwon Huh and Haldar, A.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {210-218},
  Volume                   = {60},

  Doi                      = {10.1109/TR.2010.2104191},
  File                     = {Published version:Huh-2011-p210-218.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {finite element analysis;nuclear power stations;reliability;risk management;seismology;stochastic processes;complex structural systems;nuclear power plants;reliability;risk management;seismic risk assessment;stochastic finite element method;Dynamic response;finite element methods;probabilistic risk assessment;structural engineering},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，有时间好好看看，复杂系统的风险评估},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5688471&isnumber=5721872},
  Zone                     = {2Q}
}

@Article{Huh-2011-p210-218a,
  Title                    = {A Novel Risk Assessment for Complex Structural Systems},
  Author                   = {Jungwon Huh and Haldar, A.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {210-218},
  Volume                   = {60},

  Abstract                 = {Risk management is an essential tool for safe, economical, and efficient design, operation, and maintenance of complex engineering systems. Seismic risk assessment of structures, particularly in nuclear power plants, needs special attention from the reliability community. Available risk assessment methods may not be sufficient to estimate the risk of complex systems made with different materials, numerous ways elements are connected to each other, and excited by dynamic loadings including seismic loading applied in the time domain. A hybrid risk assessment approach is proposed by intelligently integrating the stochastic finite element method, and the response surface method. It is capable of estimating the probability just before failure considering all major sources of nonlinearity and uncertainty, eliminating the deficiencies of the currently available reliability methods. With the help of the illustrative examples, it is shown that the method is robust, accurate, and efficient in estimating risk of complex systems excited by dynamic loadings applied in the time domain.},
  Doi                      = {10.1109/TR.2010.2104191},
  File                     = {Published version:Huh-2011-p210-218a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {finite element analysis;nuclear power stations;reliability;risk management;seismology;stochastic processes;complex structural systems;nuclear power plants;reliability;risk management;seismic risk assessment;stochastic finite element method;Dynamic response;finite element methods;probabilistic risk assessment;structural engineering},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，举例},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5688471&isnumber=5721872},
  Zone                     = {2Q}
}

@Article{Huh-2011-p210-218b,
  Title                    = {A Novel Risk Assessment for Complex Structural Systems},
  Author                   = {Jungwon Huh and Haldar, A.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {210-218},
  Volume                   = {60},

  Abstract                 = {Risk management is an essential tool for safe, economical, and efficient design, operation, and maintenance of complex engineering systems. Seismic risk assessment of structures, particularly in nuclear power plants, needs special attention from the reliability community. Available risk assessment methods may not be sufficient to estimate the risk of complex systems made with different materials, numerous ways elements are connected to each other, and excited by dynamic loadings including seismic loading applied in the time domain. A hybrid risk assessment approach is proposed by intelligently integrating the stochastic finite element method, and the response surface method. It is capable of estimating the probability just before failure considering all major sources of nonlinearity and uncertainty, eliminating the deficiencies of the currently available reliability methods. With the help of the illustrative examples, it is shown that the method is robust, accurate, and efficient in estimating risk of complex systems excited by dynamic loadings applied in the time domain.},
  Doi                      = {10.1109/TR.2010.2104191},
  File                     = {Published version:Huh-2011-p210-218b.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {finite element analysis;nuclear power stations;reliability;risk management;seismology;stochastic processes;complex structural systems;nuclear power plants;reliability;risk management;seismic risk assessment;stochastic finite element method;Dynamic response;finite element methods;probabilistic risk assessment;structural engineering},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5688471&isnumber=5721872},
  Zone                     = {2Q}
}

@Article{Hwang-1979-p316-319,
  Title                    = {Reliability Optimization by Generalized Lagrangian-Function and Reduced-Gradient Methods},
  Author                   = {Hwang, C.L. and Tillman, F.A. and Kuo, W.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1979},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {316-319},
  Volume                   = {R-28},

  Abstract                 = {Nonlinear optimization problems for reliability of a complex system are solved using the generalized Lagrangian function (GLF) method and the generalized reduced gradient (GRG) method. GLF is twice continuously differentiable and closely related to the generalized penalty function which includes the interior and exterior penalty functions as a special case. GRG generalizes the Wolfe reduced gradient method and has been coded in FORTRAN title ``GREG'' by Abadie et al. Two system reliability optimization problems are solved. The first maximizes complex-system reliability with a tangent cost-function; the second minimizes the cost, with a minimum system reliability. The results are compared with those using the Sequential Unconstrained Minimization Technique (SUMT) and the direct search approach by Luus and Jaakola (LJ). Many algorithms have been proposed for solving the general nonlinear programming problem. Only a few have been demonstrated to be effective when applied to large-scale nonlinear programming problems, and none has proved to be so superior that it can be classified as a universal algorithm. Both GLF and GRG methods presented here have been successfully used in solving a number of general nonlinear programming problems in a variety of engineering applications and are better methods among the many algorithms.},
  Doi                      = {10.1109/TR.1979.5220617},
  File                     = {Published version:Hwang-1979-p316-319.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Costs;Functional programming;Gradient methods;Lagrangian functions;Large-scale systems;Linear programming;Minimization methods;Optimization methods;Reliability;Vectors;Generalized Lagrangian function method (GLF);Generalized reduced gradient method (GRG);Nonlinear programming;Optimum system reliability;Sequential Unconstrained Minimization Technique (SUMT)},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5220617&isnumber=5220595},
  Zone                     = {2Q}
}

@Article{Ie拧mantas-2014-p85-90,
  Title                    = {Bayesian assessment of electrical power transmission grid outage risk },
  Author                   = {Tomas Ie拧mantas and Robertas Alzbutas},
  Journal                  = {International Journal of Electrical Power \& Energy Systems },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {85 - 90},
  Volume                   = {58},

  Abstract                 = {Abstract Lack of unified risk assessment approach to electrical power transmission grid outages when taking into account uncertain data is addressed in this paper. Authors take on Bayesian approach to analyse statistical data of electric grid outages 鈥?this enables to achieve a more coherent way to express uncertainty in data and to obtain reliability related measures of the grid. The considered methodology of how to properly manage the statistical inference process is demonstrated through real outage data collected from North American transmission grid. The different cases of electrical power lines unreliability as well as cascading outages are addressed on various levels of complexity 鈥?starting from simple Bayesian assessment and then building a more general hierarchical Bayesian model. As a result, geography and environment related variability level is found to be of significant influence suggesting that unreliability of grid lines should in general be analysed having in mind specificity of each line. In addition, such variability highly influences the reliability of the whole grid or any network, as demonstrated in the paper as well. Considering the case of cascading outages, we obtained a hierarchical model, built under the basis of Borel鈥揟anner distribution, and demonstrated the capability to simulate large blackouts, which has a non-negligible probability of occurrence, as the history of blackouts in the last decades has already demonstrated. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijepes.2014.01.006},
  File                     = {Published version:Ie拧mantas-2014-p85-90.pdf:PDF},
  Impactfactor             = {3.432},
  ISSN                     = {0142-0615},
  Keywords                 = {Electrical grid},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0142061514000076},
  Zone                     = {2Q}
}

@InProceedings{Iino-2006-p914-919,
  Title                    = {Risk based Model Predictive Control with hybrid system structure and its application to Thermal Power Plants},
  Author                   = {Iino, Y. and Matsumoto, S. and Kamito, A.},
  Booktitle                = {SICE-ICASE, 2006. International Joint Conference},
  Year                     = {2006},
  Month                    = {Oct},
  Pages                    = {914-919},

  Abstract                 = {An industrial model predictive control method, which combines usual linear control and sequence control, is reformulated to a hybrid control algorithm. The plant start up and shut down sequence is naturally used in the industrial control method, but has not been analyzed theoretically. In this paper, the thermal power plant optimal load distribution problem is formulated as model predictive control with the constraint condition, and also the embedded start up and shut down optimal control sequence is designed with different objective functions. The new hierarchical control system structure is discussed as a hybrid control system, which can dramatically reduce the calculation effort, in spite of plant non-linearity, constraints and parameter uncertainty. A simulation example applied to a thermal power plant is demonstrated to show the effectiveness of the control method},
  Doi                      = {10.1109/SICE.2006.315571},
  File                     = {Published version:Iino-2006-p914-919.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {hierarchical systems;industrial control;load distribution;optimal control;power station control;predictive control;thermal power stations;constraint condition;embedded start up;hierarchical control system;hybrid control algorithm;industrial control;linear control;optimal control;optimal load distribution problem;risk based model predictive control;sequence control;shut down sequence;thermal power plants;Control systems;Electrical equipment industry;Industrial control;Optimal control;Power generation;Power system modeling;Predictive control;Predictive models;Thermal loading;Uncertain systems;Constraint condition;Hybrid control;Model Predictive Control;Multi-objective;Sequence control},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4109086&isnumber=4108014},
  Zone                     = {-}
}

@InProceedings{Imaizumi-2004-p2-959,
  Title                    = {Experiment evaluation of reliable control using equivalent transfer function for speed feedback loop failure},
  Author                   = {Imaizumi, T. and Ohmae, T.},
  Booktitle                = {SICE 2004 Annual Conference},
  Year                     = {2004},
  Month                    = {Aug},
  Pages                    = {955-959 vol. 2},
  Volume                   = {2},

  File                     = {Published version:Imaizumi-2004-p2-959.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {closed loop systems;feedback;position control;reliability;stability;transfer functions;active redundancy compensation;equivalent transfer function;multiple loops;position control system;reliable control;speed feedback loop failure;speed sensor break down;stability},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1491553&isnumber=32062},
  Zone                     = {-}
}

@InProceedings{Ishikawa-2008-p333-337,
  Title                    = {Reliable control using equivalent transfer function for position servo system in current loop failure},
  Author                   = {Ishikawa, K. and Nakamura, T. and Osumi, H.},
  Booktitle                = {Industrial Electronics, 2008. IECON 2008. 34th Annual Conference of IEEE},
  Year                     = {2008},
  Month                    = {Nov},
  Pages                    = {333-337},

  Doi                      = {10.1109/IECON.2008.4757975},
  File                     = {Published version:Ishikawa-2008-p333-337.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1553-572X},
  Keywords                 = {compensation;failure analysis;feedback;position control;servomechanisms;transfer functions;active redundancy compensation;current loop failure;equivalent transfer function;multiple loop control system;position servo system;reliable control system;Control systems;DC motors;Electric breakdown;Feedback loop;Fluctuations;Redundancy;Sensor systems;Servomechanisms;Servomotors;Transfer functions},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4757975&isnumber=4757911},
  Zone                     = {-}
}

@Article{Islam-2012-p1243-1256,
  Title                    = {Wireless Sensor Network Reliability and Security in Factory Automation: A Survey},
  Author                   = {Islam, K. and Weiming Shen and Xianbin Wang},
  Journal                  = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {1243-1256},
  Volume                   = {42},

  Abstract                 = {Industries can benefit a lot from integrating sensors in industrial plants, structures, machinery, shop floors, and other critical places and utilizing their sensing and monitoring power, communicating and processing abilities to deliver sensed information. Proper use of wireless sensor networks (WSNs) can lower the rate of catastrophic failures, and improve the efficiency and productivity of factory operations. Ensuring reliability and providing adequate security in these crucial services provided by WSNs will reinforce their acceptability as a viable and dependable technology in the factory and industrial domain. In this paper, we examine the reliability and security challenges of WSNs and survey their practicality for industrial adoption. We discuss the unique characteristics that distinguish the factory environment from the rest, elaborate on security and reliability issues with their respective solution measures, and analyze the existing WSN architectures and standards. A number of challenges and interesting research issues have emerged from this study and have been reported for further investigation.},
  Doi                      = {10.1109/TSMCC.2012.2205680},
  File                     = {Published version:Islam-2012-p1243-1256.pdf:PDF},
  Impactfactor             = {2.548},
  ISSN                     = {1094-6977},
  Keywords                 = {telecommunication network reliability;telecommunication security;wireless sensor networks;catastrophic failures;factory automation;wireless sensor network reliability;wireless sensor network security;Communication system security;Industrial engineering;Interference;Network security;Reliability;Sensors;Wireless communication;Wireless sensor networks;Cryptography;factory automation;reliability;security;wireless sensor networks (WSNs)},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6392470&isnumber=6330018},
  Zone                     = {2Q}
}

@InProceedings{Islam-2011-p70-75,
  Title                    = {Measuring security requirements for software security},
  Author                   = {Islam, S. and Falcarin, P.},
  Booktitle                = {Cybernetic Intelligent Systems (CIS), 2011 IEEE 10th International Conference on},
  Year                     = {2011},
  Month                    = {Sept},
  Pages                    = {70-75},

  Doi                      = {10.1109/CIS.2011.6169137},
  File                     = {Published version:Islam-2011-p70-75.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {business data processing;formal specification;risk management;security of data;software metrics;asset based risk management;business goal;goal-question-metric approach;risk mitigation;secure software;security metrics;security requirement;software security goal;software security measurement;Authentication;Authorization;Availability;Software;Software measurement;Security metrics;security goal;security requirements;software security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6169137&isnumber=6169123},
  Zone                     = {-}
}

@Article{Izvercian-2014-p598-605,
  Title                    = {Semantic Assessment of the Risk: A Multidimensional Data Model for Sustainable Enterprise },
  Author                   = {Monica Izvercian and Larisa Ivascu},
  Journal                  = {Procedia Technology },
  Year                     = {2014},
  Note                     = {\{CENTERIS\} 2014 - Conference on \{ENTERprise\} Information Systems / ProjMAN 2014 - International Conference on Project \{MANagement\} / \{HCIST\} 2014 - International Conference on Health and Social Care Information Systems and Technologies },
  Number                   = {0},
  Pages                    = {598 - 605},
  Volume                   = {16},

  Abstract                 = {Abstract Risk management has gained much attention in recent years as a “challenge” for all businesses. This paper presents a multidimensional model for risk assessment in sustainable enterprises. Risk assessment is done using Web 2.0, but passing to the semantics evaluation becomes opportune. The World Wide Web as a system of interconnecting documents accessed using the Internet had complex progress over time. So semantic approach to risk assessment becomes necessary in the current conditions in which they that are demarcated (approaches made) for Web 4.0 as a web of integration. Due to the significant potential impact of these risk factors on the business objectives and the impossibility of fully controlling them by the enterprise, risk analysis is an important dimension of enterprise management being necessary to develop a semantic work model. },
  Doi                      = {http://dx.doi.org/10.1016/j.protcy.2014.10.008},
  File                     = {Published version:Izvercian-2014-p598-605.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2212-0173},
  Keywords                 = {Semantic Web},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2212017314002357},
  Zone                     = {-}
}

@Article{Jaatun-2014-p872-876,
  Title                    = {Security Checklists: A Compliance Alibi, or a Useful Tool for Water Network Operators? },
  Author                   = {M.G. Jaatun and J. Røstum and S. Petersen and R. Ugarelli},
  Journal                  = {Procedia Engineering },
  Year                     = {2014},
  Note                     = {12th International Conference on Computing and Control for the Water Industry, \{CCWI2013\} },
  Number                   = {0},
  Pages                    = {872 - 876},
  Volume                   = {70},

  Abstract                 = {Abstract Checklist Compliance is a term that has been used derisively in the information security community, implying that checklists are something used for paying lip service to security without instigating real changes to technology or processes. In this paper we argue that checklists can also be used as a practical tool to quickly establish a security baseline for water and wastewater systems. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2014.02.096},
  File                     = {Published version:Jaatun-2014-p872-876.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {information security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705814000988},
  Zone                     = {-}
}

@Article{Jackson-2012-p930-943,
  Title                    = {Virus Propagation in Heterogeneous Bluetooth Networks with Human Behaviors},
  Author                   = {Jackson, J.T. and Creese, S.},
  Journal                  = {Dependable and Secure Computing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {930-943},
  Volume                   = {9},

  Abstract                 = {The growth in the use of Smartphones and other mobile computing devices continues to grow rapidly. As mobile wireless communications become ubiquitous, the networks and systems that depend upon them will become more complex. In parallel with this, the spread of digital viruses and malicious content will be an ever increasing threat within this interconnected paradigm requiring counteracting mechanisms to continuously adapt. Current security solutions for mobile devices remain limited in their ability to protect particularly against zero-day attacks. Understanding the propagation characteristics of malware could provide a means to planning protection strategies, but modeling virus propagation behavior in mobile wireless and peer-to-peer communications devices is still immature. A compartmental-based virus propagation model has been developed for Bluetooth communication networks incorporating wireless technological traits and factors that are known to affect virus propagation including human behaviors, heterogeneous devices, and antivirus measures. The model is novel in the richness of its treatment of human factors alongside the technology factors that could impact spread. A simulation scenario, together with an analysis of the spreading dynamics has been conducted to determine how a Bluetooth virus might spread under different conditions. Although demonstrated through Bluetooth, the approach is applicable to malware propagation in general.},
  Doi                      = {10.1109/TDSC.2012.72},
  File                     = {Published version:Jackson-2012-p930-943.pdf:PDF},
  Impactfactor             = {1.059},
  ISSN                     = {1545-5971},
  Keywords                 = {Bluetooth;computer network security;computer viruses;human factors;mobile computing;peer-to-peer computing;smart phones;antivirus measure;compartmental-based virus propagation model;digital virus;heterogeneous bluetooth network;human behaviors;human factors;interconnected network;malicious content;malware propagation;mobile computing devices;mobile wireless communication;parallel computing;peer-to-peer communication;protection strategy planning;smart phone;technology factor;ubiquitous computing;virus propagation behavior modeling;zero-day attack;Bluetooth;Human factors;Mathematical model;Mobile communication;Smart phones;Ubiquitous computing;Viruses (medical);Human factors;invasive software (viruses;pervasive computing;trojan horses);wireless;worms},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6268271&isnumber=6303808},
  Zone                     = {3Q}
}

@Article{Janan-1985-p329-337,
  Title                    = {On Multistate System Analysis},
  Author                   = {Janan, Xue},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1985},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {329-337},
  Volume                   = {R-34},

  Abstract                 = {Discrete function theory, which extends switching function theory and multiple-valued logic function theory, is introduced into multistate system analysis. Some theoretical conclusions and algorithms which play key roles in multistate system analysis are presented. The concepts of s-coherence and duality in binary-state system analysis are generalized. The set of minimal upper (maximum lower) vectors for level j, which play the role of min path (cut) set, is introduced to represent the states of a monotonic multistate system. Two approaches to computing state probability of multistate systems are given, one is based on inclusion-exclusion, the other is based on enumeration. Binary-state fault-tree is extended to multistate fault-tree. A computer code (MSTA1) has been programmed and is used to evaluate a multistate fault-tree. Multistate fault-tree and the computer code have been applied to paper-making industry; the results are consistent with the field data.},
  Doi                      = {10.1109/TR.1985.5222178},
  File                     = {Published version:Janan-1985-p329-337.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Algorithm design and analysis;Computer industry;Computer science;Logic functions;Performance analysis;Pulp and paper industry;Reliability theory;System performance;Discrete function;Dual;Multistate;Multistate fault-tree;s-Coherent component},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5222178&isnumber=5222159},
  Zone                     = {2Q}
}

@Article{Jardin-2014-p-,
  Title                    = {Bankruptcy prediction using terminal failure processes },
  Author                   = {Philippe du Jardin},
  Journal                  = {European Journal of Operational Research },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract Traditional bankruptcy prediction models, designed using classification or regression techniques, achieve short-term performances (1 year) that are fairly good, but that often worsen when the prediction horizon exceeds 1 year. We show how to improve the performance of such models beyond 1 year using models that take into account the evolution of firm’s financial health over a short period of time. For this purpose, we design models that fit the underlying failure process of different groups of firms. Our results demonstrate that such models lead to better prediction accuracy at a 3-year horizon than that achieved with common models. },
  Doi                      = {http://dx.doi.org/10.1016/j.ejor.2014.09.059},
  File                     = {Published version:Jardin-2014-p-.pdf:PDF},
  Impactfactor             = {2.038},
  ISSN                     = {0377-2217},
  Keywords                 = {Forecasting},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S037722171400798X},
  Zone                     = {2Q}
}

@InProceedings{Jerman-Bla-1990-p361-365,
  Title                    = {Security in value added networks-security requirements for EDI},
  Author                   = {Jerman-Blazic, B.},
  Booktitle                = {Telecommunications Symposium, 1990. ITS '90 Symposium Record., SBT/IEEE International},
  Year                     = {1990},
  Month                    = {Sep},
  Pages                    = {361-365},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/ITS.1990.175628},
  File                     = {Published version:Jerman-Bla-1990-p361-365.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {electronic data interchange;protocols;security of data;telecommunication networks;EDI security requirement;ISO/IEC JTC1/SWG-EDI documents;X.400;electronic data interchange;protocols;security services;value added networks;Data handling;Data security;Electronic mail;IEC;ISO;Intelligent networks;Postal services;Protocols;Transaction databases;Wide area networks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=175628&isnumber=4464},
  Zone                     = {-}
}

@InProceedings{Jiang-2012-p747-751,
  Title                    = {Specificationof change points of failure rate or intensity function: A non-parametric approach},
  Author                   = {Jiang, R.},
  Booktitle                = {Industrial Engineering and Engineering Management (IEEM), 2012 IEEE International Conference on},
  Year                     = {2012},
  Month                    = {Dec},
  Pages                    = {747-751},

  Doi                      = {10.1109/IEEM.2012.6837839},
  File                     = {Published version:Jiang-2012-p747-751.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {failure analysis;nonparametric statistics;reliability theory;change point estimation;cluster analysis method;failure rate estimation;failure rate function;intensity function;nonparametric approach;Hazards;Integrated circuits;Interpolation;Jet engines;Maintenance engineering;Robustness;Change point;bathtub curve;failure rate;intensity function;non-parametric method},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6837839&isnumber=6837685},
  Zone                     = {-}
}

@InProceedings{Jiang-2010-p355-360,
  Title                    = {Periodic Real-Time Message Scheduling for Confidentiality-Aware Cyber-Physical System in Wireless Networks},
  Author                   = {Wei Jiang and Wensheng Guo and Nan Sang},
  Booktitle                = {Frontier of Computer Science and Technology (FCST), 2010 Fifth International Conference on},
  Year                     = {2010},
  Month                    = {Aug},
  Pages                    = {355-360},

  Doi                      = {10.1109/FCST.2010.117},
  File                     = {Published version:Jiang-2010-p355-360.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer network security;radio access networks;risk management;scheduling;confidentiality aware cyber physical system;periodic real time message scheduling;risk based security profit model;security problem;security slack based heuristic algorithm;wireless networks;Communication system security;Real time systems;Scheduling;Security;Wireless networks;Wireless sensor networks;Confidentiality-aware;Periodic Message scheduling;Security-critical Cyber-Physical System;Wireless networks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5575936&isnumber=5575458},
  Zone                     = {-}
}

@Article{Jiang-2014-p-,
  Title                    = {Dynamic security management for real-time embedded applications in industrial networks },
  Author                   = {Wei Jiang and Yue Ma and Nan Sang and Ziguo Zhong},
  Journal                  = {Computers \& Electrical Engineering },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract Widely deployed real-time embedded systems can improve the performance of industrial applications, but these systems also face the critical challenge of providing high quality security in an unpredictable network environment. We measure the time and energy consumptions of commonly used cryptographic algorithms on a real embedded platform and introduce a method to quantify the security risk of real-time applications. We propose a Dynamic Security Risk Management (DSRM) mechanism to manage the aperiodic real-time tasks for networked industrial applications. Inspired by the feedback design philosophy, \{DSRM\} is designed as a two-level control mechanism. The upper-level component makes efforts to admit or reject the arrival tasks and assigns the reasonable security level for each admitted task. With three proportional feedback controllers at the lower level, the security level of each ready task can be adjusted adaptively according to the dynamic environments. Simulation results show the superiority of the proposed mechanism. },
  Doi                      = {http://dx.doi.org/10.1016/j.compeleceng.2014.10.002},
  File                     = {Published version:Jiang-2014-p-.pdf:PDF},
  Impactfactor             = {0.928},
  ISSN                     = {0045-7906},
  Keywords                 = {Real-time embedded system},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0045790614002353},
  Zone                     = {4Q}
}

@Article{Jin-2012-p65-73,
  Title                    = {Method for Assessing Grid Frequency Deviation Due to Wind Power Fluctuation Based on“Time-Frequency Transformation”},
  Author                   = {Lin Jin and Sun Yuan-zhang and Sorensen, P. and Li Guo-jie and Gao Weng-zhong},
  Journal                  = {Sustainable Energy, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {65-73},
  Volume                   = {3},

  Abstract                 = {Grid frequency deviation caused by wind power fluctuation has been a major concern for secure operation of a power system with integrated large-scale wind power. Many approaches have been proposed to assess this negative effect on grid frequency due to wind power fluctuation. Unfortunately, most published studies are based entirely on deterministic methodology. This paper presents a novel assessment method based on “Time-Frequency Transformation” to overcome shortcomings of existing methods. The main contribution of the paper is to propose a stochastic process “simulation” model which is a better alternative of the existing dynamic frequency deviation simulation model. In this way, the method takes the stochastic wind power fluctuation into full account so as to give a quantitative risk assessment of grid frequency deviation to grid operators, even without using any dynamic simulation tool. The case studies show that this method can be widely used in different types of wind power system analysis scenarios.},
  Doi                      = {10.1109/TSTE.2011.2162639},
  File                     = {Published version:Jin-2012-p65-73.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1949-3029},
  Keywords                 = {power grids;power system security;risk management;time-frequency analysis;wind power;wind power plants;dynamic frequency deviation simulation model;grid frequency deviation assessment method;grid operators;integrated large-scale wind power;power system security;quantitative risk assessment;stochastic process simulation model;time-frequency transformation;wind power fluctuation;wind power system analysis;Fluctuations;Fourier transforms;Stochastic processes;Time domain analysis;Time frequency analysis;Wind power generation;Fourier transform (FT);Wiener–Khinchin theorem;frequency deviation;power spectrum density (PSD);wind power fluctuation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5959228&isnumber=6102278},
  Zone                     = {-}
}

@Article{Jirattigal-2011-p249-258,
  Title                    = {Sparse power equalization placement for limiting jamming attack propagation in transparent optical networks },
  Author                   = {Amornrat Jirattigalachote and Nina Skorin-Kapov and Marija Furdek and Jiajia Chen and Paolo Monti and Lena Wosinska},
  Journal                  = {Optical Switching and Networking },
  Year                     = {2011},
  Note                     = {Optical network architectures and management },
  Number                   = {4},
  Pages                    = {249 - 258},
  Volume                   = {8},

  Abstract                 = {The latest advances in Wavelength Division Multiplexing (WDM) technology are making it possible to build all-optical transparent \{WDM\} networks, which are expected to be able to satisfy the rapid growth of today’s capacity demand. However, the transparency of such networks makes them highly vulnerable to deliberate attacks, specifically targeting the physical layer. Physical-layer attacks, such as high-power jamming, can cause severe service disruption or even service denial, enhanced by their capability to propagate through a transparent optical network. Several attack-aware routing and wavelength assignment algorithms have been proposed to reduce the possible disruption caused by high-power jamming attacks. However, even with network planning approaches which take network security, specifically physical-layer attacks, into account, resilience to deliberate attacks in such scenarios remains an issue. In this paper, we propose the use of wavelength-selective attenuators as power equalizers inside network nodes to limit the propagation of high-power jamming attacks. Due to the increased cost of optical switching nodes associated with the addition of power equalizers, we aim at minimizing their number through sparse power equalization placement. We developed a set of greedy algorithms to solve what we call the Power Equalization Placement (PEP) problem with the objective of minimizing the number of power equalizers needed to reduce, to a desired level, the propagation of high-power jamming attacks for a given routing scheme. We further improved upon these results by proposing a \{GRASP\} (Greedy Randomized Adaptive Search Procedure) heuristic with a somewhat longer execution time, but with significantly superior results. The performance evaluation results indicate that the proposed \{GRASP\} heuristic can achieve the same attack propagation reduction as can be obtained by equipping all nodes with power equalizers by placing them at less than 50% of the nodes on average, potentially yielding significant cost savings. },
  Doi                      = {http://dx.doi.org/10.1016/j.osn.2011.06.008},
  File                     = {Published version:Jirattigal-2011-p249-258.pdf:PDF},
  Impactfactor             = {0.717},
  ISSN                     = {1573-4277},
  Keywords                 = {Physical-layer attacks},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1573427711000567},
  Zone                     = {4Q}
}

@Article{Joksimovic-1982-p959-969,
  Title                    = {Insights from On-Going PRA Studies},
  Author                   = {Joksimovich, Vojin},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {1982},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {959-969},
  Volume                   = {29},

  Abstract                 = {Earlier papers (Ref. 1, 2) dealt with the subject of use of probabilistic risk assessment (PRA) techniques in safety and licensing. The last year's paper discussed an approach to establishing quantitative safety goals in the regulatory process (Ref. 3). Other authors, notably Saul Levine (e.g. Refs. 4,5), dealt with the same subjects from the perspectives of the RSS (Reactor Safety Study). PRA, as a safety assessment tool, experienced a tortuous acceptance. Prior to the Three Mile Island accident, despite existence of several studies, PRA was on the way to extinction. In the aftermath of TMI it was noted that WASH-1400 contains a similar accident sequence. This understanding began the rebirth of WASH-1400 which was completed by virtue of Kemeny Commission and Rogovin Inquiry recommendations. As a result, use of PRA techniques is rapidly becoming widespread. The objective of this paper is to provide some insights from on-going plant specific PRA Studies. Generic studies have been prolifically discussed elsewhere and are listed here primarily for the sake of completeness. The paper discusses briefly Limerick (Ref. 6), Big Rock Point (Ref. 7) and Sequoyah (Ref.8) studies from the standpoint of an outside party.},
  Doi                      = {10.1109/TNS.1982.4335997},
  File                     = {Published version:Joksimovic-1982-p959-969.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {Economic forecasting;Fuel economy;Inductors;Industrial accidents;Licenses;Petrochemicals;Power generation economics;Risk management;Safety;Temperature distribution},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4335997&isnumber=4335780},
  Zone                     = {3Q}
}

@Article{Jones-1995-p1025-1038,
  Title                    = {A methodology for human-machine systems research: knowledge engineering, modeling, and simulation},
  Author                   = {Jones, P.M. and Chu, R.W. and Mitchell, C.M.},
  Journal                  = {Systems, Man and Cybernetics, IEEE Transactions on},
  Year                     = {1995},

  Month                    = {Jul},
  Number                   = {7},
  Pages                    = {1025-1038},
  Volume                   = {25},

  Doi                      = {10.1109/21.391285},
  File                     = {Published version:Jones-1995-p1025-1038.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0018-9472},
  Keywords                 = {knowledge based systems;knowledge engineering;man-machine systems;modelling;satellite ground stations;simulation;telecommunication control;human-machine systems;intelligent systems;interactive real-time simulation;knowledge architecture;knowledge engineering;knowledge-based support;modeling;normative model structure;satellite ground control;Automatic control;Design engineering;Humans;Intelligent systems;Knowledge engineering;Man machine systems;Modeling;NASA;Satellites;Systems engineering and theory},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=391285&isnumber=8864},
  Zone                     = {-}
}

@Article{Jonkman-2003-p1-30,
  Title                    = {An overview of quantitative risk measures for loss of life and economic damage },
  Author                   = {S.N. Jonkman and P.H.A.J.M. van Gelder and J.K. Vrijling},
  Journal                  = {Journal of Hazardous Materials },
  Year                     = {2003},
  Number                   = {1},
  Pages                    = {1 - 30},
  Volume                   = {99},

  Abstract                 = {A comprehensive overview of methods to quantify and limit risks arising from different sources is still missing in literature. Therefore, a study of risk literature was carried out by the authors. This article summarises about 25 quantitative risk measures. A risk measure is defined as a mathematical function of the probability of an event and the consequences of that event. The article focuses mainly on risk measures for loss of life (individual and societal risk) and economic risk, concentrating on risk measurement experiences in The Netherlands. Other types of consequences and some international practices are also considered. For every risk measure the most important characteristics are given: the mathematical formulation, the field of application and the standard set in this field. Some of the measures have been used in a case study to calculate the flood risks for an area in The Netherlands. },
  Doi                      = {http://dx.doi.org/10.1016/S0304-3894(02)00283-2},
  File                     = {Published version:Jonkman-2003-p1-30.pdf:PDF},
  Impactfactor             = {3.925},
  ISSN                     = {0304-3894},
  Keywords                 = {Risk measures},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~，人员损失和经济损失量化},
  Timestamp                = {2015.03.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304389402002832},
  Zone                     = {1Q}
}

@Article{Jun-1999-p623-628,
  Title                    = {A Bayesian approach to prediction of system failure rates by criticalities under event trees },
  Author                   = {Chi-Hyuck Jun and Soo Y. Chang and Yushin Hong and Heejoong Yang},
  Journal                  = {International Journal of Production Economics },
  Year                     = {1999},
  Number                   = {0},
  Pages                    = {623 - 628},
  Volume                   = {60–61},

  Abstract                 = {A Bayesian approach under a binary multi-stage event tree is proposed in estimating the system failure rate in various levels of criticality in order to incorporate failure data in one level into analyzing failure rates in any other levels. An initial failure which occurs according to a Poisson process with unknown rate is to escalate to more severe failures depending on functioning states of backup subsystems associated with the event tree. We employ the Gamma prior distribution for the initial failure rate and the Beta priors for the criticality probabilities. An approximation method is proposed to obtain the posterior distributions of the failure rates by criticalities. },
  Doi                      = {http://dx.doi.org/10.1016/S0925-5273(98)00135-2},
  File                     = {Published version:Jun-1999-p623-628.pdf:PDF},
  Impactfactor             = {2.081},
  ISSN                     = {0925-5273},
  Keywords                 = {Event tree},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925527398001352},
  Zone                     = {2Q}
}

@Article{Kalantarni-2010-p191-199,
  Title                    = {Modelling of \{BP\} Texas City refinery accident using dynamic risk assessment approach },
  Author                   = {Maryam Kalantarnia and Faisal Khan and Kelly Hawboldt},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2010},
  Number                   = {3},
  Pages                    = {191 - 199},
  Volume                   = {88},

  Abstract                 = {Process industries involve handling of hazardous substances which on release may potentially cause catastrophic consequences in terms of assets lost, human fatalities or injuries and loss of public confidence of the company. In spite of using endless end-of-the-pipe safety systems, tragic accidents such as \{BP\} Texas City refinery still occur. One of the main reasons of such rare but catastrophic events is lack of effective monitoring and modelling approaches that provide early warnings and help to prevent such event. To develop a predictive model one has to rely on past occurrence data, as such events are rare, enough data are usually not available to better understand and model such behavior. In such situations, it is advisable to use near misses and incident data to predict system performance and estimate accident likelihood. This paper is an attempt to demonstrate testing and validation of one such approach, dynamic risk assessment, using data from the \{BP\} Texas City refinery incident. Dynamic risk assessment is a novel approach which integrates Bayesian failure updating mechanism with the consequence assessment. The implementation of this methodology to the \{BP\} Texas City incident proves that the approach has the ability to learn from near misses, incident, past accidents and predict event occurrence likelihood in the next time interval. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2010.01.004},
  File                     = {Published version:Kalantarni-2010-p191-199.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Dynamic risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，比较相关，没用说用什么仿真},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095758201000008X},
  Zone                     = {3Q}
}

@Article{Kalantarni-2009-p600-606,
  Title                    = {Dynamic risk assessment using failure assessment and Bayesian theory},
  Author                   = {Maryam Kalantarnia and Faisal Khan and Kelly Hawboldt},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2009},
  Note                     = {Quantitative Risk Analysis - Special issue devoted to Norberto Piccinini },
  Number                   = {5},
  Pages                    = {600 - 606},
  Volume                   = {22},

  Abstract                 = {To ensure the safety of a process system, engineers use different methods to identify the potential hazards that may cause severe consequences. One of the most popular methods used is quantitative risk assessment (QRA) which quantifies the risk associated with a particular process activity. One of QRA's major disadvantages is its inability to update risk during the life of a process. As the process operates, abnormal events will result in incidents and near misses. These events are often called accident precursors. A conventional \{QRA\} process is unable to use the accident precursor information to revise the risk profile. To overcome this, a methodology has been proposed based on the work of Meel and Seider (2006). Similar to Meel and Seider (2006) work, this methodology uses Bayesian theory to update the likelihood of the event occurrence and also failure probability of the safety system. In this paper the proposed methodology is outlined and its application is demonstrated using a simple case study. First, potential accident scenarios are identified and represented in terms of an event tree, next, using the event tree and available failure data end-state probabilities are estimated. Subsequently, using the available accident precursor data, safety system failure likelihood and event tree end-state probabilities are revised. The methodology has been simulated using deterministic (point value) as well as probabilistic approach. This Methodology is applied to a case study demonstrating a storage tank containing highly hazardous chemicals. The comparison between conventional \{QRA\} and the results from dynamic failure assessment approach shows the significant deviation in system failure frequency throughout the life time of the process unit. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2009.04.006},
  File                     = {Published version:Kalantarni-2009-p600-606.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Dynamic failure assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，比较相关，事件树},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423009000643},
  Zone                     = {3Q}
}

@Article{Kalantarni-2009-p600-606a,
  Title                    = {Dynamic risk assessment using failure assessment and Bayesian theory },
  Author                   = {Maryam Kalantarnia and Faisal Khan and Kelly Hawboldt},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2009},
  Note                     = {Quantitative Risk Analysis - Special issue devoted to Norberto Piccinini },
  Number                   = {5},
  Pages                    = {600 - 606},
  Volume                   = {22},

  Abstract                 = {To ensure the safety of a process system, engineers use different methods to identify the potential hazards that may cause severe consequences. One of the most popular methods used is quantitative risk assessment (QRA) which quantifies the risk associated with a particular process activity. One of QRA's major disadvantages is its inability to update risk during the life of a process. As the process operates, abnormal events will result in incidents and near misses. These events are often called accident precursors. A conventional \{QRA\} process is unable to use the accident precursor information to revise the risk profile. To overcome this, a methodology has been proposed based on the work of Meel and Seider (2006). Similar to Meel and Seider (2006) work, this methodology uses Bayesian theory to update the likelihood of the event occurrence and also failure probability of the safety system. In this paper the proposed methodology is outlined and its application is demonstrated using a simple case study. First, potential accident scenarios are identified and represented in terms of an event tree, next, using the event tree and available failure data end-state probabilities are estimated. Subsequently, using the available accident precursor data, safety system failure likelihood and event tree end-state probabilities are revised. The methodology has been simulated using deterministic (point value) as well as probabilistic approach. This Methodology is applied to a case study demonstrating a storage tank containing highly hazardous chemicals. The comparison between conventional \{QRA\} and the results from dynamic failure assessment approach shows the significant deviation in system failure frequency throughout the life time of the process unit. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2009.04.006},
  File                     = {Published version:Kalantarni-2009-p600-606a.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Dynamic failure assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423009000643},
  Zone                     = {3Q}
}

@Article{Kameyama-1977-p1297-1302,
  Title                    = {Synthesis of Multiple-Valued Logic Networks Based on Tree-Type Universal Logic Module},
  Author                   = {Kameyama, M. and Higuchi, T.},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1977},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {1297-1302},
  Volume                   = {C-26},

  Abstract                 = {This correspondence presents a theoretical study on the synthesis of multiple-valued logic networks based on tree-type universal logic modules (T-ULM's). The mathematical notation of T-ULM is introduced. On the basis of the mathematical properties, an algorithm for synthesizing an arbitrary logic function of n variables with a smaller number of modules is presented. In this algorithm, only true and constant inputs are allowed at input terminals. The algorithm consists of two parts. The first one is a functional decomposition, and the other one is the proper order of the expansion which is the problem of zinding the most incomplete tree structure. Thus it is established that the systematic and nonexhaustive procedure of the algorithm gives a suboptimal solution for the reduction of the number of T-ULM's.},
  Doi                      = {10.1109/TC.1977.1674797},
  File                     = {Published version:Kameyama-1977-p1297-1302.pdf:PDF},
  Impactfactor             = {1.379},
  ISSN                     = {0018-9340},
  Keywords                 = {Compatible set, complex disjunctive decomposition, control variables, dependency, expansion, residue functions, row multiplicity, ternary T-gate, tree-type universal logic module (T-ULM), true and constant inputs.;Algebra;Control system synthesis;Integrated circuit technology;Logic circuits;Logic design;Logic functions;Minimization;Multivalued logic;Network synthesis;Tree data structures;Compatible set, complex disjunctive decomposition, control variables, dependency, expansion, residue functions, row multiplicity, ternary T-gate, tree-type universal logic module (T-ULM), true and constant inputs.},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1674797&isnumber=35154},
  Zone                     = {3Q}
}

@Article{Kapela-2004-p1915-1922,
  Title                    = {A theoretical model of the high-frequency arrhythmogenic depolarization signal following myocardial infarction},
  Author                   = {Kapela, A. and Bezerianos, A.},
  Journal                  = {Biomedical Engineering, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {1915-1922},
  Volume                   = {51},

  Abstract                 = {Theoretical body-surface potentials were computed from single, branching and tortuous strands of Luo-Rudy dynamic model cells, representing different areas of an infarct scar. When action potential (AP) propagation either in longitudinal or transverse direction was slow (3-12 cm/s), the depolarization signals contained high-frequency (100-300 Hz) oscillations. The frequencies were related to macroscopic propagation velocity and strand architecture by simple formulas. Next, we extended a mathematical model of the QRS-complex presented in our earlier work to simulate unstable activation wavefront. It combines signals from different strands with small timing fluctuations relative to a large repetitive QRS-like waveform and can account for dynamic changes of real arrhythmogenic micropotentials. Variance spectrum of wavelet coefficients calculated from the composite QRS-complex contained the high frequencies of the individual abnormal signals. We conclude that slow AP propagation through fibrotic regions after myocardial infarction is a source of high-frequency arrhythmogenic components that increase beat-to-beat variability of the QRS, and wavelet variance parameters can be used for ventricular tachycardia risk assessment.},
  Doi                      = {10.1109/TBME.2004.834277},
  File                     = {Published version:Kapela-2004-p1915-1922.pdf:PDF},
  Impactfactor             = {2.348},
  ISSN                     = {0018-9294},
  Keywords                 = {bioelectric potentials;electrocardiography;medical signal processing;muscle;physiological models;wavelet transforms;100 to 300 Hz;3 to 12 cm/s;Luo-Rudy dynamic model cells;QRS-complex;action potential propagation;beat-to-beat variability;high-frequency arrhythmogenic depolarization signal;infarct scar;myocardial infarction;theoretical body-surface potentials;unstable activation wavefront;ventricular tachycardia risk assessment;wavelet variance parameters;Electrocardiography;Fluctuations;Frequency;Mathematical model;Myocardium;Pathology;Physics;Risk management;Timing;Wavelet coefficients;Arrhythmias, Cardiac;Body Surface Potential Mapping;Diagnosis, Computer-Assisted;Heart Conduction System;Humans;Models, Cardiovascular;Models, Neurological;Myocardial Infarction;Reproducibility of Results;Sensitivity and Specificity},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1344194&isnumber=29600},
  Zone                     = {3Q}
}

@Article{Kaplan-1981-p11-27,
  Title                    = {On the quantitative definition of risk},
  Author                   = {Kaplan, Stanley and Garrick, B John},
  Journal                  = {Risk analysis},
  Year                     = {1981},
  Number                   = {1},
  Pages                    = {11--27},
  Volume                   = {1},

  File                     = {Published version:Kaplan-1981-p11-27.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Publisher                = {Wiley Online Library},
  Review                   = {风险的定义},
  Timestamp                = {2015.04.15},
  Url                      = {http://josiah.berkeley.edu/2007Fall/NE275/CourseReader/3.pdf},
  Zone                     = {-}
}

@Article{Karabacak-2005-p147-159,
  Title                    = {ISRAM: information security risk analysis method },
  Author                   = {Bilge Karabacak and Ibrahim Sogukpinar},
  Journal                  = {Computers \& Security },
  Year                     = {2005},
  Number                   = {2},
  Pages                    = {147 - 159},
  Volume                   = {24},

  Abstract                 = {Continuously changing nature of technological environment has been enforcing to revise the process of information security risk analysis accordingly. A number of quantitative and qualitative risk analysis methods have been proposed by researchers and vendors. The purpose of these methods is to analyze today's information security risks properly. Some of these methods are supported by a software package. In this study, a survey based quantitative approach is proposed to analyze security risks of information technologies by taking current necessities into consideration. The new method is named as Information Security Risk Analysis Method (ISRAM). Case study has shown that \{ISRAM\} yields consistent results in a reasonable time period by allowing the participation of the manager and staff of the organization. },
  Doi                      = {http://dx.doi.org/10.1016/j.cose.2004.07.004},
  File                     = {Published version:Karabacak-2005-p147-159.pdf:PDF},
  Impactfactor             = {1.158},
  ISSN                     = {0167-4048},
  Keywords                 = {Information security},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {~，损失期望和风险的关系},
  Timestamp                = {2015.03.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167404804001890},
  Zone                     = {3Q}
}

@Article{KarimiAzar-2011-p9105-9111b,
  Title                    = {Risk assessment model selection in construction industry },
  Author                   = {AmirReza KarimiAzari and Neda Mousavi and S. Farid Mousavi and SeyedBagher Hosseini},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2011},
  Number                   = {8},
  Pages                    = {9105 - 9111},
  Volume                   = {38},

  Abstract                 = {Construction industry faces a lot of inherent uncertainties and issues. As this industry is plagued by risk, risk management is an important part of the decision-making process of these companies. Risk assessment is the critical procedure of risk management. Despite many scholars and practitioners recognizing the risk assessment models in projects, insufficient attention has been paid by researchers to select the suitable risk assessment model. In general, many factors affect this problem which adheres to uncertain and imprecise data and usually several people are involved in the selection process. Using the fuzzy \{TOPSIS\} method, this study provides a rational and systematic process for developing the best model under each of the selection criteria. Decision criteria are obtained from the nominal group technique (NGT). The proposed method can discriminate successfully and clearly among risk assessment methods. The proposed approach is demonstrated using a real case involving an Iranian construction corporation. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2010.12.110},
  File                     = {Published version:KarimiAzar-2011-p9105-9111b.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Risk management},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417410014739},
  Zone                     = {3Q}
}

@InProceedings{Karnouskos-2011-p4490-4494,
  Title                    = {Stuxnet worm impact on industrial cyber-physical system security},
  Author                   = {Karnouskos, S.},
  Booktitle                = {IECON 2011 - 37th Annual Conference on IEEE Industrial Electronics Society},
  Year                     = {2011},
  Pages                    = {4490--4494},

  Doi                      = {10.1109/IECON.2011.6120048},
  File                     = {Published version:Karnouskos-2011-p4490-4494.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6120048},
  Zone                     = {-}
}

@Article{Karyotis-2007-p2397-2410,
  Title                    = {Risk-based attack strategies for mobile ad hoc networks under probabilistic attack modeling framework },
  Author                   = {Vasileios Karyotis and Symeon Papavassiliou},
  Journal                  = {Computer Networks },
  Year                     = {2007},
  Note                     = {(1) Advances in Smart Cards and (2) Topics in Wireless Broadband Systems },
  Number                   = {9},
  Pages                    = {2397 - 2410},
  Volume                   = {51},

  Abstract                 = {In this paper, we introduce and design a modeling framework that allows for the study and analysis of attack propagation in mobile ad hoc networks. The choice of a statistical approach for the problem is motivated by the dynamic characteristics of the ad hoc topology and the stochastic nature of threat propagation. Based on this probabilistic modeling framework, we study the impact of topology and mobility in the propagation of software threats over ad hoc networks. We design topology control algorithms that indicate how to properly adjust an attacker’s transmission radius, according to the measured topological characteristics and availability of its resources, in the process of infecting a network more effectively. Then based on these topology control algorithms we develop different attack strategies that may range from independent attacks to cooperative scenarios in order to increase the negative impact of an attack on the network. Our performance evaluation results demonstrate that the proposed topology control algorithms and respective attack strategies effectively balance the tradeoffs between the potential network damage and the attackers’ lifetime, and as a result significantly outperform any other flat and threshold-based approaches. },
  Doi                      = {http://dx.doi.org/10.1016/j.comnet.2007.01.028},
  File                     = {Published version:Karyotis-2007-p2397-2410.pdf:PDF},
  Impactfactor             = {1.231},
  ISSN                     = {1389-1286},
  Keywords                 = {Attack modeling},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1389128607000424},
  Zone                     = {3Q}
}

@InProceedings{Kassou-2012-p7-11,
  Title                    = {A maturity metric based approach for eliciting SOA security requirements},
  Author                   = {Kassou, M. and Kjiri, L.},
  Booktitle                = {Network Security and Systems (JNS2), 2012 National Days of},
  Year                     = {2012},
  Month                    = {April},
  Pages                    = {7-11},

  Doi                      = {10.1109/JNS2.2012.6249231},
  File                     = {Published version:Kassou-2012-p7-11.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {security of data;service-oriented architecture;SOA Security requirement elicitation;SOA application;SOA security requirements identification;business risk;failure;maturity assessment;maturity metric based approach;organization;quality attribute requirement;security process;service-oriented architecture;Context;ISO standards;Measurement;Process control;Security;Semiconductor optical amplifiers;Service oriented architecture;Maturity Models;Metrics;SOA;Security Assessment;Security Requirement;Security elicitation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6249231&isnumber=6249222},
  Zone                     = {-}
}

@Article{Kawada-2005-p1341-1353,
  Title                    = {An efficient parallel video transmission system including codecs with functions of failure detection and coding noise evaluation},
  Author                   = {Kawada, R. and Koike, A. and Wada, M. and Matsumoto, S.},
  Journal                  = {Circuits and Systems for Video Technology, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {1341-1353},
  Volume                   = {15},

  Abstract                 = {For contribution and primary distribution of television, i.e., television transmission to broadcasting stations, parallel transmission links that include codecs are often set up for reliability. However, this configuration still faces the following issues: 1) during normal periods, the reserve link is totally wasted and 2) when a failure occurs, the disrupted signal continues to be transmitted until the system switches to the reserve link. In addition, in digital video transmission, automatic assessment of received video quality is an important issue. To address these issues, the authors developed a processing system for parallelly transmitted video signals, which mainly consists of the following three elements: 1) when both links are normal, outputting the average signal of both links reduces compression coding noise and improves picture quality compared to only one link (efficiency); 2) comparison of the pictures from the two links enables failure detection in either link, to switch to the normal link (support of failure detection); and 3) comparison of the two normally decoded pictures enables evaluation of picture quality (automatic coding noise evaluation). This unique system improves the reliability and operational efficiency of conventional parallel video transmission links.},
  Doi                      = {10.1109/TCSVT.2005.856930},
  File                     = {Published version:Kawada-2005-p1341-1353.pdf:PDF},
  Impactfactor             = {1.819},
  ISSN                     = {1051-8215},
  Keywords                 = {data compression;digital video broadcasting;television stations;video codecs;video coding;automatic assessment;broadcasting station;codecs;coding noise evaluation;digital video transmission;failure detection;parallel video transmission link;received video quality;reliability;television transmission;Codecs;Computerized monitoring;Condition monitoring;Decoding;Degradation;Remote monitoring;Signal processing;Switches;TV;Video compression;Coding noise evaluation;efficiency;failure detection;parallel transmission;television transmission},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1522261&isnumber=32556},
  Zone                     = {3Q}
}

@Article{Kawamura-2014-p143-152,
  Title                    = {Failure of Volcanic Slopes in Cold Regions and its Prediction },
  Author                   = {Shima Kawamura and Seiichi Miura},
  Journal                  = {Procedia Earth and Planetary Science },
  Year                     = {2014},
  Note                     = {The Third Italian Workshop on Landslides: Hydrological Response of Slopes through Physical Experiments, Field Monitoring and Mathematical Modeling },
  Number                   = {0},
  Pages                    = {143 - 152},
  Volume                   = {9},

  Abstract                 = {Abstract The aim of this study is to reveal failure mechanisms of volcanic slopes in cold regions and to propose a prediction method on the failure. To achieve the purposes of this study, a full scale-embankment and -cut slope, which was 5 m in height, 12 m and 4 m in length and the angle of 45 degree, were constructed. A typical volcanic soil in Hokkaido as soil material was adopted. After the constructions of embankment and cut slope, the changes in soil moisture, temperature, pore pressure in the embankment and the cut slope were investigated using monitoring devices such as soil moisture meters and so on. Simultaneously, a series of model tests was performed on small size slopes constructed by the same soil material and compaction conditions as that in the real field. In consideration of the results of model test and field monitoring, it was found that the slope failures can be uniquely assessed by the changes in water content in zones subjected to rainfall and freeze-thaw actions. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeps.2014.06.010},
  File                     = {Published version:Kawamura-2014-p143-152.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1878-5220},
  Keywords                 = {slope failure},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1878522014000393},
  Zone                     = {-}
}

@Article{Keller-1991-p82-87,
  Title                    = {Applying knowledge compilation techniques to model-based reasoning},
  Author                   = {Keller, R.M.},
  Journal                  = {IEEE Expert},
  Year                     = {1991},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {82-87},
  Volume                   = {6},

  Doi                      = {10.1109/64.79715},
  File                     = {Published version:Keller-1991-p82-87.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0885-9000},
  Keywords                 = {knowledge based systems;knowledge engineering;design;diagnosis;knowledge compilation techniques;knowledge-based systems;model-based reasoning;simulation;Application software;Artificial intelligence;Context modeling;Inference mechanisms;Knowledge representation;Machine learning;NASA;Problem-solving;Software engineering;Software systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=79715&isnumber=2610},
  Zone                     = {-}
}

@Article{Kelly-1990-p1001-1021,
  Title                    = {An analysis of containment venting as a severe accident mitigation strategy for the BWR Mark II containment},
  Author                   = {Kelly, D.L. and Galyean, W.J.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {1990},

  Month                    = {Apr},
  Number                   = {2},
  Pages                    = {1001-1021},
  Volume                   = {37},

  Doi                      = {10.1109/23.106749},
  File                     = {Published version:Kelly-1990-p1001-1021.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {accidents;fission reactor safety;nuclear power stations;BWR Mark II containment;BWR/4 reactor;Barseback Nuclear Power Station;Filtra system;HVAC systems;containment failure mode;containment venting;core damage frequency;harsh reactor building environment;off-site consequences;probabilistic risk assessment;severe accident mitigation strategy;Accidents;Filters;Frequency;Inductors;Laboratories;Performance analysis;Performance evaluation;Probability;Risk analysis;Vents},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=106749&isnumber=3253},
  Zone                     = {3Q}
}

@Article{Kelly-2009-p628-643,
  Title                    = {Bayesian inference in probabilistic risk assessment鈥擳he current state of the art },
  Author                   = {Dana L. Kelly and Curtis L. Smith},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {628 - 643},
  Volume                   = {94},

  Abstract                 = {Markov chain Monte Carlo (MCMC) approaches to sampling directly from the joint posterior distribution of aleatory model parameters have led to tremendous advances in Bayesian inference capability in a wide variety of fields, including probabilistic risk analysis. The advent of freely available software coupled with inexpensive computing power has catalyzed this advance. This paper examines where the risk assessment community is with respect to implementing modern computational-based Bayesian approaches to inference. Through a series of examples in different topical areas, it introduces salient concepts and illustrates the practical application of Bayesian inference via \{MCMC\} sampling to a variety of important problems. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2008.07.002},
  File                     = {Published version:Kelly-2009-p628-643.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Bayesian inference},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832008001919},
  Zone                     = {2Q}
}

@Article{Kemerer-1992-p1011-1024,
  Title                    = {Improving the reliability of function point measurement: an empirical study},
  Author                   = {Kemerer, C.F. and Porter, B.S.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {1992},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {1011-1024},
  Volume                   = {18},

  Abstract                 = {One measure of the size and complexity of information systems that is growing in acceptance and adoption is function points, a user-oriented, nonsource line of code metric of the systems development product. Previous research has documented the degree of reliability of function points as a metric. This research extends that work by (a) identifying the major sources of variation through a survey of current practice, and (b) estimating the magnitude of the effect of these sources of variation using detailed case study data from commercial systems. The results of this research show that a relatively small number of factors has the greatest potential for affecting reliability, and recommendations are made for using these results to improve the reliability of function point counting in organizations},
  Doi                      = {10.1109/32.177370},
  File                     = {Published version:Kemerer-1992-p1011-1024.pdf:PDF},
  Impactfactor             = {2.588},
  ISSN                     = {0098-5589},
  Keywords                 = {software metrics;software quality;software reliability;complexity;function point counting;function point measurement;information systems;nonsource line of code metric;reliability;size;software complexity measures;systems development product;user-oriented;Application software;Area measurement;Costs;Information systems;Management information systems;Productivity;Programming;Size measurement;Software maintenance;Software measurement},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=177370&isnumber=4477},
  Zone                     = {1Q}
}

@Article{Kenarangui-1991-p120-124,
  Title                    = {Event-tree analysis by fuzzy probability},
  Author                   = {Kenarangui, R.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1991},

  Month                    = {Apr},
  Number                   = {1},
  Pages                    = {120-124},
  Volume                   = {40},

  Abstract                 = {A method is presented for dealing with event-tree analysis under uncertainty. Fuzzy-set logic is used to account for imprecision and uncertainty in data while employing event-tree analysis. The fuzzy event-tree logic allows the use of verbal statements for the probabilities and consequences, such as very high, moderate, and low probability. The technique permits an analysis of the qualitative evaluation of the event tree to gain the quantitative results. The application of fuzzy event trees is further demonstrated by using a set of event trees for an electric power protection system to assess the viability of the method in complex situations},
  Doi                      = {10.1109/24.75348},
  File                     = {Published version:Kenarangui-1991-p120-124.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {fuzzy logic;power system protection;probability;electric power protection system;event-tree analysis;fuzzy probability;Arithmetic;Fuzzy logic;Fuzzy sets;Probability;Protection;Reliability engineering;Reliability theory;Risk analysis;Statistics;Uncertainty},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=75348&isnumber=2503},
  Zone                     = {2Q}
}

@Article{Khakzad-2013-p46-53,
  Title                    = {Dynamic safety analysis of process systems by mapping bow-tie into Bayesian network },
  Author                   = {Nima Khakzad and Faisal Khan and Paul Amyotte},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2013},
  Number                   = {1–2},
  Pages                    = {46 - 53},
  Volume                   = {91},

  Abstract                 = {Among the various techniques used for safety analysis of process systems, bow-tie (BT) analysis is becoming a popular technique as it represents an accident scenario from causes to effects. However, the \{BT\} application in the dynamic safety analysis is limited due to the static nature of its components, i.e. fault tree and event tree. It is therefore difficult in \{BT\} to take accident precursors into account to update the probability of events and the consequent risk. Also, \{BT\} is unable to represent conditional dependency. Event dependency is common among primary events and safety barriers. The current paper illustrates how Bayesian network (BN) helps to overcome these limitations. It has also been shown that \{BN\} can be used in dynamic safety analysis of a wide range of accident scenarios due to its flexible structure. This paper also introduces the application of probability adapting in dynamic safety analysis rather than probability updating. A case study from the U.S. Chemical Safety Board has been used to illustrate the application of both \{BT\} and \{BN\} techniques, with a comparison of the results from each technique. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2012.01.005},
  File                     = {Published version:Khakzad-2013-p46-53.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Dynamic safety analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957582012000067},
  Zone                     = {3Q}
}

@Article{Khakzad-2011-p925-932,
  Title                    = {Safety analysis in process facilities: Comparison of fault tree and Bayesian network approaches },
  Author                   = {Nima Khakzad and Faisal Khan and Paul Amyotte},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2011},
  Number                   = {8},
  Pages                    = {925 - 932},
  Volume                   = {96},

  Abstract                 = {Safety analysis in gas process facilities is necessary to prevent unwanted events that may cause catastrophic accidents. Accident scenario analysis with probability updating is the key to dynamic safety analysis. Although conventional failure assessment techniques such as fault tree (FT) have been used effectively for this purpose, they suffer severe limitations of static structure and uncertainty handling, which are of great significance in process safety analysis. Bayesian network (BN) is an alternative technique with ample potential for application in safety analysis. \{BNs\} have a strong similarity to \{FTs\} in many respects; however, the distinct advantages making them more suitable than \{FTs\} are their ability in explicitly representing the dependencies of events, updating probabilities, and coping with uncertainties. The objective of this paper is to demonstrate the application of \{BNs\} in safety analysis of process systems. The first part of the paper shows those modeling aspects that are common between \{FT\} and BN, giving preference to \{BN\} due to its ability to update probabilities. The second part is devoted to various modeling features of BN, helping to incorporate multi-state variables, dependent failures, functional uncertainty, and expert opinion which are frequently encountered in safety analysis, but cannot be considered by FT. The paper concludes that \{BN\} is a superior technique in safety analysis because of its flexible structure, allowing it to fit a wide variety of accident scenarios. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2011.03.012},
  File                     = {Published version:Khakzad-2011-p925-932.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Bayesian network},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832011000408},
  Zone                     = {2Q}
}

@Article{Khanmohamm-2015-p1782-1785,
  Title                    = {Diagnostic prediction of renal failure from blood serum analysis by \{FTIR\} spectrometry and chemometrics },
  Author                   = {Mohammdreza Khanmohammadi and Keyvan Ghasemi and Amir Bagheri Garmarudi and Mehdi Ramin},
  Journal                  = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {1782 - 1785},
  Volume                   = {136, Part C},

  Abstract                 = {Abstract A new diagnostic approach based on Attenuated Total Reflectance–Fourier Transform Infrared (ATR–FTIR) spectrometry and classification algorithm has been introduced which provides a rapid, reliable, and easy way to perform blood test for the diagnosis of renal failure. Blood serum samples from 35 renal failure patients and 40 healthy persons were analyzed by ATR–FTIR spectrometry. The resulting data was processed by Quadratic Discriminant Analysis (QDA) and \{QDA\} combined with simple filtered method. Spectroscopic studies were performed in 900–2000&#xa0;cm−1 spectral region with 3.85&#xa0;cm−1 data space. Results showed 93.33% and 100% of accuracy for \{QDA\} and filter-QDA models, respectively. In the first step, 30 samples were applied to construct the model. In order to modify the capability of \{QDA\} in prediction of test samples, filter-based feature selection methods were applied. It was found that the filtered spectra coupled with \{QDA\} could correctly predict the test samples in most of the cases. },
  Doi                      = {http://dx.doi.org/10.1016/j.saa.2014.10.082},
  File                     = {Published version:Khanmohamm-2015-p1782-1785.pdf:PDF},
  Impactfactor             = {1.977},
  ISSN                     = {1386-1425},
  Keywords                 = {Renal failure},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1386142514015790},
  Zone                     = {3Q}
}

@InProceedings{Khanna-2009-p1-6,
  Title                    = {Modeling the risks to complex industrial networks due to loss of natural capital},
  Author                   = {Khanna, V. and Bakshi, B.R.},
  Booktitle                = {Sustainable Systems and Technology, 2009. ISSST '09. IEEE International Symposium on},
  Year                     = {2009},
  Pages                    = {1--6},

  Doi                      = {10.1109/ISSST.2009.5156771},
  File                     = {Published version:Khanna-2009-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5156771},
  Zone                     = {-}
}

@Article{Kharlamov-2014-p657-668,
  Title                    = {Investment Security of Industries: Rating Assessment (Case of Ukraine) },
  Author                   = {Ganna Kharlamova},
  Journal                  = {Procedia Economics and Finance },
  Year                     = {2014},
  Note                     = {21st International Economic Conference of Sibiu 2014, \{IECS\} 2014 Prospects of Economic Recovery in a Volatile International Context: Major Obstacles, Initiatives and Projects },
  Number                   = {0},
  Pages                    = {657 - 668},
  Volume                   = {16},

  Abstract                 = {Abstract The paper reflects on the phenomenon of investment security and its assessment. Investment opportunities and potential of industries of Ukraine are analyzed. Economic activity sectors are ranked according to their level of investment potential and the degree of investment risks, as well as an experience of previous investment activity in the industry. Ranked groups of industries are considered for further marketing approach to be implemented with the aim to propose investment strategies for every particular industry/economic activity sector to attract maximum efficient volume of investing and to provide the most secure investment policy. The novelty of this paper mostly is about the discloser of the nature and content of an investment security of the national economy at the industrial level. The research is focused on potential investors, governors, investment advisers and analytics. },
  Doi                      = {http://dx.doi.org/10.1016/S2212-5671(14)00854-5},
  File                     = {Published version:Kharlamov-2014-p657-668.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2212-5671},
  Keywords                 = {Investment security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2212567114008545},
  Zone                     = {-}
}

@Article{Khattab-2006-p1152-1164,
  Title                    = {Honeypot back-propagation for mitigating spoofing distributed Denial-of-Service attacks },
  Author                   = {Sherif Khattab and Rami Melhem and Daniel Mossé and Taieb Znati},
  Journal                  = {Journal of Parallel and Distributed Computing },
  Year                     = {2006},
  Note                     = {Special Issue: Security in grid and distributed systems },
  Number                   = {9},
  Pages                    = {1152 - 1164},
  Volume                   = {66},

  Abstract                 = {The Denial-of-Service (DoS) attack is a challenging problem in the current Internet. Many schemes have been proposed to trace spoofed (forged) attack packets back to their sources. Among them, hop-by-hop schemes are less vulnerable to router compromise than packet marking schemes, but they require accurate attack signatures, high storage or bandwidth overhead, and cooperation of many ISPs. In this paper, we propose honeypot back-propagation, an efficient hop-by-hop traceback mechanism, in which accurate attack signatures are obtained by a novel leverage of the roaming honeypots scheme. The reception of attack packets by a roaming honeypot (a decoy machine camouflaged within a server pool) triggers the activation of a tree of honeypot sessions rooted at the honeypot under attack toward attack sources. The tree is formed hierarchically, first at Autonomous system (AS) level and then at router level. Honeypot back-propagation supports incremental deployment by providing incentives for \{ISPs\} even with partial deployment. Against low-rate attackers, most traceback schemes would take a long time to collect the needed number of packets. To address this problem, we also propose progressive back-propagation to handle low-rate attacks, such as on-off attacks with short bursts. Analytical and simulation results demonstrate the effectiveness of the proposed schemes under a variety of \{DDoS\} attack scenarios. },
  Doi                      = {http://dx.doi.org/10.1016/j.jpdc.2006.04.016},
  File                     = {Published version:Khattab-2006-p1152-1164.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0743-7315},
  Keywords                 = {Network security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0743731506000864},
  Zone                     = {-}
}

@InProceedings{Khoshgofta-1996-p244-251,
  Title                    = {A tree-based classification model for analysis of a military software system},
  Author                   = {Khoshgoftaar, T.M. and Allen, E.B. and Bullard, L.A. and Halstead, R. and Trio, G.P.},
  Booktitle                = {High-Assurance Systems Engineering Workshop, 1996. Proceedings., IEEE},
  Year                     = {1996},
  Month                    = {Oct},
  Pages                    = {244-251},

  Abstract                 = {Tactical military software is required to have high reliability. Each software function is often considered mission critical, and the lives of military personnel often depend on mission success. The paper presents a tree based modeling method for identifying fault prone software modules, which has been applied to a subsystem of the Joint Surveillance Target Attack Radar System, JSTPARS, a large tactical military system. We developed a decision tree model using software product metrics from one iteration of a spiral life cycle to predict whether or not each module in the next iteration would be considered fault prone. Model results could be used to identify those modules that would probably benefit from extra reviews and testing and thus reduce the risk of discovering faults later on. Identifying fault prone modules early in the development can lead to better reliability. High reliability of each iteration translates into a highly reliable final product. A decision tree also facilitates interpretation of software product metrics to characterize the fault prone class. The decision tree was constructed using the TREED-ISC algorithm which is a refinement of the CHAID algorithm. This algorithm partitions the ranges of independent variables based on chi squared tests with the dependent variable. In contrast to algorithms used by previous tree based studies of software metric data, there is no restriction to binary trees, and statistically significant relationships with the dependent variable are the basis for branching.},
  Doi                      = {10.1109/HASE.1996.618605},
  File                     = {Published version:Khoshgofta-1996-p244-251.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {decision theory;military computing;software fault tolerance;software metrics;software performance evaluation;trees (mathematics);CHAID algorithm;JSTPARS;Joint Surveillance Target Attack Radar System;TREED-ISC algorithm;chi squared tests;decision tree model;fault prone class;fault prone modules;fault prone software modules;highly reliable final product;independent variables;large tactical military system;military personnel;military software systems analysis;mission critical;software function;software product metrics;spiral life cycle;statistically significant relationships;tactical military software;tree based classification model;tree based modeling method;Classification tree analysis;Decision trees;Fault diagnosis;Mission critical systems;Partitioning algorithms;Personnel;Predictive models;Radar;Surveillance;Testing},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=618605&isnumber=13475},
  Zone                     = {-}
}

@Article{Khouzani-2012-p1347-1360,
  Title                    = {Maximum Damage Malware Attack in Mobile Wireless Networks},
  Author                   = {Khouzani, M.H.R. and Sarkar, S. and Altman, E.},
  Journal                  = {Networking, IEEE/ACM Transactions on},
  Year                     = {2012},

  Month                    = {Oct},
  Number                   = {5},
  Pages                    = {1347-1360},
  Volume                   = {20},

  Abstract                 = {Malware attacks constitute a serious security risk that threatens to slow down the large-scale proliferation of wireless applications. As a first step toward thwarting this security threat, we seek to quantify the maximum damage inflicted on the system due to such outbreaks and identify the most vicious attacks. We represent the propagation of malware in a battery-constrained mobile wireless network by an epidemic model in which the worm can dynamically control the rate at which it kills the infected node and also the transmission ranges and/or the media scanning rates. At each moment of time, the worm at each node faces the following tradeoffs: 1) using larger transmission ranges and media scanning rates to accelerate its spread at the cost of exhausting the battery and thereby reducing the overall infection propagation rate in the long run; or 2) killing the node to inflict a large cost on the network, however at the expense of losing the chance of infecting more susceptible nodes at later times. We mathematically formulate the decision problems and utilize Pontryagin Maximum Principle from optimal control theory to quantify the damage that the malware can inflict on the network by deploying optimum decision rules. Next, we establish structural properties of the optimal strategy of the attacker over time. Specifically, we prove that it is optimal for the attacker to defer killing of the infective nodes in the propagation phase until reaching a certain time and then start the slaughter with maximum effort. We also show that in the optimal attack policy, the battery resources are used according to a decreasing function of time, i.e., most aggressively during the initial phase of the outbreak. Finally, our numerical investigations reveal a framework for identifying intelligent defense strategies that can limit the damage by appropriately selecting network parameters.},
  Doi                      = {10.1109/TNET.2012.2183642},
  File                     = {Published version:Khouzani-2012-p1347-1360.pdf:PDF},
  Impactfactor             = {2.014},
  ISSN                     = {1063-6692},
  Keywords                 = {decision theory;invasive software;mobile radio;telecommunication security;Pontryagin maximum principle;battery resources;battery-constrained mobile wireless network;decision problems;epidemic model;infection propagation rate;intelligent defense strategy;maximum damage malware attack;media scanning rates;optimal attack policy;optimal control theory;optimum decision rules;propagation phase;security risk;security threat;structural property;Communication system security;Grippers;Malware;Media;Optimal control;Wireless networks;Communication systems security;epidemic modeling;mean-field convergence;optimal control},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6169963&isnumber=6329526},
  Zone                     = {2Q}
}

@InProceedings{Kikuchi-2001-p4-2293,
  Title                    = {Characteristics of function emergence in evolutionary robotic systems dependency on environment and task},
  Author                   = {Kikuchi, K. and Hara, F. and Kobayashi, H.},
  Booktitle                = {Intelligent Robots and Systems, 2001. Proceedings. 2001 IEEE/RSJ International Conference on},
  Year                     = {2001},
  Pages                    = {2288-2293 vol.4},
  Volume                   = {4},

  Abstract                 = {Effective robot functions emerge from not only according to the level of system control, i.e., intelligence, but also according to the conformity between morphology, intelligence, task and environment. This paper investigates the relationship between robotic morphology and intelligence against task and/or environmental conditions and the characteristics of functions emerging in evolutionary robotic systems. The robotic system used in the study has a reconfigurable morphology and intelligence, which were designed by genetic programming. The robotic morphology is represented as a graph structure constructed from cells with motor and visual sensors, and the intelligence is a computer program defined by a parse tree. In this study, a simulation of a task in which the robot had to maintain a certain distance from an object was executed. The results clarify the importance of conformity between morphology and intelligence, and show that effective functions emerge as the result of such conformity between morphology, intelligence, task and environment},
  Doi                      = {10.1109/IROS.2001.976411},
  File                     = {Published version:Kikuchi-2001-p4-2293.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {artificial intelligence;genetic algorithms;graph grammars;mathematical morphology;robots;trees (mathematics);artificial intelligence;conformity;evolutionary robotic systems;fitness criterion;genetic programming;graph structure;parse tree;reconfigurable morphology;robotic intelligence;robotic morphology;Control systems;Genetic programming;Intelligent control;Intelligent robots;Intelligent sensors;Intelligent structures;Intelligent systems;Morphology;Robot sensing systems;Sensor phenomena and characterization},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=976411&isnumber=21067},
  Zone                     = {-}
}

@Article{Kim-2014-p119-134,
  Title                    = {Assessing redox zones and seawater intrusion in a coastal aquifer in South Korea using hydrogeological, chemical and isotopic approaches },
  Author                   = {Duk-Min Kim and Seong-Taek Yun and Man Jae Kwon and Bernhard Mayer and Kyoung-Ho Kim},
  Journal                  = {Chemical Geology },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {119 - 134},
  Volume                   = {390},

  Abstract                 = {Abstract A shallow (&lt;&#xa0;25&#xa0;m), coastal alluvial groundwater system underneath a paddy field in the Yangyang area of South Korea was investigated to examine the occurrence of redox processes. The aquifer is affected by seawater intrusion, and is characterized by a highly reducing environment facilitated by high organic matter in the sediments. Hydrochemical data with δ34S and δ18O of sulfate were examined for depth-specific groundwater from two multilevel samplers that were installed at seaward (YY2) and landward (YY1) locations. Shallow groundwater showed distinct patterns of redox zoning. Evidence of significant bacterial sulfate reduction (BSR) was observed throughout the nearly entire depths of the two boreholes, while at the depths of active seawater intrusion in YY2, conditions suitable for methanogenesis were never reached. Thus, at \{YY2\} the deep zone of intense \{BSR\} was overlain by a zone in which methanogenesis occurred in a low-sulfate environment. In contrast, concurrent \{BSR\} and methanogenesis in \{YY1\} occurred at depths with high sedimentary organic matter and low dissolved sulfate due to intensive BSR. Considerable \{BSR\} in the groundwater representing trapped seawater in a clay layer had resulted in a very strong increase of δ34Ssulfate up to 99.9‰. The inferred sulfur isotopic enrichment factor (ε) for \{BSR\} in the lower part of \{YY2\} was −&#xa0;12.3‰, while ε at \{YY1\} was much higher (−&#xa0;45.9‰). In addition, the observed trends of δ18Osulfate at \{YY1\} indicated significant oxygen isotope exchange of sulfate-oxygen with ambient water, likely because of lower cell-specific rates of \{BSR\} and higher sulfur isotope fractionation as indicated by the δ34S. In contrast, there was little evidence of oxygen isotope exchange between water and SO42−&#xa0; at YY2. This study indicates that in coastal aquifers with sulfate-reducing activity, δ34S and δ18O of sulfate can reveal zones of active seawater intrusion and of trapped seawater. This study provides an example of the application of sulfur and oxygen isotope data with hydrochemical and hydrogeologic data to interpret complex redox zonation in an organic-rich coastal environment. },
  Doi                      = {http://dx.doi.org/10.1016/j.chemgeo.2014.10.024},
  File                     = {Published version:Kim-2014-p119-134.pdf:PDF},
  Impactfactor             = {3.154},
  ISSN                     = {0009-2541},
  Keywords                 = {Bacterial sulfate reduction (BSR)},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0009254114004641},
  Zone                     = {2Q}
}

@Article{Kim-2014-p124-133,
  Title                    = {Evaluation of passenger health risk assessment of sustainable indoor air quality monitoring in metro systems based on a non-Gaussian dynamic sensor validation method },
  Author                   = {MinJeong Kim and Hongbin Liu and Jeong Tai Kim and ChangKyoo Yoo},
  Journal                  = {Journal of Hazardous Materials },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {124 - 133},
  Volume                   = {278},

  Abstract                 = {Abstract Sensor faults in metro systems provide incorrect information to indoor air quality (IAQ) ventilation systems, resulting in the miss-operation of ventilation systems and adverse effects on passenger health. In this study, a new sensor validation method is proposed to (1) detect, identify and repair sensor faults and (2) evaluate the influence of sensor reliability on passenger health risk. To address the dynamic non-Gaussianity problem of \{IAQ\} data, dynamic independent component analysis (DICA) is used. To detect and identify sensor faults, the DICA-based squared prediction error and sensor validity index are used, respectively. To restore the faults to normal measurements, a DICA-based iterative reconstruction algorithm is proposed. The comprehensive indoor air-quality index (CIAI) that evaluates the influence of the current \{IAQ\} on passenger health is then compared using the faulty and reconstructed \{IAQ\} data sets. Experimental results from a metro station showed that the DICA-based method can produce an improved \{IAQ\} level in the metro station and reduce passenger health risk since it more accurately validates sensor faults than do conventional methods. },
  Doi                      = {http://dx.doi.org/10.1016/j.jhazmat.2014.05.098},
  File                     = {Published version:Kim-2014-p124-133.pdf:PDF},
  Impactfactor             = {3.925},
  ISSN                     = {0304-3894},
  Keywords                 = {Sensor fault validation},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，评价标准：SPE，平方预期误差值},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304389414004531},
  Zone                     = {1Q}
}

@Article{Kim-2011-p496-506,
  Title                    = {A Physical and Logical Security Framework for Multilevel AFCI Systems in Smart Grid},
  Author                   = {Seong-Woo Kim and Eun-Dong Lee and Dong-Hyun Je and Seung-Woo Seo},
  Journal                  = {Smart Grid, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {496-506},
  Volume                   = {2},

  Abstract                 = {The arc fault circuit interrupter (AFCI) is expected to be one of the most essential components in smart grid systems for providing physical security and safety against electrical fire hazards caused by arc faults. As AFCIs are widely deployed as a part of mandatory installation requirements, a hierarchy consisting of multilevel AFCIs has been established, where a portable AFCI is connected serially to an outlet box AFCI. However, this multilevel AFCI structure causes serious problems when the AFCI detects arc faults in the surveillance area of its descendant AFCIs. This can cause de-energizing of the entire area covered by the upper-level AFCI, which may lead to blackouts over large areas. This paper proposes an integrated security framework comprised of physical and logical security measures as a solution for this problem. Firstly, the problem is tackled through communication between the hierarchy levels. Since the proposed system deals with physical security and safety, the communication must guarantee reliable message delivery within the specified deadlines. A controller area network (CAN) is chosen as the communication technology because it provides deterministic message delivery that meets the system requirements. Moreover, CAN has the advantages of verified performance and cost competitiveness through accelerated industrial adoption. Along with the physical security framework, a logical security framework is also proposed with group key management that prevents unauthorized access. Finally, this paper reports an integrated methodology for optimizing the design parameters satisfying the bandwidth and security demands of physical and logical measures because both demands competitively share a common communication resource.},
  Doi                      = {10.1109/TSG.2011.2159869},
  File                     = {Published version:Kim-2011-p496-506.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1949-3053},
  Keywords                 = {circuit breakers;circuit-breaking arcs;controller area networks;electrical safety;power system reliability;power system security;smart power grids;CAN;blackout;controller area network;electrical fire hazard;integrated security framework;logical security framework;multilevel AFCI system;multilevel arc fault circuit interrupter system;outlet box AFCI;physical security framework;reliable message delivery;smart grid;Bandwidth;Circuit faults;Delay;Mathematical model;Safety;Security;Smart grids;Arc fault circuit interrupter;control network;grid security;logical key hierarchy;matching problem;multilevel AFCI;secure group communication;smart grid},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5982117&isnumber=5993785},
  Zone                     = {-}
}

@Article{Kini-1982-p752-771,
  Title                    = {Automatic Generation of Symbolic Reliability Functions for Processor-Memory-Switch Structures},
  Author                   = {Kini, V. and Siewiorek, D.P.},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1982},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {752-771},
  Volume                   = {C-31},

  Abstract                 = {Calculation of the reliability of computer system architectures with built-in redundancy, such as multiprocessors, is gaining in importance. The task of computing the reliability function for arbitrary Processor-Memory-Switch (PMS) interconnection structures, however, is tedious and prone to human error. Existing reliability computation programs make one of two assumptions: ? That the case analysis of success states of the system has been carried out. Such analysis must be done manually. In this instance, input to the program is usually in the form of an intermediate representation (e.g., fault tree, reliability graph). ? That the interconnection structure is a member of, or can be partitioned into, some limited class of structures for which a parametric family of equations exists (e.g., N-modular redundant systems, hybrid redundant systems).},
  Doi                      = {10.1109/TC.1982.1676082},
  File                     = {Published version:Kini-1982-p752-771.pdf:PDF},
  Impactfactor             = {1.379},
  ISSN                     = {0018-9340},
  Keywords                 = {Automatic generation of symbolic reliability functions;processor-memory-switch (PMS) structures;reliability computation program;symbolic hard-failure reliability functions;Communication switching;Computer architecture;Computer errors;Equations;Fault trees;Humans;Redundancy;Reliability;Switches;Vehicles;Automatic generation of symbolic reliability functions;processor-memory-switch (PMS) structures;reliability computation program;symbolic hard-failure reliability functions},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，ponential failure 潜在的失效},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1676082&isnumber=35218},
  Zone                     = {3Q}
}

@Article{Kirschen-2007-p527-533,
  Title                    = {Comparison of risk-based and deterministic security assessments},
  Author                   = {Kirschen, D.S. and Jayaweera, D.},
  Journal                  = {Generation, Transmission Distribution, IET},
  Year                     = {2007},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {527-533},
  Volume                   = {1},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {The traditional N - 1 security criterion provides only a limited perspective on the actual level of security of a power system and a risk-based approach to security assessment provides considerably more information on which to base operating decisions. It further argues that this risk should be measured in terms of expected outage costs to the consumers. Furthermore, the risk calculation should not be limited to the consideration of a predefined set of contingencies but should factor in the actual probabilities of outages leading to load disconnections. A case study based on the IEEE-Reliability Test System shows that different operating points on the system's deterministic security boundary have very different levels of risk. For a given operating point, the risk level changes considerably between fair, average and bad weather conditions. Finally, the paper shows how, using adaptive deterministic security boundaries, it is possible to compare the cost and benefit of relaxing operating limits.},
  Doi                      = {10.1049/iet-gtd:20060368},
  File                     = {Published version:Kirschen-2007-p527-533.pdf:PDF},
  Impactfactor             = {1.414},
  ISSN                     = {1751-8687},
  Keywords                 = {power system reliability;power system security;probability;risk management;IEEE-Reliability Test System;adaptive deterministic security boundaries;deterministic security assessments;deterministic security boundary;outages probability;power system security;risk-based security assessments},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B， 也用到了 IEEE-Reliability Test System，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4264406&isnumber=4264405},
  Zone                     = {3Q}
}

@Article{Kirschen-2007-p527-533a,
  Title                    = {Comparison of risk-based and deterministic security assessments},
  Author                   = {Kirschen, D.S. and Jayaweera, D.},
  Journal                  = {Generation, Transmission Distribution, IET},
  Year                     = {2007},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {527-533},
  Volume                   = {1},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {The traditional N - 1 security criterion provides only a limited perspective on the actual level of security of a power system and a risk-based approach to security assessment provides considerably more information on which to base operating decisions. It further argues that this risk should be measured in terms of expected outage costs to the consumers. Furthermore, the risk calculation should not be limited to the consideration of a predefined set of contingencies but should factor in the actual probabilities of outages leading to load disconnections. A case study based on the IEEE-Reliability Test System shows that different operating points on the system's deterministic security boundary have very different levels of risk. For a given operating point, the risk level changes considerably between fair, average and bad weather conditions. Finally, the paper shows how, using adaptive deterministic security boundaries, it is possible to compare the cost and benefit of relaxing operating limits.},
  Doi                      = {10.1049/iet-gtd:20060368},
  File                     = {Published version:Kirschen-2007-p527-533.pdf:PDF},
  Impactfactor             = {1.414},
  ISSN                     = {1751-8687},
  Keywords                 = {power system reliability;power system security;probability;risk management;IEEE-Reliability Test System;adaptive deterministic security boundaries;deterministic security assessments;deterministic security boundary;outages probability;power system security;risk-based security assessments},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B， 也用到了 IEEE-Reliability Test System，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4264406&isnumber=4264405},
  Zone                     = {3Q}
}

@InProceedings{Kizony-2009-p211-211,
  Title                    = {Virtual Environments for the study of the effect of context on dual tasking},
  Author                   = {Kizony, R. and Fung, J. and Katz, N. and Siev-Ner, I and Bahat, Y. and Elion, O.},
  Booktitle                = {Virtual Rehabilitation International Conference, 2009},
  Year                     = {2009},
  Month                    = {June},
  Pages                    = {211-211},

  Abstract                 = {The purpose of this poster is to present the development of functional and non-functional virtual environments for the investigation of dual tasking capabilities after traumatic brain injury.},
  Doi                      = {10.1109/ICVR.2009.5174249},
  File                     = {Published version:Kizony-2009-p211-211.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {medical computing;patient rehabilitation;virtual reality;context effect;dual tasking capability;functional virtual environment;nonfunctional virtual environment;traumatic brain injury;Brain injuries;Data analysis;Educational institutions;Feedback;Food products;Medical treatment;Multitasking;Navigation;Shape control;Virtual environment;TBI;context-dependency, executive- functions;dual task},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5174249&isnumber=5174190},
  Zone                     = {-}
}

@Article{Kleindorfe-2012-p85-96,
  Title                    = {Assessment of catastrophe risk and potential losses in industry },
  Author                   = {Paul Kleindorfer and Ulku G. Oktem and Ankur Pariyani and Warren D. Seider},
  Journal                  = {Computers \& Chemical Engineering },
  Year                     = {2012},
  Note                     = {\{FOCAPO\} 2012 },
  Number                   = {0},
  Pages                    = {85 - 96},
  Volume                   = {47},

  Abstract                 = {This paper describes the potential contribution of near-miss management systems to improving company profitability and reducing the frequency and severity of major industrial accidents. The near-miss concept has long been understood in different industries, as examples in this paper illustrate. However, what has been largely missing is the integration of near-miss management into the culture and day to day operations in a manner that underlines the critical connections between near-misses and behavior. Often, near-miss management has played an ex post forensic role in risk management rather than an alerting one, summarizing leading indicators and precursors of hazardous conditions. This paper describes several strands of recent research that aim to correct this and to make near-miss management an organic element of Enterprise Risk Management. In this respect, a new concept, “potential safety profit loss”, is introduced to calculate the potential monetary losses due to unexpected shutdowns and accidents. },
  Doi                      = {http://dx.doi.org/10.1016/j.compchemeng.2012.06.033},
  File                     = {Published version:Kleindorfe-2012-p85-96.pdf:PDF},
  Impactfactor             = {2.091},
  ISSN                     = {0098-1354},
  Keywords                 = {Near-miss},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0098135412002177},
  Zone                     = {3Q}
}

@Article{Kleyle-1978-p153-160,
  Title                    = {Approximate Lower Confidence Limits for the Weibull Reliability Function},
  Author                   = {Kleyle, Robert},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1978},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {153-160},
  Volume                   = {R-27},

  Abstract                 = {An iterative technique for obtaining large sample approximate lower s-confidence limits (LCL) for the reliability function of a Weibull probability model is proposed. The approximations are based on the asymptotic distribution of the maximum likelihood (ML) estimator of the reliability function and are valid whenever the ML estimates are obtained from singly censored samples of type I or type II. The iterated approximation is compared with an approximation based on the F distribution and with exact LCLs obtained by Monte Carlo simulation. The main results are: 1. In most circumstances the iterated LCLs are reasonably good approximations of the exact LCLs. 2. At the 95%, and especially at the 90o%, s-confidence levels the iterated approximations are sometimes too high, but never seriously high as is often the case with the non-iterated ML approximation. 3. At the 99% s-confidence level both the iterated and F approximations tend to be low. 4. The F approximation sometimes fails because the degrees of freedom in the approximating F distribution are too low. This happens when the samples are relatively small and the censoring is heavy. 5. The F approximation seems to work best when the estimated reliability is high (> 0.95). When the ML estimator is less than 0.90, the iterated approximation is generally superior. 6. Both approximations improve as the sample size increases and deteriorate as the amount of censoring increases. 7. The iterated approximation does not depend on special tables and is easier to apply than the F approximation.},
  Doi                      = {10.1109/TR.1978.5220292},
  File                     = {Published version:Kleyle-1978-p153-160.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Life estimation;Life testing;Maximum likelihood estimation;Monte Carlo methods;Probability;Reliability theory;Shape;State estimation;Statistics;Weibull distribution;F approximation;Iterated approximation;Lower s-confidence limits;Maximum likelihood estimators;Weibull reliability function},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5220292&isnumber=5220260},
  Zone                     = {2Q}
}

@Article{Kordy-2014-p1-38,
  Title                    = {DAG-based attack and defense modeling: Don’t miss the forest for the attack trees },
  Author                   = {Barbara Kordy and Ludovic Piètre-Cambacédès and Patrick Schweitzer},
  Journal                  = {Computer Science Review },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {1 - 38},
  Volume                   = {13–14},

  Abstract                 = {Abstract This paper presents the current state of the art on attack and defense modeling approaches that are based on directed acyclic graphs (DAGs). \{DAGs\} allow for a hierarchical decomposition of complex scenarios into simple, easily understandable and quantifiable actions. Methods based on threat trees and Bayesian networks are two well-known approaches to security modeling. However there exist more than 30 DAG-based methodologies, each having different features and goals. The objective of this survey is to summarize the existing methodologies, compare their features, and propose a taxonomy of the described formalisms. This article also supports the selection of an adequate modeling technique depending on user requirements. },
  Doi                      = {http://dx.doi.org/10.1016/j.cosrev.2014.07.001},
  File                     = {Published version:Kordy-2014-p1-38.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1574-0137},
  Keywords                 = {Graphical models for security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1574013714000100},
  Zone                     = {-}
}

@Article{Kordy-2014-p1-38a,
  Title                    = {DAG-based attack and defense modeling: Don’t miss the forest for the attack trees },
  Author                   = {Barbara Kordy and Ludovic Piètre-Cambacédès and Patrick Schweitzer},
  Journal                  = {Computer Science Review },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {1 - 38},
  Volume                   = {13–14},

  Abstract                 = {Abstract This paper presents the current state of the art on attack and defense modeling approaches that are based on directed acyclic graphs (DAGs). \{DAGs\} allow for a hierarchical decomposition of complex scenarios into simple, easily understandable and quantifiable actions. Methods based on threat trees and Bayesian networks are two well-known approaches to security modeling. However there exist more than 30 DAG-based methodologies, each having different features and goals. The objective of this survey is to summarize the existing methodologies, compare their features, and propose a taxonomy of the described formalisms. This article also supports the selection of an adequate modeling technique depending on user requirements. },
  Doi                      = {http://dx.doi.org/10.1016/j.cosrev.2014.07.001},
  File                     = {Published version:Kordy-2014-p1-38a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1574-0137},
  Keywords                 = {Graphical models for security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1574013714000100},
  Zone                     = {-}
}

@Article{Kordy-2014-p1-38b,
  Title                    = {DAG-based attack and defense modeling: Don’t miss the forest for the attack trees },
  Author                   = {Barbara Kordy and Ludovic Piètre-Cambacédès and Patrick Schweitzer},
  Journal                  = {Computer Science Review },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {1 - 38},
  Volume                   = {13–14},

  Abstract                 = {Abstract This paper presents the current state of the art on attack and defense modeling approaches that are based on directed acyclic graphs (DAGs). \{DAGs\} allow for a hierarchical decomposition of complex scenarios into simple, easily understandable and quantifiable actions. Methods based on threat trees and Bayesian networks are two well-known approaches to security modeling. However there exist more than 30 DAG-based methodologies, each having different features and goals. The objective of this survey is to summarize the existing methodologies, compare their features, and propose a taxonomy of the described formalisms. This article also supports the selection of an adequate modeling technique depending on user requirements. },
  Doi                      = {http://dx.doi.org/10.1016/j.cosrev.2014.07.001},
  File                     = {Published version:Kordy-2014-p1-38b.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1574-0137},
  Keywords                 = {Graphical models for security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1574013714000100},
  Zone                     = {-}
}

@Article{Koren-1977-p1154-1158,
  Title                    = {Diagnosis of Intermittent Faults in Combinational Networks},
  Author                   = {Koren, I. and Kohavi, Zvi},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1977},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {1154-1158},
  Volume                   = {C-26},

  Abstract                 = {Detection of intermittent faults was considered in a number of publications [1],[2]. Recently, Kamal [3] presented an approach to the diagnosis of intermittent faults in combinational networks. The methods developed so far, however, do not yield a minimal diagnosis experiment. Moreover, complete diagnosis is not obtained even in cases where it is possible.},
  Doi                      = {10.1109/TC.1977.1674766},
  File                     = {Published version:Koren-1977-p1154-1158.pdf:PDF},
  Impactfactor             = {1.379},
  ISSN                     = {0018-9340},
  Keywords                 = {Combinational networks, dynamic programming, fault diagnosis, intermittent faults, sequential decision tree, weighting function.;Application software;Arithmetic;Decision trees;Dynamic programming;Fault diagnosis;Hardware;Intelligent networks;Roundoff errors;Stability;Tin;Combinational networks, dynamic programming, fault diagnosis, intermittent faults, sequential decision tree, weighting function.},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1674766&isnumber=35153},
  Zone                     = {3Q}
}

@InProceedings{Kornecki-2013-p1393-1399,
  Title                    = {Studying interrelationships of safety and security for software assurance in cyber-physical systems: Approach based on bayesian belief networks},
  Author                   = {Kornecki, AJ. and Subramanian, N. and Zalewski, J.},
  Booktitle                = {Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on},
  Year                     = {2013},
  Month                    = {Sept},
  Pages                    = {1393-1399},

  File                     = {Published version:Kornecki-2013-p1393-1399.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {belief networks;safety-critical software;security of data;BBN approach;Bayesian belief networks;CPS;NFR;SCADA system;cyber-physical systems;nonfunctional requirements approach;oil pipeline control;safety interrelationships;safety property;safety violation;secuirty interrelationships;security property;security violation;software assurance;system environment;Bayes methods;Encryption;Pipelines;Safety;Software;Valves},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6644199&isnumber=6643962},
  Zone                     = {-}
}

@Article{Kujawski-2013-p979-987a,
  Title                    = {Analysis and Critique of the System Readiness Level},
  Author                   = {Kujawski, E.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {979-987},
  Volume                   = {43},

  Abstract                 = {The system readiness level (SRL) developed by Sauser <;etal/> (referred to as the S&C_SRL in this paper) has recently been introduced as a system development metric on several major defense acquisition programs without having received adequate scrutiny for modeling realism and mathematical validity. The use of matrix operations on a technology readiness level (TRL) vector and an integration readiness level (IRL) matrix gives the S&C_SRL characteristics of a quantitative measure. Given that the TRL and IRL are ordinal data, these operations are meaningless, and the S&C_SRL is a potentially misleading metric. Its use can have harmful consequences on a system's development. Fortunately, there is no need for the S&C_SRL given the availability of valid qualitative models that provide visibility and valid information on both the achieved system readiness and the difficulty to achieve operational readiness; but they have limited usefulness. The effective and efficient development of technically advanced systems and upgrades of heritage systems requires a systems engineering (SE) process based on a sound quantitative assessment and management of technical, cost, and schedule risks. The use of the S&C_SRL and other flawed decision and risk analysis models is symptomatic of a mathematical vulnerability in the SE practice. Corrective actions are presented.},
  Doi                      = {10.1109/TSMCA.2012.2209868},
  File                     = {Published version:Kujawski-2013-p979-987a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {matrix algebra;military computing;software metrics;systems engineering;IRL matrix;S&C_SRL;SE practice;SE process;TRL vector;cost risk;defense acquisition program;integration readiness level;mathematical validity;mathematical vulnerability;matrix operation;operational readiness;realism modeling;schedule risk;system development metric;system readiness level;systems engineering;systems upgrade;technical risk;technology readiness level;Humans;Measurement;Modeling;NASA;US Department of Defense;Vectors;Integration readiness level (IRL);measurement scales;ordinal data;risk analysis;system readiness level (SRL);technology maturity;technology readiness level (TRL)},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6301765&isnumber=6530659},
  Zone                     = {-}
}

@Article{Kull-2008-p409-419,
  Title                    = {A Supply Risk Reduction Model Using Integrated Multicriteria Decision Making},
  Author                   = {Kull, T.J. and Talluri, S.},
  Journal                  = {Engineering Management, IEEE Transactions on},
  Year                     = {2008},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {409-419},
  Volume                   = {55},

  Abstract                 = { Risk management in supply chains is receiving increasing attention in both academia and industry. Firms are recognizing the importance of considering supply risk in evaluating and selecting suppliers for strategic partnerships. One of the critical issues faced by purchasing managers is in effectively defining, operationalizing, and incorporating supply risk measures in the supplier evaluation process. Due to the multidimensional nature of supply risk, analytical tools that can effectively integrate various risk measures into the decision process can prove useful for managers. To this end, the contribution of this paper is twofold. First, we consider extant research in supply risk in developing a framework for risk assessment based on various categories and types of risks. Second, we propose a combination of analytic hierarchy process and goal programming as a decision tool for supplier selection in the presence of risk measures and product life cycle considerations. The efficacy of the model is tested at a mid-sized automotive supplier and managerial implications are discussed. },
  Doi                      = {10.1109/TEM.2008.922627},
  File                     = {Published version:Kull-2008-p409-419.pdf:PDF},
  Impactfactor             = {0.893},
  ISSN                     = {0018-9391},
  Keywords                 = {decision making;mathematical programming;product life cycle management;risk management;supply chain management;analytic hierarchy process;goal programming;integrated multicriteria decision making;product life cycle;risk management;strategic partnerships;supplier evaluation process;supply chains;supply risk reduction model;Automotive engineering;Costs;Decision making;Multidimensional systems;Programmable control;Risk analysis;Risk management;Supply chain management;Supply chains;Testing;Analytic hierarchy process (AHP);case analysis;goal programming (GP);product life cycle (PLC);risk management;supplier selection;supply chain risk},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4542978&isnumber=4578909},
  Zone                     = {4Q}
}

@Article{KumarSoma-2014-p1391-1412,
  Title                    = {An Industrial Perspective on Wireless Sensor Networks— A Survey of Requirements, Protocols, and Challenges},
  Author                   = {Kumar Somappa, A.A. and ?vsthus, K. and Kristensen, L.M.},
  Journal                  = {Communications Surveys Tutorials, IEEE},
  Year                     = {2014},

  Month                    = {Third},
  Number                   = {3},
  Pages                    = {1391-1412},
  Volume                   = {16},

  Abstract                 = {Wireless Sensor Networks (WSNs) are applicable in numerous domains, including industrial automation where WSNs may be used for monitoring and control of industrial plants and equipment. However, the requirements in the industrial systems differ from the general WSN requirements. In recent years, standards have been defined by several industrial alliances. These standards are specified as frameworks with modifiable parts that can be defined based on the particular application of WSN. However, limited work has been done on defining industry-specific protocols that could be used as a part of these standards. In this survey, we discuss representative protocols that meet some of the requirements of the industrial applications. Since the industrial applications domain in itself is a vast area, we divide them into classes with similar requirements. We discuss these industrial classes, set of common requirements and various state-of-the-art WSN standards proposed to satisfy these requirements. We then present a broader view towards the WSN solution by discussing important functions like medium access control, routing, and transport in detail to give some insight into specific requirements and the classification of protocols based on certain factors. We list and discuss representative protocols for each of these functions that address requirements defined in the industrial classes. Security function is discussed in brief, mainly in relation to industrial standards. Finally, we identify unsolved challenges that are encountered during design of protocols and standards. In addition some new challenges are introduced and discussed.},
  Doi                      = {10.1109/SURV.2014.012114.00058},
  File                     = {Published version:KumarSoma-2014-p1391-1412.pdf:PDF},
  Impactfactor             = {4.818},
  ISSN                     = {1553-877X},
  Keywords                 = {protocols;telecommunication security;wireless sensor networks;WSN requirements;industrial automation;industrial equipment;industrial perspective;industrial plants;industry specific protocols;medium access control;representative protocols;security function;wireless sensor networks;Monitoring;Protocols;Quality of service;Routing;Standards;Wireless communication;Wireless sensor networks;Industrial automation;Medium Access Control;Routing;Security;Standards;Transport;Wireless Sensor and Actuator Networks},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6728782&isnumber=6880447},
  Zone                     = {1Q}
}

@InProceedings{Kuo-2011-p643-646,
  Title                    = {A Mechanism on Risk Analysis of Information Security with Dynamic Assessment},
  Author                   = {Chien-Ting Kuo and He-Ming Ruan and Chin-Laung Lei and Shih-Jen Chen},
  Booktitle                = {Intelligent Networking and Collaborative Systems (INCoS), 2011 Third International Conference on},
  Year                     = {2011},
  Month                    = {Nov},
  Pages                    = {643-646},

  Abstract                 = {Information security audit has become more and more important to organizations. Human is usually the most vulnerable part about information security in an organization. In this paper, we propose a dynamic risk assessment mechanism to help the information security manager realizing the human security awareness and vulnerability assessment in end client devices. The proposed mechanism will generate graphic reports to help security manager to improve the information security of organizations. A practical implementation demonstrates that the proposed mechanism is both useful and effective.},
  Doi                      = {10.1109/INCoS.2011.25},
  File                     = {Published version:Kuo-2011-p643-646.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer graphics;electronic commerce;organisational aspects;risk analysis;security of data;dynamic risk assessment;graphic report;human security awareness;organisation information security;vulnerability assessment;Databases;Information security;Measurement;Organizations;Risk management;Software;Risk analysis;dynamic assessment;information security audit;vulnerability scanner},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6132884&isnumber=6132770},
  Zone                     = {-}
}

@Article{Kurubetti-2014-p86-94,
  Title                    = {Industrial Relations at Cummins India Limited - A Case Study },
  Author                   = {Arti Kurubetti and Asha Nagendra},
  Journal                  = {Procedia Economics and Finance },
  Year                     = {2014},
  Note                     = {Shaping the Future of Business and Society Symbiosis Institute of Management Studies (SIMS) },
  Number                   = {0},
  Pages                    = {86 - 94},
  Volume                   = {11},

  Abstract                 = {Abstract An industrial relations system consists of the whole gamut of relationships between employees and employees and between employees and employers which are managed by means of conflict and cooperation. It includes matter pertaining to employment conditions like pay, hours of works, leave with wages, health, and safety disciplinary actions, lay-off, dismissals retirements etc., laws relating to such activities, regulations governing labour welfare, social security, industrial relations, issues concerning with workers participation in management, collective bargaining, etc. The present research is a case study of the industrial relations scenario at Cummins India Limited. 50 respondents from the organization had been picked on the basis of random convenience sampling. It was found that industrial relations have been cordial, harmonious and collaborative since 2010.Communication and meeting with union representatives and workers is there on regular basis. The management representative, store managers regularly meet on weekly and monthly basis. Industrial relations manager conducts weekly meetings and resolves grievances immediately. In addition to this the senior leadership meets on monthly basis with union.CEO of the company also communicates the goals of the business and situations of the business to the employees on quarterly basis. The voices of the employees are captured during these meetings and the issues/grievances are resolved as per feasibility. Line managers and shop floor managers are regularly interacting with \{IR\} Department for day to day issues of production associates and communicating the issues to \{IR\} Department for resolving. },
  Doi                      = {http://dx.doi.org/10.1016/S2212-5671(14)00179-8},
  File                     = {Published version:Kurubetti-2014-p86-94.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2212-5671},
  Keywords                 = {Industrial Relations},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2212567114001798},
  Zone                     = {-}
}

@Article{Kutylowsk-2015-p41-48,
  Title                    = {Neural network approach for failure rate prediction },
  Author                   = {Małgorzata Kutyłowska},
  Journal                  = {Engineering Failure Analysis },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {41 - 48},
  Volume                   = {47, Part A},

  Abstract                 = {Abstract The aim of this paper was to present the possibility of artificial neural networks application to the failure rate modeling. Operating data from one Polish water utility were used to forecast output value of failure frequency. The prediction results indicate that artificial networks may be used to model the damages frequency in the water supply systems. It was found that the artificial neural network (multilayer perceptron) trained by quasi-Newton approach gave acceptable, from engineering point of view, convergence. The network was learnt using 173 and 147 data (house connections and distribution pipes, respectively). 50% of all data was chosen for training, 25% for testing and 25% for validation. In prognosis phase, the best created network used 100% of 133 and 114 values for testing. The correlation between experimental and predicted data (relating to house connections and distribution pipes, respectively) was characterized by indicator R2&#xa0;=&#xa0;0.9510 and R2&#xa0;=&#xa0;0.9268 (learning phase). Worse results were obtained in prognosis phase. In this step of modeling once created network predicted failure rate using not known input signals. The coefficient \{R2\} was equal to 0.4142 for house connections. For the distribution pipes the significant relation between experimental and modeled data was not found. The created model could be used by water utility in the future to establish the level of failure frequency and to plan the renovation of the most deteriorated pipes. },
  Doi                      = {http://dx.doi.org/10.1016/j.engfailanal.2014.10.007},
  File                     = {Published version:Kutylowsk-2015-p41-48.pdf:PDF},
  Impactfactor             = {0.855},
  ISSN                     = {1350-6307},
  Keywords                 = {Artificial neural network},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1350630714003070},
  Zone                     = {3Q}
}

@Article{Kvam-1998-p30-34,
  Title                    = {A parametric mixture-model for common-cause failure data [of nuclear power plants]},
  Author                   = {Kvam, P.H.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {30-34},
  Volume                   = {47},

  Abstract                 = {This paper introduces a statistical reliability model for common-cause failure data from the nuclear industry. To achieve target reliability, many components in power plants are placed in parallel systems. The benefits of redundancy can be negated if multiple component failures occur due to a common external event. To model the possibility of multiple failures, a mixture-model based on the binomial failure-rate model is derived using reasonable assumptions of multiple failure events at a nuclear power plant (NPP). In many applications, the original binomial failure-rate model fits failure data poorly, and the model has not typically been applied to probabilistic risk assessments in the nuclear industry. This mixture-model fits better. This paper presents a least-squares solution to the mixture-model parameters and the model fit is investigated. Methods developed here are motivated by, and illustrated with, discrete failure data collected from several US NPP since about 1980},
  Doi                      = {10.1109/24.690894},
  File                     = {Published version:Kvam-1998-p30-34.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {failure analysis;fission reactor safety;fission reactor theory;least squares approximations;nuclear power stations;probability;reliability;statistical analysis;USA;binomial failure-rate model;common-cause failure data;discrete failure data;least-squares solution;mixture-model parameters;model fit;multiple component failures;multiple failure events;nuclear industry;nuclear power plants;parallel systems;parametric mixture-model;probabilistic risk assessments;redundancy;statistical reliability model;target reliability;Data mining;Electric shock;Failure analysis;Maximum likelihood estimation;Nuclear power generation;Power generation;Power system modeling;Power system reliability;Redundancy;Risk management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=690894&isnumber=15138},
  Zone                     = {2Q}
}

@Article{Ladbury-2013-p4457-4463,
  Title                    = {Use of Commercial FPGA-Based Evaluation Boards for Single-Event Testing of DDR2 and DDR3 SDRAMs},
  Author                   = {Ladbury, R.L. and Berg, M.D. and Wilcox, E.P. and LaBel, K.A. and Kim, H.S. and Phan, A.M. and Seidleck, C.M.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {4457-4463},
  Volume                   = {60},

  Doi                      = {10.1109/TNS.2013.2285517},
  File                     = {Published version:Ladbury-2013-p4457-4463.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {DRAM chips;field programmable gate arrays;integrated circuit testing;radiation hardening (electronics);DDR2 SDRAMs;DDR3 SDRAMs;FPGA-based evaluation boards;complicated error behavior;data quality;radiation testing;single-event testing;Field programmable gate arrays;Quality assurance;Radiation effects;Radiation hardening (electronics);Reliability;Risk management;SDRAM;Probabilistic risk assessment;quality assurance;radiation effects in ICs;radiation hardness assurance;reliability estimation;testing techniques},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6665158&isnumber=6678952},
  Zone                     = {3Q}
}

@Article{Ladbury-2013-p4464-4469,
  Title                    = {Bayesian Methods for Bounding Single-Event Related Risk in Low-Cost Satellite Missions},
  Author                   = {Ladbury, R.L. and Campola, M.J.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {4464-4469},
  Volume                   = {60},

  Doi                      = {10.1109/TNS.2013.2285519},
  File                     = {Published version:Ladbury-2013-p4464-4469.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {radiation hardening (electronics);risk management;space vehicle electronics;statistical distributions;Bayesian methods;SEE hardness assurance approach;confidence probability;heritage data;historical data;low-cost risk tolerant missions;low-cost satellite missions;single-event related risk bounding;single-event-risk prior probability distributions;success probability;Bayes methods;Operational amplifiers;Quality assurance;Radiation effects;Radiation hardening (electronics);Reliability;Risk management;Transient analysis;Probabilistic risk assessment;quality assurance;radiation effects;radiation hardness assurance methodology;reliability estimation},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6665099&isnumber=6678952},
  Zone                     = {3Q}
}

@Article{Ladbury-2011-p3004-3010,
  Title                    = {A Bayesian Approach for Total Ionizing Dose Hardness Assurance},
  Author                   = {Ladbury, R. and Triggs, B.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {3004-3010},
  Volume                   = {58},

  Doi                      = {10.1109/TNS.2011.2172461},
  File                     = {Published version:Ladbury-2011-p3004-3010.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {hardness;probability;radiation effects;Bayesian RHA methodology;Bayesian approach;risk mitigation;total ionizing dose hardness assurance;Bayesian methods;Degradation;Probability distribution;Radiation effects;Transistors;Probabilistic risk assessment;quality assurance radiation hardness assurance methodology;radiation effects;reliability estimation},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6074974&isnumber=6093861},
  Zone                     = {3Q}
}

@Article{Lagani-2015-p-,
  Title                    = {Development and validation of risk assessment models for diabetes-related complications based on the DCCT/EDIC data },
  Author                   = {Vincenzo Lagani and Franco Chiarugi and Shona Thomson and Jo Fursse and Edin Lakasing and Russell W. Jones and Ioannis Tsamardinos},
  Journal                  = {Journal of Diabetes and its Complications },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {AbstractAim To derive and validate a set of computational models able to assess the risk of developing complications and experiencing adverse events for patients with diabetes. The models are developed on data from the Diabetes Control and Complications Trial (DCCT) and the Epidemiology of Diabetes Interventions and Complications (EDIC) studies, and are validated on an external, retrospectively collected cohort. Methods We selected fifty-one clinical parameters measured at baseline during the \{DCCT\} as potential risk factors for the following adverse outcomes: Cardiovascular Diseases (CVD), Hypoglycemia, Ketoacidosis, Microalbuminuria, Proteinuria, Neuropathy and Retinopathy. For each outcome we applied a data-mining analysis protocol in order to identify the best-performing signature, i.e., the smallest set of clinical parameters that, considered jointly, are maximally predictive for the selected outcome. The predictive models built on the selected signatures underwent both an interval validation on the DCCT/EDIC data and an external validation on a retrospective cohort of 393 diabetes patients (49 Type I and 344 Type II) from the Chorleywood Medical Center, UK. Results The selected predictive signatures contain five to fifteen risk factors, depending on the specific outcome. Internal validation performances, as measured by the Concordance Index (CI), range from 0.62 to 0.83, indicating good predictive power. The models achieved comparable performances for the Type I and, quite surprisingly, Type \{II\} external cohort. Conclusions Data-mining analyses of the DCCT/EDIC data allow the identification of accurate predictive models for diabetes-related complications. We also present initial evidences that these models can be applied on a more recent, European population. },
  Doi                      = {http://dx.doi.org/10.1016/j.jdiacomp.2015.03.001},
  File                     = {Published version:Lagani-2015-p-.pdf:PDF},
  Impactfactor             = {2.056},
  ISSN                     = {1056-8727},
  Keywords                 = {Risk assessment models},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1056872715000768},
  Zone                     = {4Q}
}

@InProceedings{Lai-2013-p366-371,
  Title                    = {A Software Security Requirement Quality Improvement Procedure to Increase E-Commerce Security},
  Author                   = {Sen-Tarng Lai and Fang-Yie Leu and Chu, W.C.-C.},
  Booktitle                = {Broadband and Wireless Computing, Communication and Applications (BWCCA), 2013 Eighth International Conference on},
  Year                     = {2013},
  Month                    = {Oct},
  Pages                    = {366-371},

  Doi                      = {10.1109/BWCCA.2013.64},
  File                     = {Published version:Lai-2013-p366-371.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {electronic commerce;formal specification;security of data;software quality;SSR quality improvement procedure;SSR quality measurement model;SSRQIP;SSRQM model;business transaction system;digital age;e-commerce SSR;e-commerce security;malicious users;network age;network intrusion;quality impact factor;software security requirement quality improvement procedure;system security defects;system security flaw;system security vulnerabilities;virus attack;Availability;Business;Complexity theory;Q-factor;Security;Software;Weight measurement;E-Commerce;Quality Measurement Model;SSR;SSRQIP;Security Vulnerability},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6690912&isnumber=6690821},
  Zone                     = {-}
}

@Article{Lang-2000-p279-284,
  Title                    = {Power distribution system reliability planning using a fuzzy knowledge-based approach},
  Author                   = {Lang, B.P. and Pahwa, A.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {279-284},
  Volume                   = {15},

  Abstract                 = {Electric power distribution system reliability is defined as the ability to deliver uninterrupted service to customers. Deregulation of the electric power industry has forced utilities to face new challenges and to evaluate the cost-benefit implications of providing an acceptable level of service. This paper presents a fuzzy knowledge-based approach for reliability planning purposes. This approach makes a direct assessment of the circuit configuration and hazards and assigns each section and feeder a relative risk index by expressing the configuration variables mathematically using fuzzy logic. Results obtained using the fuzzy model on actual utility feeder data are presented and the correlation between the fuzzy model results and actual feeder performance is obtained. Determination of the highest risk feeders and of the feeders which benefit the most from a given reliability improvement project are the major goals of the paper},
  Doi                      = {10.1109/61.847263},
  File                     = {Published version:Lang-2000-p279-284.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {fuzzy logic;fuzzy set theory;knowledge engineering;power distribution planning;power distribution reliability;risk management;circuit configuration;electric power industry deregulation;feeder;fuzzy knowledge-based approach;fuzzy model;power distribution system reliability planning;relative risk index;reliability planning;utility feeder data;Circuits;Costs;Fuzzy logic;Fuzzy systems;Load flow;Load flow analysis;Minimization;Power distribution;Power system planning;Power system reliability},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=847263&isnumber=18392},
  Zone                     = {3Q}
}

@InProceedings{Lang-2010-p1-4,
  Title                    = {A New Attribute Dependency Function in Information System},
  Author                   = {Guang-ming Lang and Qing-Guo Li},
  Booktitle                = {Computational Intelligence and Software Engineering (CiSE), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {Dec},
  Pages                    = {1-4},

  Abstract                 = {Attribute dependency function is very important for feature selection in data mining, pattern recognition and machine learning. However, Pawlak's is inadequate for some information systems, and Daisuke's definition is only for categorical attribute. In this paper, we introduce a new definition based on partition for numerical attribute. The advantage of the definition is that heterogeneous features can be dealt with. At last, we apply the function to local reduction, the experimental results show that the definition is more flexible to deal with heterogeneous features as a new quantitative analysis tool for local reduction.},
  Doi                      = {10.1109/CISE.2010.5677264},
  File                     = {Published version:Lang-2010-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data mining;information systems;learning (artificial intelligence);pattern recognition;attribute dependency function;categorical attribute;data mining;feature selection;heterogeneous features;information systems;local reduction;machine learning;numerical attribute;pattern recognition;quantitative analysis tool;Approximation methods;Cognition;Information systems;Pattern recognition;Probabilistic logic;Rough sets;Support vector machines},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5677264&isnumber=5676710},
  Zone                     = {-}
}

@Article{Langner-2011-p49-51,
  Title                    = {Stuxnet: Dissecting a Cyberwarfare Weapon},
  Author                   = {Langner, R.},
  Journal                  = {Security Privacy, IEEE},
  Year                     = {2011},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {49-51},
  Volume                   = {9},

  Doi                      = {10.1109/MSP.2011.67},
  File                     = {Published version:Langner-2011-p49-51.pdf:PDF},
  Impactfactor             = {0.962},
  ISSN                     = {1540-7993},
  Keywords                 = {security of data;Stuxnet;availability thinking;confidentiality;cybersecurity;cyberwarfare weapon;integrity;malware;military target;Aircraft propulsion;Computer security;Driver circuits;Industrial engineering;Malware;Military communication;Process control;Ralph Langner;SCADA;Stuxnet;cyberwarfare;digital code signing},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5772960&isnumber=5772950},
  Zone                     = {4Q}
}

@Article{Lanza-2014-p319-325,
  Title                    = {Population Heterogeneity in the Salience of Multiple Risk Factors for Adolescent Delinquency },
  Author                   = {Stephanie T. Lanza and Brittany R. Cooper and Bethany C. Bray},
  Journal                  = {Journal of Adolescent Health },
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {319 - 325},
  Volume                   = {54},

  Abstract                 = {AbstractPurpose To present mixture regression analysis as an alternative to more standard regression analysis for predicting adolescent delinquency. We demonstrate how mixture regression analysis allows for the identification of population subgroups defined by the salience of multiple risk factors. Methods We identified population subgroups (i.e., latent classes) of individuals based on their coefficients in a regression model predicting adolescent delinquency from eight previously established risk indices drawn from the community, school, family, peer, and individual levels. The study included N = 37,763 10th-grade adolescents who participated in the Communities That Care Youth Survey. Standard, zero-inflated, and mixture Poisson and negative binomial regression models were considered. Results Standard and mixture negative binomial regression models were selected as optimal. The five-class regression model was interpreted based on the class-specific regression coefficients, indicating that risk factors had varying salience across classes of adolescents. Conclusions Standard regression showed that all risk factors were significantly associated with delinquency. Mixture regression provided more nuanced information, suggesting a unique set of risk factors that were salient for different subgroups of adolescents. Implications for the design of subgroup-specific interventions are discussed. },
  Doi                      = {http://dx.doi.org/10.1016/j.jadohealth.2013.09.007},
  File                     = {Published version:Lanza-2014-p319-325.pdf:PDF},
  Impactfactor             = {2.966},
  ISSN                     = {1054-139X},
  Keywords                 = {Adolescence},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {风险构成},
  Timestamp                = {2014.11.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1054139X1300503X},
  Zone                     = {2Q}
}

@Article{Laugier-2011-p4-19,
  Title                    = {Probabilistic Analysis of Dynamic Scenes and Collision Risks Assessment to Improve Driving Safety},
  Author                   = {Laugier, C. and Paromtchik, I.E. and Perrollaz, M. and Yong, M.Y. and Yoder, J.-D. and Tay, C. and Mekhnacha, K. and Negre, A.},
  Journal                  = {Intelligent Transportation Systems Magazine, IEEE},
  Year                     = {2011},

  Month                    = {winter},
  Number                   = {4},
  Pages                    = {4-19},
  Volume                   = {3},

  Doi                      = {10.1109/MITS.2011.942779},
  File                     = {Published version:Laugier-2011-p4-19.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1939-1390},
  Keywords                 = {Gaussian processes;accident prevention;hidden Markov models;object detection;object tracking;risk management;road safety;road traffic;sensor fusion;traffic engineering computing;Bayesian occupancy filter;Gaussian processes;Lexus car;collision risks assessment;driving safety;driving simulator;dynamic object detection;dynamic object tracking;dynamic scenes;ego-vehicle;environment monitoring;hidden Markov models;probabilistic analysis;sensor fusion;spatio-temporal grid representation;stochastic variables;urban driving;Collision avoidance;Motion control;Probabilistic logic;Risk management;Safety;Sensor fusion},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，汽车相撞风险的评估，好像关系不是很大},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6025208&isnumber=6025204},
  Zone                     = {-}
}

@Article{Laugier-2011-p4-19a,
  Title                    = {Probabilistic Analysis of Dynamic Scenes and Collision Risks Assessment to Improve Driving Safety},
  Author                   = {Laugier, C. and Paromtchik, I.E. and Perrollaz, M. and Yong, M.Y. and Yoder, J.-D. and Tay, C. and Mekhnacha, K. and Negre, A.},
  Journal                  = {Intelligent Transportation Systems Magazine, IEEE},
  Year                     = {2011},

  Month                    = {winter},
  Number                   = {4},
  Pages                    = {4-19},
  Volume                   = {3},

  Abstract                 = {The article deals with the analysis and interpretation of dynamic scenes typical of urban driving. The key objective is to assess risks of collision for the ego-vehicle. We describe our concept and methods, which we have integrated and tested on our experimental platform on a Lexus car and a driving simulator. The on-board sensors deliver visual, telemetric and inertial data for environment monitoring. The sensor fusion uses our Bayesian Occupancy Filter for a spatio-temporal grid representation of the traffic scene. The underlying probabilistic approach is capable of dealing with uncertainties when modeling the environment as well as detecting and tracking dynamic objects. The collision risks are estimated as stochastic variables and are predicted for a short period ahead with the use of Hidden Markov Models and Gaussian processes. The software implementation takes advantage of our methods, which allow for parallel computation. Our tests have proven the relevance and feasibility of our approach for improving the safety of car driving.},
  Doi                      = {10.1109/MITS.2011.942779},
  File                     = {Published version:Laugier-2011-p4-19a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1939-1390},
  Keywords                 = {Gaussian processes;accident prevention;hidden Markov models;object detection;object tracking;risk management;road safety;road traffic;sensor fusion;traffic engineering computing;Bayesian occupancy filter;Gaussian processes;Lexus car;collision risks assessment;driving safety;driving simulator;dynamic object detection;dynamic object tracking;dynamic scenes;ego-vehicle;environment monitoring;hidden Markov models;probabilistic analysis;sensor fusion;spatio-temporal grid representation;stochastic variables;urban driving;Collision avoidance;Motion control;Probabilistic logic;Risk management;Safety;Sensor fusion},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6025208&isnumber=6025204},
  Zone                     = {-}
}

@Article{Laugier-2011-p4-19b,
  Title                    = {Probabilistic Analysis of Dynamic Scenes and Collision Risks Assessment to Improve Driving Safety},
  Author                   = {Laugier, C. and Paromtchik, I.E. and Perrollaz, M. and Yong, M.Y. and Yoder, J.-D. and Tay, C. and Mekhnacha, K. and Negre, A.},
  Journal                  = {Intelligent Transportation Systems Magazine, IEEE},
  Year                     = {2011},

  Month                    = {winter},
  Number                   = {4},
  Pages                    = {4-19},
  Volume                   = {3},

  Abstract                 = {The article deals with the analysis and interpretation of dynamic scenes typical of urban driving. The key objective is to assess risks of collision for the ego-vehicle. We describe our concept and methods, which we have integrated and tested on our experimental platform on a Lexus car and a driving simulator. The on-board sensors deliver visual, telemetric and inertial data for environment monitoring. The sensor fusion uses our Bayesian Occupancy Filter for a spatio-temporal grid representation of the traffic scene. The underlying probabilistic approach is capable of dealing with uncertainties when modeling the environment as well as detecting and tracking dynamic objects. The collision risks are estimated as stochastic variables and are predicted for a short period ahead with the use of Hidden Markov Models and Gaussian processes. The software implementation takes advantage of our methods, which allow for parallel computation. Our tests have proven the relevance and feasibility of our approach for improving the safety of car driving.},
  Doi                      = {10.1109/MITS.2011.942779},
  File                     = {Published version:Laugier-2011-p4-19b.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1939-1390},
  Keywords                 = {Gaussian processes;accident prevention;hidden Markov models;object detection;object tracking;risk management;road safety;road traffic;sensor fusion;traffic engineering computing;Bayesian occupancy filter;Gaussian processes;Lexus car;collision risks assessment;driving safety;driving simulator;dynamic object detection;dynamic object tracking;dynamic scenes;ego-vehicle;environment monitoring;hidden Markov models;probabilistic analysis;sensor fusion;spatio-temporal grid representation;stochastic variables;urban driving;Collision avoidance;Motion control;Probabilistic logic;Risk management;Safety;Sensor fusion},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6025208&isnumber=6025204},
  Zone                     = {-}
}

@InProceedings{Laukonen-1994-p184-189,
  Title                    = {Fuzzy systems for function approximation with applications to failure estimation},
  Author                   = {Laukonen, E.G. and Passino, K.M.},
  Booktitle                = {Intelligent Control, 1994., Proceedings of the 1994 IEEE International Symposium on},
  Year                     = {1994},
  Month                    = {Aug},
  Pages                    = {184-189},

  Doi                      = {10.1109/ISIC.1994.367820},
  File                     = {Published version:Laukonen-1994-p184-189.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2158-9860},
  Keywords                 = {function approximation;fuzzy set theory;fuzzy systems;identification;numerical analysis;F-16 aircraft;actuator failure detection and identification;failure estimation;function approximation;fuzzy system;input-output data pair associations;nonlinear estimation;system identification;Actuators;Aerospace engineering;Aircraft manufacture;Aircraft propulsion;Elevators;Function approximation;Fuzzy systems;Least squares approximation;State estimation;System identification},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=367820&isnumber=8414},
  Zone                     = {-}
}

@Article{Launer-1993-p71-80,
  Title                    = {Graphical techniques for analyzing failure data with the percentile residual-life function},
  Author                   = {Launer, R.L.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1993},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {71-75, 80},
  Volume                   = {42},

  Abstract                 = {Graphical data-analysis techniques for studying the empirical behavior of the hazard (failure) rate, based on a relationship between the maximum and minimum of the α-percentile residual life function to the minimum and maximum, respectively, of the hazard rate, are presented. Knowledge of these critical points is useful in controlling the system percentile life through burn-in. The graphical techniques are illustrated with a previously published data-set consisting of the empirical hazard rate of aircraft-engine components},
  Doi                      = {10.1109/24.210273},
  File                     = {Published version:Launer-1993-p71-80.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {failure analysis;reliability theory;statistical analysis;α-percentile;aircraft-engine components;burn-in;data-analysis techniques;empirical behavior;failure rate analysis;graphical techniques;hazard rate;percentile residual-life function;reliability;Control systems;Data analysis;Data engineering;Failure analysis;Hazards;Probability;Reliability engineering;Time measurement},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Function 表示函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=210273&isnumber=5455},
  Zone                     = {2Q}
}

@Article{Lavasani-2011-p277-294,
  Title                    = {Fuzzy risk assessment of oil and gas offshore wells},
  Author                   = {S.M. Miri Lavasani and Z. Yang and J. Finlay and J. Wang},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {277 - 294},
  Volume                   = {89},

  Abstract                 = {Risk evaluation of offshore wells is a challenging task, given that much of the available data is highly uncertain and vague, and many of the mechanisms are complex and difficult to understand. Consequently, a systematic approach is required to handle both quantitative and qualitative data as well as means to update existing information when new knowledge and data become available. Each Basic Risk Item (BRI) in a hierarchical framework is expressed as a fuzzy number, which is a combination of the likelihood of a failure event and the associated failure consequence. Analytical Hierarchy Process (AHP) is used to estimate weights required for grouping non-commensurate risk sources. Evidential Reasoning (ER) is employed to incorporate new data for updating existing risk estimates. It is envisaged that the proposed approach could serve as a basis for benchmarking acceptable risks in offshore wells. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2011.06.006},
  File                     = {Published version:Lavasani-2011-p277-294.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Analytical hierarchy process},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957582011000425},
  Zone                     = {3Q}
}

@Article{Lavasani-2015-p75-88,
  Title                    = {An extension to Fuzzy Fault Tree Analysis (FFTA) application in petrochemical process industry },
  Author                   = {Seyed Miri Lavasani and Anousheh Zendegani and Metin Celik},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {75 - 88},
  Volume                   = {93},

  Abstract                 = {Abstract Fault Tree Analysis (FTA) is an established technique in risk management associated with identified hazards specific to focused fields. It is a comprehensive, structured and logical analysis method aimed at identifying and assessing hazards of complex systems. To conduct a quantitative FTA, it is essential to have sufficient data. By considering the fact that sufficient data is not always available, the \{FTA\} method can be adopted into the problems under fuzzy environment, so called as Fuzzy Fault Tree Analysis (FFTA). This research extends \{FFTA\} methodology to petrochemical process industry in which fire, explosion and toxic gas releases are recognized as potential hazards. Specifically, the case study focuses on Deethanizer failure in petrochemical plant operations to demonstrate the proposed methodology. Consequently, the study has provided theoretical and practical values to challenge with operational data shortage in risk assessment. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2014.05.001},
  File                     = {Published version:Lavasani-2015-p75-88.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957582014000706},
  Zone                     = {3Q}
}

@Article{Lee-2002-p5-22,
  Title                    = {Toward cost-sensitive modeling for intrusion detection and response},
  Author                   = {Lee, Wenke and Fan, Wei and Miller, Matthew and Stolfo, Salvatore J and Zadok, Erez},
  Journal                  = {Journal of Computer Security},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {5--22},
  Volume                   = {10},

  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Publisher                = {IOS Press},
  Timestamp                = {2015.03.12},
  Zone                     = {-}
}

@Article{Lee-2015-p40-47,
  Title                    = {Accuracy enhancement in estimation of the initiating event frequencies in risk monitor application on Kuosheng \{NPP\} },
  Author                   = {Yun-Ju Becker Lee and Tsu-Jen Lin and Sheau-Farn Max Liang},
  Journal                  = {Annals of Nuclear Energy },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {40 - 47},
  Volume                   = {76},

  Abstract                 = {Abstract Risk-Informed Regulation (RIR) based on the technology of the Probabilistic Risk Assessment (PRA) is an important issue in the safety management of Nuclear Power Plants (NPPs). With the advancement of the state-of-art PRA, On-Line Maintenance (OLM) has been widely used by \{NPPs\} in the USA. A new risk monitor for OLM, Maintenance Integrated Risk Utilities (MIRU), was developed by the Institute of Nuclear Energy Research (INER), in Taiwan, to facilitate the daily management and risk evaluation of maintenance activities. The initiating event frequencies (IEFs) estimated in \{MIRU\} was requested to be changed with the plant status to reflect the effects of varied maintenance activities. However, few researches have empirically documented the link between \{IEFs\} and maintenance. This article attempts to explore a way to enhance the accuracy of \{IEFs\} estimated in MIRU. A new fault tree analysis method named Degraded Fault Tree (DFT) was developed. The results have shown that use of \{DFT\} can estimate the \{IEFs\} correctly. The \{DFT\} can also alert \{NPPs\} not to enter any maintenance configurations with high potential of causing initiating events, which can further ensure the safety of OLMs. },
  Doi                      = {http://dx.doi.org/10.1016/j.anucene.2014.09.034},
  File                     = {Published version:Lee-2015-p40-47.pdf:PDF},
  Impactfactor             = {0.8},
  ISSN                     = {0306-4549},
  Keywords                 = {Probabilistic Risk Assessment (PRA)},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0306454914005131},
  Zone                     = {4Q}
}

@InProceedings{LeitedaS-1999-p1-205,
  Title                    = {Dynamic security risk assessment},
  Author                   = {Leite da Silva, A.M. and Jardim, J.L. and Rei, A.M. and Mello JCO},
  Booktitle                = {Power Engineering Society Summer Meeting, 1999. IEEE},
  Year                     = {1999},
  Month                    = {Jul},
  Pages                    = {198-205 vol.1},
  Volume                   = {1},

  Abstract                 = {This paper summarizes part of the research under development Brazil, related to the risk assessment of dynamic security. It treats the problem from three connected time frameworks: on-line operation, operation planning and expansion planning. It is divided into three major sections covering the three time frameworks: on-line security risk assessment, voltage collapse risk assessment and reliability security assessment. At the end of each section, future trends for implementation and research are discussed},
  Doi                      = {10.1109/PESS.1999.784346},
  File                     = {Published version:LeitedaS-1999-p1-205.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {power system dynamic stability;power system planning;power system reliability;power system security;risk management;Brazil;composite reliability;dynamic security risk assessment;expansion planning;on-line operation;on-line security risk assessment;operation planning;reliability security assessment;research;transient stability;voltage collapse risk assessment;Data security;Power system analysis computing;Power system dynamics;Power system reliability;Power system security;Power system stability;Power system transients;Risk management;Senior members;Voltage},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=784346&isnumber=17001},
  Zone                     = {-}
}

@Article{Leith-2013-p982-993,
  Title                    = {Identification and application of security measures for petrochemical industrial control systems },
  Author                   = {H.M. Leith and John W. Piper},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {982 - 993},
  Volume                   = {26},

  Abstract                 = {Abstract The financial success of the chemical and petrochemical industry will increasingly depend upon the security of process control systems. This paper presents recommendations and insights gleaned from over 100 security risk assessment (SRA) and process control analyses, using requirements baselines extracted from the National Institute of Standards and Technology (NIST) special publication 800-53 (and Appendix A), the Recommended Security Controls for Federal Information Systems and Organizations, in conjunction with \{NIST\} special publication 800-82, Guide to Industrial Control Systems(ICS) Security, to provide the bridge in application of 800-53 controls to IC/SCADA. The paper identifies how current and projected malevolent threats posed by insiders, outsiders, collusion, and system-induced threats can erode system performance in terms of shut downs, sabotage, production disruption, and contamination. The issue is not whether there are clear and present cyber threats, nor whether there are business prudent practices that can be implemented to counter those threats; but rather that there is such a diverse compendium, at times conflicting and often technically obtuse guidance, that clarity is needed to narrow the focus of this guidance to assist those responsible for implementing effective process control security. The paper focuses on application of business-prudent controls and discusses how disparities in implementation of controls can exacerbate system vulnerabilities. Topics include issues of processes control system management, systems documentation, use of contractors and remote contractor access, system authorities that exceed user needs, misalignment of staff perception of information asset values, exposures related to use of \{USB\} ports, lack of encryption, and background surety gaps for individuals and contractor companies with access to process control systems. The paper examines the dynamics of communicating information from process control systems to business \{IT\} systems and the pressure from business operations to capture process data and make it available in near real-time through administrative networks. Such pressures may influence systems administrators to overlook or ignore firewall and systems engineering architecture, increasing potentials for two-way interface between business and process control that significantly increases exploit exposures. Despite the availability of excellent guidelines for physical and technical security of \{IT\} related assets, these practices are too often unheeded in favor of expediency or expanded access. The paper includes a discussion of Risk Management Framework models that should be considered to enhance the correspondences and relationships between multiple organizational domains, thereby promoting more effective cyber security for current and future process control systems. The paper summarizes the process for establishing security for industrial control systems (ICS), and addresses cyber security baseline requirements and expectations, within a risk management framework that provides a decision basis, threat dynamics, common vulnerabilities, and prudent mitigation measures. Much of this summary has been derived from The Information Technology Laboratory at the National Institute of Standards and Technology (NIST) Special Publication (SP) 800-53, Recommended Security Controls for Federal Information Systems and \{NIST\} \{SP\} 800-82, Guide to Industrial Control Systems (ICS) Security. \{NIST\} has also published Applying \{NIST\} \{SP\} 800-53 to Industrial Control Systems which demonstrates the relationship of 800-53 to \{ICS\} security and the application of more than 20 control families and over 625 control elements to \{ICS\} security. Although originally designed for Federal systems, portions of these publications also provide a solid foundation for critical commercial and industrial information control systems in terms of addressing the basic questions that companies in the process industry should consider when selecting security controls, including: • What controls are actually needed to protect process systems, while supporting operations and safeguarding critical assets? • Can the selected controls suggested for Federal systems effectively be implemented for systems in the process industry? • Once selected and implemented, will these controls really be effective in protecting the processes? \{NIST\} \{SP\} 800-53, Recommended Security Controls for Federal Information Systems, helps answer questions to strengthen commercial processes information security programs. The security controls articulated in \{NIST\} \{SP\} 800-53 provide guidance and recommend practices applicable to security systems in process industries, to provide a foundation for understanding the fundamental concepts of security controls. The introductory material presents the concept of security controls and their use within well-defined information security programs. Some of the issues discussed include the structural components of controls, how the controls are organized into families, and the use of controls to support information security programs. The guide outlines the essential steps that should be followed to determine needed controls, to assure the effectiveness of controls, and to maintain the effectiveness of installed controls. The appendices in \{NIST\} \{SP\} 800-53 provide additional resources including general references, definitions, explanation of acronyms, a breakdown of security controls for graduated levels of security requirements, a catalog of security controls, and information relating security controls to other standards and control sets. The controls are organized into classes of operational, management, and technical controls, and then into families within each class. To maintain parity and applicability with advances in technology, \{NIST\} also plans to review and to update the controls in the catalog as technology changes and new safeguards and new information security countermeasures are identified. \{NIST\} \{SP\} 800-53 and related documents are available at http://csrc.nist.gov/publications/nistpubs/index.html. The extensive reference list in \{SP\} 800-53 includes standards, guidelines, and recommendations that process industry companies can use as the foundation for comprehensive security planning and lifecycle management processes. Additionally, a significant effort of broad commercial and government cooperation, the Consensus Audit Guideline (CAG) provides a 20-element cyber security controls roster supporting a common commercial framework for cyber security, correlating to the \{NIST\} 800-53 Control Library. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2013.10.009},
  File                     = {Published version:Leith-2013-p982-993.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Industrial control systems},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423013002015},
  Zone                     = {3Q}
}

@Article{Leszczyna-2015-p81-89,
  Title                    = {Approaching secure industrial control systems},
  Author                   = {Leszczyna, R.},
  Journal                  = {Information Security, IET},
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {81-89},
  Volume                   = {9},

  Abstract                 = {This study presents a systematic approach to secure industrial control systems based on establishing a business case followed by the development of a security programme. To support these two fundamental activities the authors propose a new method for security cost estimation and a security assessment scheme. In this study they explain the cost evaluation technique and illustrate with a case study concerning the assessment of the cost of information security assurance activities in a division of a Polish manufacturer of passenger and commercial tyres. They further present the steps of their security assessment scheme and demonstrate how they integrate with the overall approach for protecting industrial control systems.},
  Doi                      = {10.1049/iet-ifs.2013.0159},
  File                     = {Published version:Leszczyna-2015-p81-89.pdf:PDF},
  Impactfactor             = {0.623},
  ISSN                     = {1751-8709},
  Keywords                 = {costing;industrial control;production engineering computing;production management;security of data;Polish manufacturer;commercial tyres;cost evaluation technique;information security assurance activities;passenger tyres;secure industrial control systems;security assessment scheme;security cost estimation;security programme},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6983734&isnumber=6983695},
  Zone                     = {4Q}
}

@Article{Leu2013,
  Title                    = {Bayesian-network-based safety risk assessment for steel construction projects },
  Author                   = {Sou-Sen Leu and Ching-Miao Chang},
  Journal                  = {Accident Analysis \& Prevention },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {122 - 133},
  Volume                   = {54},

  Abstract                 = {There are four primary accident types at steel building construction (SC) projects: falls (tumbles), object falls, object collapse, and electrocution. Several systematic safety risk assessment approaches, such as fault tree analysis (FTA) and failure mode and effect criticality analysis (FMECA), have been used to evaluate safety risks at \{SC\} projects. However, these traditional methods ineffectively address dependencies among safety factors at various levels that fail to provide early warnings to prevent occupational accidents. To overcome the limitations of traditional approaches, this study addresses the development of a safety risk-assessment model for \{SC\} projects by establishing the Bayesian networks (BN) based on fault tree (FT) transformation. The BN-based safety risk-assessment model was validated against the safety inspection records of six \{SC\} building projects and nine projects in which site accidents occurred. The ranks of posterior probabilities from the \{BN\} model were highly consistent with the accidents that occurred at each project site. The model accurately provides site safety-management abilities by calculating the probabilities of safety risks and further analyzing the causes of accidents based on their relationships in BNs. In practice, based on the analysis of accident risks and significant safety factors, proper preventive safety management strategies can be established to reduce the occurrence of accidents on \{SC\} sites. },
  Doi                      = {http://dx.doi.org/10.1016/j.aap.2013.02.019},
  File                     = {Published version:Leu-2013-p122-133.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0001-4575},
  Keywords                 = {Bayesian network},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0001457513000602},
  Zone                     = {-}
}

@Article{Leu-2013-p122-133,
  Title                    = {Bayesian-network-based safety risk assessment for steel construction projects },
  Author                   = {Sou-Sen Leu and Ching-Miao Chang},
  Journal                  = {Accident Analysis \& Prevention },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {122 - 133},
  Volume                   = {54},

  Abstract                 = {There are four primary accident types at steel building construction (SC) projects: falls (tumbles), object falls, object collapse, and electrocution. Several systematic safety risk assessment approaches, such as fault tree analysis (FTA) and failure mode and effect criticality analysis (FMECA), have been used to evaluate safety risks at \{SC\} projects. However, these traditional methods ineffectively address dependencies among safety factors at various levels that fail to provide early warnings to prevent occupational accidents. To overcome the limitations of traditional approaches, this study addresses the development of a safety risk-assessment model for \{SC\} projects by establishing the Bayesian networks (BN) based on fault tree (FT) transformation. The BN-based safety risk-assessment model was validated against the safety inspection records of six \{SC\} building projects and nine projects in which site accidents occurred. The ranks of posterior probabilities from the \{BN\} model were highly consistent with the accidents that occurred at each project site. The model accurately provides site safety-management abilities by calculating the probabilities of safety risks and further analyzing the causes of accidents based on their relationships in BNs. In practice, based on the analysis of accident risks and significant safety factors, proper preventive safety management strategies can be established to reduce the occurrence of accidents on \{SC\} sites. },
  Doi                      = {http://dx.doi.org/10.1016/j.aap.2013.02.019},
  File                     = {Published version:Leu-2013-p122-133.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0001-4575},
  Keywords                 = {Bayesian network},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0001457513000602},
  Zone                     = {-}
}

@Article{Levy-2005-p63-65,
  Title                    = {Worm propagation and generic attacks},
  Author                   = {Levy, E.},
  Journal                  = {Security Privacy, IEEE},
  Year                     = {2005},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {63-65},
  Volume                   = {3},

  Abstract                 = {The defining task of propagating malicious code is to locate new targets to attack. Viruses search for files in a computer system to which to attach, whereas worms search for new targets to which to transmit themselves. Depending on their method of transmission, malicious code writers have developed different strategies for finding new victims. Worms transmitted via email have had great success propagating themselves because they find their next targets either by raiding a user's email address book or by searching through the user's mailbox. Such addresses are almost certain to be valid, permitting the worm to hijack the user's social web and exploit trust relationships. In most cases, the worm will craft its own message to send to the target, but some will wait for the user to send a message and attach themselves to it. Network worms, those that attack network services, must determine their next victim's IP address.},
  Doi                      = {10.1109/MSP.2005.57},
  File                     = {Published version:Levy-2005-p63-65.pdf:PDF},
  Impactfactor             = {0.962},
  ISSN                     = {1540-7993},
  Keywords                 = {Internet;electronic mail;invasive software;IP address;email;generic attacks;malicious code;worm propagation;Computer security;Computer worms;Crawlers;Data security;HTML;Internet;Privacy;Search engines;Web pages;Web search;Santy;Web applications;virus;worms},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1423964&isnumber=30742},
  Zone                     = {4Q}
}

@Article{Levy-2014-p437-439,
  Title                    = {Heart Failure Risk Prediction Models: What Have We Learned? },
  Author                   = {Wayne C. Levy and Inder S. Anand},
  Journal                  = {JACC: Heart Failure },
  Year                     = {2014},
  Number                   = {5},
  Pages                    = {437 - 439},
  Volume                   = {2},

  Doi                      = {http://dx.doi.org/10.1016/j.jchf.2014.05.006},
  File                     = {Published version:Levy-2014-p437-439.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2213-1779},
  Keywords                 = {heart failure},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2213177914002534},
  Zone                     = {-}
}

@Article{Li-2011-p3878-3885,
  Title                    = {Corrosion propagation of prestressing steel strands in concrete subject to chloride attack },
  Author                   = {Fumin Li and Yingshu Yuan and Chun-Qing Li},
  Journal                  = {Construction and Building Materials },
  Year                     = {2011},
  Number                   = {10},
  Pages                    = {3878 - 3885},
  Volume                   = {25},

  Abstract                 = {A review of research literature suggests that steel corrosion in concrete can accelerate if the corroding steel is under high levels of stress but this important phenomenon has not been investigated thoroughly. Both laboratory tests and field surveys indicate that the effects of corrosion on structural behaviour in reinforced and prestressed concrete structures are different, with the latter having less concrete cracking but more serious structural collapses. The intention of this paper is to investigate the corrosion propagation of prestressing steel strands in chloride-laden environment. A long term experimental program is presented in which the corrosion of twelve prestressing steel strands embedded in concrete was examined. The effects of the level of stresses in the steel and type of steels on corrosion rate are studied. It is found that the corrosion rate of prestressing steel strands in concrete increases with the increase of the level of stresses applied and that the pitting corrosion is the main form of prestressing steel corrosion in chloride-laden environment. The paper concludes that the corrosion of prestressing steel in concrete structures poses higher risk to the structure than that of reinforcing steel in terms of structural collapse. },
  Doi                      = {http://dx.doi.org/10.1016/j.conbuildmat.2011.04.011},
  File                     = {Published version:Li-2011-p3878-3885.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0950-0618},
  Keywords                 = {Prestressing steel},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950061811001516},
  Zone                     = {2Q}
}

@Article{Li-2014-p422-434,
  Title                    = {Mean Variance Analysis of Fast Fashion Supply Chains With Returns Policy},
  Author                   = {Jian Li and Tsan-Ming Choi and Cheng, T.C.E.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {422-434},
  Volume                   = {44},

  Abstract                 = {This paper is motivated by observed industrial practices. We conduct a mean variance (MV) analysis of a fast fashion supply chain with returns policy. Different from the conventional newsvendor type products, fast fashion brands plan to have stock-out because it is a feature of fast fashion and can bring some benefit. Based on the fast fashion features, we build an analytical MV optimization model for a two-echelon fast fashion supply chain to address the following research questions. 1) What are the differences and similarities in the structural properties between the supply chains that carry fast fashion products and conventional newsvendor type products? 2) How do we optimize a fast fashion supply chain with multiple retailers under the MV framework? 3) Can a simple returns policy optimize (and “coordinate”) such a multiretailer supply chain? 4) How do individual retailers' degrees of risk aversion affect the achievability of coordination? 5) Can the above simple contract help coordinate the supply chain under information asymmetry? We propose a novel approach called “negotiated space” in the analysis. We generate several important insights which include an interesting finding that a simple returns policy can be applied to coordinate the fast fashion supply chain even in the presence of multiple retailers.},
  Doi                      = {10.1109/TSMC.2013.2264934},
  File                     = {Published version:Li-2014-p422-434.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {clothing;statistical analysis;supply chain management;MV framework;MV optimization model;fast fashion brand plan;fast fashion supply chains;industrial practices;information asymmetry;mean variance analysis;multiretailer supply chain;negotiated space;newsvendor type products;retailer degrees of risk aversion affect;returns policy;structural properties;two-echelon fast fashion supply chain;Analytical models;Contracts;Cybernetics;Educational institutions;Optimization;Supply chains;Fast fashion product;mean variance analysis;supply chain coordination},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6568899&isnumber=6766681},
  Zone                     = {-}
}

@Article{Li-2014-p59-65,
  Title                    = {A vulnerability model for power system dynamic security assessment},
  Author                   = {Jianlan Li and Shuhong Huang},
  Journal                  = {International Journal of Electrical Power \& Energy Systems },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {59 - 65},
  Volume                   = {62},

  Abstract                 = {Abstract Traditional security assessment based on risk takes into account the failure probability and the failure consequence. This paper provides a modification on the risk calculation by replacing the failure probability with the current status to overcome the dependence on mass statistical data. The vulnerability model is proposed that incorporates the two key factors of the risk along with the deterioration trend factor to define the system security level, which efficiently indicates the current and potential danger. Grey system theory is used to predict the equipment deterioration trend and the grey correlation moment is defined to evaluate equipment state with multi-parameters. The classification and quantification of equipment state, failure consequence, risk, deterioration trend and vulnerability are discussed. A feed water pump case demonstrates the effectiveness of vulnerability in system dynamic security assessment. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijepes.2014.03.026},
  File                     = {Published version:Li-2014-p59-65.pdf:PDF},
  Impactfactor             = {3.432},
  ISSN                     = {0142-0615},
  Keywords                 = {Power system},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，我们学校的论文，没有仿真},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0142061514001288},
  Zone                     = {2Q}
}

@Article{Li-2001-p84-88,
  Title                    = {Influence of DC supply systems on unplanned reactor trips in nuclear power plants},
  Author                   = {Li, Junli and Tong, Jiejuan and Mao, Dingyuan},
  Journal                  = {Tsinghua Science and Technology},
  Year                     = {2001},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {84-88},
  Volume                   = {6},

  File                     = {Published version:Li-2001-p84-88.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Electromagnetics;Inductors;Power supplies;Relays;Turbines;Valves;Failure Mode Effect Analysis (FMEA);Fault Tree Analysis (FTA);Nuclear Power Plant (NPP);Probabilistic Risk Assessment (PRA)},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6083378&isnumber=6083358},
  Zone                     = {-}
}

@Article{Li-2014-p63-72,
  Title                    = {A novelty detection machine and its application to bank failure prediction },
  Author                   = {Shukai Li and Whye Loon Tung and Wee Keong Ng},
  Journal                  = {Neurocomputing },
  Year                     = {2014},
  Note                     = {Track on Intelligent Computing and Applications Selected papers from the 2012 International Workshop on Information, Intelligence and Computing (IWIIC 2012) Complex Learning in Connectionist Networks Selected papers from the World Congress on Nature and Biologically Inspired Computing (NaBIC) },
  Number                   = {0},
  Pages                    = {63 - 72},
  Volume                   = {130},

  Abstract                 = {Abstract Novelty detection has been well-studied for many years and has found a wide range of applications, but correctly identifying the outliers is still a hard problem because of the diverse variation and the small quantity of such outliers. We address the problem using several distinct characteristics of the outliers and the normal patterns. First, normal patterns are usually grouped together, forming clusters in the high density regions of the data space. Second, outliers are characteristically very different from the normal patterns, and hence tend to be located far away from the normal patterns in the data space. Third, the number of outliers is generally very small in a given dataset. Based on these observations, we can envisage that the appropriate decision boundary segregating the outliers and the normal patterns usually lies in some low density regions of the data space. This is referred to as cluster assumption. The resultant optimization problem to learn the decision function can be solved using the mixed integer programming approach. Following that, we present a cutting plane algorithm together with a multiple kernel learning technique to solve the convex relaxation of the optimization problem. Specifically, we make use of the scarcity of the outliers to find a violating solution to the cutting plane algorithm. Experimental results with several benchmark datasets show that our proposed novelty detection method outperforms existing hyperplane and density estimation-based novelty detection techniques. We subsequently apply our method to the prediction of banking failures to identify potential bank failures or high risk banks through the traits of financial distress. },
  Doi                      = {http://dx.doi.org/10.1016/j.neucom.2013.02.043},
  File                     = {Published version:Li-2014-p63-72.pdf:PDF},
  Impactfactor             = {1.634},
  ISSN                     = {0925-2312},
  Keywords                 = {Novelty detection},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925231213006942},
  Zone                     = {3Q}
}

@Article{Li-2014-p42-53,
  Title                    = {Surface vs. interior failure behaviors in a structural steel under gigacycle fatigue: Failure analysis and life prediction },
  Author                   = {Wei Li and Huang Yuan and Zhenduo Sun and Zhenyu Zhang},
  Journal                  = {International Journal of Fatigue },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {42 - 53},
  Volume                   = {64},

  Abstract                 = {Abstract The failure behaviors of a structural Cr–Ni–W steel under gigacycle fatigue were investigated, and a life prediction method based on the crack growth was proposed. The discovery of two failure phenomena, including the coexistence of surface and interior cracks and the coexistence of multiple fish-eyes, verifies the possibility of competing failure. The developed interior crack growth model reveals the failure progress at a slow rate below 10−10&#xa0;m/cycle within the fish-eye induced by the inclusion or the inhomogeneous matrix area. The validity of the life prediction method is proved by the good agreement between the predicted and the experimental lives. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijfatigue.2014.02.022},
  File                     = {Published version:Li-2014-p42-53.pdf:PDF},
  Impactfactor             = {1.976},
  ISSN                     = {0142-1123},
  Keywords                 = {Gigacycle fatigue},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0142112314000814},
  Zone                     = {2Q}
}

@InProceedings{Li-2010-p120-125,
  Title                    = {Tactical guarantee network survivability cascade failures analysis based on probability generating function},
  Author                   = {Yong Li and Zhong-chi Qin and An-quan Zou},
  Booktitle                = {Industrial Engineering and Engineering Management (IE EM), 2010 IEEE 17Th International Conference on},
  Year                     = {2010},
  Month                    = {Oct},
  Pages                    = {120-125},

  Doi                      = {10.1109/ICIEEM.2010.5646627},
  File                     = {Published version:Li-2010-p120-125.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer networks;probability;system recovery;cascade failure survivability model;network cascade collapse;network load;network model;network node initial capacity;probability function method analysis;probability generating function;random node failure;tactical guarantee network;uniformly random degree distribution;Analytical models;Computer crashes;Load modeling;cascade failure;guarantee network;probability generating function;survivability},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5646627&isnumber=5645891},
  Zone                     = {-}
}

@InProceedings{Li-2013-p413-418,
  Title                    = {Quantitative area risk assessment and safety planning on chemical industry parks},
  Author                   = {Yang Li and Hui Wang},
  Booktitle                = {Quality, Reliability, Risk, Maintenance, and Safety Engineering (QR2MSE), 2013 International Conference on},
  Year                     = {2013},
  Month                    = {July},
  Pages                    = {413-418},

  Abstract                 = {Chemical industry park development has become an inevitable trend. However, due to the high density of numerous enterprises and workers, it easily leads to mass death, casualty and domino effects in case of any accidents, so the quantitative area risk assessment (QARA) and safety planning are particularly important. Taking a chemical industry park as an example and based on the current research, this paper conducts quantitative risk assessment with CASSTQRA software to work out the integrated individual risk contour distribution and social risk capacity curve and proposes a safety planning combining with an acceptable risk criterion to ensure the safety and long-term development of the park.},
  Doi                      = {10.1109/QR2MSE.2013.6625615},
  File                     = {Li-2013-p413-418.pdf:Li-2013-p413-418.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {chemical industry;industrial plants;occupational safety;risk analysis;strategic planning;CASSTQRA software;accidents;chemical industry parks;domino effects;individual risk contour distribution;quantitative area risk assessment;risk criterion;safety planning;social risk capacity curve;Accidents;Chemical industry;Hazards;Personnel;Planning;Risk management;acceptable risk criterion;chemical industry parks;risk assessment;safety planning},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6625615&isnumber=6625514},
  Zone                     = {-}
}

@InProceedings{Li-2008-p455-460,
  Title                    = {Hazards and Dynamic Risk Assessment of Power Supply Enterprise},
  Author                   = {Yanbin Li and Peng Wei},
  Booktitle                = {Risk Management Engineering Management, 2008. ICRMEM '08. International Conference on},
  Year                     = {2008},
  Month                    = {Nov},
  Pages                    = {455-460},

  Abstract                 = {Hazards is an objective existence that is the root cause of the accident, but it will not necessarily become an angerous source of danger, so we should carry out risk assessment. This idea will be applied to the security management of power supply enterprise, and proposed the risk evaluation index system based on the dangerous source of power supply enterprise which in accordance with the idea of three types of dangerous source, and then constructed the risk assessment model of this system using AHP and the theory of dynamic fuzzy mathematics. Example analysis of the power company in a certain area has proved that this method can conduct effective evaluation on the security situation of electricity supply companies and can provide scientific information for managers to raise the level of safety management.},
  Doi                      = {10.1109/ICRMEM.2008.9},
  File                     = {Published version:Li-2008-p455-460.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {accidents;electricity supply industry;fuzzy set theory;hazards;risk management;security;statistical analysis;AHP;accident;dynamic fuzzy mathematics;dynamic risk assessment;hazards risk assessment;power supply enterprise;safety management;security management;Accidents;Energy management;Hazards;Load management;Mathematical model;Power supplies;Power system management;Power system modeling;Power system security;Risk management;AHP;dangerous source;dynamic fuzzy evaluation;index system;power supply enterprise},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4673272&isnumber=4673183},
  Zone                     = {-}
}

@InProceedings{Liankun-2014-p524-527,
  Title                    = {Hybrid Neural Network Based Risk Assessment Method for Large Scale Sports Events},
  Author                   = {Liankun, Jia and Yangyong},
  Booktitle                = {Intelligent Systems Design and Engineering Applications (ISDEA), 2014 Fifth International Conference on},
  Year                     = {2014},
  Month                    = {June},
  Pages                    = {524-527},

  Abstract                 = {This paper focuses on the problem of risk assessment method for large scale sports events which is an important problem in modern sports management. The index system for large scale sports events risk assessment is proposed in advance, which is made up of three categories: 1) risk before match, 2) risk in match and 3) risk after match. Particularly, eighteen influencing factors are design which can cover all aspects of the large scale sports events risk assessment. Structure of the hybrid neural network is contructed by three layers, which are "the input layer", "the hidden layer", and "the output layer". Particularly, the output layers can compute the regression function values, and the regression function of the artificial neural network can be computed through a linear integration of some nonlinear basis functions. Afterwards, utilizing a training dataset and its updating version, values of the decision functions of the multi-class support vector classifier can be obtained by the regression functions based on the artificial neural network. Finally, experiments are conducted to test the effectiveness of our algorithm. The conclusions can be drawn that compared with the artificial neural network, the proposed hybrid neural network is more suitable for the risk assessment of large scale sports events.},
  Doi                      = {10.1109/ISDEA.2014.124},
  File                     = {Published version:Liankun-2014-p524-527.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Artificial neural networks;Classification algorithms;Indexes;Risk management;Support vector machines;Training;Artificial neural network;Hybrid neural network;Nonlinear basis function;Risk assessment;Sports events},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6977654&isnumber=6977522},
  Zone                     = {-}
}

@InProceedings{Lijian-2010-p3-604,
  Title                    = {Research the information security risk assessment technique based on Bayesian network},
  Author                   = {Wang Lijian and Wang Bin and Peng Yongjun},
  Booktitle                = {Advanced Computer Theory and Engineering (ICACTE), 2010 3rd International Conference on},
  Year                     = {2010},
  Month                    = {Aug},
  Pages                    = {V3-600-V3-604},
  Volume                   = {3},

  Abstract                 = {Information security risk assessment is a new technology in China, there are so many difficulties in its application. In this paper, we introduced Bayesian network into information security risk assessment system based on risk analysis studies. Firstly, we create a risk assessment model based on Bayesian network, which used probabilistic reasoning to seek value at risk and combined with expert knowledge. And then we give a risk assessment model system architecture network based on Bayesian analysis and the corresponding security knowledge base in detail. Finally, we present an implementation of risk analysis module based on Bayesian network, and used it to analyze an example. We believe that the model of information security risk assessment based on Bayesian analysis is an effective model of risk assessment system.},
  Doi                      = {10.1109/ICACTE.2010.5579740},
  File                     = {Published version:Lijian-2010-p3-604.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2154-7491},
  Keywords                 = {Bayes methods;inference mechanisms;risk management;security of data;Bayesian network;China;information security risk assessment technique;probabilistic reasoning;risk assessment model system architecture network;Atmospheric measurements;Bayesian methods;Markov processes;Mechatronics;Particle measurements;Pipelines;Risk management;Bayesian Network;Information Security;Risk Analysis;Risk Assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5579740&isnumber=5579079},
  Zone                     = {-}
}

@Article{Lin-2013-p379-392,
  Title                    = {Wind Power Fluctuation Smoothing Controller Based on Risk Assessment of Grid Frequency Deviation in an Isolated System},
  Author                   = {Jin Lin and Yuanzhang Sun and Yonghua Song and Wenzhong Gao and Sorensen, P.},
  Journal                  = {Sustainable Energy, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {379-392},
  Volume                   = {4},

  Abstract                 = {Wind power fluctuation raises the security concern of grid frequency deviation, especially for an isolated power system. Thus, better control methodology needs to be developed to smooth the fluctuation without excessive spillage. Based on an actual industrial power system, this paper proposes a smoothing controller to suppress the power fluctuation from doubly-fed induction generator (DFIG)-based wind farm. This controller consists of three main functionality components: risk assessment model, wind turbine rotor speed optimizer, and rotor speed upper limiter. In order to avoid unnecessary energy loss, this paper designs a risk assessment model of grid frequency deviation, which is capable of locally estimating the maximum grid frequency deviation risk of the next dispatch cycle. A wind turbine speed optimizer then uses the estimated frequency deviation risk to search for the optimal power curve with reduced output so that a trade-off between fluctuation smoothing and energy loss is achieved. Subsequently, the controller limits the maximum rotor speed to shift down the power curve of wind power plant based on the optimal wind turbine rotor speed. Therefore, the power fluctuation is smoothed along with the down-regulated power curve. A numerical case study demonstrates the effectiveness and economy of this smoothing controller for the studied isolated system.},
  Doi                      = {10.1109/TSTE.2012.2225853},
  File                     = {Published version:Lin-2013-p379-392.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1949-3029},
  Keywords                 = {angular velocity control;asynchronous generators;frequency control;frequency estimation;losses;power control;power generation control;power generation dispatch;power generation economics;power grids;power system security;risk management;wind power plants;wind turbines;DFIG;dispatch cycle;doubly-fed induction generator;energy loss;industrial power system;isolated power system;maximum grid frequency deviation risk estimation;maximum rotor speed controller;optimal down-regulated power curve;power system economics;risk assessment model;rotor speed upper limiter;security concern;wind farm;wind power fluctuation smoothing controller;wind power plant;wind turbine rotor speed optimizer;Frequency control;Rotors;Smoothing methods;Wind farms;Wind power generation;Wind speed;Wind turbines;“Time-Frequency” assessment model;Binary search algorithm;doubly-fed induction generator (DFIG);energy loss;frequency deviation;maximum speed limiter;power fluctuation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6387354&isnumber=6481467},
  Zone                     = {-}
}

@InProceedings{Lin-2003-p371-372,
  Title                    = {Introducing abuse frames for analysing security requirements},
  Author                   = {Lin, L. and Nuseibeh, B. and Ince, D. and Jackson, M. and Moffett, J.},
  Booktitle                = {Requirements Engineering Conference, 2003. Proceedings. 11th IEEE International},
  Year                     = {2003},
  Month                    = {Sept},
  Pages                    = {371-372},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/ICRE.2003.1232791},
  File                     = {Published version:Lin-2003-p371-372.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1090-705X},
  Keywords                 = {computer crime;data privacy;formal specification;formal verification;security of data;systems analysis;Jackson Problem Frames;abuse frames;data privacy;malicious user;security vulnerability;system security requirement analysis;Automation;Computer science;Computer security;Design engineering;Engineering management;Information security;Internet;Mission critical systems;Protection;Systems engineering and theory},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1232791&isnumber=27626},
  Zone                     = {-}
}

@InProceedings{Lin-2003-p371-372a,
  Title                    = {Introducing abuse frames for analysing security requirements},
  Author                   = {Lin, L. and Nuseibeh, B. and Ince, D. and Jackson, M. and Moffett, J.},
  Booktitle                = {Requirements Engineering Conference, 2003. Proceedings. 11th IEEE International},
  Year                     = {2003},
  Month                    = {Sept},
  Pages                    = {371-372},

  Abstract                 = {We are developing an approach using Jackson's Problem Frames to analyse security problems in order to determine security vulnerabilities. We introduce the notion of an anti-requirement as the requirement of a malicious user that can subvert an existing requirement. We incorporate anti-requirements into so-called abuse frames to represent the notion of a security threat imposed by malicious users in a particular problem context. We suggest how abuse frames can provide a means for bounding the scope of security problems in order to analyse security threats and derive security requirements.},
  Doi                      = {10.1109/ICRE.2003.1232791},
  File                     = {Published version:Lin-2003-p371-372a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1090-705X},
  Keywords                 = {computer crime;data privacy;formal specification;formal verification;security of data;systems analysis;Jackson Problem Frames;abuse frames;data privacy;malicious user;security vulnerability;system security requirement analysis;Automation;Computer science;Computer security;Design engineering;Engineering management;Information security;Internet;Mission critical systems;Protection;Systems engineering and theory},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1232791&isnumber=27626},
  Zone                     = {-}
}

@Article{Lin-2009-p59-69,
  Title                    = {Fingerprinting With Minimum Distance Decoding},
  Author                   = {Shih-Chun Lin and Shahmohammadi, M. and El Gamal, H.},
  Journal                  = {Information Forensics and Security, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {59-69},
  Volume                   = {4},

  Abstract                 = {This paper adopts an information-theoretic framework for the design of collusion-resistant coding/decoding schemes for digital fingerprinting. More specifically, the minimum distance decision rule is used to identify 1 out of t pirates. Achievable rates, under this detection rule, are characterized in two scenarios. First, we consider the averaging attack where a random coding argument is used to show that the rate 1/2 is achievable with t=2 pirates. Our study is then extended to the general case of arbitrary t highlighting the underlying complexity-performance tradeoff. Overall, these results establish the significant performance gains offered by minimum distance decoding compared to other approaches based on orthogonal codes and correlation detectors which can support only a subexponential number of users (i.e., a zero rate). In the second scenario, we characterize the achievable rates, with minimum distance decoding, under any collusion attack that satisfies the marking assumption. For t=2 pirates, we show that the rate 1-H(0.25) ap 0.188 is achievable using an ensemble of random linear codes. For t ges 3, the existence of a nonresolvable collusion attack, with minimum distance decoding, for any nonzero rate is established. Inspired by our theoretical analysis, we then construct coding/decoding schemes for fingerprinting based on the celebrated belief-propagation framework. Using an explicit repeat-accumulate code, we obtain a vanishingly small probability of misidentification at rate 1/3 under averaging attack with t=2. For collusion attacks, which satisfy the marking assumption, we use a more sophisticated accumulate repeat accumulate code to obtain a vanishingly small misidentification probability at rate 1/9 with t=2. These results represent a marked improvement over the best available designs in the literature.},
  Doi                      = {10.1109/TIFS.2008.2012201},
  File                     = {Published version:Lin-2009-p59-69.pdf:PDF},
  Impactfactor             = {1.895},
  ISSN                     = {1556-6013},
  Keywords                 = {cryptography;decoding;belief-propagation framework;collusion attack;collusion-resistant coding-decoding;complexity-performance tradeoff;correlation detectors;digital fingerprinting;marking assumption;minimum distance decoding;misidentification probability;orthogonal codes;random linear codes;repeat-accumulate code;Belief propagation;digital fingerprinting;minimum distance decoding;repeat-accumulate codes},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4773150&isnumber=4782049},
  Zone                     = {2Q}
}

@Article{Lin-2002-p41-46,
  Title                    = {Using minimal cuts to evaluate the system reliability of a stochastic-flow network with failures at nodes and arcs },
  Author                   = {Yi-Kuei Lin},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {41 - 46},
  Volume                   = {75},

  Abstract                 = {This paper deals with a stochastic-flow network in which each node and arc has a designated capacity, which will have different lower levels due to various partial and complete failures. We try to evaluate the system reliability that the maximum flow of the network is not less than a demand (d+1). A simple algorithm in terms of minimal cuts is first proposed to generate all upper boundary points for d, and then the system reliability can be calculated in terms of such points. The upper boundary point for d is a maximal vector, which represents the capacity of each component (arc or node), such that the maximum flow of the network is d. A computer example is shown to illustrate the solution procedure. },
  Doi                      = {http://dx.doi.org/10.1016/S0951-8320(01)00110-7},
  File                     = {Published version:Lin-2002-p41-46.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Node failure},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832001001107},
  Zone                     = {2Q}
}

@Article{Linda-2011-p194-200,
  Title                    = {Online Spatio-Temporal Risk Assessment for Intelligent Transportation Systems},
  Author                   = {Linda, O. and Manic, M.},
  Journal                  = {Intelligent Transportation Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {194-200},
  Volume                   = {12},

  Abstract                 = {Due to modern pervasive wireless technologies and high-performance monitoring systems, spatio-temporal information plays an important role in areas such as intelligent transportation systems (ITS), surveillance, scheduling, planning, or industrial automation. Security or criminal/terrorist threat prevention in modern ITS is one of today's most relevant concerns. This paper presents an algorithm for online spatio-temporal risk assessment in urban environments. In its first phase, the algorithm uses the online nearest neighbor clustering (NNC) algorithm to identify a set of significant places. In the second phase, a fuzzy inference engine is employed to quantify the level of risk that each significant place poses to the place of interest (e.g., vehicle, person, building, or an object of high assets). The contributions of the presented algorithm are given as follows: 1) recognition and extraction of the set of the most significant places; 2) dynamic adaptation of the solution to time-dependent traffic distributions; 3) parametric control by adjusting geographical proximity threshold, significance threshold, and discount factor; and 4) online risk assessment. The performance of the algorithm was demonstrated on a problem of traffic density estimation and risk assessment in a virtual urban environment.},
  Doi                      = {10.1109/TITS.2010.2076807},
  File                     = {Published version:Linda-2011-p194-200.pdf:PDF},
  Impactfactor             = {3.064},
  ISSN                     = {1524-9050},
  Keywords                 = {risk management;security;traffic engineering computing;ubiquitous computing;criminal threat prevention;discount factor;dynamic adaptation;fuzzy inference engine;geographical proximity threshold;high-performance monitoring systems;industrial automation;intelligent transportation systems;modern ITS;online nearest neighbor clustering;online risk assessment;online spatio-temporal risk assessment;parametric control;pervasive wireless technologies;planning;scheduling;security;significance threshold;spatio-temporal information;surveillance;terrorist threat prevention;time-dependent traffic distribution;traffic density estimation;virtual urban environment;Clustering;fuzzy control;pattern recognition;risk assessment;traffic analysis},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5598528&isnumber=5720481},
  Zone                     = {1Q}
}

@Article{Linda-2011-p194-200a,
  Title                    = {Online Spatio-Temporal Risk Assessment for Intelligent Transportation Systems},
  Author                   = {Linda, O. and Manic, M.},
  Journal                  = {Intelligent Transportation Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {194-200},
  Volume                   = {12},

  Abstract                 = {Due to modern pervasive wireless technologies and high-performance monitoring systems, spatio-temporal information plays an important role in areas such as intelligent transportation systems (ITS), surveillance, scheduling, planning, or industrial automation. Security or criminal/terrorist threat prevention in modern ITS is one of today's most relevant concerns. This paper presents an algorithm for online spatio-temporal risk assessment in urban environments. In its first phase, the algorithm uses the online nearest neighbor clustering (NNC) algorithm to identify a set of significant places. In the second phase, a fuzzy inference engine is employed to quantify the level of risk that each significant place poses to the place of interest (e.g., vehicle, person, building, or an object of high assets). The contributions of the presented algorithm are given as follows: 1) recognition and extraction of the set of the most significant places; 2) dynamic adaptation of the solution to time-dependent traffic distributions; 3) parametric control by adjusting geographical proximity threshold, significance threshold, and discount factor; and 4) online risk assessment. The performance of the algorithm was demonstrated on a problem of traffic density estimation and risk assessment in a virtual urban environment.},
  Doi                      = {10.1109/TITS.2010.2076807},
  File                     = {Published version:Linda-2011-p194-200a.pdf:PDF},
  Impactfactor             = {3.064},
  ISSN                     = {1524-9050},
  Keywords                 = {risk management;security;traffic engineering computing;ubiquitous computing;criminal threat prevention;discount factor;dynamic adaptation;fuzzy inference engine;geographical proximity threshold;high-performance monitoring systems;industrial automation;intelligent transportation systems;modern ITS;online nearest neighbor clustering;online risk assessment;online spatio-temporal risk assessment;parametric control;pervasive wireless technologies;planning;scheduling;security;significance threshold;spatio-temporal information;surveillance;terrorist threat prevention;time-dependent traffic distribution;traffic density estimation;virtual urban environment;Clustering;fuzzy control;pattern recognition;risk assessment;traffic analysis},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {这个Background介绍概念},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5598528&isnumber=5720481},
  Zone                     = {1Q}
}

@Article{Linda-2011-p194-200b,
  Title                    = {Online Spatio-Temporal Risk Assessment for Intelligent Transportation Systems},
  Author                   = {Linda, O. and Manic, M.},
  Journal                  = {Intelligent Transportation Systems, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {194-200},
  Volume                   = {12},

  Abstract                 = {Due to modern pervasive wireless technologies and high-performance monitoring systems, spatio-temporal information plays an important role in areas such as intelligent transportation systems (ITS), surveillance, scheduling, planning, or industrial automation. Security or criminal/terrorist threat prevention in modern ITS is one of today's most relevant concerns. This paper presents an algorithm for online spatio-temporal risk assessment in urban environments. In its first phase, the algorithm uses the online nearest neighbor clustering (NNC) algorithm to identify a set of significant places. In the second phase, a fuzzy inference engine is employed to quantify the level of risk that each significant place poses to the place of interest (e.g., vehicle, person, building, or an object of high assets). The contributions of the presented algorithm are given as follows: 1) recognition and extraction of the set of the most significant places; 2) dynamic adaptation of the solution to time-dependent traffic distributions; 3) parametric control by adjusting geographical proximity threshold, significance threshold, and discount factor; and 4) online risk assessment. The performance of the algorithm was demonstrated on a problem of traffic density estimation and risk assessment in a virtual urban environment.},
  Doi                      = {10.1109/TITS.2010.2076807},
  File                     = {Published version:Linda-2011-p194-200b.pdf:PDF},
  Impactfactor             = {3.064},
  ISSN                     = {1524-9050},
  Keywords                 = {risk management;security;traffic engineering computing;ubiquitous computing;criminal threat prevention;discount factor;dynamic adaptation;fuzzy inference engine;geographical proximity threshold;high-performance monitoring systems;industrial automation;intelligent transportation systems;modern ITS;online nearest neighbor clustering;online risk assessment;online spatio-temporal risk assessment;parametric control;pervasive wireless technologies;planning;scheduling;security;significance threshold;spatio-temporal information;surveillance;terrorist threat prevention;time-dependent traffic distribution;traffic density estimation;virtual urban environment;Clustering;fuzzy control;pattern recognition;risk assessment;traffic analysis},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5598528&isnumber=5720481},
  Zone                     = {1Q}
}

@InProceedings{Linda-2011-p26-32,
  Title                    = {Towards resilient critical infrastructures: Application of Type-2 Fuzzy Logic in embedded network security cyber sensor},
  Author                   = {Linda, O. and Manic, M. and Alves-Foss, J. and Vollmer, T.},
  Booktitle                = {Resilient Control Systems (ISRCS), 2011 4th International Symposium on},
  Year                     = {2011},
  Month                    = {Aug},
  Pages                    = {26-32},

  Doi                      = {10.1109/ISRCS.2011.6016083},
  File                     = {Published version:Linda-2011-p26-32.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer network security;critical infrastructures;fuzzy logic;anomaly detection;cyber security state awareness;embedded network security cyber sensor;resilient critical infrastructures;type-2 fuzzy logic;Clustering algorithms;Computer security;Control systems;Feature extraction;Fuzzy logic;Uncertainty;Anomaly Detection;Cyber Sensor;Embedded Systems;Online Clustering;Type-2 Fuzzy Logic},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6016083&isnumber=6016075},
  Zone                     = {-}
}

@Article{Lipow-1987-p355-361,
  Title                    = {Implications of R M 2000 on Software},
  Author                   = {Lipow, M. and Book, Erwin},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1987},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {355-361},
  Volume                   = {R-36},

  Abstract                 = {This paper presents the implications on software of the US Air Force Reliability and Maintainability Action Plan: R&M 2000, whose objective is to improve defense system reliability and maintainability. These implications are very important, even though R&M 2000 does not explicitly mention software. However, Air Force Regulation 800-18, which implements R&M 2000, does state that ``system ... comprises both hardware and software elements'' and includes the requirement to ``integrate the development of reliable software into the overall system development and acquisition program.'' Further, the US Department of Defense (DoD) Software Initiatives, new DoD Directives, and other Air Force Regulations on software, together with industry/academic initiatives, are intended to result in policies, standards, and specifications which lead to defense system software with requisite reliability and maintainability. This difficult goal can be achieved provided that: ??The DoD Software Initiatives successfully create new software technology and transfer it to industry. ??Industry associations (eg, Electronic Industries Association [EIA], National Industrial Security Association INSIA]) and professional societies (eg, Institute of Electrical and Electronics Engineers [IEEE], American Institute of Aeronautics and Astronautics [AIAA]) create and improve software reliability and maintainability standards through effective committee work. ??Industry/Academe upgrade software development procedures to be consistent with software engineering principles based on Ada?? technology.},
  Doi                      = {10.1109/TR.1987.5222399},
  File                     = {Published version:Lipow-1987-p355-361.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Aerospace industry;Computer industry;Defense industry;Electronics industry;Hardware;Reliability;Software maintenance;Software standards;Space technology;Standards development;Software maintainability;Software productivity;Software reliability;Software reusability},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5222399},
  Zone                     = {2Q}
}

@Article{Liu-2012-p600-609,
  Title                    = {The Security Risk Assessment Methodology },
  Author                   = {Chunlin Liu and Chong-Kuan Tan and Yea-Saen Fang and Tat-Seng Lok},
  Journal                  = {Procedia Engineering },
  Year                     = {2012},
  Note                     = {International Symposium on Safety Science and Engineering in China, 2012 },
  Number                   = {0},
  Pages                    = {600 - 609},
  Volume                   = {43},

  __markedentry            = {[Qiqi:5]},
  Abstract                 = {There is an increasing demand for physical security risk assessments in which the span of assessment usually encompasses threats from terrorism. This paper presents a brief description of the approach taken by the author's organization based on a systematic computation of ratings, which are further supported by logical arguments backed by factual data. The procedure compiles the results of the threat assessment, vulnerability assessment and impact assessment to arrive at a numeric value for the risk to each asset against a specific threat given by: Risk Rating(R) = Threat Rating (T) x Vulnerability Rating (V) x Impact Rating (I) This systematic approach could assist decision-makers in selecting risk management strategy by ranking various threats in accordance to their respective Risk Profile. Following which mitigation measures can be explored to reduce the risk for valuable assets, and a logical prioritization for implementation can be achieved },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2012.08.106},
  File                     = {Published version:Liu-2012-p600-609.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {Safety Rating},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，无仿真},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705812031177},
  Zone                     = {-}
}

@InProceedings{Liu-2002-p2-954,
  Title                    = {Learning control of failure avoidance problems with known analytical form of cost function},
  Author                   = {Derong Liu},
  Booktitle                = {Intelligent Control and Automation, 2002. Proceedings of the 4th World Congress on},
  Year                     = {2002},
  Pages                    = {950-954 vol.2},
  Volume                   = {2},

  Doi                      = {10.1109/WCICA.2002.1020716},
  File                     = {Published version:Liu-2002-p2-954.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {cost optimal control;discrete time systems;dynamic programming;failure analysis;learning (artificial intelligence);neurocontrollers;nonlinear control systems;action-dependent heuristic dynamic programming;adaptive critic designs;approximate optimal control;cart-pole problem;critic network;discrete-time nonlinear dynamical system;dynamic programming;failure avoidance control problems;infinite summation;learning control;local cost function;neural network learning;nonlinear environments;overall cost function;Adaptive control;Control systems;Cost function;Dynamic programming;Failure analysis;Neural networks;Nonlinear control systems;Optimal control;Programmable control;Signal design},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1020716&isnumber=21957},
  Zone                     = {-}
}

@Article{Liu-2007-p2-17,
  Title                    = {Resilience of all-optical network architectures under in-band crosstalk attacks: a probabilistic graphical model approach},
  Author                   = {Guanglei Liu and Chuanyi Ji},
  Journal                  = {Selected Areas in Communications, IEEE Journal on},
  Year                     = {2007},

  Month                    = {April},
  Number                   = {3},
  Pages                    = {2-17},
  Volume                   = {25},

  Abstract                 = {An important question for secure all-optical networks (AONs) is how to incorporate security against attacks in the design and engineering of network architectures. In this work, we study the resilience of AON architectures under in-band crosstalk attacks. Crosstalk attack propagation depends on both optical devices at the physical layer and wavelength usage at the network layer. This motivates us to employ probabilistic graphical models to model attack propagation. At the physical layer, we use a directed probabilistic graph (Bayesian Belief Network) to model the attack propagation under static network traffic and a given source of attack. At the network layer, we use an undirected probabilistic graph to represent the probability distribution of active connections in the network. The cross-layer model is obtained by combining the physical- and the network-layer models into a factor graph representation. Graphical models provide an explicit representation of interactions between the physical- and the network layer. Furthermore, graphical models facilitate derivations of analytical results on resilience with respect to physical-layer vulnerability, physical topology, and network load. Specifically, we derive bounds on the network resilience for regular topologies. For ring, star, and mesh-torus networks with link-shortest path routing and all-to-all traffic, we show that the average network resilience loss grows linearly with respect to the network load when the network load is small, and polynomially with respect to the probability of attack propagation from node to node along the attacker's route. In addition, numerical results suggest that the sum-product algorithm based on the factor graph representation can be used for computationally efficient evaluation of network resilience for irregular/large topologies},
  Doi                      = {10.1109/JSAC-OCN.2007.022305},
  File                     = {Published version:Liu-2007-p2-17.pdf:PDF},
  Impactfactor             = {3.121},
  ISSN                     = {0733-8716},
  Keywords                 = {graph theory;optical crosstalk;optical fibre networks;probability;telecommunication network topology;telecommunication security;telecommunication traffic;AON architecture;all-optical network;in-band crosstalk attack;network topology;probabilistic graphical model;security incorporation;static network traffic;All-optical networks;Crosstalk;Design engineering;Graphical models;Network topology;Optical propagation;Physical layer;Resilience;Telecommunication traffic;Traffic control},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4202534&isnumber=4202533},
  Zone                     = {2Q}
}

@InProceedings{Liu-2006-p2914-2918,
  Title                    = {Resilient Architecture of All-Optical Networks: Probabilistic Graphical Models for Crosstalk Attack Propagation},
  Author                   = {Guanglei Liu and Chuanyi Ji},
  Booktitle                = {Information Theory, 2006 IEEE International Symposium on},
  Year                     = {2006},
  Month                    = {July},
  Pages                    = {2914-2918},

  Abstract                 = {We study the resilience of all-optical network (AON) architectures under in-band crosstalk attacks. We first develop a cross-layer model that captures attack propagation based on probabilistic graphical models. At the physical layer, we use a directed probabilistic graph (Bayesian belief network) to model the attack propagation under static network traffic and a given source of attack. At the network layer, we use an undirected probabilistic graph (random field) to represent the probability distribution of active connections in the network. The cross-layer model is obtained by combining the physicaland the network-layer models into a factor graph representation. We then derive bounds on the network resilience for regular topologies. We show that for ring, star, and mesh-torus networks with link-shortest path routing and all-to-all traffic, the average network resilience loss grows linearly with respect to the network load when the network load is light; grows polynomially with respect to the probability of attack propagation from node to node along the attacker's route. We then show that the sum-product algorithm can be used for computationally efficient evaluation of network resilience for irregular topologies},
  Doi                      = {10.1109/ISIT.2006.261673},
  File                     = {Published version:Liu-2006-p2914-2918.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {belief networks;optical crosstalk;optical fibre networks;statistical distributions;telecommunication network routing;telecommunication network topology;telecommunication traffic;Bayesian belief network;all-optical networks;all-to-all traffic;cross-layer model;crosstalk attack propagation;in-band crosstalk attacks;link-shortest path routing;network resilience;network topology;probabilistic graphical models;probability distribution;random field;static network traffic;sum-product algorithm;undirected probabilistic graph;All-optical networks;Bayesian methods;Crosstalk;Graphical models;Network topology;Physical layer;Probability distribution;Resilience;Telecommunication traffic;Traffic control},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4036506&isnumber=4035896},
  Zone                     = {-}
}

@Article{Liu-2013-p23-36,
  Title                    = {Fuzzy Failure Mode and Effects Analysis Using Fuzzy Evidential Reasoning and Belief Rule-Based Methodology},
  Author                   = {Hu-Chen Liu and Long Liu and Qing-Lian Lin},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {23-36},
  Volume                   = {62},

  Abstract                 = {The main objective of this paper is to propose a new risk priority model for prioritizing failures in failure mode and effects analysis (FMEA) on the basis of fuzzy evidential reasoning (FER) and belief rule-based (BRB) methodology. The technique is particularly intended to resolve some of the shortcomings in fuzzy FMEA (i.e., fuzzy rule-based) approaches. In the proposed approach, risk factors like occurrence (O), severity (S), and detection (D), along with their relative importance weights, are described using fuzzy belief structures. The FER approach is used to capture and aggregate the diversified, uncertain assessment information given by the FMEA team members; the BRB methodology is used to model the uncertainty, and nonlinear relationships between risk factors and corresponding risk level; and the inference of the rule-based system is implemented using the weighted average-maximum composition algorithm. The Dempster rule of combination is then used to aggregate all relevant rules for assessing and prioritizing the failure modes that have been identified in FMEA. A case study concerning an ocean going fishing vessel in a marine industry is provided and conducted using the proposed model to illustrate its potential applications and benefits.},
  Doi                      = {10.1109/TR.2013.2241251},
  File                     = {Published version:Liu-2013-p23-36.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Knight shift;case-based reasoning;failure analysis;knowledge based systems;Dempster rule;belief rule based methodology;effects analysis;fuzzy belief structures;fuzzy evidential reasoning;fuzzy failure mode;marine industry;risk priority model;rule based system;uncertain assessment information;weighted average maximum composition algorithm;Aggregates;Cognition;Erbium;Fuzzy logic;Pragmatics;Risk management;Uncertainty;Belief rule-base system;failure mode and effects analysis;fuzzy evidential reasoning;risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6423858&isnumber=6471782},
  Zone                     = {2Q}
}

@Article{Liu-2012-p1067-1078,
  Title                    = {A fuzzy risk assessment approach for occupational hazards in the construction industry },
  Author                   = {Hao-Tien Liu and Yieh-lin Tsai},
  Journal                  = {Safety Science },
  Year                     = {2012},
  Note                     = {First International Symposium on Mine Safety Science and Engineering 2011 },
  Number                   = {4},
  Pages                    = {1067 - 1078},
  Volume                   = {50},

  Abstract                 = {The techniques in the construction industry have been improved due to the rapid development of science and technology. However, the constructional hazards are not decreased as expected. To reduce or prevent occupational hazards in the construction industry, a fuzzy risk assessment method was proposed to provide a prevention and improvement technique against occupational hazards. This method used two-stage quality function deployment (QFD) tables to represent the relationships among construction items, hazard types and hazard causes. A fuzzy analytic network process (ANP) method was developed to identify important hazard types and hazard causes. Failure modes and effect analysis (FMEA) was performed to assess the risk value of hazard causes based on the fuzzy inference approach. The proposed method was applied to a telecom engineering company in southern Taiwan. The performance evaluation result indicated that this method can provide satisfactory risk assessment values of hazard causes and relevant improvement strategies.},
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2011.11.021},
  File                     = {Published version:Liu-2012-p1067-1078.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Construction industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {利用模糊量化工业危害性，有参考价值},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753511003110},
  Zone                     = {3Q}
}

@Article{Liu-2012-p1067-1078a,
  Title                    = {A fuzzy risk assessment approach for occupational hazards in the construction industry },
  Author                   = {Hao-Tien Liu and Yieh-lin Tsai},
  Journal                  = {Safety Science },
  Year                     = {2012},
  Note                     = {First International Symposium on Mine Safety Science and Engineering 2011 },
  Number                   = {4},
  Pages                    = {1067 - 1078},
  Volume                   = {50},

  Abstract                 = {The techniques in the construction industry have been improved due to the rapid development of science and technology. However, the constructional hazards are not decreased as expected. To reduce or prevent occupational hazards in the construction industry, a fuzzy risk assessment method was proposed to provide a prevention and improvement technique against occupational hazards. This method used two-stage quality function deployment (QFD) tables to represent the relationships among construction items, hazard types and hazard causes. A fuzzy analytic network process (ANP) method was developed to identify important hazard types and hazard causes. Failure modes and effect analysis (FMEA) was performed to assess the risk value of hazard causes based on the fuzzy inference approach. The proposed method was applied to a telecom engineering company in southern Taiwan. The performance evaluation result indicated that this method can provide satisfactory risk assessment values of hazard causes and relevant improvement strategies. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2011.11.021},
  File                     = {Published version:Liu-2012-p1067-1078a.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Construction industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753511003110},
  Zone                     = {3Q}
}

@Article{Liu-2013-p115-127,
  Title                    = {A New Method for Knowledge and Information Management Domain Ontology Graph Model},
  Author                   = {Liu, J.N.K. and Yu-Lin He and Lim, E.H.Y. and Xi-Zhao Wang},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {115-127},
  Volume                   = {43},

  Doi                      = {10.1109/TSMCA.2012.2196431},
  File                     = {Published version:Liu-2013-p115-127.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {graph theory;knowledge management;learning (artificial intelligence);natural language processing;ontologies (artificial intelligence);pattern classification;text analysis;Chinese text data;DOG;classification accuracy;document ontology graph generation;domain knowledge representation;domain ontology graph model;domain ontology learning model;domain-specific text document;f-measure;information management;knowledge conceptualization model;knowledge management;ontological operation;ontology graph knowledge model;ontology-graph-based text classification;term-frequency-inverse-document-frequency approach;Accuracy;Computational modeling;Dictionaries;Dogs;Humans;Learning systems;Ontologies;Chinese text analysis;domain ontology graph (DOG);information management;knowledge representation;ontology learning},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6205648&isnumber=6356011},
  Zone                     = {-}
}

@Article{Liu-2011-p393-397b,
  Title                    = {Transportation Risk Assessment Of Chemical Industry Supply Chain Based On A Dual Model },
  Author                   = {Li-ping Liu and Shu-xia Li and Ti-jun Fan and Xiang-yun Chang},
  Journal                  = {Procedia Environmental Sciences },
  Year                     = {2011},
  Note                     = {2011 2nd International Conference on Challenges in Environmental Science and Computer Engineering (CESCE 2011) },
  Number                   = {0},
  Pages                    = {393 - 397},
  Volume                   = {11, Part A},

  Abstract                 = {Research on chemical industry supply chain transportation risk assessment has very important significance not only for secure operation of chemical supply chain but also for protection of energy and resource, improvement of social environment. This paper reviews literatures of transportation risk assessment. On this basis, transportation assessment dual model of chemical industry supply chain is presented considering the transportation risk impaction on both inner and external chemical industry supply chain. Then corresponding transportation assessment process of chemical industry is given. },
  Doi                      = {http://dx.doi.org/10.1016/j.proenv.2011.12.063},
  File                     = {Published version:Liu-2011-p393-397b.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1878-0296},
  Keywords                 = {Chemical industry supply chain},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1878029611008863},
  Zone                     = {-}
}

@Article{Liu-2011-p869-875,
  Title                    = {Asset Analysis of Risk Assessment for IEC 61850-Based Power Control Systems—Part I: Methodology},
  Author                   = {Nian Liu and Jianhua Zhang and Xu Wu},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {869-875},
  Volume                   = {26},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {Information security risk assessment of IEC 61850-based power control systems is currently an unsolved problem. One of the reasons is a lack of methodology for asset analysis, which is an important process of risk assessment. As the features of IEC 61850-based power control systems are different from general IT systems, a specific methodology of asset analysis is introduced. Based on the requirements of risk assessment proposed in the BS ISO/IEC 27005 standard, the methodology for asset analysis is separated into asset identification and valuation. For asset identification, a structured asset model is defined to distinguish the assets, and a function-oriented business process model is defined to identify the business process and describe the relations between assets and business processes. For asset valuation, in order to objectively reflect the consequence incurred due to the loss of security properties, three levels of value are defined, which is value of information exchange, asset value of function level, and asset value of system level, respectively. Finally, the implementation procedure of the methodology is described. In the companion paper (Part II), an application instance is presented to support the usefulness of the methodology.},
  Doi                      = {10.1109/TPWRD.2010.2090950},
  File                     = {Published version:Liu-2011-p869-875.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {IEC standards;power control;risk management;BS ISO/IEC 27005 standard;IEC 61850;asset analysis;function-oriented business process;information exchange;information security risk assessment;power control systems;Asset identification;asset valuation;cyber security;power control system;risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，Case Study，an instance system based on IEC 61850},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5671522&isnumber=5738430},
  Zone                     = {3Q}
}

@Article{Liu-2011-p876-881,
  Title                    = {Asset Analysis of Risk Assessment for IEC 61850-Based Power Control Systems—Part II: Application in Substation},
  Author                   = {Nian Liu and Jianhua Zhang and Xu Wu},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {876-881},
  Volume                   = {26},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {The information security risk assessment of IEC 61850-based power control systems is currently an unsolved problem. One of the reasons is a lack of methodology for asset analysis, which is an important process of risk assessment. In the companion paper (Part I), a specific methodology of asset analysis for the IEC 61850-based power control systems is introduced. To explain and verify the proposed methodology, the substation automation systems are selected as a typical application field. Before the case study, a basic principle for value assignment in a specific qualitative scale is proposed as a foundation for asset valuation. Then, an instance system based on IEC 61850 is introduced to apply the methodology. The overall procedures of the asset identification and asset valuation are represented step by step. From the results of the application, the methodology can meet the requirements of risk assessment.},
  Doi                      = {10.1109/TPWRD.2010.2090951},
  File                     = {Published version:Liu-2011-p876-881.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {power system management;power system security;risk management;substation automation;IEC 61850;asset analysis;asset identification;asset valuation;information security risk assessment;power control systems;substation automation systems;Asset identification;asset valuation;cybersecurity;risk assessment;substation automation system},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，Case Study，an instance system based on IEC 61850},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5671523&isnumber=5738430},
  Zone                     = {3Q}
}

@Article{Liu-2015-p180-197,
  Title                    = {Low-Ti iron oxide deposits in the Emeishan large igneous province related to low-Ti basalts and gabbroic intrusions },
  Author                   = {Wenhao Liu and Jun Zhang and Teng Sun and Li Zhou and Anlu Liu},
  Journal                  = {Ore Geology Reviews },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {180 - 197},
  Volume                   = {65, Part 1},

  Abstract                 = {Abstract The Yanyuan region in the Emeishan large igneous province (ELIP) contains some low-Ti iron oxide deposits that are spatially associated with low-Ti basalts and related gabbroic intrusions. The Pingchuan deposit is the largest of this type and the adjacent Lanzhichang deposit is the first stratiform deposit hosted in the Emeishan basalts discovered in 2005. The Pingchuan deposit occurs in veins and is hosted in caldera near the Dabanshan low-Ti gabbroic complex, which is composed of gabbro and gabbronorite that intruded the gabbro. The adjacent Lanzhichang deposit is hosted in the lower-section of the low-Ti Emeishan basalts and geological characteristics obviously indicate a volcanic sedimentary genesis. The magnetite of these two deposits contains minor TiO2 (~&#xa0;0.002%) and \{V2O3\} (~&#xa0;0.027%), and there is no ilmenite in the mineral assemblages. Apatite U–Pb and fission-track (FT) dating of the Pingchuan deposit indicates a U–Pb age of 245&#xa0;±&#xa0;26&#xa0;Ma and \{FT\} age of 51.8&#xa0;±&#xa0;4.9&#xa0;Ma. Zircon U–Pb dating of the Dabanshan complex indicates two similar ages of 259.9&#xa0;±&#xa0;1.1&#xa0;Ma and 260.3&#xa0;±&#xa0;1.3&#xa0;Ma within the error range for the gabbro and gabbronorite, respectively. Three U–Pb ages together with geological evidence indicate that the Pingchuan deposit is formed after both the basalts and Dabanshan gabbroic complex. Reported bulk-rock major element data indicate that the basalts and the Dabanshan gabbroic intrusions belong to the low-Ti basalts and associated intrusions. The gabbro has similar chemical composition to the basalt, but the gabbronorite is distinctly different from gabbro and basalt. Using zircon U–Pb ages, we found that the Fe2O3⁎ decreased by 4.51% from the gabbro to the gabbronorite within a very short time interval. This sudden drop of Fe2O3⁎ was probably caused by immiscibility of the low-Ti basaltic magma, and the isolated low-Ti Fe-rich melts can account for the mineralization of the Lanzhichang and Pingchuan deposits from two ascents. Contribution of phosphorus from the country rocks may have triggered this liquid immiscibility. The Sr–Nd isotope compositions of the basalts and gabbroic intrusions indicate a transition between the igneous rocks related to the Fe–Ti–V oxide deposits and the Cu–Ni–(PGE) sulfide deposits in the ELIP. All of these factors indicate the Pingchuan and Lanzhichang deposits in the Yanyuan region belong to a specific type of low-Ti iron oxide deposits that are related to the low-Ti basalts and gabbroic intrusions in the ELIP. },
  Doi                      = {http://dx.doi.org/10.1016/j.oregeorev.2014.08.015},
  File                     = {Published version:Liu-2015-p180-197.pdf:PDF},
  Impactfactor             = {2.417},
  ISSN                     = {0169-1368},
  Keywords                 = {Pingchuan},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0169136814002273},
  Zone                     = {2Q}
}

@Article{Liu-2010-p198-209,
  Title                    = {Measurement-based method for wind farm power system oscillations monitoring},
  Author                   = {Liu, X. and McSwiggan, D. and Littler, T.B. and Kennedy, J.},
  Journal                  = {Renewable Power Generation, IET},
  Year                     = {2010},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {198-209},
  Volume                   = {4},

  Abstract                 = {The global increase in the penetration of renewable energy is pushing electrical power systems into uncharted territory, especially in terms of transient and dynamic stability. In particular, the greater penetration of wind generation in European power networks is, at times, displacing a significant capacity of conventional synchronous generation with fixed-speed induction generation and now more commonly, doubly fed induction generators. The impact of such changes in the generation mix requires careful monitoring to assess the impact on transient and dynamic stability. This study presents a measurement-based method for the early detection of power system oscillations, with consideration of mode damping, in order to raise alarms and develop strategies to actively improve power system dynamic stability and security. A method is developed based on wavelet-based support vector data description (SVDD) to detect oscillation modes in wind farm output power, which may excite dynamic instabilities in the wider system. The wavelet transform is used as a filter to identify oscillations in frequency bands, whereas the SVDD method is used to extract dominant features from different scales and generate an assessment boundary according to the extracted features. Poorly damped oscillations of a large magnitude, or that are resonant, can be alarmed to the system operator, to reduce the risk of system instability. The proposed method is exemplified using measured data from a chosen wind farm site.},
  Doi                      = {10.1049/iet-rpg.2009.0110},
  File                     = {Published version:Liu-2010-p198-209.pdf:PDF},
  Impactfactor             = {1.718},
  ISSN                     = {1752-1416},
  Keywords                 = {asynchronous generators;power system measurement;wind power plants;doubly fed induction generators;electrical power systems;fixed-speed induction generation;measurement based method;renewable energy;synchronous generation;wavelet-based support vector data description;wind farm power system oscillations monitoring},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5423984&isnumber=5423975},
  Zone                     = {4Q}
}

@InProceedings{Liu-2013-p75-79,
  Title                    = {The e-government system risk assessment model based on dynamic threat and fuzzy neural network},
  Author                   = {Xin Liu and Yongjun Shen and Guidong Zhang and Shuxian Li},
  Booktitle                = {Broadband Network Multimedia Technology (IC-BNMT), 2013 5th IEEE International Conference on},
  Year                     = {2013},
  Month                    = {Nov},
  Pages                    = {75-79},

  Abstract                 = {Researchers regard the Information Security Risk Assessment Model as the authoritative model for the security performance of information system. Its value at risk is the directly important indicator of information system security. This paper proposes a e-government risk model based on dynamic threat value and fuzzy neural network for the e-government information system. This model effectively avoids human judgment factors of traditional model and the neural network experiment indicates this model is the effective information security assessment model for the e-government information system.},
  Doi                      = {10.1109/ICBNMT.2013.6823918},
  File                     = {Published version:Liu-2013-p75-79.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {fuzzy neural nets;government data processing;risk management;security of data;authoritative model;dynamic threat value;e-government information system;e-government system risk assessment model;fuzzy neural network;human judgment factor avoidance;information security risk assessment model;information system security performance;Biological neural networks;Electronic government;Information security;Risk management;Dynamic threat value;E-government protection;Information security;Neural network;Risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6823918&isnumber=6823902},
  Zone                     = {-}
}

@Article{Liu-2015-p1917-1926,
  Title                    = {An approach for developing diagnostic Bayesian network based on operation procedures },
  Author                   = {Zengkai Liu and Yonghong Liu and Baoping Cai and Chao Zheng},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2015},
  Number                   = {4},
  Pages                    = {1917 - 1926},
  Volume                   = {42},

  Abstract                 = {Abstract In this paper, a novel approach of developing the Bayesian network for fault diagnosis based on operation procedures is presented. The proposed Bayesian network consists of operation procedure layer, fault layer and fault symptom layer. First, operation procedure layer containing procedure nodes and state decision nodes is developed. Second, the fault layer is determined based on the state decision nodes in the operation procedure layer. Then fault symptom layer including symptoms sensitive to the concerned faults is developed. Finally, the entire Bayesian network is established by integrating the three layers. The presented approach is applied to hydraulic control system of subsea blowout preventer (BOP). Taking an example of closing the BOP, the operation procedures are illustrated. The entire Bayesian network for fault diagnosis of closing the \{BOP\} is established. Several cases possible to appear during the closing process are studied to evaluate the developed model. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2014.10.020},
  File                     = {Published version:Liu-2015-p1917-1926.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Fault diagnosis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417414006447},
  Zone                     = {3Q}
}

@Article{Lo-2012-p247-257,
  Title                    = {A hybrid information security risk assessment procedure considering interdependences between controls },
  Author                   = {Chi-Chun Lo and Wan-Jia Chen},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {247 - 257},
  Volume                   = {39},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {Risk assessment is the core process of information security risk management. Organizations use risk assessment to determine the risks within an information system and provide sufficient means to reduce these risks. In this paper, a hybrid procedure for evaluating risk levels of information security under various security controls is proposed. First, this procedure applies the Decision Making Trial and Evaluation Laboratory (DEMATEL) approach to construct interrelations among security control areas. Secondly, likelihood ratings are obtained through the Analytic Network Process (ANP) method; as a result, the proposed procedure can detect the interdependences and feedback between security control families and function in real world situations. Lastly, the Fuzzy Linguistic Quantifiers-guided Maximum Entropy Order-Weighted averaging (FLQ-MEOWA) operator is used to aggregate impact values assessed by experts, applied to diminish the influence of extreme evaluations such as personal views and drastic perspectives. A real world application in a branch office of the health insurance institute in Taiwan was examined to verify the proposed procedure. By analyzing the acquired data, we confirm the proposed procedure certainly detects the influential factors among security control areas. This procedure also evaluates risk levels more accurately by coping with the interdependencies among security control families and determines the information systems safeguards required for better security, therefore enabling organizations to accomplish their missions. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2011.07.015},
  File                     = {Published version:Lo-2012-p247-257.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Information security},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，没有仿真，对着一个健康保险研究所描述如何进行风险评估},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417411009778},
  Zone                     = {3Q}
}

@Article{Lo-2003-p573-581,
  Title                    = {Risk assessment due to local demand forecast uncertainty in the competitive supply industry},
  Author                   = {Lo, K.L. and Wu, V.K.},
  Journal                  = {Generation, Transmission and Distribution, IEE Proceedings-},
  Year                     = {2003},

  Month                    = {Sept},
  Number                   = {5},
  Pages                    = {573-581},
  Volume                   = {150},

  Abstract                 = {A risk assessment on local demand forecast uncertainty is presented. The aim is to highlight high-risk periods over different lengths of time and daily value-at-risk (VAR) due to load forecast errors. A number of load forecasts have been performed, and the load forecast is based on ARIMA models and ANN structures. With the residuals from load forecasting, the risk indexes over different time periods and seasons are formed. Moreover, a new methodology using the standard deviation of load increment on evaluating the risk is proposed. In contrast with the standard forecasting method that relies on a sophisticated forecast procedure, the new approach provides a useful and fast method to evaluate the risk due to load forecast uncertainty for a variety of local demand profiles. Finally, the VAR methodology combined with the NETA system is applied to a local electricity supplier in the UK.},
  Doi                      = {10.1049/ip-gtd:20030641},
  File                     = {Published version:Lo-2003-p573-581.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1350-2360},
  Keywords                 = {load forecasting;neural nets;power system analysis computing;risk management;ANN structures;ARIMA models;NETA system;UK;competitive supply industry;daily value-at-risk;high-risk periods;load forecast errors;load increment;local demand forecast uncertainty;local electricity supplier;risk assessment;risk indexes},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1233541&isnumber=27637},
  Zone                     = {-}
}

@Article{Lo-2003-p573-581a,
  Title                    = {Risk assessment due to local demand forecast uncertainty in the competitive supply industry},
  Author                   = {Lo, K.L. and Wu, V.K.},
  Journal                  = {Generation, Transmission and Distribution, IEE Proceedings-},
  Year                     = {2003},

  Month                    = {Sept},
  Number                   = {5},
  Pages                    = {573-581},
  Volume                   = {150},

  Abstract                 = {A risk assessment on local demand forecast uncertainty is presented. The aim is to highlight high-risk periods over different lengths of time and daily value-at-risk (VAR) due to load forecast errors. A number of load forecasts have been performed, and the load forecast is based on ARIMA models and ANN structures. With the residuals from load forecasting, the risk indexes over different time periods and seasons are formed. Moreover, a new methodology using the standard deviation of load increment on evaluating the risk is proposed. In contrast with the standard forecasting method that relies on a sophisticated forecast procedure, the new approach provides a useful and fast method to evaluate the risk due to load forecast uncertainty for a variety of local demand profiles. Finally, the VAR methodology combined with the NETA system is applied to a local electricity supplier in the UK.},
  Doi                      = {10.1049/ip-gtd:20030641},
  File                     = {Published version:Lo-2003-p573-581a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1350-2360},
  Keywords                 = {load forecasting;neural nets;power system analysis computing;risk management;ANN structures;ARIMA models;NETA system;UK;competitive supply industry;daily value-at-risk;high-risk periods;load forecast errors;load increment;local demand forecast uncertainty;local electricity supplier;risk assessment;risk indexes},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1233541&isnumber=27637},
  Zone                     = {-}
}

@Article{Loncomilla-2014-p406-415,
  Title                    = {A Novel Methodology for Assessing the Fall Risk Using Low-Cost and Off-the-Shelf Devices},
  Author                   = {Loncomilla, P. and Tapia, C. and Daud, O. and Ruiz-del-Solar, J.},
  Journal                  = {Human-Machine Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {3},
  Pages                    = {406-415},
  Volume                   = {44},

  Abstract                 = {Early detection of fall risk can reduce health costs associated with surgery, rehabilitation, imaging studies, hospitalizations, and medical evaluations. This paper proposes a measurement-focused study oriented to evaluate a new methodology for assessing fall risk using low-cost and off-the-shelf devices. The proposed methodology consists of a data acquisition system, a data analysis system, and a fall risk assessment system. The data acquisition system is composed by a standard notebook computer and video game input devices: a Kinect, a Wii balance board, and two Wii motion controllers. The data analysis system and the fall risk assessment system, in turn, use signal processing, data mining, and computational intelligence methods, in order to analyze the acquired data for determining the fall risk of the subject under analysis. This methodology includes six static and two dynamic tests. Experiments were conducted on a population of 37 subjects: 16 with falling background, and 21 with nonfalling background. These two groups have the same age distribution. As nonlinear binary classification techniques were used, methodologies based on confidence intervals are not applicable and then tenfold cross validation was used to estimate accuracy. Hence, such a methodology can classify the fall risk as high or low, with an accuracy of 89.2%. The proposed methodology allows the construction of low-cost, portable, replicable, objective, and reliable fall risk assessment systems.},
  Doi                      = {10.1109/THMS.2014.2309493},
  File                     = {Published version:Loncomilla-2014-p406-415.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2291},
  Keywords                 = {biomedical equipment;data acquisition;data mining;interactive devices;notebook computers;patient rehabilitation;risk management;surgery;Wii balance board;Wii motion controllers;computational intelligence methods;data acquisition system;data analysis;data mining;fall risk assessment system;fall risk detection;hospitalizations;low-cost devices;medical evaluations;off-the-shelf devices;patient rehabilitation;signal processing;standard notebook computer;surgery;video game input devices;Accuracy;Data acquisition;Data analysis;Risk management;Senior citizens;Standards;Vectors;Fall risk assessment;functional tests;statistical classifiers},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6776554&isnumber=6814814},
  Zone                     = {-}
}

@Article{Loncomilla-2014-p406-415a,
  Title                    = {A Novel Methodology for Assessing the Fall Risk Using Low-Cost and Off-the-Shelf Devices},
  Author                   = {Loncomilla, P. and Tapia, C. and Daud, O. and Ruiz-del-Solar, J.},
  Journal                  = {Human-Machine Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {3},
  Pages                    = {406-415},
  Volume                   = {44},

  Abstract                 = {Early detection of fall risk can reduce health costs associated with surgery, rehabilitation, imaging studies, hospitalizations, and medical evaluations. This paper proposes a measurement-focused study oriented to evaluate a new methodology for assessing fall risk using low-cost and off-the-shelf devices. The proposed methodology consists of a data acquisition system, a data analysis system, and a fall risk assessment system. The data acquisition system is composed by a standard notebook computer and video game input devices: a Kinect, a Wii balance board, and two Wii motion controllers. The data analysis system and the fall risk assessment system, in turn, use signal processing, data mining, and computational intelligence methods, in order to analyze the acquired data for determining the fall risk of the subject under analysis. This methodology includes six static and two dynamic tests. Experiments were conducted on a population of 37 subjects: 16 with falling background, and 21 with nonfalling background. These two groups have the same age distribution. As nonlinear binary classification techniques were used, methodologies based on confidence intervals are not applicable and then tenfold cross validation was used to estimate accuracy. Hence, such a methodology can classify the fall risk as high or low, with an accuracy of 89.2%. The proposed methodology allows the construction of low-cost, portable, replicable, objective, and reliable fall risk assessment systems.},
  Doi                      = {10.1109/THMS.2014.2309493},
  File                     = {Published version:Loncomilla-2014-p406-415a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2291},
  Keywords                 = {biomedical equipment;data acquisition;data mining;interactive devices;notebook computers;patient rehabilitation;risk management;surgery;Wii balance board;Wii motion controllers;computational intelligence methods;data acquisition system;data analysis;data mining;fall risk assessment system;fall risk detection;hospitalizations;low-cost devices;medical evaluations;off-the-shelf devices;patient rehabilitation;signal processing;standard notebook computer;surgery;video game input devices;Accuracy;Data acquisition;Data analysis;Risk management;Senior citizens;Standards;Vectors;Fall risk assessment;functional tests;statistical classifiers},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6776554&isnumber=6814814},
  Zone                     = {-}
}

@Article{Lorin-2014-p50-57,
  Title                    = {Transformer Condition Assessment: Methodologies and On-Site Repair Solutions},
  Author                   = {Lorin, P. and Khanna, P. and Werle, P. and Eklund, L.},
  Journal                  = {Industry Applications Magazine, IEEE},
  Year                     = {2014},

  Month                    = {Sept},
  Number                   = {5},
  Pages                    = {50-57},
  Volume                   = {20},

  Abstract                 = {In this paper, the transformer condition assessment methodologies were describe as well as on-site repair solutions as a means to assess the reliability of transformers. The diagnosis of the transformer condition is used to recommend maintenance actions and identify defects before untanking the transformer. This allows maintenance personnel to anticipate the potential need for repair, thereby reducing the amount of time spent in the process, especially when transformers are being repaired on site. Repairing transformers on site usually allows them to be put back in service in a shorter time period (up to three months) by avoiding transportation from the site to the factory and back. In addition, it reduces the costs and risks associated with heavy transport. To date, more than 400 transformers, including utility, industrial, high-voltage direct current (HVdc), and reactors have been successfully repaired on site. In many cases, transformers were upgraded to provide an increased rating.},
  Doi                      = {10.1109/MIAS.2013.2288382},
  File                     = {Published version:Lorin-2014-p50-57.pdf:PDF},
  Impactfactor             = {0.483},
  ISSN                     = {1077-2618},
  Keywords                 = {maintenance engineering;power transformers;reliability;high-voltage direct current;onsite repair solution;transformer condition assessment methodology;transformers reliability;Maintenance engineering;Oil insulation;Power system reliability;Power transformer insulation;Production facilities;Transformer cores;Windings},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6847131&isnumber=6872616},
  Zone                     = {4Q}
}

@Article{Loutchkina2014,
  Title                    = {Systems' Integration Technical Risks' Assessment Model (SITRAM)},
  Author                   = {Loutchkina, I. and Jain, L.C. and Thong Nguyen and Nesterov, S.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {342-352},
  Volume                   = {44},

  Doi                      = {10.1109/TSMC.2013.2256126},
  File                     = {Published version:Loutchkina-2014-p342-352a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {belief networks;project management;risk management;software development management;statistical analysis;BBN;BBN experimental model design;Bayesian belief networks;HHM approach;PM design principles;SITRAM model;commercial applications;conceptual modeling framework;conditional probability tables;defense applications;heuristic techniques;hierarchical holographic modeling;modeling objectives;parametric models;risk contributing factors;risk interrelations;risk management;risk taxonomy;software intensive systems;statistical information;system integration technical risk assessment model;Bayesian networks;expert knowledge elicitation;risk assessment;system integration risk modeling;system integration risks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6519934&isnumber=6739146},
  Zone                     = {-}
}

@Article{Loutchkina-2014-p342-352,
  Title                    = {Systems' Integration Technical Risks' Assessment Model (SITRAM)},
  Author                   = {Loutchkina, I. and Jain, L.C. and Thong Nguyen and Nesterov, S.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {342-352},
  Volume                   = {44},

  Abstract                 = {This paper presents a novel system integration technical risk assessment model (SITRAM), which is based on Bayesian belief networks (BBN) coupled with parametric models (PM). This model provides statistical information for decision makers, improving risk management of complex projects. System integration technical risks (SITR) represent a significant part of project risks associated with the development of large software intensive systems for defense and commercial applications. We propose a conceptual modeling framework to address the problem of SITR assessment in the early stages of a system life cycle. Initial risks' taxonomy and risks' interrelations have been identified using a hierarchical holographic modeling (HHM) approach. The framework includes a set of BBN models, representing relations between risk contributing factors, and complementing PMs, used to provide input data to the BBN models. In this paper, we present the rationale and the modeling objectives, and describe the concepts and details of BBN experimental model design and implementation. To address practical limitations, heuristic techniques have been proposed for easing the generation of conditional probability tables. PM design principles are described and examples are presented. In conclusion, we summarize the benefits and constraints of SITR assessment based on BBN models. Further research directions and model improvements are also presented.},
  Doi                      = {10.1109/TSMC.2013.2256126},
  File                     = {Published version:Loutchkina-2014-p342-352.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {belief networks;project management;risk management;software development management;statistical analysis;BBN;BBN experimental model design;Bayesian belief networks;HHM approach;PM design principles;SITRAM model;commercial applications;conceptual modeling framework;conditional probability tables;defense applications;heuristic techniques;hierarchical holographic modeling;modeling objectives;parametric models;risk contributing factors;risk interrelations;risk management;risk taxonomy;software intensive systems;statistical information;system integration technical risk assessment model;Bayesian networks;expert knowledge elicitation;risk assessment;system integration risk modeling;system integration risks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6519934&isnumber=6739146},
  Zone                     = {-}
}

@Article{Loutchkina-2014-p342-352a,
  Title                    = {Systems' Integration Technical Risks' Assessment Model (SITRAM)},
  Author                   = {Loutchkina, I. and Jain, L.C. and Thong Nguyen and Nesterov, S.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {342-352},
  Volume                   = {44},

  Doi                      = {10.1109/TSMC.2013.2256126},
  File                     = {Published version:Loutchkina-2014-p342-352a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {belief networks;project management;risk management;software development management;statistical analysis;BBN;BBN experimental model design;Bayesian belief networks;HHM approach;PM design principles;SITRAM model;commercial applications;conceptual modeling framework;conditional probability tables;defense applications;heuristic techniques;hierarchical holographic modeling;modeling objectives;parametric models;risk contributing factors;risk interrelations;risk management;risk taxonomy;software intensive systems;statistical information;system integration technical risk assessment model;Bayesian networks;expert knowledge elicitation;risk assessment;system integration risk modeling;system integration risks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6519934&isnumber=6739146},
  Zone                     = {-}
}

@Article{Loutchkina-2014-p342-352c,
  Title                    = {Systems' Integration Technical Risks' Assessment Model (SITRAM)},
  Author                   = {Loutchkina, I. and Jain, L.C. and Thong Nguyen and Nesterov, S.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {342-352},
  Volume                   = {44},

  Abstract                 = {This paper presents a novel system integration technical risk assessment model (SITRAM), which is based on Bayesian belief networks (BBN) coupled with parametric models (PM). This model provides statistical information for decision makers, improving risk management of complex projects. System integration technical risks (SITR) represent a significant part of project risks associated with the development of large software intensive systems for defense and commercial applications. We propose a conceptual modeling framework to address the problem of SITR assessment in the early stages of a system life cycle. Initial risks' taxonomy and risks' interrelations have been identified using a hierarchical holographic modeling (HHM) approach. The framework includes a set of BBN models, representing relations between risk contributing factors, and complementing PMs, used to provide input data to the BBN models. In this paper, we present the rationale and the modeling objectives, and describe the concepts and details of BBN experimental model design and implementation. To address practical limitations, heuristic techniques have been proposed for easing the generation of conditional probability tables. PM design principles are described and examples are presented. In conclusion, we summarize the benefits and constraints of SITR assessment based on BBN models. Further research directions and model improvements are also presented.},
  Doi                      = {10.1109/TSMC.2013.2256126},
  File                     = {Published version:Loutchkina-2014-p342-352c.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {belief networks;project management;risk management;software development management;statistical analysis;BBN;BBN experimental model design;Bayesian belief networks;HHM approach;PM design principles;SITRAM model;commercial applications;conceptual modeling framework;conditional probability tables;defense applications;heuristic techniques;hierarchical holographic modeling;modeling objectives;parametric models;risk contributing factors;risk interrelations;risk management;risk taxonomy;software intensive systems;statistical information;system integration technical risk assessment model;Bayesian networks;expert knowledge elicitation;risk assessment;system integration risk modeling;system integration risks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6519934&isnumber=6739146},
  Zone                     = {-}
}

@Article{Lu-2010-p1483-1493,
  Title                    = {Pi: A practical incentive protocol for delay tolerant networks},
  Author                   = {Rongxing Lu and Xiaodong Lin and Haojin Zhu and Xuemin Shen and Preiss, B.},
  Journal                  = {Wireless Communications, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {1483-1493},
  Volume                   = {9},

  Abstract                 = {Delay Tolerant Networks (DTNs) are a class of networks characterized by lack of guaranteed connectivity, typically low frequency of encounters between DTN nodes and long propagation delays within the network. As a result, the message propagation process in DTNs follows a store-carryand- forward manner, and the in-transit bundle messages can be opportunistically routed towards the destinations through intermittent connections under the hypothesis that each individual DTN node is willing to help with forwarding. Unfortunately, there may exist some selfish nodes, especially in a cooperative network like DTN, and the presence of selfish DTN nodes could cause catastrophic damage to any well designed opportunistic routing scheme and jeopardize the whole network. In this paper, to address the selfishness problem in DTNs, we propose a practical incentive protocol, called Pi, such that when a source node sends a bundle message, it also attaches some incentive on the bundle, which is not only attractive but also fair to all participating DTN nodes. With the fair incentive, the selfish DTN nodes could be stimulated to help with forwarding bundles to achieve better packet delivery performance. In addition, the proposed Pi protocol can also thwart various attacks, which could be launched by selfish DTN nodes, such as free ride attack, layer removing and adding attacks. Extensive simulation results demonstrate the effectiveness of the proposed Pi protocol in terms of high delivery ratio and lower average delay.},
  Doi                      = {10.1109/TWC.2010.04.090557},
  File                     = {Published version:Lu-2010-p1483-1493.pdf:PDF},
  Impactfactor             = {2.418},
  ISSN                     = {1536-1276},
  Keywords                 = {ad hoc networks;computer network security;mobile radio;protocols;Pi protocol;bundle message;catastrophic damage;delay tolerant networks;message propagation process;opportunistic routing scheme;practical incentive protocol;Ad hoc networks;Computer science;Delay effects;Disruption tolerant networking;Frequency;Information technology;Niobium;Propagation delay;Protocols;Routing;Delay tolerant networks;fairness;practical incentive;selfish node},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5441369&isnumber=5441339},
  Zone                     = {2Q}
}

@InProceedings{Luo-2011-p1448-1453,
  Title                    = {Risk Based Mobile Access Control (RiBMAC) policy framework},
  Author                   = {Jim Luo and Myong Kang},
  Booktitle                = {MILITARY COMMUNICATIONS CONFERENCE, 2011 - MILCOM 2011},
  Year                     = {2011},
  Month                    = {Nov},
  Pages                    = {1448-1453},

  Abstract                 = {Mobile devices are increasingly being deployed by enterprises, governments, and the military. Protecting sensitive data that will invariably reside on them is critical. Mobile devices cannot be protected by physical security the same way as stationary systems. Therefore, they must deploy strong internal protection mechanisms for access control. Policies for access control must be driven by context and risk in the environmental in which they operate. This is inherently different from traditional policy models that focus on the multi-user access control. We propose the Risk Based Mobile Access Control (RiBMAC) policy framework for mobile device access control. It uses risk factor abstractions to break down the complexity in the specification, management and evaluation of risk based policies. Its agent-centric approach can effectively integrate a large number of onboard sensors and risk assessment components in different hardware and operational configurations. RiBMAC is a simple yet powerful policy framework that is expressive, practical and scalable. RiBMAC, in conjunction with the appropriate enforcement mechanisms, can greatly improve security for tactical mobile devices.},
  Doi                      = {10.1109/MILCOM.2011.6127510},
  File                     = {Published version:Luo-2011-p1448-1453.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2155-7578},
  Keywords                 = {authorisation;data privacy;mobile computing;mobile radio;risk analysis;risk management;RiBMAC policy framework;agent-centric approach;enforcement mechanisms;internal protection mechanisms;mobile device access control;multiuser access control;onboard sensors;operational configurations;physical security;policy models;risk assessment components;risk based mobile access control policy framework;risk based policy;risk factor abstractions;sensitive data protection;stationary systems;tactical mobile devices;Access control;Authentication;Complexity theory;Context;Mobile handsets;Risk management;access control;mobility;security;tactical},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6127510},
  Zone                     = {-}
}

@Article{Luo-2014-p86-102,
  Title                    = {Power Series Representation Model of Text Knowledge Based on Human Concept Learning},
  Author                   = {Xiangfeng Luo and Jun Zhang and Feiyue Ye and Peng Wang and Chuanliang Cai},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {86-102},
  Volume                   = {44},

  Doi                      = {10.1109/TSMCC.2012.2231674},
  File                     = {Published version:Luo-2014-p86-102.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {algebra;inference mechanisms;knowledge representation;learning (artificial intelligence);series (mathematics);text analysis;PSR model;Web resources;computational complexity;concept algebra;degree-2 power series hypothesis-based reasoning operations;human concept learning;low complex computation;power series representation model;reasoning ability;reasoning-based knowledge services;text knowledge constructing process;text knowledge representation model;Association rules;Cognition;Computational modeling;Knowledge representation;Linearity;OWL;Cognitive informatics;human concept learning;knowledge representation;semantic search;text understanding},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6570748&isnumber=6690269},
  Zone                     = {-}
}

@Article{Lv-2014-p71-79,
  Title                    = {Creep lifetime prediction of polypropylene/clay nanocomposites based on a critical failure strain criterion },
  Author                   = {Yadong Lv and Yajiang Huang and Miqiu Kong and Junlong Yang and Qi Yang and Guangxian Li},
  Journal                  = {Composites Science and Technology },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {71 - 79},
  Volume                   = {96},

  Abstract                 = {Abstract This work presents a new approach for the creep lifetime prediction of polypropylene (PP)/organoclay nanocomposites. The creep failure of \{PP\} and its nanocomposites with and without \{PP\} grafted maleic anhydride (PP-g-MAH) is found to occur at a critical strain independent of the stress level investigated. This peculiar critical strain is combined with a time-strain superposition method to predict the creep failure lifetime of \{PP\} nanocomposites. In this method, a generalized creep compliance curves followed a relatively simple power law are generated from nonlinear creep data obtained by accelerated tests at high stresses. The creep lifetime under lower stresses is then predicted successfully by using the critical strain as the failure criterion. The lifetime predictions for the \{PP\} materials considered based on this approach are satisfactory compared with the experimental results. The approach proposed is intuitive and convenient, and has the potential to be extended to other polymeric materials which possess a critical failure strain behavior. },
  Doi                      = {http://dx.doi.org/10.1016/j.compscitech.2014.03.011},
  File                     = {Published version:Lv-2014-p71-79.pdf:PDF},
  Impactfactor             = {3.328},
  ISSN                     = {0266-3538},
  Keywords                 = {A. Nanocomposites},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0266353814000864},
  Zone                     = {1Q}
}

@Article{Ma-2013-p457-466,
  Title                    = {Analysis of Frequency Dynamics in Power Grid: A Bayesian Structure Learning Approach},
  Author                   = {Hannan Ma and Husheng Li},
  Journal                  = {Smart Grid, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {March},
  Number                   = {1},
  Pages                    = {457-466},
  Volume                   = {4},

  Abstract                 = {The oscillation of frequency in power grid is studied in this paper. The possibility association of frequencies measured at different locations are modeled by a Bayesian network with the logical structure learned using Bayesian structure learning and real measurements in the U.S. power grid. Frequency data analysis and the detection of incorrect frequency measurements (caused by equipment error or malicious attack) are performed over the logical Bayesian network structure. Such application of Bayesian network is a powerful mathematical tool in computational intelligence. Without the physical power network topology information, a two-branch search-and-score structure learning algorithm with L -1 regulation is proposed to learn the logical structure, achieving around 97% correct prediction rate for future frequency and 92% detection rate for false frequency data with 2% false alarm rate. The tool of epidemic propagation over this logical network is also exploited to analyze the propagation of frequency changes. Using the Kolmogorov-Smirnov test, such logical structure is demonstrated to be well approximated by the Small World network model. And the propagation of frequency changes is demonstrated to be described by the Susceptible-Infectious-Susceptible (SIS) model quite well. The Bayesian structure obtained from the real measurement is statistically validated using a 5-fold training data and the Pearson system.},
  Doi                      = {10.1109/TSG.2012.2226066},
  File                     = {Published version:Ma-2013-p457-466.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1949-3053},
  Keywords                 = {belief networks;learning (artificial intelligence);power engineering computing;power grids;statistical analysis;Bayesian structure learning approach;Bayesian structure learning measurement;Kolmogorov-Smirnov test;L-1 regulation;Pearson system;SIS model;Small World network model;US power grid;computational intelligence;epidemic propagation tool;equipment error;frequency change propagation;frequency data analysis;frequency dynamics;frequency oscillation;incorrect frequency measurement detection;logical Bayesian network structure;malicious attack;physical power network topology information;real measurement;susceptible-infectious-susceptible model;two-branch search-and-score structure learning algorithm;Bayesian methods;Heuristic algorithms;Power grids;Power system dynamics;Prediction algorithms;Time frequency analysis;Artificial intelligence;dynamics;frequency estimation;inference mechanisms;security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6451181&isnumber=6471805},
  Zone                     = {-}
}

@InProceedings{Ma-2014-p642-646,
  Title                    = {Study of the Application of LOPA in Risk Assessment of Multi-initiating Events},
  Author                   = {Ma, Lan and Jiang, Yongqing and Liang, Chen and Liu, Zhengmei},
  Booktitle                = {Instrumentation and Measurement, Computer, Communication and Control (IMCCC), 2014 Fourth International Conference on},
  Year                     = {2014},
  Month                    = {Sept},
  Pages                    = {642-646},

  Abstract                 = {There are some defects when dealing some types of complex accident scenes which using LOPA as the strict definition of independent protection layer limits its application scope. The purpose of this paper is aimed at the accident scenarios where multi-initiating event trigger a single accident consequence, putting forward strategy of selecting the aggregation method and the maximum screening method, so that the effectiveness of LOPA applying in such accident scenarios can be expanded. Meanwhile, for the condition that the passive protective layer is not completely effective in the accident scenarios, consequence reduction coefficient is introduced in and correctional program based on accident consequence is presented. All these work is to make the accident frequency calculation results closer to its actual value and finally enhance the accuracy of LOPA analysis.},
  Doi                      = {10.1109/IMCCC.2014.137},
  File                     = {Published version:Ma-2014-p642-646.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Accidents;Accuracy;Fires;Phase frequency detector;Risk management;Statistical analysis;LOPA;Multi-initiating Event;Passive Protective Layer;Risk Assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，好好看看},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6995107&isnumber=6994949},
  Zone                     = {-}
}

@InProceedings{Ma-2010-p2-66,
  Title                    = {How multiple-dependency structure of classes affects their functions a statistical perspective},
  Author                   = {Yutao Ma and Keqing He and Bing Li and Xiaoyan Zhou},
  Booktitle                = {Software Technology and Engineering (ICSTE), 2010 2nd International Conference on},
  Year                     = {2010},
  Month                    = {Oct},
  Pages                    = {V2-60-V2-66},
  Volume                   = {2},

  Abstract                 = {Networks have been considered an effective tool to describe the topological structure of software. Recently, many object-oriented software systems were found to share global network features such as scale free and small world. In this paper, we defined a special structural feature of individual class to investigate the relationship between external structure and internal function. The metric (m) takes in-degree and reachable set of nodes in class-level dependency graphs derived from source code into consideration. Experimental results show that (1) the distribution of frequency-of-occurrence of m roughly follows a power-law distribution, and (2) there is a weak but distinct positive correlation between m and either WMC or LCOM, and classes with complex multiple-dependency structure have on average more complicated functions (and less cohesion) than other classes within specific groups classified by m. Thus, the metric could be a useful complementary measure for traditional OO metrics to analyze the complexity, stability and maintainability of classes.},
  Doi                      = {10.1109/ICSTE.2010.5608763},
  File                     = {Published version:Ma-2010-p2-66.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {graph theory;object-oriented programming;statistical analysis;class-level dependency graphs;classes;complex multiple-dependency structure;global network features;object-oriented software systems;power-law distribution;reachable set;scale free;small world;source code;statistical perspective;topological structure;Complexity theory;Correlation;Couplings;Java;Measurement;Software systems;correlation;dependency graph;objectoriented;program analysis;software metric},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5608763&isnumber=5608746},
  Zone                     = {-}
}

@InProceedings{Mabrouk-2014-p1148-1151,
  Title                    = {PET 18F-FDG kinetic modeling using the iterative two-stage population approach for the assessment of left ventricular function in case of small animal heart failure},
  Author                   = {Mabrouk, R. and Dubeau, F. and Bentabet, L.},
  Booktitle                = {Biomedical Imaging (ISBI), 2014 IEEE 11th International Symposium on},
  Year                     = {2014},
  Month                    = {April},
  Pages                    = {1148-1151},

  Doi                      = {10.1109/ISBI.2014.6868078},
  File                     = {Published version:Mabrouk-2014-p1148-1151.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Blood;Heart;Kinetic theory;Mathematical model;Myocardium;Positron emission tomography;Signal to noise ratio;PET;iterative two-stage method;kinetic modeling;nonlinear weighted least squares},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6868078&isnumber=6867780},
  Zone                     = {-}
}

@Article{Machias-1990-p701-708,
  Title                    = {Application of a deep level knowledge model to dynamic behavior analysis of power systems},
  Author                   = {Machias, A.V. and Souflis, J.L. and Papadias, B.C.},
  Journal                  = {Systems, Man and Cybernetics, IEEE Transactions on},
  Year                     = {1990},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {701-708},
  Volume                   = {20},

  Doi                      = {10.1109/21.57280},
  File                     = {Published version:Machias-1990-p701-708.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0018-9472},
  Keywords                 = {expert systems;inference mechanisms;knowledge acquisition;power system analysis computing;stability;Hellenic transmission system;deep level knowledge model;dynamic behavior analysis;knowledge acquisition;power systems;qualitative reasonings;state transition models;transient stability;Buildings;Computer science;Data analysis;Human factors;Mathematical model;Power system analysis computing;Power system dynamics;Power system modeling;Testing;Text analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=57280&isnumber=2075},
  Zone                     = {-}
}

@Article{Maleki-2014-p36-58,
  Title                    = {Comparison of different failure criteria in prediction of safe mud weigh window in drilling practice },
  Author                   = {Shahoo Maleki and Raoof Gholami and Vamegh Rasouli and Ali Moradzadeh and Reza Ghavami Riabi and Farhad Sadaghzadeh},
  Journal                  = {Earth-Science Reviews },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {36 - 58},
  Volume                   = {136},

  Abstract                 = {Abstract Wellbore stability is one of the most significant problems observed during drilling practice. Determination of safe mud weigh window is hence critically important in order to avoid instability problems. Mechanical properties of formations along with magnitude and direction of ins-situ stress are necessary parameters for evaluating the possibility. However, important role of an efficient failure criterion should never be neglected. In this paper three failure criteria known as Mohr-Coulomb, Hoek-Brown and Mogi-Coulomb were used for prediction of safe mud weigh window. Mechanical earth models were built using wireline logs and to get a rational conclusion, three wellbores drilled in sandstone, carbonate and gas shale reservoirs were taken into consideration. The results of this study indicated that Mohr-Coulomb criterion underestimate the strength and estimates the highest minimum mud pressure required to stabilize the wellbore. This is due to the fact that this criterion is linear and neglects the effect of intermediate stress. The Hoek–Brown criterion on the other hand due to having a non-linear equation is able to provide better estimation compared to that of the Mohr-Coulomb. However Mogi-Coulomb criterion is the best failure criterion for determination of safe mud weight window and this is due mainly to considering the effect of intermediate stress. },
  Doi                      = {http://dx.doi.org/10.1016/j.earscirev.2014.05.010},
  File                     = {Published version:Maleki-2014-p36-58.pdf:PDF},
  Impactfactor             = {7.339},
  ISSN                     = {0012-8252},
  Keywords                 = {Wireline logs},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0012825214000944},
  Zone                     = {1Q}
}

@InProceedings{Malik-2010-p950-955,
  Title                    = {Predicting Function Changes by Mining Revision History},
  Author                   = {Malik, H. and Shakshuki, E.},
  Booktitle                = {Information Technology: New Generations (ITNG), 2010 Seventh International Conference on},
  Year                     = {2010},
  Month                    = {April},
  Pages                    = {950-955},

  Abstract                 = {Software is consistently changing and evolving to new circumstances. Modifications to software do not always involve changes to a single, well-encapsulated module. Software developers are often faced with modification task that involve changes to source code artifacts such as function and comments that are spread across the code base. Developer must ensure that related entities are updated accordingly to be consistent with changes. In this paper, we propose hybrid approach that combines the best of data mining and impact analysis techniques to improve the overall performance (precision and recall) of change propagation heuristics. Our aim is to investigate the Function co-change in large software systems over period of time by utilizing various heuristics such as Function dependencies and History. It is not always every heuristic is good predictor for each entity. Therefore, we augment our effort to provide recommendations to programmers based on the prediction of heuristics that are best for entity needed to be changed. In this paper, we identify the best performing change propagation heuristic based on empirical case study, using a large open source system PostgreSQL database. This database consists of 31,000 functions and 1,493 files over the period of 12 years.},
  Doi                      = {10.1109/ITNG.2010.19},
  File                     = {Published version:Malik-2010-p950-955.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data analysis;data mining;software engineering;PostgreSQL database;data mining;function change prediction;function dependency;function history;impact analysis;revision history mining;software development;Computer science;Data mining;Databases;History;Information technology;Performance analysis;Predictive models;Programming profession;Software maintenance;Software systems;Measurement;modeling and prediction},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5501511&isnumber=5501429},
  Zone                     = {-}
}

@Article{Mandelli-2013-p146-160,
  Title                    = {Scenario clustering and dynamic probabilistic risk assessment },
  Author                   = {Diego Mandelli and Alper Yilmaz and Tunc Aldemir and Kyle Metzroth and Richard Denning},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {146 - 160},
  Volume                   = {115},

  Abstract                 = {A challenging aspect of dynamic methodologies for probabilistic risk assessment (PRA), such as the Dynamic Event Tree (DET) methodology, is the large number of scenarios generated for a single initiating event. Such large amounts of information can be difficult to organize for extracting useful information. Furthermore, it is not often sufficient to merely calculate a quantitative value for the risk and its associated uncertainties. The development of risk insights that can increase system safety and improve system performance requires the interpretation of scenario evolutions and the principal characteristics of the events that contribute to the risk. For a given scenario dataset, it can be useful to identify the scenarios that have similar behaviors (i.e., identify the most evident classes), and decide for each event sequence, to which class it belongs (i.e., classification). It is shown how it is possible to accomplish these two objectives using the Mean-Shift Methodology (MSM). The \{MSM\} is a kernel-based, non-parametric density estimation technique that is used to find the modes of an unknown data distribution. The algorithm developed finds the modes of the data distribution in the state space corresponding to regions with highest data density as well as grouping the scenarios generated into clusters based on scenario temporal similarities. The \{MSM\} is illustrated using the data generated by a \{DET\} algorithm for the analysis of a simple level/temperature controller and reactor vessel auxiliary cooling system. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2013.02.013},
  File                     = {Published version:Mandelli-2013-p146-160.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Transient analysis},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832013000483},
  Zone                     = {2Q}
}

@Article{Manivannan-1994-p534-548,
  Title                    = {A knowledge-based fatal incident decision model},
  Author                   = {Manivannan, S. and Guthrie, S.},
  Journal                  = {Knowledge and Data Engineering, IEEE Transactions on},
  Year                     = {1994},

  Month                    = {Aug},
  Number                   = {4},
  Pages                    = {534-548},
  Volume                   = {6},

  Doi                      = {10.1109/69.298171},
  File                     = {Published version:Manivannan-1994-p534-548.pdf:PDF},
  Impactfactor             = {1.892},
  ISSN                     = {1041-4347},
  Keywords                 = {anthropology;decision support systems;disasters;emergency services;expert systems;knowledge acquisition;knowledge representation;pattern recognition;FINDM;IBM PC;antemortem data;assignment advisor;conflict resolution;contradiction factor;decision support;domain knowledge,;forensic anthropology;forensic scientists;knowledge acquisition;knowledge base;knowledge representation;knowledge-based fatal incident decision model;mass disaster;network flow algorithm;observation advisor;postmortem analysis;regression equations;remains identification;skeletal ID process;trait evaluations;Computer crashes;Data analysis;Earthquakes;Equations;Forensics;Humans;Knowledge acquisition;Large-scale systems;Systems engineering and theory;US Government},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=298171&isnumber=7391},
  Zone                     = {3Q}
}

@Article{Marhavilas-2008-p596-603,
  Title                    = {A risk-estimation methodological framework using quantitative assessment techniques and real accidents’ data: Application in an aluminum extrusion industry },
  Author                   = {Pan. K. Marhavilas and D.E. Koulouriotis},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2008},
  Number                   = {6},
  Pages                    = {596 - 603},
  Volume                   = {21},

  Abstract                 = {The risk estimation presumably is the most crucial part of the entire procedure of assessing hazards/unsafe situations in the work, and especially in the industries’ and constructions’ worksites, where the working conditions are unstable. We can consider the risk as a quantity, which can be estimated and expressed by a mathematical relation, under the help of real accidents’ data. The aim of this work is quadruplicate: (a) the development, elaboration and explanation of two new quantitative risk-assessment techniques, (b) the improvement of specific points of other scientific works, as far as concerns quantitative risk estimation, (c) the application of these techniques on an industrial productive procedure (as a case study) and (d) the comparison of their outcome risk-estimation results. Particularly, we develop and analyze the theoretical background of the two techniques, which we call as “proportional technique” and “decision matrix technique”, and apply them on an aluminum extrusion industry's worksite, which is situated in Greece, by using real data of potential sources of hazards, recorded by safety managers, during the 5.5-year time period of 1999–2004. Comparing the results of the two quantitative risk-assessment techniques, we infer that they are compatible. Therefore, the most important hazard source in the aluminum industry is the “squeezing and hits by dropping objects (transported by derricks)”, and imposes that immediate suppressive measures must be taken place to abolish the danger source. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2008.04.009},
  File                     = {Published version:Marhavilas-2008-p596-603.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Risk estimation},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423008000351},
  Zone                     = {3Q}
}

@InProceedings{Markose-2008-p75-81a,
  Title                    = {A Systematic Framework for Structured Object-Oriented Security Requirements Analysis in Embedded Systems},
  Author                   = {Markose, S. and Liu, X.F. and McMillin, B.},
  Booktitle                = {Embedded and Ubiquitous Computing, 2008. EUC '08. IEEE/IFIP International Conference on},
  Year                     = {2008},
  Month                    = {Dec},
  Pages                    = {75-81},
  Volume                   = {1},

  Abstract                 = {The primary goal of this paper is to develop a structured object-oriented security requirements analysis methodology for the elicitation and analysis of security requirements in embedded systems. There are several approaches to elicit, analyze and specify security requirements in embedded systems ranging from formal mathematical models for proof of certain security properties to informal methods that are easily understood. Applicability of formal security models is limited because they are complex and it is time consuming to develop. On the other hand, informal security requirements analysis methods are not integrated with conceptual models in requirements analysis, and although both external and internal threats have been dealt using use cases and misuse cases, they provide no process for analyzing both internal and external threats in a structured manner. This paper discusses a structured object-oriented security requirements analysis methodology for the elicitation and analysis of security requirements in embedded systems. It is capable of identifying hierarchically both external and internal threats posed by both external and internal actors of a system level by level. It is illustrated and validated by security requirements analysis for an advanced embedded power grid control system.},
  Doi                      = {10.1109/EUC.2008.92},
  File                     = {Published version:Markose-2008-p75-81a.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {embedded systems;formal specification;object-oriented programming;security of data;systems analysis;embedded system;formal mathematical model;security requirement elicitation;structured object-oriented security requirements analysis methodology;Computer science;Computer security;Control systems;Embedded system;Mathematical model;Object oriented modeling;Power grids;Power system security;Ubiquitous computing;Unified modeling language;Security requirements;and security requirements;misuse cases;security goal;structured object-oriented analysis;use cases},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4756323&isnumber=4756301},
  Zone                     = {-}
}

@Article{Markowski-2011-p780-790,
  Title                    = {Application of fuzzy logic to explosion risk assessment},
  Author                   = {Adam S. Markowski and M. Sam Mannan and Agata Kotynia and Henryk Pawlak},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2011},
  Note                     = {Papers Presented at the 2010 International Symposium of the Mary Kay O'Connor Process Safety Center MKOPSC-2010 symposium },
  Number                   = {6},
  Pages                    = {780 - 790},
  Volume                   = {24},

  Abstract                 = {Safety and health of workers potentially being at risk from explosive atmospheres are regulated by separate regulations (ANSI/AIHA in \{USA\} and \{ATEX\} in the European Union). The ANSI/AIHA does not require risk assessment whereas it is compulsory for ATEX. There is no standard method to do that assessment. For that purpose we have applied the explosion Layer of Protection Analysis (ExLOPA), which enables semi-quantitative risk assessment for process plants where explosive atmospheres occur. The?ExLOPA is based on the original work of \{CCPS\} for \{LOPA\} taking into account an explosion accident scenario at workplace. That includes typical variables appropriate for workplace explosion like occurrence of the explosive atmosphere, the presence of effective ignition sources, activity of the explosion prevention and mitigation independent protection layers as well as the severity of consequences. All those variables are expressed in the form of qualitative linguistic categories and relations between them are presented using expert based engineering knowledge, expressed in the form of appropriate set of rules. In this way the category of explosion risk may be estimated by the semi-quantitative analysis. However, this simplified method is connected with essential uncertainties providing over or under estimation of the explosion risk and may not provide real output data. In order to overcome this problem and receive more detailed quantitative results, the fuzzy logic system was applied. In the first stage called fuzzification, all linguistic categories of the variables are mapped by fuzzy sets. In the second stage, the number of relation between all variables of analysis are determined by the enumerative combinatorics and the set of the 810 fuzzy rules “IF-THEN” is received. Each rule enables determination of the fuzzy risk level for a particular accident scenario. In the last stage, called defuzzification, the crisp value of final risk is obtained using a centroid method. The final result of the risk presents a contribution of each risk category represented by the fuzzy sets (A, TA, \{TNA\} and NA) and is therefore more precise and readable than the traditional approach producing one category of risk only. Fuzzy logic gives a possibility of better insights into hazards and safety phenomena for each explosion risk scenario. It is not possible to receive such conclusions from the traditional ExLOPA calculation results. However it requires the application of computer-aided analyses which may be partially in conflict with a simplicity of ExLOPA. The practical example provides a comparison between the traditional results obtained by ExLOPA and by fuzzy ExLOPA methods. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2011.06.002},
  File                     = {Published version:Markowski-2011-p780-790.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {\{ATEX\} regulation},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423011000830},
  Zone                     = {3Q}
}

@Article{Marvel-2015-p260-275a,
  Title                    = {Characterizing Task-Based Human–Robot Collaboration Safety in Manufacturing},
  Author                   = {Marvel, J.A. and Falco, J. and Marstio, I.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {260-275},
  Volume                   = {45},

  Abstract                 = {A new methodology for describing the safety of human-robot collaborations is presented. Taking a task-based perspective, a risk assessment of a collaborative robot system safety can be evaluated offline during the initial design stages. This risk assessment factors in such elements as tooling, the nature and duration of expected contacts, and any amortized transfer of pressures and forces onto a human operator. Risk assessments of example tasks are provided for illustrative purposes.},
  Doi                      = {10.1109/TSMC.2014.2337275},
  File                     = {Published version:Marvel-2015-p260-275a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {human-robot interaction;industrial robots;manufacturing systems;occupational safety;risk management;collaborative robot system safety;manufacturing;risk assessment factors;task-based human-robot collaboration safety;Collaboration;Hazards;Manufacturing;Risk management;Service robots;Collaborative work;human-machine interaction;human???machine interaction;manufacturing automation;risk analysis;safety},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6866878&isnumber=7008575},
  Zone                     = {-}
}

@Article{Matsuura-2012-p1972-1984,
  Title                    = {Sensing-based risk mitigation control of hydrogen dispersion and accumulation in a partially open space with low-height openings by forced ventilation },
  Author                   = {Kazuo Matsuura and Masami Nakano and Jun Ishimoto},
  Journal                  = {International Journal of Hydrogen Energy },
  Year                     = {2012},
  Note                     = {10th International Conference on Clean Energy 2010 },
  Number                   = {2},
  Pages                    = {1972 - 1984},
  Volume                   = {37},

  Abstract                 = {This paper presents the real-time sensing-based risk-mitigation control of hydrogen dispersion and accumulation in a partially open space with low-height openings by forced ventilation. In the partially open space we previously considered (Matsuura et al., Int J Hydrogen Energy, 35(10), p. 4776–4786 (2010)), a hydrogen buoyant plume is subjected to cross flows during forced ventilation, and hydrogen travels over a long distance in the lower part of the space, which enhances the hydrogen concentration there. On the basis of those results, we alternatively propose in this paper a new partially open space that permits the almost vertical rising of hydrogen from a leak source, and exhausts, by forced ventilation, hydrogen temporarily accumulated near the roof. We first describe computational geometries and scenarios, mathematical models and numerical procedures. Related to this model, an acceptability diagram of exhaust flow rates for various leak flow rates and leak positions, an estimation method of the instantaneous hydrogen amount accumulating near the roof, and a hunting-preventive control scheme of the exhaust flow rate based on the least-squares method are constructed. A sensing-based risk mitigation control strategy to change the exhaust volume flow rate is then proposed. The effects of the number of sensors on the proposed control system are also investigated. The proposed system is validated for various leak positions and leak modes. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijhydene.2011.08.006},
  File                     = {Published version:Matsuura-2012-p1972-1984.pdf:PDF},
  Impactfactor             = {3.548},
  ISSN                     = {0360-3199},
  Keywords                 = {Safety},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0360319911018520},
  Zone                     = {3Q}
}

@Article{Mattiasson-2014-p175-191,
  Title                    = {On the prediction of failure in metal sheets with special reference to strain path dependence },
  Author                   = {Kjell Mattiasson and Johan Jergéus and Paul DuBois},
  Journal                  = {International Journal of Mechanical Sciences },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {175 - 191},
  Volume                   = {88},

  Abstract                 = {Abstract Prediction of failure in metal sheets is an important topic for the sheet forming community, as well as for the automotive crash community. The word ‘failure’ can have different meaning for different individuals within these communities. Methods for failure prediction within this area can either focus on the prediction of plastic instability (necking), or on the actual fracture phenomenon. The pros and cons of these approaches are discussed in this paper. The current authors have chosen to favour methods for necking prediction. The traditional method for necking prediction is to use a limit curve in the principal strain space (FLD). The great disadvantage of this approach is that it is only applicable for linear strain paths. In fact, the necking phenomenon can be shown to be strongly strain path dependent. In the current report, four different numerical methods for instability prediction are discussed, and compared in applications to some simple problems involving broken strain paths. It is shown that these methods can yield dramatically different results in some particular cases. Based on the findings of this study, the paper concludes with some recommendations for how the failure prediction problem best can be handled in industrial sheet forming and crash simulations. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijmecsci.2014.08.006},
  File                     = {Published version:Mattiasson-2014-p175-191.pdf:PDF},
  Impactfactor             = {1.613},
  ISSN                     = {0020-7403},
  Keywords                 = {Sheet metal},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0020740314002719},
  Zone                     = {2Q}
}

@Article{McCaffery-2007-p24-31,
  Title                    = {Adept: A Unified Assessment Method for Small Software Companies},
  Author                   = {Mc Caffery, F. and Taylor, P.S. and Coleman, G.},
  Journal                  = {Software, IEEE},
  Year                     = {2007},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {24-31},
  Volume                   = {24},

  Abstract                 = {Software process assessments are typically the first step to commencing software process improvement. Small software companies find that many assessment methods are linked to plan-driven improvement models and can be expensive in terms of the resources required. The authors support these claims with statistics from the Irish software industry and present a lightweight assessment method called Adept. Adept unifies an existing plan-driven assessment method and an adapted, risk-based agility/discipline assessment method. Adept has a low resource overhead and does not dictate either a plan-driven or agile process improvement model, making it an attractive assessment method for small software companies},
  Doi                      = {10.1109/MS.2007.3},
  File                     = {Published version:McCaffery-2007-p24-31.pdf:PDF},
  Impactfactor             = {1.616},
  ISSN                     = {0740-7459},
  Keywords                 = {DP industry;software process improvement;Adept method;Irish software industry;plan-driven assessment;plan-driven improvement models;risk-based agility assessment;risk-based discipline assessment;small software companies;software engineering;software process assessments;software process improvement;software process measurement;software process models;Appraisal;Companies;Costs;IEC standards;ISO standards;Project management;Quality management;Research and development management;Software development management;Software engineering;D.2.18 Software Engineering Process;D.2.18.c Process Measurement;D.2.18.e Software Process Models.},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4052548&isnumber=4052534},
  Zone                     = {3Q}
}

@Article{McCalley-1999-p1400-1406,
  Title                    = {Reliability of special protection systems},
  Author                   = {McCalley, J.D. and Weihui Fu},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {1400-1406},
  Volume                   = {14},

  Abstract                 = {Special protection systems (SPS) have been widely used to increase the transfer capability of the network by assisting system operators in administering fast corrective actions. Compared with constructing new transmission facilities, SPS can be placed in service relatively quickly and inexpensively. However, increased reliance on SPS results in additional risks to system security. The purpose of this paper is to elucidate the importance of developing a systematic and comprehensive reliability framework for SPS. We review types of SPS and provide several examples of actual SPS failures. The need for SPS reliability assessment is discussed. Several reliability assessment methods are described, and efforts to deal with a similar problem in the process control industry are summarized. We also provide several recommendations},
  Doi                      = {10.1109/59.801903},
  File                     = {Published version:McCalley-1999-p1400-1406.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {power system protection;power system reliability;power system security;power system security;process control industry;reliability assessment methods;special protection systems reliability;system operators assistance;transfer capability;Communication system control;Electrical equipment industry;Industrial control;Power system dynamics;Power system protection;Power system reliability;Power system security;Process control;Senior members;Student members},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=801903&isnumber=17392},
  Zone                     = {2Q}
}

@Article{McCalley-1999-p1400-1406a,
  Title                    = {Reliability of special protection systems},
  Author                   = {McCalley, J.D. and Weihui Fu},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {1999},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {1400-1406},
  Volume                   = {14},

  Abstract                 = {Special protection systems (SPS) have been widely used to increase the transfer capability of the network by assisting system operators in administering fast corrective actions. Compared with constructing new transmission facilities, SPS can be placed in service relatively quickly and inexpensively. However, increased reliance on SPS results in additional risks to system security. The purpose of this paper is to elucidate the importance of developing a systematic and comprehensive reliability framework for SPS. We review types of SPS and provide several examples of actual SPS failures. The need for SPS reliability assessment is discussed. Several reliability assessment methods are described, and efforts to deal with a similar problem in the process control industry are summarized. We also provide several recommendations},
  Doi                      = {10.1109/59.801903},
  File                     = {Published version:McCalley-1999-p1400-1406a.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {power system protection;power system reliability;power system security;power system security;process control industry;reliability assessment methods;special protection systems reliability;system operators assistance;transfer capability;Communication system control;Electrical equipment industry;Industrial control;Power system dynamics;Power system protection;Power system reliability;Power system security;Process control;Senior members;Student members},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=801903&isnumber=17392},
  Zone                     = {2Q}
}

@Article{McLean-1990-p9-16,
  Title                    = {The specification and modeling of computer security},
  Author                   = {McLean, J.},
  Journal                  = {Computer},
  Year                     = {1990},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {9-16},
  Volume                   = {23},

  Abstract                 = {A description is given of computer security models in general and the model of D. Bell and L. LaPadula (Tech. Rep. MTR-2997, Mitre Corp., 1976) in particular. The Bell and LaPadula (BLP) model is the backbone of the National Computer Security Center's evaluation process for trusted computer systems. Although discretionary access control is briefly addressed, the focus is on mandatory access control (MAC) in national security. However, the issues addressed are relevant to any setting in which MAC-like restrictions arise. It is shown that security is a fruitful research area for those interested in software specification, since some of the most difficult issues in specifying security have analogs in other domains. The limitations of the BLP model are examined. For example, it has little relevance for systems in which users can change their own security levels or those of their files, and it is inadequate for expressing requirements that certain operations cannot be performed by a single individual working alone. It is shown how BLP's limitations can be remedied by a framework of models, making it more useful to those interested in industrial security.<>},
  Doi                      = {10.1109/2.48795},
  File                     = {Published version:McLean-1990-p9-16.pdf:PDF},
  Impactfactor             = {1.675},
  ISSN                     = {0018-9162},
  Keywords                 = {security of data;BLP model;Bell and LaPadula model;MAC;computer security models;discretionary access control;files;industrial security;mandatory access control;national security;security levels;software specification;trusted computer systems;Access control;Computer security;Horses;Information security;Invasive software;Laboratories;National security;Permission;Power system protection;Power system security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=48795&isnumber=1814},
  Zone                     = {2Q}
}

@Article{Mei-2009-p814-823,
  Title                    = {An Improved OPA Model and Blackout Risk Assessment},
  Author                   = {Shengwei Mei and Fei He and Xuemin Zhang and Shengyu Wu and Gang Wang},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {May},
  Number                   = {2},
  Pages                    = {814-823},
  Volume                   = {24},

  Abstract                 = {The ORNL-PSerc-Alaska (OPA) model is a blackout model proposed by researchers at Oak Ridge National Laboratory (ORNL), Power System Engineering Research Center of Wisconsin University (PSerc), and Alaska University (Alaska). Although the OPA model is a landmark study, it has two limitations. First, there is a significant difference between simulation and practice in transmission line outage and update; and second, the simulation of cascading failure and the probability distribution of blackout size are in general not accurate enough. Hence, an improved OPA model is proposed in this paper to address these limitations. The proposed model contains two layers of iteration. The inner iteration describes the fast dynamics of the system and considers the influence of power flow, dispatching, automation, relay protection, and so on. The outer iteration describes the slow overall system evolution and is concerned with the update of the power grid, operation modes and planning. Such a model can be applied to practical large-scale systems. Furthermore, based on the Value at Risk (VaR) and Conditional Value at Risk (CVaR), two new complementary blackout risk indices are defined, which reveal critical characteristics of blackouts and are used to evaluate security levels of power systems. The effectiveness of the improved OPA model is verified by the simulations concerning the Northeast Power Grid of China.},
  Doi                      = {10.1109/TPWRS.2009.2016521},
  File                     = {Published version:Mei-2009-p814-823.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {power system faults;power system management;power system reliability;power system security;statistical distributions;Alaska University;China;Northeast Power Grid;OPA model;ORNL-PSerc-Alaska model;Oak Ridge National Laboratory;Power System Engineering Research Center;Wisconsin University;blackout risk assessment;cascading failure;conditional value at risk;probability distribution;security levels;transmission line outage;CVaR;Cascading failure;OPA model;VaR},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4815410&isnumber=4839059},
  Zone                     = {2Q}
}

@InProceedings{Mellado-2006-p8-8,
  Title                    = {A comparison of the Common Criteria with proposals of information systems security requirements},
  Author                   = {Mellado, D. and Fernandez-Medina, E. and Piattini, M.},
  Booktitle                = {Availability, Reliability and Security, 2006. ARES 2006. The First International Conference on},
  Year                     = {2006},
  Month                    = {April},
  Pages                    = {8 pp.-},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/ARES.2006.2},
  File                     = {Published version:Mellado-2006-p8-8.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {information systems;security of data;Common Criteria standard;IS design;information systems security requirement;Information management;Information security;Information systems;Information technology;Management information systems;Proposals;Protection;Quality management;Research and development;Software systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1625370&isnumber=34117},
  Zone                     = {-}
}

@Article{Mendiluce-2014-p34-41,
  Title                    = {Risky Business: Building a Resilient Power Sector},
  Author                   = {Mendiluce, M.},
  Journal                  = {Power and Energy Magazine, IEEE},
  Year                     = {2014},

  Month                    = {Sept},
  Number                   = {5},
  Pages                    = {34-41},
  Volume                   = {12},

  Abstract                 = {The evidence of human-induced climate change grows stronger with every scientific report. While action to mitigate rising temperatures is increasingly urgent, it is also essential to consider how to adapt to the consequences of global warming. The long-term investment horizons used in the electricity industry require an early risk assessment for its assets. Electric utilities must prepare to maintain supply in the face of different weather patterns and more frequent extreme weather events, as confirmed by the Fifth Assessment Report of the Intergovernmental Panel on Climate Change.. Members of the electric utilities project of the World Business Council on Sustainable Development (WBCSD) have recently published a report, ?Building a Resilient Power Sector,? to share their learning and understanding of best practices in increasing the resilience of the power sector (see www.wbcsd.org/resilience.aspx). This report contributes to WBCSD?s Action2020 climate change business solutions for resilience (see ?WBCD?s Action2020 Framework?).},
  Doi                      = {10.1109/MPE.2014.2331892},
  File                     = {Published version:Mendiluce-2014-p34-41.pdf:PDF},
  Impactfactor             = {1.582},
  ISSN                     = {1540-7977},
  Keywords                 = {climate mitigation;global warming;investment;power markets;risk analysis;sustainable development;WBCSD;World Business Council on Sustainable Development;climate change;electric utilities;electricity industry;global warming;investment horizon;power sector;rising temperature mitigation;risk assessment;weather event;Electricity supply industry;Environmental factors;Global warming;Greenhouse effect;Human factors;Meteorology;Ocean temperature;Pollution;Power systems planning;Sustainable development},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6878538&isnumber=6878510},
  Zone                     = {3Q}
}

@Article{Meng-2012-p1454-1464,
  Title                    = {Uncertainty Propagation in Quantitative Risk Assessment Modeling for Fire in Road Tunnels},
  Author                   = {Qiang Meng and Xiaobo Qu},
  Journal                  = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {1454-1464},
  Volume                   = {42},

  Abstract                 = {Road tunnels are critical transportation infrastructures that provide underground passageways for motorists and commuters. Fire in road tunnels in combination with tunnel safety provisions failure may lead to catastrophic consequences, and thus, necessitates a robust and reliable approach to assess tunnel risks. This article proposes a quantitative risk assessment model for fire in road tunnel by taking into consideration two types of uncertainties. A Monte Carlo-based estimation method is developed to propagate parameter uncertainty in quantitative risk assessment model consisting of event tree analysis as well as consequence estimation models. The percentile-based individual risks and α-cut-based societal risks are put up and the risk indices are proven to be very useful for tunnel operators with distinct risk attitudes to assess the safety level of a road tunnel. Finally, the proposed research methodology is applied to Singapore KPE road tunnels.},
  Doi                      = {10.1109/TSMCC.2012.2190982},
  File                     = {Published version:Meng-2012-p1454-1464.pdf:PDF},
  Impactfactor             = {2.548},
  ISSN                     = {1094-6977},
  Keywords                 = {Monte Carlo methods;critical infrastructures;failure analysis;fires;risk analysis;road safety;structural engineering computing;trees (mathematics);tunnels;uncertainty handling;α-cut based societal risk;Monte Carlo-based estimation method;catastrophic consequence;commuters;consequence estimation model;critical transportation infrastructure;event tree analysis;failure analysis;fire;motorists;percentile-based individual risk;quantitative risk assessment modeling;road tunnel;tunnel safety;uncertainty propagation;underground passageway;Analytical models;Estimation;Monte Carlo methods;Risk management;Road vehicles;Tunnels;Uncertainty;Fire;fuzzy sets;parameter uncertainty;quantitative risk assessment (QRA);road tunnel},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6190763&isnumber=6330018},
  Zone                     = {2Q}
}

@InProceedings{Menzel-2009-p41-48,
  Title                    = {Security Requirements Specification in Service-Oriented Business Process Management},
  Author                   = {Menzel, M. and Thomas, I and Meinel, C.},
  Booktitle                = {Availability, Reliability and Security, 2009. ARES '09. International Conference on},
  Year                     = {2009},
  Month                    = {March},
  Pages                    = {41-48},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/ARES.2009.90},
  File                     = {Published version:Menzel-2009-p41-48.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Web services;business data processing;formal specification;object-oriented programming;protocols;risk analysis;security of data;software architecture;workflow management software;business process management;business process modelling;concrete protocol;domain-independent security model;model-driven approach;organisational workflow;security pattern system;security requirement specification;security risk;service-oriented architecture;software component;Authorization;Availability;Business communication;Computer architecture;Concrete;Conference management;Information security;Protocols;Service oriented architecture;Software development management;Business Process Management;SOA Security;Security Requirement Specification;Web Service Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5066453&isnumber=5066394},
  Zone                     = {-}
}

@InProceedings{Merle-2011-p1-6,
  Title                    = {Dynamic fault tree analysis based on the structure function},
  Author                   = {Merle, G. and Roussel, J.-M. and Lesage, J.},
  Booktitle                = {Reliability and Maintainability Symposium (RAMS), 2011 Proceedings - Annual},
  Year                     = {2011},
  Month                    = {Jan},
  Pages                    = {1-6},

  Abstract                 = {This paper presents an algebraic approach allowing to perform the analysis of any Dynamic Fault Tree (DFT). This approach is based on the ability to formally express the structure function of DFTs. We first present the algebraic framework that we introduced to model dynamic gates and hence be able to determine the structure function of DFTs. Then, we show that this structure function can be rewritten under a canonical form from which the qualitative analysis of DFTs can be performed directly. We finally provide a probabilistic model of dynamic gates to be able to perform the quantitative analysis of DFTs from their structure function.},
  Doi                      = {10.1109/RAMS.2011.5754452},
  File                     = {Published version:Merle-2011-p1-6.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0149-144X},
  Keywords                 = {fault trees;logic gates;probability;algebraic framework;dynamic fault tree analysis;dynamic gates modelling;probabilistic model;structure function;Analytical models;Computational modeling;Computers;Discrete Fourier transforms;Fault trees;Logic gates;Probabilistic logic;Boolean function;fault trees;probability;reliability;symbol manipulation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5754452&isnumber=5754421},
  Zone                     = {-}
}

@Article{Mili-2004-p38-63,
  Title                    = {Risk assessment of catastrophic failures in electric power systems},
  Author                   = {Mili, Lamine and Qiu, Q and Phadke, Arun G},
  Journal                  = {International Journal of Critical Infrastructures},
  Year                     = {2004},
  Number                   = {1},
  Pages                    = {38--63},
  Volume                   = {1},

  File                     = {Mili-2004-p38-63.pdf:Mili-2004-p38-63.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Publisher                = {Inderscience},
  Timestamp                = {2015.04.21},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.96.5608&rep=rep1&type=pdf},
  Zone                     = {-}
}

@InProceedings{Miller-2012-p51-56,
  Title                    = {A Survey SCADA of and Critical Infrastructure Incidents},
  Author                   = {Miller, Bill and Rowe, Dale},
  Booktitle                = {Proceedings of the 1st Annual Conference on Research in Information Technology},
  Year                     = {2012},

  Address                  = {New York, NY, USA},
  Pages                    = {51--56},
  Publisher                = {ACM},
  Series                   = {RIIT '12},

  Acmid                    = {2380805},
  Doi                      = {10.1145/2380790.2380805},
  File                     = {Published version:Miller-2012-p51-56.pdf:PDF},
  Impactfactor             = {-},
  ISBN                     = {978-1-4503-1643-9},
  Keywords                 = {critical infrastructure, cyber attack, cyber security, information assurance and security, scada, security},
  Level                    = {-},
  Location                 = {Calgary, Alberta, Canada},
  Numpages                 = {6},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://doi.acm.org/10.1145/2380790.2380805},
  Zone                     = {-}
}

@Article{Miranda-1999-p265-267,
  Title                    = {A function-fit model for the soft breakdown failure mode},
  Author                   = {Miranda, Enrique and Sune, J. and Rodriguez, R. and Nafria, M. and Aymerich, X.},
  Journal                  = {Electron Device Letters, IEEE},
  Year                     = {1999},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {265-267},
  Volume                   = {20},

  Abstract                 = {An empirical one parameter-based power law model for the leakage current through one or more soft breakdown spots in ultrathin (<5 nm) gate oxides is presented. Good fit to data can be obtained in nearly five decades of current from 0.5 to 5 V. In addition, it is shown that there exists a slight correlation between the parameters which describe the soft breakdown conduction characteristic and the stressing condition which triggers it.},
  Doi                      = {10.1109/55.767093},
  File                     = {Published version:Miranda-1999-p265-267.pdf:PDF},
  Impactfactor             = {2.789},
  ISSN                     = {0741-3106},
  Keywords                 = {MIS devices;dielectric thin films;failure analysis;semiconductor device breakdown;semiconductor device models;semiconductor device reliability;MOS devices;breakdown conduction characteristic;function-fit model;one parameter-based power law model;soft breakdown failure mode;soft breakdown spots;stressing condition;ultrathin gate oxides;Breakdown voltage;Capacitors;Dielectric breakdown;Electric breakdown;Fabrication;Lead compounds;Leakage current;MOS devices;Tunneling;Ultra large scale integration},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=767093&isnumber=16625},
  Zone                     = {2Q}
}

@Article{Mo-2014-p757-769,
  Title                    = {MDD-Based Method for Efficient Analysis on Phased-Mission Systems With Multimode Failures},
  Author                   = {Yuchang Mo and Liudong Xing and Dugan, J.B.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {757-769},
  Volume                   = {44},

  Abstract                 = {Many practical systems are phased-mission systems with multimode failures (MFPMSs) where the mission consists of multiple nonoverlapping phases of operation, and the system components may assume more than one failure mode. In MFPMSs, dependence arises among different phases and among different failure modes of the same component, which makes the reliability analysis of MFPMSs difficult. This paper proposes a new analytical method based on multivalued decision diagrams (MDDs) for the reliability analysis of nonrepairable MFPMSs. MDDs have recently been applied to the reliability analysis of single-phase systems with multiple component states. In this paper, we make the new contribution by proposing a novel way to adapt MDDs for the reliability analysis of systems with multiple phases and multimode failures. Examples show how the MDD models are generated and evaluated to obtain the mission reliability measures. Performance of the MDD-based method is compared with an existing binary decision diagram (BDD)-based method for MFPMS analysis through several examples and a comprehensive benchmark study. Empirical results show that the proposed MDD-based method can offer lower computational complexity and simpler model construction and evaluation algorithms than the BDD-based method, and it can be effectively applied to large practical cases.},
  Doi                      = {10.1109/TSMC.2013.2277692},
  File                     = {Published version:Mo-2014-p757-769.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {binary decision diagrams;computational complexity;failure analysis;reliability theory;BDD-based method;MDD-based method;MFPMS analysis;binary decision diagram-based method;computational complexity;evaluation algorithm;failure mode;mission reliability measure;model construction;multimode failures;multiple component states;multiple nonoverlapping phases;multivalued decision diagrams;nonrepairable MFPMS;phased-mission system;reliability analysis;single-phase systems;system components;Analytical models;Boolean functions;Computational modeling;Data structures;Encoding;Fault trees;Reliability;Binary decision diagram (BDD);fault tree;multivalued decision diagram (MDD);phased-mission system (PMS)},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6587608&isnumber=6814830},
  Zone                     = {-}
}

@Article{Modarres-1984-p325-328,
  Title                    = {A Truncation Methodology for Evaluating Large Fault Trees},
  Author                   = {Modarres, M. and Dezfuli, H.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1984},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {325-328},
  Volume                   = {R-33},

  Abstract                 = {Probabilistic risk assessment (PRA) of a nuclear power plant often involves evaluating large fault trees (eg, trees with more than 1000 basic events and 1000 gates). It is desired to generate the dominant minimal cut sets (MCSs) in order to gain engineering insight, to estimate the top event probability, and to calculate the frequency of reactor core damage. To estimate the top event probability with computers, the usual practice has been to neglect those MCSs of the fault tree that have a probability of less than some cut-off value; the cut-off value is subjectively selected by the analyst. This method of truncation eliminates some MCSs, and thus simplifies the fault tree but does not help the analyst to estimate the truncation error. This source of uncertainty has been a concern in PRA calculations. A better method of truncation, based on both cut-set size and cut-set probability [Combined Truncation (CT) methodology] is developed and discussed in this paper. With this method, the analyst is able to estimate the maximum error that can occur in the truncation process. In the CT methodology the analyst estimates a parameter which is a function of the number of primary events in the fault tree, and the highest probability associated with the basic events in the fault tree. From that parameter value, the analyst then determines the level of truncation necessary and the associated maximum error of truncation.},
  Doi                      = {10.1109/TR.1984.5221845},
  File                     = {Published version:Modarres-1984-p325-328.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Fault trees;Finite wordlength effects;Frequency estimation;Inductors;Nuclear power generation;Power engineering and energy;Power generation;Probability;Risk management;Uncertainty;Accident sequence quantification;Cut-set generation;Fault-tree analysis;Fault-tree truncation;Top-event probability},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5221845&isnumber=5221811},
  Zone                     = {2Q}
}

@Article{Mollegen-1966-p414-419,
  Title                    = {Determining if Sonar Contact Can be Maintained},
  Author                   = {Mollegen, A.T.},
  Journal                  = {Aerospace and Electronic Systems, IEEE Transactions on},
  Year                     = {1966},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {414-419},
  Volume                   = {AES-2},

  Abstract                 = {ASW effectiveness is strongly dependent on whether sonar contact can be maintained long enough for a attack to be accomplished. Proper design and use of weapons systems both depend upon the probable duration of contact maintenance. A number of stochastic models of propagation and detection variability are considered. A method is described of fast-time tactical simulation which makes use of such models to evaluate the overall effectiveness of the sonar/weapon system combination.},
  Doi                      = {10.1109/TAES.1966.4502036},
  File                     = {Published version:Mollegen-1966-p414-419.pdf:PDF},
  Impactfactor             = {1.299},
  ISSN                     = {0018-9251},
  Keywords                 = {Event detection;Helicopters;Motion analysis;Motion detection;Remote sensing;Sonar;Stochastic processes;Target tracking;Underwater vehicles;Weapons},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4502036&isnumber=4501949},
  Zone                     = {2Q}
}

@Article{Moore-2013-p1685-1689,
  Title                    = {Security Risk Assessment Methodology for the petroleum and petrochemical industries },
  Author                   = {David A. Moore},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {1685 - 1689},
  Volume                   = {26},

  Abstract                 = {Abstract The American National Standards Institute (ANSI)/American Petroleum Institute (API) Standard 780 Security Risk Assessment (SRA) Methodology was published in June 2013 as a U. S. standard for security risk assessments on petroleum and petrochemical facilities. The standard represents a model standard for evaluating all security risks of petroleum and petrochemical infrastructure and operations and assists industries in more thoroughly and consistently conducting SRAs. The 2013 Standard is an update from the previous API/NPRA \{SRA\} Methodology (2004) and focuses on expanding functional utility without changing the basic methodology. The methodology can be applied to a wide range of assets even beyond the typical operating facilities of the industry. This includes refining and petrochemical manufacturing operations, pipelines, and transportation operations including truck, marine, and rail, as well as worker and executive security, housing compounds, and remote operational sites. The new standard describes the most efficient and thorough approach for assessing security risks widely applicable to the types of facilities operated by the industry and the security issues they face. It is voluntary but has been adopted by the Kingdom of Saudi Arabia Ministry of Interior High Commission for Industrial Security as the mandatory security risk assessment methodology for industrial facilities. This paper examines the key elements of the ANSI/API \{SRA\} process and discusses how forward thinking organizations may use risk-based performance metrics to systematically analyze facility security postures and identify appropriately scaled and fiscally responsible countermeasures based on current and projected threats. The AcuTech Consulting Group developed the methodology under contract to the API, and the author was the project manager for the project. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2013.10.012},
  File                     = {Published version:Moore-2013-p1685-1689.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Security risk management},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {石油工业的信息安全风险评估，讲的比较粗略},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423013002040},
  Zone                     = {3Q}
}

@Article{Moore-2013-p1685-1689a,
  Title                    = {Security Risk Assessment Methodology for the petroleum and petrochemical industries },
  Author                   = {David A. Moore},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {1685 - 1689},
  Volume                   = {26},

  __markedentry            = {[Qiqi:5]},
  Abstract                 = {Abstract The American National Standards Institute (ANSI)/American Petroleum Institute (API) Standard 780 Security Risk Assessment (SRA) Methodology was published in June 2013 as a U. S. standard for security risk assessments on petroleum and petrochemical facilities. The standard represents a model standard for evaluating all security risks of petroleum and petrochemical infrastructure and operations and assists industries in more thoroughly and consistently conducting SRAs. The 2013 Standard is an update from the previous API/NPRA \{SRA\} Methodology (2004) and focuses on expanding functional utility without changing the basic methodology. The methodology can be applied to a wide range of assets even beyond the typical operating facilities of the industry. This includes refining and petrochemical manufacturing operations, pipelines, and transportation operations including truck, marine, and rail, as well as worker and executive security, housing compounds, and remote operational sites. The new standard describes the most efficient and thorough approach for assessing security risks widely applicable to the types of facilities operated by the industry and the security issues they face. It is voluntary but has been adopted by the Kingdom of Saudi Arabia Ministry of Interior High Commission for Industrial Security as the mandatory security risk assessment methodology for industrial facilities. This paper examines the key elements of the ANSI/API \{SRA\} process and discusses how forward thinking organizations may use risk-based performance metrics to systematically analyze facility security postures and identify appropriately scaled and fiscally responsible countermeasures based on current and projected threats. The AcuTech Consulting Group developed the methodology under contract to the API, and the author was the project manager for the project. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2013.10.012},
  File                     = {Published version:Moore-2013-p1685-1689a.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Security risk management},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，R = f(C,V,T)，没试验，没有仿真},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423013002040},
  Zone                     = {3Q}
}

@Article{Moore-2013-p1685-1689b,
  Title                    = {Security Risk Assessment Methodology for the petroleum and petrochemical industries },
  Author                   = {David A. Moore},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2013},
  Number                   = {6},
  Pages                    = {1685 - 1689},
  Volume                   = {26},

  Abstract                 = {Abstract The American National Standards Institute (ANSI)/American Petroleum Institute (API) Standard 780 Security Risk Assessment (SRA) Methodology was published in June 2013 as a U. S. standard for security risk assessments on petroleum and petrochemical facilities. The standard represents a model standard for evaluating all security risks of petroleum and petrochemical infrastructure and operations and assists industries in more thoroughly and consistently conducting SRAs. The 2013 Standard is an update from the previous API/NPRA \{SRA\} Methodology (2004) and focuses on expanding functional utility without changing the basic methodology. The methodology can be applied to a wide range of assets even beyond the typical operating facilities of the industry. This includes refining and petrochemical manufacturing operations, pipelines, and transportation operations including truck, marine, and rail, as well as worker and executive security, housing compounds, and remote operational sites. The new standard describes the most efficient and thorough approach for assessing security risks widely applicable to the types of facilities operated by the industry and the security issues they face. It is voluntary but has been adopted by the Kingdom of Saudi Arabia Ministry of Interior High Commission for Industrial Security as the mandatory security risk assessment methodology for industrial facilities. This paper examines the key elements of the ANSI/API \{SRA\} process and discusses how forward thinking organizations may use risk-based performance metrics to systematically analyze facility security postures and identify appropriately scaled and fiscally responsible countermeasures based on current and projected threats. The AcuTech Consulting Group developed the methodology under contract to the API, and the author was the project manager for the project. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2013.10.012},
  File                     = {Published version:Moore-2013-p1685-1689b.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Security risk management},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423013002040},
  Zone                     = {3Q}
}

@InProceedings{Mougouei-2012-p111-116,
  Title                    = {Evaluating fault tolerance in security requirements of web services},
  Author                   = {Mougouei, D. and Rahman, W.N.W.A and Moein Almasi, M.},
  Booktitle                = {Cyber Security, Cyber Warfare and Digital Forensic (CyberSec), 2012 International Conference on},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {111-116},

  Doi                      = {10.1109/CyberSec.2012.6246125},
  File                     = {Published version:Mougouei-2012-p111-116.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Web services;security of data;software fault tolerance;Web services;computational security requirement;external security faults;fault tolerance evaluation;internal security faults;security failure;security requirements;Discrete Fourier transforms;Equations;Fault tolerance;Fault tolerant systems;Mathematical model;Security;Web services;security fault;threat;vulnerability;web service},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6246125&isnumber=6246077},
  Zone                     = {-}
}

@Article{Mukashema-2014-p331-340,
  Title                    = {Automated high resolution mapping of coffee in Rwanda using an expert Bayesian network },
  Author                   = {A. Mukashema and A. Veldkamp and A. Vrieling},
  Journal                  = {International Journal of Applied Earth Observation and Geoinformation },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {331 - 340},
  Volume                   = {33},

  Abstract                 = {Abstract African highland agro-ecosystems are dominated by small-scale agricultural fields that often contain a mix of annual and perennial crops. This makes such systems difficult to map by remote sensing. We developed an expert Bayesian network model to extract the small-scale coffee fields of Rwanda from very high resolution data. The model was subsequently applied to aerial orthophotos covering more than 99% of Rwanda and on one QuickBird image for the remaining part. The method consists of a stepwise adjustment of pixel probabilities, which incorporates expert knowledge on size of coffee trees and fields, and on their location. The initial naive Bayesian network, which is a spectral-based classification, yielded a coffee map with an overall accuracy of around 50%. This confirms that standard spectral variables alone cannot accurately identify coffee fields from high resolution images. The combination of spectral and ancillary data (DEM and a forest map) allowed mapping of coffee fields and associated uncertainties with an overall accuracy of 87%. Aggregated to district units, the mapped coffee areas demonstrated a high correlation with the coffee areas reported in the detailed national coffee census of 2009 (R2&#xa0;=&#xa0;0.92). Unlike the census data our map provides high spatial resolution of coffee area patterns of Rwanda. The proposed method has potential for mapping other perennial small scale cropping systems in the East African Highlands and elsewhere. },
  Doi                      = {http://dx.doi.org/10.1016/j.jag.2014.05.005},
  File                     = {Published version:Mukashema-2014-p331-340.pdf:PDF},
  Impactfactor             = {2.176},
  ISSN                     = {0303-2434},
  Keywords                 = {Coffee},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0303243414001251},
  Zone                     = {3Q}
}

@Article{Mukherjee-2012-p119-128,
  Title                    = {Intrusion Detection using Naive Bayes Classifier with Feature Reduction },
  Author                   = {Saurabh Mukherjee and Neelam Sharma},
  Journal                  = {Procedia Technology },
  Year                     = {2012},
  Note                     = {2nd International Conference on Computer, Communication, Control and Information Technology( C3IT-2012) on February 25 - 26, 2012 },
  Number                   = {0},
  Pages                    = {119 - 128},
  Volume                   = {4},

  Abstract                 = {Intrusion detection is the process of monitoring and analyzing the events occurring in a computer system in order to detect signs of security problems. Today most of the intrusion detection approaches focused on the issues of feature selection or reduction, since some of the features are irrelevant and redundant which results lengthy detection process and degrades the performance of an intrusion detection system (IDS). The purpose of this study is to identify important reduced input features in building \{IDS\} that is computationally efficient and effective. For this we investigate the performance of three standard feature selection methods using Correlation-based Feature Selection, Information Gain and Gain Ratio. In this paper we propose method Feature Vitality Based Reduction Method, to identify important reduced input features. We apply one of the efficient classifier naive bayes on reduced datasets for intrusion detection. Empirical results show that selected reduced attributes give better performance to design \{IDS\} that is efficient and effective for network intrusion detection. },
  Doi                      = {http://dx.doi.org/10.1016/j.protcy.2012.05.017},
  File                     = {Published version:Mukherjee-2012-p119-128.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2212-0173},
  Keywords                 = {Network Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2212017312002964},
  Zone                     = {-}
}

@Article{Murphy-1989-p28-31,
  Title                    = {Aging aircraft: too old to fly?},
  Author                   = {Murphy, E.E.},
  Journal                  = {Spectrum, IEEE},
  Year                     = {1989},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {28-31},
  Volume                   = {26},

  Abstract                 = {The problem of metal fatigue of aircraft is discussed in the context of risk analysis and assessment. The steps taken by aircraft designers to minimize risk are examined. A major defect of fatigue testing, which is its failure to take into account the effects of corrosion on fatigue cracks, is addressed. The pursuit of more effective methods of nondestructive testing is noted. The role played by fatigue and corrosion in the Aloha Airlines accident of April 28, 1988, in which a flight attendant was lost when a 5.5 meter-long portion of the front fuselage of a Boeing 737 ripped off during flight, is cited throughout, and Boeing's approach to the problem is questioned.<>},
  Doi                      = {10.1109/6.29336},
  File                     = {Published version:Murphy-1989-p28-31.pdf:PDF},
  Impactfactor             = {1.322},
  ISSN                     = {0018-9235},
  Keywords                 = {aircraft;corrosion testing;nondestructive testing;reliability;safety;Boeing 737;accident;aircraft;corrosion;fatigue cracks;fatigue testing;metal fatigue;nondestructive testing;reliability;risk analysis;risk assessment;safety;Aerospace electronics;Aging;Aircraft manufacture;Corrosion;FAA;Fatigue;Industrial accidents;Manufacturing;Out of order;Risk management},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=29336&isnumber=1249},
  Zone                     = {3Q}
}

@Article{Na-2014-p499-501,
  Title                    = {Automated Knowledge-Based Neural Network Modeling for Microwave Applications},
  Author                   = {Wei Cong Na and Qi Jun Zhang},
  Journal                  = {Microwave and Wireless Components Letters, IEEE},
  Year                     = {2014},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {499-501},
  Volume                   = {24},

  Doi                      = {10.1109/LMWC.2014.2316251},
  File                     = {Published version:Na-2014-p499-501.pdf:PDF},
  Impactfactor             = {1.784},
  ISSN                     = {1531-1309},
  Keywords                 = {microwave circuits;neural nets;AMG method;automated knowledge;automated model generation method;data distribution;data generation;knowledge-based NN models;microwave applications;model structure adaptation;model training;neural network modeling;systematic framework;Adaptation models;Data models;Integrated circuit modeling;Knowledge based systems;Microwave theory and techniques;Training;Training data;Design automation;knowledge-based neural network (KBNN);modeling;optimization},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6827985&isnumber=6840864},
  Zone                     = {3Q}
}

@Article{Nait-Abdes-2008-p127-133,
  Title                    = {Detecting and avoiding wormhole attacks in wireless ad hoc networks},
  Author                   = {Nait-Abdesselam, F. and Bensaou, B. and Taleb, T.},
  Journal                  = {Communications Magazine, IEEE},
  Year                     = {2008},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {127-133},
  Volume                   = {46},

  Abstract                 = {A particularly severe attack on routing protocols in ad hoc networks is the so-called worm- hole attack in which two or more colluding attackers record packets at one location, and tunnel them to another location for replay at that remote location. When this attack targets specifically routing control packets, the nodes that are close to the attackers are shielded from any alternative routes with more than one or two hops to the remote location. All routes are thus directed to the wormhole established by the attackers. In the optimized link state routing protocol, if a wormhole attack is launched during the propagation of link state packets, the wrong link information percolates throughout the network, leading to routing disruption. In this article we devise an efficient method to detect and avoid wormhole attacks in the OLSR protocOLSR protocolol. This method first attempts to pinpoint links that may potentially be part of a wormhole tunnel. Then a proper wormhole detection mechanism is applied to suspicious links by means of an exchange of encrypted probing packets between the two supposed neighbors (endpoints of the wormhole). The proposed solution exhibits several advantages, among which are its nonreliance on any time synchronization or location information, and its high detection rate under various scenarios.},
  Doi                      = {10.1109/MCOM.2008.4481351},
  File                     = {Published version:Nait-Abdes-2008-p127-133.pdf:PDF},
  Impactfactor             = {3.661},
  ISSN                     = {0163-6804},
  Keywords                 = {ad hoc networks;routing protocols;OLSR protocol;colluding attackers record packets;routing protocols;synchronization;wireless ad hoc networks;worm-hole attack;wormhole tunnel;Ad hoc networks;Delay;Directional antennas;Frequency synchronization;Hardware;Mobile ad hoc networks;Optical propagation;Protection;Routing protocols;Upper bound},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4481351&isnumber=4481327},
  Zone                     = {1Q}
}

@Article{Nava-2004-p657-662,
  Title                    = {A probabilistic approach for secondary arc risk assessment},
  Author                   = {Nava, J.H.V. and Rivas, R.A. and Urdaneta, A.J.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2004},

  Month                    = {April},
  Number                   = {2},
  Pages                    = {657-662},
  Volume                   = {19},

  Doi                      = {10.1109/TPWRD.2003.820426},
  File                     = {Published version:Nava-2004-p657-662.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {EMTP;arcs (electric);probability;risk management;EMTP-type simulations;deterministic model;electromagnetic transients program;extinction time calculations;pessimistic characteristic;probabilistic approach;probability distribution;reclosing time;secondary arc reignition voltage curves;secondary arc risk assessment;single-pole reclosing time schemes;single-pole switching;Circuit faults;Delay;Insulation;Power system security;Power system transients;Power transmission lines;Risk management;Time measurement;Voltage;Weather forecasting},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {A，次生弧的风险评估，不相关},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1278423&isnumber=28583},
  Zone                     = {3Q}
}

@Article{Navarro-1996-p685-690,
  Title                    = {Failure-rate functions for doubly-truncated random variables},
  Author                   = {Navarro, J. and Ruiz, J.M.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1996},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {685-690},
  Volume                   = {45},

  Doi                      = {10.1109/24.556594},
  File                     = {Published version:Navarro-1996-p685-690.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {estimation theory;failure analysis;random processes;Kaplan-Meier estimators;continuous random variables;cumulative-hazard estimators;discrete random variables;distribution function;doubly-truncated random variables;failure rate function;generalized function;nonparametric estimators;Distribution functions;Hazards;Random variables},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=556594&isnumber=12134},
  Zone                     = {2Q}
}

@Article{Navarro-1996-p685-690a,
  Title                    = {Failure-rate functions for doubly-truncated random variables},
  Author                   = {Navarro, J. and Ruiz, J.M.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1996},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {685-690},
  Volume                   = {45},

  Abstract                 = {The failure rate function is generalized for doubly-truncated random variables. This generalized function is used to characterize the distribution function and to obtain the properties that any function must have to be a generalized failure rate function, both for continuous and discrete random variables. From the theoretical results are constructed nonparametric estimators of the Kaplan-Meier type and cumulative-hazard type},
  Doi                      = {10.1109/24.556594},
  File                     = {Published version:Navarro-1996-p685-690a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {estimation theory;failure analysis;random processes;Kaplan-Meier estimators;continuous random variables;cumulative-hazard estimators;discrete random variables;distribution function;doubly-truncated random variables;failure rate function;generalized function;nonparametric estimators;Distribution functions;Hazards;Random variables},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=556594&isnumber=12134},
  Zone                     = {2Q}
}

@Article{Neri-2008-p397-415,
  Title                    = {Developing an Event Tree for probabilistic hazard and risk assessment at Vesuvius },
  Author                   = {A. Neri and W.P. Aspinall and R. Cioni and A. Bertagnini and P.J. Baxter and G. Zuccaro and D. Andronico and S. Barsotti and P.D. Cole and T. Esposti Ongaro and T.K. Hincks and G. Macedonio and P. Papale and M. Rosi and R. Santacroce and G. Woo},
  Journal                  = {Journal of Volcanology and Geothermal Research },
  Year                     = {2008},
  Note                     = {Evaluating Explosive Eruption Risk at European Volcanoes Contribution from the \{EXPLORIS\} Project },
  Number                   = {3},
  Pages                    = {397 - 415},
  Volume                   = {178},

  Abstract                 = {Probabilistic characterizations of possible future eruptive scenarios at Vesuvius volcano are elaborated and organized within a risk-based framework. In the \{EXPLORIS\} project, a wide variety of topics relating to this basic problem have been pursued: updates of historical data, reinterpretation of previous geological field data and the collection of new fieldwork results, the development of novel numerical modelling codes and of risk assessment techniques have all been completed. To achieve coherence, many diverse strands of evidence had to be unified within a formalised structure, and linked together by expert knowledge. For this purpose, a Vesuvius ‘Event Tree’ (ET) was created to summarise in a numerical-graphical form, at different levels of detail, all the relative likelihoods relating to the genesis and style of eruption, development and nature of volcanic hazards, and the probabilities of occurrence of different volcanic risks in the next eruption crisis. The Event Tree formulation provides a logical pathway connecting generic probabilistic hazard assessment to quantitative risk evaluation. In order to achieve a complete parameterization for this all-inclusive approach, exhaustive hazard and risk models were needed, quantified with comprehensive uncertainty distributions for all factors involved, rather than simple ‘best-estimate’ or nominal values. Thus, a structured expert elicitation procedure was implemented to complement more traditional data analysis and interpretative approaches. The structure of the Vesuvius Event Tree is presented, and some of the data analysis findings and elicitation outcomes that have provided initial indicative probability distributions to be associated with each of its branches are summarized. The Event Tree extends from initiating volcanic eruption events and hazards right through to human impact and infrastructure consequences, with the complete tree and its parameterisation forming a quantitative synoptic framework for comprehensive hazard evaluation and mapping of risk impacts. The organization of the Event Tree allows easy updating, as and when new information becomes available. },
  Doi                      = {http://dx.doi.org/10.1016/j.jvolgeores.2008.05.014},
  File                     = {Published version:Neri-2008-p397-415.pdf:PDF},
  Impactfactor             = {2.193},
  ISSN                     = {0377-0273},
  Keywords                 = {Vesuvius},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，火山爆发概率},
  Timestamp                = {2015.01.14},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0377027308002655},
  Zone                     = {3Q}
}

@Article{Ni-2003-p1165-1172,
  Title                    = {Software implementation of online risk-based security assessment},
  Author                   = {Ming Ni and McCalley, J.D. and Vittal, V. and Greene, S. and Chee-Wooi Ten and Ganugula, V.S. and Tayyib, T.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {1165-1172},
  Volume                   = {18},

  Abstract                 = {This paper describes software implementation for online risk-based security assessment which computes indices based on probabilistic risk for use by operators in the control room to assess system security levels as a function of existing and near-future network conditions. The paper focuses on speed enhancement techniques that are essential for online application and result visualization methods that offer clear and meaningful ways to enhance human assimilation and comprehension of security levels. Results of testing on a series of 1600 bus power flow models retrieved from the energy management system of a large US utility are presented and serve to illustrate the benefits of the software.},
  Doi                      = {10.1109/TPWRS.2003.814909},
  File                     = {Published version:Ni-2003-p1165-1172.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {energy management systems;load flow;power system analysis computing;power system security;probability;risk management;US utility;bus power flow models;control room;energy management system;human assimilation enhancement;near-future network conditions;online risk-based security assessment;probabilistic risk;result visualization methods;software implementation;Application software;Computer networks;Control systems;Humans;Load flow;Power system modeling;Power system security;Software testing;System testing;Visualization},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，开发了风险可视化软件},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1216160&isnumber=27352},
  Zone                     = {2Q}
}

@InProceedings{Nie-2008-p1-4,
  Title                    = {The basis functions involving propagating wave phase dependency for solving the scattering from electrically large targets},
  Author                   = {Zaiping Nie and Su Yan and Shiquan He and Jun Hu},
  Booktitle                = {Antennas and Propagation Society International Symposium, 2008. AP-S 2008. IEEE},
  Year                     = {2008},
  Month                    = {July},
  Pages                    = {1-4},

  Abstract                 = {Expressing the traveling wave phase dependency, the use of PE bases is able to reduce the number of unknowns dramatically, and lead to the sparsification of the impedance matrix, which will be reported in our other paper. Secondly, the PE bases cause better condition number and faster convergence rate in iteration solution because of showing intrinsic property of the induced current. Thirdly, the PE bases have the excellent characteristics for wide band analysis because the bases include the frequency (phase) factor. Finally, the PE bases result in efficient interpolation in angle domain because the amplitude factor of PE bases varies slowly while the incident (or scattering) angle changes. All those abilities are very valuable for solutions of the electrically large problems, particularly for electrically large scatterer with smooth convex surface. As a conclusion, describing the phase variation of the induced current successfully, the PE bases can be defined on the electrically large patches with the size up to 1.5lambda-2.0lambda and even larger. A lot of challenging real world problem (the scattering from electrically large and extra large complex 3D objects, for example) can be simply solved with high efficiency and satisfied accuracy by using Method of Moment.},
  Doi                      = {10.1109/APS.2008.4618950},
  File                     = {Published version:Nie-2008-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {convergence of numerical methods;electromagnetic wave propagation;electromagnetic wave scattering;impedance matrix;interpolation;iterative methods;method of moments;Method of Moment;amplitude factor;convergence rate;electrically large targets;impedance matrix;interpolation;iteration solution;phase extracted basis functions;propagating wave phase dependency;smooth convex surface;traveling wave scattering;wide band analysis;Extrapolation;Frequency;Helium;Integral equations;Magnetic fields;Magnetic properties;Microwave propagation;Optical scattering;Optical surface waves;Surface waves},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4618950&isnumber=4618896},
  Zone                     = {-}
}

@Article{Niskanen-2012-p535-543,
  Title                    = {Results of Finnish national survey in the chemical industry on \{EU\} legislation concerned with risk assessment and safety compliance climate },
  Author                   = {Toivo Niskanen},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {535 - 543},
  Volume                   = {25},

  Abstract                 = {A questionnaire study about the Finnish chemical industry was devised to explore the implementation of the risk assessment obligations of the \{EU\} Chemical Agents Directive (98/24/EC), and the role of risk assessment activities as a part of \{OSH\} management. The quantitative method involved an assessment of online questionnaire. The respondents were employers' (N = 49) and workers' (N = 105) \{OSH\} representatives from different companies'. The regression analysis of the responses indicated that prioritizations in the risk assessment had a highly significant (p &lt; 0.001) effect both on the prevention measures in the management and on the safe use of chemicals. The present results showed that development measures have to be directed at the most important problems ensuring that sub-contractors carry out hazard identification and arranging their on-the-job training so that it matches the chemical plant enterprise's own workers. Furthermore, the results of risk assessment should be used in the planning and management of the work, in the development of the supervisors' leadership skills as well as when following up the improvements.},
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2011.12.010},
  File                     = {Published version:Niskanen-2012-p535-543.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {\{EU\}},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，芬兰化工工业立法调查，无关文章},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095042301100218X},
  Zone                     = {3Q}
}

@Article{Niskanen-2012-p535-543a,
  Title                    = {Results of Finnish national survey in the chemical industry on \{EU\} legislation concerned with risk assessment and safety compliance climate },
  Author                   = {Toivo Niskanen},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {535 - 543},
  Volume                   = {25},

  Abstract                 = {A questionnaire study about the Finnish chemical industry was devised to explore the implementation of the risk assessment obligations of the \{EU\} Chemical Agents Directive (98/24/EC), and the role of risk assessment activities as a part of \{OSH\} management. The quantitative method involved an assessment of online questionnaire. The respondents were employers' (N = 49) and workers' (N = 105) \{OSH\} representatives from different companies'. The regression analysis of the responses indicated that prioritizations in the risk assessment had a highly significant (p &lt; 0.001) effect both on the prevention measures in the management and on the safe use of chemicals. The present results showed that development measures have to be directed at the most important problems ensuring that sub-contractors carry out hazard identification and arranging their on-the-job training so that it matches the chemical plant enterprise's own workers. Furthermore, the results of risk assessment should be used in the planning and management of the work, in the development of the supervisors' leadership skills as well as when following up the improvements. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2011.12.010},
  File                     = {Published version:Niskanen-2012-p535-543a.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {\{EU\}},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095042301100218X},
  Zone                     = {3Q}
}

@Article{Njah-2015-p404-416,
  Title                    = {Weighted ensemble learning of Bayesian network for gene regulatory networks },
  Author                   = {Hasna Njah and Salma Jamoussi},
  Journal                  = {Neurocomputing },
  Year                     = {2015},
  Note                     = {Special Issue on Information Processing and Machine Learning for Applications of Engineering Selected papers from the \{IEEE\} 17th International Conference on Intelligent Engineering Systems (INES’13) Solving Complex Machine Learning Problems with Ensemble Methods Visual Analytics using Multidimensional Projections Selected papers from the Workshop on Visual Analytics using Multidimensional Projections, held at EuroVis 2013 },
  Number                   = {0},
  Pages                    = {404 - 416},
  Volume                   = {150, Part B},

  Abstract                 = {Abstract Gene Regulatory Network (GRN) is known as the most adequate representation of genes׳ interactions based on microarray datasets. One of the most performing modeling tools that enable the inference of these networks is a Bayesian network (BN). When preceded by an efficient pre-processing step, \{BN\} learning can unveil possible relationships between key disease genes and allows biologists to analyze these interactions and to exploit them. However, the layout of microarray data is different from classic data. This particularity engenders challenges to \{BN\} learning in terms of dimensionality and data over-fitting. In this paper, we propose a fuzzy ensemble clustering method that allows outputting small and highly inter-correlated partitions of genes so that we can overcome dimensionality problem. We present a weighted committee based structure algorithm for learning \{BNs\} of each partition without over-fitting training dataset. Moreover, we offer an approach for assembling the sub-BNs through genes in common. We also statistically verify and biologically validate our approach. },
  Doi                      = {http://dx.doi.org/10.1016/j.neucom.2014.05.078},
  File                     = {Published version:Njah-2015-p404-416.pdf:PDF},
  Impactfactor             = {1.634},
  ISSN                     = {0925-2312},
  Keywords                 = {Ensemble learning},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925231214012260},
  Zone                     = {3Q}
}

@Article{Noori-2013-p229-234,
  Title                    = {Fuzzy Reliability-Based Traction Control Model for Intelligent Transportation Systems},
  Author                   = {Noori, K. and Jenab, K.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {229-234},
  Volume                   = {43},

  Abstract                 = {In this paper, a fuzzy Bayesian traction control system was developed for rail vehicles with speed sensors in intelligent transportation systems. The system included three main components to sense, process, and classify the traction conditions. The information received from the speed sensors is used to avoid any error that might cause service interruption and unnecessary maintenance. There are, however, occasions when these signals may not be sensed, transmitted, or received precisely due to unexpected conditions such as noise. Therefore, in this study, the γ-level fuzzy Bayesian model was proposed for sensor-based traction control systems. In order to apply the fuzzy Bayesian concept, the wheel acceleration was assumed to be a fuzzy random variable for membership function with fuzzy prior distribution. Using the fuzzy signals, the intelligent model calculates the risk of classification for the system that results in determining the misclassification decision at a minimum cost. The model's engine involves a mathematical problem which can be solved in any programming language in onboard or embedded computers. The conceptual model was applied to a case study with promising results, which can be used for target systems or simulation.},
  Doi                      = {10.1109/TSMCA.2012.2204047},
  File                     = {Published version:Noori-2013-p229-234.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Bayes methods;control system synthesis;fuzzy control;fuzzy set theory;intelligent control;rails;railway engineering;random processes;reliability;sensors;traction;transportation;vehicle dynamics;velocity measurement;wheels;γ-level fuzzy Bayesian model;embedded computers;fuzzy Bayesian traction control system;fuzzy prior distribution;fuzzy random variable;fuzzy reliability-based traction control model;fuzzy signals;intelligent model;intelligent transportation systems;mathematical problem;membership function;misclassification decision;onboard computers;programming language;rail vehicles;sensor-based traction control systems;service interruption;speed sensors;traction condition classification;traction condition processing;wheel acceleration;Acceleration;Bayesian methods;Mathematical model;Noise;Sensors;Vehicles;Wheels;Computer-based transportation system;fuzzy Bayesian decision theory;intelligent systems;rail transit system;traction control systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6380603&isnumber=6356011},
  Zone                     = {-}
}

@Article{Office-1999-p-,
  Title                    = {Information Security Risk Assessment: Practices of Leading Organizations},
  Author                   = {US Government Accountability Office},
  Year                     = {1999},

  File                     = {Published version:Office-1999-p-.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.21},
  Url                      = {http://www.gao.gov/special.pubs/ai00033.pdf},
  Zone                     = {-}
}

@InProceedings{Oke-2007-p1-6,
  Title                    = {Detecting Denial of Service Attacks with Bayesian Classifiers and the Random Neural Network},
  Author                   = {Oke, G. and Loukas, G. and Gelenbe, E.},
  Booktitle                = {Fuzzy Systems Conference, 2007. FUZZ-IEEE 2007. IEEE International},
  Year                     = {2007},
  Month                    = {July},
  Pages                    = {1-6},

  Doi                      = {10.1109/FUZZY.2007.4295666},
  Impactfactor             = {-},
  ISSN                     = {1098-7584},
  Keywords                 = {Bayes methods;neural nets;security of data;Bayesian classifiers;denial of service attack;likelihood estimation;network resource;protection;random neural network;Bayesian methods;Communication system traffic control;Computer crime;Continuous wavelet transforms;Neural networks;Proposals;Protection;Recurrent neural networks;Telecommunication traffic;Testing},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4295666&isnumber=4295329},
  Zone                     = {-}
}

@InProceedings{Okubo-2012-p367-372,
  Title                    = {Mutual Refinement of Security Requirements and Architecture Using Twin Peaks Model},
  Author                   = {Okubo, T. and Kaiya, H. and Yoshioka, N.},
  Booktitle                = {Computer Software and Applications Conference Workshops (COMPSACW), 2012 IEEE 36th Annual},
  Year                     = {2012},
  Month                    = {July},
  Pages                    = {367-372},

  Doi                      = {10.1109/COMPSACW.2012.72},
  File                     = {Published version:Okubo-2012-p367-372.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {formal specification;security of data;software architecture;TMP-SA;architecture-independent security issues;architecture-specific security issues;iterative development;mutual refinement;security architecture;security countermeasures;software architecture;software security requirement specification;twin peaks model application for security analysis;Analytical models;Computer architecture;Digital signatures;Java;Software;Spirals;application security;software requirements engineering;twin peaks model},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6341603&isnumber=6341540},
  Zone                     = {-}
}

@Article{Oliner-1969-p812-826,
  Title                    = {Microwave Network Methods for Guided Elastic Waves},
  Author                   = {Oliner, A.A.},
  Journal                  = {Microwave Theory and Techniques, IEEE Transactions on},
  Year                     = {1969},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {812-826},
  Volume                   = {17},

  Abstract                 = {The possibility of obtaining true microminiaturization by the use of elastic wave circuitry on solids is hampered by the lack of knowledge regarding the behavior of the constituents of those circuits. Since boundary value problems involving elastic waves in solids are generally very intricate and difficult to solve, a direct frontal attack on those problems will in many cases lead to frustration. In this paper, a series of steps is outlined which avoids the frontal attack and lends itself to a systematic procedure for achieving the understanding sought. It involves the application of concepts and techniques of proven value in electromagnetic microwaves to corresponding categories of problems in elastic guided waves. To demonstrate the value of this approach, it is used to derive the properties of several well-known types of elastic wave on layered media, such as Rayleigh surface waves, leaky Rayleigh waves, Lamb waves, and Love waves. In the building-block approach employed, the results derived separately include transmission-line models for body waves in fluids and isotropic solids, with expressions for the characteristic impedances and the velocity and stress vector mode functions, and equivalent networks for several types of interface which are constituents of the layered media mentioned above. The propagation properties of the guided waves are then obtained by the use of the transverse resonance procedure in a systematic, simple, and direct fashion.},
  Doi                      = {10.1109/TMTT.1969.1127071},
  File                     = {Published version:Oliner-1969-p812-826.pdf:PDF},
  Impactfactor             = {2.229},
  ISSN                     = {0018-9480},
  Keywords                 = {Boundary value problems;Circuits;Electromagnetic scattering;Microwave theory and techniques;Nonhomogeneous media;Solid modeling;Stress;Surface impedance;Surface waves;Transmission lines},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1127071&isnumber=24951},
  Zone                     = {2Q}
}

@InProceedings{Oman-2000-p-,
  Title                    = {Concerns about intrusions into remotely accessible substation controllers and SCADA systems},
  Author                   = {Oman, Paul and Schweitzer, Edmund and Frincke, Deborah},
  Booktitle                = {Proceedings of the Twenty-Seventh Annual Western Protective Relay Conference},
  Year                     = {2000},
  Organization             = {Citeseer},
  Volume                   = {160},

  File                     = {Oman-2000-p-.pdf:Oman-2000-p-.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.21},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6519&rep=rep1&type=pdf},
  Zone                     = {-}
}

@InProceedings{Pallotti-2011-p1-4,
  Title                    = {Smart grid cyber security requirements},
  Author                   = {Pallotti, E. and Mangiatordi, F.},
  Booktitle                = {Environment and Electrical Engineering (EEEIC), 2011 10th International Conference on},
  Year                     = {2011},
  Month                    = {May},
  Pages                    = {1-4},

  Doi                      = {10.1109/EEEIC.2011.5874822},
  File                     = {Published version:Pallotti-2011-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {distribution networks;environmental factors;power system security;security of data;smart power grids;communication system;environmental impact;intelligent energy distribution network;power distribution network;smart energy network;smart grid cyber security;smart power grid;Companies;Information security;Power system reliability;Smart grids;Cyber Security;Electrical power grid;Security Standards;Smart Grid},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5874822&isnumber=5874563},
  Zone                     = {-}
}

@Article{Paltrinier-2014-p669-679,
  Title                    = {Dynamic approach to risk management: Application to the Hoeganaes metal dust accidents },
  Author                   = {Nicola Paltrinieri and Faisal Khan and Paul Amyotte and Valerio Cozzani},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2014},
  Number                   = {6},
  Pages                    = {669 - 679},
  Volume                   = {92},

  Abstract                 = {Abstract Several major accidents caused by metal dusts were recorded in the past few years. For instance, in 2011, three accidents caused by iron dust killed five workers at the Hoeganaes Corp. facility in Gallatin, Tennessee (USA). In order to prevent such accidents, a dynamic approach to risk management was defined in this study. The method is able to take into account new risk notions and early warnings and to systematically update the related risk. It may be applied not only in the design phase of a system, but also throughout the system lifetime as a support to a more precise and robust decision making process. The synergy of two specific techniques for hazard identification and risk assessment was obtained: the Dynamic Procedure for Atypical Scenarios Identification (DyPASI) and the Dynamic Risk Assessment (DRA) methods. To demonstrate its effectiveness, this approach was applied to the analysis of Gallatin metal dust accidents. The application allowed collecting a number of risk notions related to the plant, equipment and materials used. The analysis of risk notions by means of this dynamic approach could have led to enhanced hazard identification and dynamic real-time risk assessment. However, the approach described is effective only if associated to a proper safety culture, in order to produce an appropriate and robust decision making response to emerging risk issues. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2013.11.008},
  File                     = {Published version:Paltrinier-2014-p669-679.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Dynamic risk management},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095758201300089X},
  Zone                     = {3Q}
}

@InProceedings{Paluri-2005-p6561-6566,
  Title                    = {Extrapolation based interval approach to compute the frequency responses of non-rational transfer functions with nonlinear parametric dependencies},
  Author                   = {Paluri, N.S.V. and Sondur, S.},
  Booktitle                = {Decision and Control, 2005 and 2005 European Control Conference. CDC-ECC '05. 44th IEEE Conference on},
  Year                     = {2005},
  Month                    = {Dec},
  Pages                    = {6561-6566},

  Abstract                 = {We consider the problem of computing the frequency response of a non-rational transfer function having uncertainties in the parameters. A complete characterization of the frequency plots of such a transfer function is provided and an efficient and guaranteed algorithm for computing the envelope of the Bode plot using interval analysis (IA) is devised. In particular, it is shown that the range enclosure property of the interval analysis can be used to compute the Bode envelopes of the non-rational transfer function with parametric dependencies. The range enclosures computed using IA are guaranteed to contain the minima and maxima of the function over a given box. The IA is combined with uniform subdivision technique to obtain the range enclosures of desired accuracy. We also show, how the well known Richardson extrapolation technique helps in accelerating the convergence process of the algorithm.},
  Doi                      = {10.1109/CDC.2005.1583215},
  File                     = {Published version:Paluri-2005-p6561-6566.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Bode plot;Extrapolation;Frequency response;Interval analysis;Linear systems;Robust control;Uncertain systems;Acceleration;Algorithm design and analysis;Control engineering;Convergence;Extrapolation;Frequency domain analysis;Frequency response;Robust control;Transfer functions;Uncertainty;Bode plot;Extrapolation;Frequency response;Interval analysis;Linear systems;Robust control;Uncertain systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1583215&isnumber=33412},
  Zone                     = {-}
}

@Article{Park-2014-p123-136,
  Title                    = {How coupon and element tests reduce conservativeness in element failure prediction },
  Author                   = {Chan Y. Park and Nam H. Kim and Raphael T. Haftka},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {123 - 136},
  Volume                   = {123},

  Abstract                 = {Abstract Structural elements, such as stiffened panels, are designed by combining material strength data obtained from coupon tests with a failure theory for 3D stress field. Material variability is captured by dozens of coupon tests, but there remains epistemic uncertainty due to error in the failure theory, which can be reduced by element tests. Conservativeness to compensate for the uncertainty in failure prediction (as in the A- or B-basis allowables) results in a weight penalty. A key question, addressed here, is what weight penalty is associated with this conservativeness and how much it can be reduced by using coupon and element tests. In this paper, a probabilistic approach is used to estimate the conservative element failure strength by quantifying uncertainty in the element strength prediction. A convolution integral is used to efficiently combine uncertainty from coupon tests and that from the failure theory. Bayesian inference is then employed to reduce the epistemic uncertainty using element test results. The methodology is examined with typical values of material variability (7%), element test variability (3%), and the error in the failure theory (5%). It is found that the weight penalty associated with no element test is significant (20% heavier than an infinite number of element tests), and it is greatly reduced by more element tests (4.5% for 5 element tests), but the effect of the number of coupon tests is much smaller. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2013.10.012},
  File                     = {Published version:Park-2014-p123-136.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Failure prediction},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832013002962},
  Zone                     = {2Q}
}

@InProceedings{Parrondo-2014-p1-27,
  Title                    = {Industrial cyber security solutions for the connected enterprise},
  Author                   = {Parrondo, L.},
  Booktitle                = {Cyber Security for Industrial Control Systems, IET Seminar on},
  Year                     = {2014},
  Pages                    = {1--27},

  Doi                      = {10.1049/ic.2014.0012},
  File                     = {Published version:Parrondo-2014-p1-27.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6845061},
  Zone                     = {-}
}

@Article{Patel-2013-p25-41,
  Title                    = {An intrusion detection and prevention system in cloud computing: A systematic review },
  Author                   = {Ahmed Patel and Mona Taghavi and Kaveh Bakhtiyari and Joaquim Celestino Júnior},
  Journal                  = {Journal of Network and Computer Applications },
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {25 - 41},
  Volume                   = {36},

  Abstract                 = {The distributed and open structure of cloud computing and services becomes an attractive target for potential cyber-attacks by intruders. The traditional Intrusion Detection and Prevention Systems (IDPS) are largely inefficient to be deployed in cloud computing environments due to their openness and specific essence. This paper surveys, explores and informs researchers about the latest developed \{IDPSs\} and alarm management techniques by providing a comprehensive taxonomy and investigating possible solutions to detect and prevent intrusions in cloud computing systems. Considering the desired characteristics of \{IDPS\} and cloud computing systems, a list of germane requirements is identified and four concepts of autonomic computing self-management, ontology, risk management, and fuzzy theory are leveraged to satisfy these requirements. },
  Doi                      = {http://dx.doi.org/10.1016/j.jnca.2012.08.007},
  File                     = {Published version:Patel-2013-p25-41.pdf:PDF},
  Impactfactor             = {1.467},
  ISSN                     = {1084-8045},
  Keywords                 = {Intrusion detection and prevention},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S108480451200183X},
  Zone                     = {3Q}
}

@Article{Paul-2014-p165-181,
  Title                    = {Unifying traditional risk assessment approaches with attack trees },
  Author                   = {Stéphane Paul and Raphaël Vignon-Davillier},
  Journal                  = {Journal of Information Security and Applications },
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {165 - 181},
  Volume                   = {19},

  Abstract                 = {Abstract As software-intensive systems become more and more complex, so does the assessment of the risks that these systems may have on people's businesses, privacy, livelihoods, and very lives. For very large long-lived industrial programmes, such as the Galileo programme of the European Space Agency (ESA), or the French Pentagon programme for the Ministry of Defence, traditional risk management approaches are now reaching their limit. This is true for tooling, but even more so for humans. This paper proposes novel techniques to deal with cognitive scalability issues in risk assessment studies, amongst which graphical extensions to traditional risk management approaches, such as chain diagrams, and the seamless integration of attack trees. Feedback and results were collected from security experts and other stakeholders, in a large industrial context (namely, the Galileo risk assessment programme) and through dedicated research and development demonstrations. The feedback and results show effective improvements with respect to standard practices, even though fine tuning is still needed to reach an adequate and financially acceptable equilibrium between: (i) dealing with a large number of small independent problems; and (ii) maintaining an overall understanding of the system’s risks and risks treatment. },
  Doi                      = {http://dx.doi.org/10.1016/j.jisa.2014.03.006},
  File                     = {Published version:Paul-2014-p165-181.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2214-2126},
  Keywords                 = {Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S2214212614000180},
  Zone                     = {-}
}

@Article{Pearson-2012-p1026-1031,
  Title                    = {Contextual representations increase analogue traumatic intrusions: Evidence against a dual-representation account of peri-traumatic processing },
  Author                   = {David G. Pearson},
  Journal                  = {Journal of Behavior Therapy and Experimental Psychiatry },
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {1026 - 1031},
  Volume                   = {43},

  Abstract                 = {Background and objectives Information processing accounts of post-traumatic stress disorder (PTSD) state that intrusive memories emerge due to a lack of integration between perceptual and contextual trauma representations in autobiographical memory. This hypothesis was tested experimentally using an analogue trauma paradigm in which participants viewed an aversive film designed to elicit involuntary recollections. Method Participants viewed scenes from the film either paired with contextual information or with the contextual information omitted. After viewing the film participants were asked to record for one week any involuntary intrusions for the film using a provided intrusions diary. Results The results revealed a significant increase in analogue intrusions for the film when viewed with contextual information in comparison to when the film was viewed with the contextual information omitted. In contrast there was no effect of contextual information on valence ratings or voluntary memory for the film, or on the reported vividness and emotionality of the intrusions. Limitations The analogue trauma paradigm may have failed to reproduce the effect of extreme stress on encoding that is postulated to occur during PTSD. Conclusions The findings have potential implications for trauma intervention as they suggest that the contextual understanding of a scene during encoding can be integral to the subsequent occurrence of traumatic intrusions. The pattern of results found in the study are inconsistent with dual-representation accounts of intrusive memory formation, and instead provide new evidence that contextual representations play a casual role in increasing the frequency of involuntary intrusions for traumatic material. },
  Doi                      = {http://dx.doi.org/10.1016/j.jbtep.2012.04.002},
  File                     = {Published version:Pearson-2012-p1026-1031.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0005-7916},
  Keywords                 = {Intrusive memories},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0005791612000365},
  Zone                     = {-}
}

@InProceedings{Peng-2013-p442-447,
  Title                    = {Cyber-physical System Risk Assessment},
  Author                   = {Yong Peng and Tianbo Lu and Jingli Liu and Yang Gao and Xiaobo Guo and Feng Xie},
  Booktitle                = {Intelligent Information Hiding and Multimedia Signal Processing, 2013 Ninth International Conference on},
  Year                     = {2013},
  Month                    = {Oct},
  Pages                    = {442-447},

  Doi                      = {10.1109/IIH-MSP.2013.116},
  File                     = {Published version:Peng-2013-p442-447.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {critical infrastructures;risk management;CPS risk assessment;CPS security status;IT system security;chemical industry;critical national infrastructure;cyber physical system security;cyber systems;cyber-physical system risk assessment;electric power;petroleum industry;risk assessment idea;risk assessment method;safeguard resources;security features;well-designed risk assessment;Computer crime;Data communication;Information security;Quantization (signal);Risk management;Software;CPS;attack tree;risk assessment;simulation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6846672&isnumber=6846555},
  Zone                     = {-}
}

@Article{Penna-2012-p1806-1822,
  Title                    = {Detecting and Counteracting Statistical Attacks in Cooperative Spectrum Sensing},
  Author                   = {Penna, F. and Yifan Sun and Dolecek, L. and Cabric, D.},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {April},
  Number                   = {4},
  Pages                    = {1806-1822},
  Volume                   = {60},

  Abstract                 = {In this paper we propose a novel Bayesian method to improve the robustness of cooperative spectrum sensing against misbehaving secondary users, which may send wrong sensing reports in order to artificially increase or reduce the throughput of a cognitive network. We adopt a statistical attack model in which every malicious node is characterized by a certain probability of attack. The key features of the proposed method are: (i) combined spectrum sensing, identification of malicious users, and estimation of their attack probabilities; (ii) use of belief propagation on factor graphs to efficiently solve the Bayesian estimation problem. Our analysis shows that the proposed joint estimation approach outperforms traditional cooperation schemes based on exclusion of the unreliable nodes from the spectrum sensing process, and that it nearly achieves the performance of an ideal maximum likelihood estimation if attack probabilities remain constant over a sufficient number of sensing time slots. Results illustrate that belief propagation applied to the considered problem is robust with respect to different network parameters (e.g., numbers of reliable and malicious nodes, attack probability values, sensing duration). Finally, spectrum sensing estimates obtained via belief propagation are proved to be consistent on average for arbitrary graph size.},
  Doi                      = {10.1109/TSP.2011.2181839},
  File                     = {Published version:Penna-2012-p1806-1822.pdf:PDF},
  Impactfactor             = {2.813},
  ISSN                     = {1053-587X},
  Keywords                 = {Bayes methods;cognitive radio;cooperative communication;graph theory;maximum likelihood estimation;radio spectrum management;telecommunication security;Bayesian estimation problem;arbitrary graph size;attack probability estimation;belief propagation;cognitive network;combined spectrum sensing;cooperative spectrum sensing;factor graph;joint estimation approach;malicious user identification;maximum likelihood estimation;statistical attack counteracting;statistical attack detection;Bayesian methods;Belief propagation;Estimation;Mathematical model;Sensors;Vectors;Bayesian inference;belief propagation;cognitive radio;information fusion;multisensor signal detection;network security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6112695&isnumber=6165492},
  Zone                     = {2Q}
}

@Article{Pereira-2012-p1226-1234,
  Title                    = {An Optimum-Path Forest framework for intrusion detection in computer networks },
  Author                   = {Clayton R. Pereira and Rodrigo Y.M. Nakamura and Kelton A.P. Costa and João P. Papa},
  Journal                  = {Engineering Applications of Artificial Intelligence },
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {1226 - 1234},
  Volume                   = {25},

  Abstract                 = {Intrusion detection systems that make use of artificial intelligence techniques in order to improve effectiveness have been actively pursued in the last decade. However, their complexity to learn new attacks has become very expensive, making them inviable for a real time retraining. In order to overcome such limitations, we have introduced a new pattern recognition technique called optimum-path forest (OPF) to this task. Our proposal is composed of three main contributions: to apply \{OPF\} for intrusion detection, to identify redundancy in some public datasets and also to perform feature selection over them. The experiments have been carried out on three datasets aiming to compare \{OPF\} against Support Vector Machines, Self Organizing Maps and a Bayesian classifier. We have showed that \{OPF\} has been the fastest classifier and the always one with the top results. Thus, it can be a suitable tool to detect intrusions on computer networks, as well as to allow the algorithm to learn new attacks faster than other techniques. },
  Doi                      = {http://dx.doi.org/10.1016/j.engappai.2012.03.008},
  File                     = {Published version:Pereira-2012-p1226-1234.pdf:PDF},
  Impactfactor             = {1.625},
  ISSN                     = {0952-1976},
  Keywords                 = {Intrusion detection system},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0952197612000644},
  Zone                     = {3Q}
}

@Article{Perkusich-2015-p437-450,
  Title                    = {A procedure to detect problems of processes in software development projects using Bayesian networks },
  Author                   = {Mirko Perkusich and Gustavo Soares and Hyggo Almeida and Angelo Perkusich},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2015},
  Number                   = {1},
  Pages                    = {437 - 450},
  Volume                   = {42},

  Abstract                 = {Abstract There are several software process models and methodologies such as waterfall, spiral and agile. Even so, the rate of successful software development projects is low. Since software is the major output of software processes, increasing software process management quality should increase the project’s chances of success. Organizations have invested to adapt software processes to their environments and the characteristics of projects to improve the productivity and quality of the products. In this paper, we present a procedure to detect problems of processes in software development projects using Bayesian networks. The procedure was successfully applied to Scrum-based software development projects. The research results should encourage the usage of Bayesian networks to manage software processes and increase the rate of successful software development projects. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2014.08.015},
  File                     = {Published version:Perkusich-2015-p437-450.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Software process simulation modeling},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417414004916},
  Zone                     = {3Q}
}

@Article{Petakos-2000-p163-164,
  Title                    = {Failure coefficients and a characterization function for them},
  Author                   = {Petakos, K.I.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {163-164},
  Volume                   = {49},

  Doi                      = {10.1109/24.877333},
  File                     = {Published version:Petakos-2000-p163-164.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {consecutive system reliability;failure analysis;reliability theory;characterization function;failure coefficients calculation;general s-coherent system;k-out-of-n:F systems;parallel system;reliability structures;series system;Electric shock},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=877333&isnumber=18992},
  Zone                     = {2Q}
}

@Article{Petakos-2000-p163-164a,
  Title                    = {Failure coefficients and a characterization function for them},
  Author                   = {Petakos, K.I.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {163-164},
  Volume                   = {49},

  Abstract                 = {A function that characterizes (characterization function) the failure coefficients of a general s-coherent system, and its forms, are studied. Formulas are given for calculating the “failure coefficients” in specific reliability structures},
  Doi                      = {10.1109/24.877333},
  File                     = {Published version:Petakos-2000-p163-164a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {consecutive system reliability;failure analysis;reliability theory;characterization function;failure coefficients calculation;general s-coherent system;k-out-of-n:F systems;parallel system;reliability structures;series system;Electric shock},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Function 表示函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=877333&isnumber=18992},
  Zone                     = {2Q}
}

@Article{Petrovi-2014-p8157-8164,
  Title                    = {Risk assessment model of mining equipment failure based on fuzzy logic},
  Author                   = {Dejan V. Petrovi and Milo Tanasijevi and Vitomir Mili and Nikola Lili and Saa Stojadinovi and Igor Svrkota},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2014},
  Number                   = {18},
  Pages                    = {8157 - 8164},
  Volume                   = {41},

  Abstract                 = {Abstract The systematic maintenance of mining machinery and equipment is the crucial factor for the proper functioning of a mine without production process interruption. For high-quality maintenance of the technical systems in mining, it is necessary to conduct a thorough analysis of machinery and accompanying elements in order to determine the critical elements in the system which are prone to failures. The risk assessment of the failures of system parts leads to obtaining precise indicators of failures which are also excellent guidelines for maintenance services. This paper presents a model of the risk assessment of technical systems failure based on the fuzzy sets theory, fuzzy logic and min–max composition. The risk indicators, severity, occurrence and detectability are analyzed. The risk indicators are given as linguistic variables. The model presented was applied for assessing the risk level of belt conveyor elements failure which works in severe conditions in a coal mine. Moreover, this paper shows the advantages of this model when compared to a standard procedure of \{RPN\} calculating – in the \{FMEA\} method of risk assessment. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2014.06.042},
  File                     = {Published version:Petrovi-2014-p8157-8164.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417414003832},
  Zone                     = {3Q}
}

@Article{Petrovic-2014-p8157-8164,
  Title                    = {Risk assessment model of mining equipment failure based on fuzzy logic },
  Author                   = {Dejan V. Petrović and Miloš Tanasijević and Vitomir Milić and Nikola Lilić and Saša Stojadinović and Igor Svrkota},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2014},
  Number                   = {18},
  Pages                    = {8157 - 8164},
  Volume                   = {41},

  Abstract                 = {Abstract The systematic maintenance of mining machinery and equipment is the crucial factor for the proper functioning of a mine without production process interruption. For high-quality maintenance of the technical systems in mining, it is necessary to conduct a thorough analysis of machinery and accompanying elements in order to determine the critical elements in the system which are prone to failures. The risk assessment of the failures of system parts leads to obtaining precise indicators of failures which are also excellent guidelines for maintenance services. This paper presents a model of the risk assessment of technical systems failure based on the fuzzy sets theory, fuzzy logic and min–max composition. The risk indicators, severity, occurrence and detectability are analyzed. The risk indicators are given as linguistic variables. The model presented was applied for assessing the risk level of belt conveyor elements failure which works in severe conditions in a coal mine. Moreover, this paper shows the advantages of this model when compared to a standard procedure of \{RPN\} calculating – in the \{FMEA\} method of risk assessment. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2014.06.042},
  File                     = {Published version:Petrovic-2014-p8157-8164.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417414003832},
  Zone                     = {3Q}
}

@Article{Pham-2007-p454-458,
  Title                    = {On Recent Generalizations of the Weibull Distribution},
  Author                   = {Pham, H. and Chin-Diew Lai},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {454-458},
  Volume                   = {56},

  Abstract                 = {This short communication first offers a clarification to a claim by Nadarajah & Kotz. We then present a short summary (by no means exhaustive) of some well-known, recent generations of Weibull-related lifetime models for quick information. A brief discussion on the properties of this general class is also given. Some future research directions on this topic are also discussed.},
  Doi                      = {10.1109/TR.2007.903352},
  File                     = {Published version:Pham-2007-p454-458.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Weibull distribution;reliability theory;Weibull distribution;bathtub shape;failure rate function;lifetime models;reliability theory;Context modeling;Distribution functions;Hazards;Humans;Probability density function;Random variables;Reliability engineering;Shape;Systems engineering and theory;Weibull distribution;Bathtub shape;Weibull distribution;failure rate function},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，这里的 Function 是函数的意思},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4298248&isnumber=4298224},
  Zone                     = {2Q}
}

@Article{Pierce-1982-p1452-1458,
  Title                    = {The Characterization of Transistor Electrical Overstress Failure Probability Density Functions},
  Author                   = {Pierce, Donald G. and Mason, Robert M.},
  Journal                  = {Nuclear Science, IEEE Transactions on},
  Year                     = {1982},

  Month                    = {Dec},
  Number                   = {6},
  Pages                    = {1452-1458},
  Volume                   = {29},

  Abstract                 = {An empirical approach for characterizing transistor emitter-base failure threshold probability density functions is presented. The data analyzed come from a program of experiments designed to test component failures due to electrical overstress transients. First, an empirical distribution is obtained which describes the variation of the relative width of a large set of measured failure threshold distributions. Using this distribution, a technique is presented for obtaining threshold distribution parameters when the mean failure threshold is assumed to be known, Second, a technique is presented for combining the distribution of relative width with a derived mean uncertainty estimate. This yields a probabilistic statement of the threshold distribution parameters based on the estimated mean and the described uncertainty distributions. This approach is used to provide probabilistic statements on threshold lower bounds. An example of the use of these techniques is included.},
  Doi                      = {10.1109/TNS.1982.4336385},
  File                     = {Published version:Pierce-1982-p1452-1458.pdf:PDF},
  Impactfactor             = {1.219},
  ISSN                     = {0018-9499},
  Keywords                 = {Earth Observing System;Failure analysis;Manufacturing;Predictive models;Probability density function;Process design;Risk analysis;Testing;Topology;Uncertainty},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，Function 表示函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4336385&isnumber=4336371},
  Zone                     = {3Q}
}

@Article{Piggin-2014-p70-74,
  Title                    = {Industrial systems: cyber-security's new battlefront [Information Technology Operational Technology]},
  Author                   = {Piggin, R.},
  Journal                  = {Engineering Technology},
  Year                     = {2014},

  Month                    = {Sept},
  Number                   = {8},
  Pages                    = {70-74},
  Volume                   = {9},

  Abstract                 = {Hackers are now directing their activities toward the technology commonly found in power stations, factories and other infrastructural facilities. The paper discusses how engineers managing these systems must understand the rising risk, and ensure that safeguards are implemented.},
  Doi                      = {10.1049/et.2014.0810},
  File                     = {Published version:Piggin-2014-p70-74.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1750-9637},
  Keywords                 = {critical infrastructures;security of data;cyber-security;factories;industrial systems;infrastructural facilities;power stations},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6905657&isnumber=6905592},
  Zone                     = {-}
}

@InProceedings{Piggin-2013-p1-6,
  Title                    = {Development of industrial cyber security standards: {IEC} 62443 for {SC}ADA and Industrial Control System security},
  Author                   = {Piggin, R.S.H.},
  Booktitle                = {Control and Automation 2013: Uniting Problems and Solutions, IET Conference on},
  Year                     = {2013},
  Pages                    = {1--6},

  Doi                      = {10.1049/cp.2013.0001},
  File                     = {Published version:Piggin-2013-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6613714},
  Zone                     = {-}
}

@Article{Pinto-2014-p57-76,
  Title                    = {{QRAM} a Qualitative Occupational Safety Risk Assessment Model for the construction industry that incorporate uncertainties by the use of fuzzy sets},
  Author                   = {Abel Pinto},
  Journal                  = {Safety Science },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {57 - 76},
  Volume                   = {63},

  Abstract                 = {Abstract Occupational safety risk assessment is the core of safety practices. Is a complex process that requires the consideration of sundry parameters, which are often difficult to quantify. This paper presents the new developed fuzzy \{QRAM\} model, which intends to support construction companies in carrying out their responsibilities to reduce occupational safety risks. The innovative aspects of \{QRAM\} model is to embody assess of the safety climate and the safety barriers effectiveness as assessment dimensions and the use of fuzzy sets theory to enhance the use of imprecise and incomplete information, elicited by linguistic variables. The \{QRAM\} model was designed based on the best academic and empirical knowledge about safety risks on construction industry, biomechanical data and laws of physics, chemistry and engineering. The lack of credible and accurate data, resulting from the in-depth investigation of work accidents in construction industry was the greatest difficulty in carry-out this work. \{QRAM\} was, firstly evaluated by “peer” review, with 12 safety experts from Brazil (2), Bulgaria (1), Greece (3), Turkey (3) and Portugal (3) and, then, appraised by comparing \{QRAM\} with 2 other occupational safety risk assessment techniques. The safety experts evaluators concluded that: (a) \{QRAM\} is a versatile tool to assess occupational safety risk assessment on construction sites; (b) the specific checklists for knowledge elicitation are a good aid and enhance the process objectivity, and (c) the use of linguistic variables is a better way to rate the risk factors thus to render the risk assessment process more objective and reliable. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2013.10.019},
  File                     = {Published version:Pinto-2014-p57-76.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Qualitative risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，定性职业安全风险评估模型，建筑业，利用模糊集解决其中的不确定因素。},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S092575351300252X},
  Zone                     = {3Q}
}

@Article{Pinto-2014-p57-76a,
  Title                    = {\{QRAM\} a Qualitative Occupational Safety Risk Assessment Model for the construction industry that incorporate uncertainties by the use of fuzzy sets },
  Author                   = {Abel Pinto},
  Journal                  = {Safety Science },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {57 - 76},
  Volume                   = {63},

  Abstract                 = {Abstract Occupational safety risk assessment is the core of safety practices. Is a complex process that requires the consideration of sundry parameters, which are often difficult to quantify. This paper presents the new developed fuzzy \{QRAM\} model, which intends to support construction companies in carrying out their responsibilities to reduce occupational safety risks. The innovative aspects of \{QRAM\} model is to embody assess of the safety climate and the safety barriers effectiveness as assessment dimensions and the use of fuzzy sets theory to enhance the use of imprecise and incomplete information, elicited by linguistic variables. The \{QRAM\} model was designed based on the best academic and empirical knowledge about safety risks on construction industry, biomechanical data and laws of physics, chemistry and engineering. The lack of credible and accurate data, resulting from the in-depth investigation of work accidents in construction industry was the greatest difficulty in carry-out this work. \{QRAM\} was, firstly evaluated by “peer” review, with 12 safety experts from Brazil (2), Bulgaria (1), Greece (3), Turkey (3) and Portugal (3) and, then, appraised by comparing \{QRAM\} with 2 other occupational safety risk assessment techniques. The safety experts evaluators concluded that: (a) \{QRAM\} is a versatile tool to assess occupational safety risk assessment on construction sites; (b) the specific checklists for knowledge elicitation are a good aid and enhance the process objectivity, and (c) the use of linguistic variables is a better way to rate the risk factors thus to render the risk assessment process more objective and reliable. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2013.10.019},
  File                     = {Published version:Pinto-2014-p57-76a.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Qualitative risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S092575351300252X},
  Zone                     = {3Q}
}

@Article{Pinto-2014-p57-76b,
  Title                    = {\{QRAM\} a Qualitative Occupational Safety Risk Assessment Model for the construction industry that incorporate uncertainties by the use of fuzzy sets },
  Author                   = {Abel Pinto},
  Journal                  = {Safety Science },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {57 - 76},
  Volume                   = {63},

  Abstract                 = {Abstract Occupational safety risk assessment is the core of safety practices. Is a complex process that requires the consideration of sundry parameters, which are often difficult to quantify. This paper presents the new developed fuzzy \{QRAM\} model, which intends to support construction companies in carrying out their responsibilities to reduce occupational safety risks. The innovative aspects of \{QRAM\} model is to embody assess of the safety climate and the safety barriers effectiveness as assessment dimensions and the use of fuzzy sets theory to enhance the use of imprecise and incomplete information, elicited by linguistic variables. The \{QRAM\} model was designed based on the best academic and empirical knowledge about safety risks on construction industry, biomechanical data and laws of physics, chemistry and engineering. The lack of credible and accurate data, resulting from the in-depth investigation of work accidents in construction industry was the greatest difficulty in carry-out this work. \{QRAM\} was, firstly evaluated by “peer” review, with 12 safety experts from Brazil (2), Bulgaria (1), Greece (3), Turkey (3) and Portugal (3) and, then, appraised by comparing \{QRAM\} with 2 other occupational safety risk assessment techniques. The safety experts evaluators concluded that: (a) \{QRAM\} is a versatile tool to assess occupational safety risk assessment on construction sites; (b) the specific checklists for knowledge elicitation are a good aid and enhance the process objectivity, and (c) the use of linguistic variables is a better way to rate the risk factors thus to render the risk assessment process more objective and reliable. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2013.10.019},
  File                     = {Published version:Pinto-2014-p57-76b.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Qualitative risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S092575351300252X},
  Zone                     = {3Q}
}

@Article{Pinto-2011-p616-624b,
  Title                    = {Occupational risk assessment in construction industry – Overview and reflection },
  Author                   = {Abel Pinto and Isabel L. Nunes and Rita A. Ribeiro},
  Journal                  = {Safety Science },
  Year                     = {2011},
  Number                   = {5},
  Pages                    = {616 - 624},
  Volume                   = {49},

  Abstract                 = {The construction industry is plagued by occupational risky situations and poor working conditions. Occupational risk assessment (ORA) on workplace sites is the first and key step to achieve adequate safety levels, particularly to support decision-making in safety programs. Most construction safety efforts are applied informally under the premise that simply allocating more resources to safety management will improve safety on site. Moreover, there are many traditional methods to address ORA, but few have been adapted and validated for use in the construction industry, particularly in the design stage, for which traditional approaches do not give adequate answers. This paper presents a state-of-the-art on \{ORA\} traditional methods, for the construction industry, discussing their limitations and pointing advantages of using fuzzy sets approaches to deal with ill-defined situations. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2011.01.003},
  File                     = {Published version:Pinto-2011-p616-624b.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Occupational risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753511000051},
  Zone                     = {3Q}
}

@Article{Pinzon-2013-p15-31,
  Title                    = {idMAS-SQL: Intrusion Detection Based on \{MAS\} to Detect and Block \{SQL\} injection through data mining },
  Author                   = {Cristian I. Pinzón and Juan F. De Paz and Álvaro Herrero and Emilio Corchado and Javier Bajo and Juan M. Corchado},
  Journal                  = {Information Sciences },
  Year                     = {2013},
  Note                     = {Data Mining for Information Security },
  Number                   = {0},
  Pages                    = {15 - 31},
  Volume                   = {231},

  Abstract                 = {This study presents a multiagent architecture aimed at detecting \{SQL\} injection attacks, which are one of the most prevalent threats for modern databases. The proposed architecture is based on a hierarchical and distributed strategy where the functionalities are structured on layers. SQL-injection attacks, one of the most dangerous attacks to online databases, are the focus of this research. The agents in each one of the layers are specialized in specific tasks, such as data gathering, data classification, and visualization. This study presents two key agents under a hybrid architecture: a classifier agent that incorporates a Case-Based Reasoning engine employing advanced algorithms in the reasoning cycle stages, and a visualizer agent that integrates several techniques to facilitate the visual analysis of suspicious queries. The former incorporates a new classification model based on a mixture of a neural network and a Support Vector Machine in order to classify \{SQL\} queries in a reliable way. The latter combines clustering and neural projection techniques to support the visual analysis and identification of target attacks. The proposed approach was tested in a real-traffic case study and its experimental results, which validate the performance of the proposed approach, are presented in this paper. },
  Doi                      = {http://dx.doi.org/10.1016/j.ins.2011.06.020},
  File                     = {Published version:Pinzon-2013-p15-31.pdf:PDF},
  Impactfactor             = {3.643},
  ISSN                     = {0020-0255},
  Keywords                 = {Intrusion Detection},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0020025511003148},
  Zone                     = {1Q}
}

@Article{Poolsappas-2012-p61-74,
  Title                    = {Dynamic Security Risk Management Using Bayesian Attack Graphs},
  Author                   = {Poolsappasit, N. and Dewri, R. and Ray, I.},
  Journal                  = {Dependable and Secure Computing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {61-74},
  Volume                   = {9},

  Abstract                 = {Security risk assessment and mitigation are two vital processes that need to be executed to maintain a productive IT infrastructure. On one hand, models such as attack graphs and attack trees have been proposed to assess the cause-consequence relationships between various network states, while on the other hand, different decision problems have been explored to identify the minimum-cost hardening measures. However, these risk models do not help reason about the causal dependencies between network states. Further, the optimization formulations ignore the issue of resource availability while analyzing a risk model. In this paper, we propose a risk management framework using Bayesian networks that enable a system administrator to quantify the chances of network compromise at various levels. We show how to use this information to develop a security mitigation and management plan. In contrast to other similar models, this risk model lends itself to dynamic analysis during the deployed phase of the network. A multiobjective optimization platform provides the administrator with all trade-off information required to make decisions in a resource constrained environment.},
  Doi                      = {10.1109/TDSC.2011.34},
  File                     = {Published version:Poolsappas-2012-p61-74.pdf:PDF},
  Impactfactor             = {1.059},
  ISSN                     = {1545-5971},
  Keywords                 = {belief networks;information technology;optimisation;risk management;security of data;Bayesian attack graphs;Bayesian networks;IT infrastructure;administrator system;attack trees;dynamic security risk management;multiobjective optimization platform;resource constrained environment;security mitigation;Analytical models;Bayesian methods;Computational modeling;Computer crime;Computer hacking;Computer security;Risk management;Bayesian belief networks;Security risk assessment;attack graph.;mitigation analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5936075&isnumber=6078484},
  Zone                     = {3Q}
}

@Article{Poolsappas-2012-p61-74a,
  Title                    = {Dynamic Security Risk Management Using Bayesian Attack Graphs},
  Author                   = {Poolsappasit, N. and Dewri, R. and Ray, I.},
  Journal                  = {Dependable and Secure Computing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {61-74},
  Volume                   = {9},

  Abstract                 = {Security risk assessment and mitigation are two vital processes that need to be executed to maintain a productive IT infrastructure. On one hand, models such as attack graphs and attack trees have been proposed to assess the cause-consequence relationships between various network states, while on the other hand, different decision problems have been explored to identify the minimum-cost hardening measures. However, these risk models do not help reason about the causal dependencies between network states. Further, the optimization formulations ignore the issue of resource availability while analyzing a risk model. In this paper, we propose a risk management framework using Bayesian networks that enable a system administrator to quantify the chances of network compromise at various levels. We show how to use this information to develop a security mitigation and management plan. In contrast to other similar models, this risk model lends itself to dynamic analysis during the deployed phase of the network. A multiobjective optimization platform provides the administrator with all trade-off information required to make decisions in a resource constrained environment.},
  Doi                      = {10.1109/TDSC.2011.34},
  File                     = {Published version:Poolsappas-2012-p61-74a.pdf:PDF},
  Impactfactor             = {1.059},
  ISSN                     = {1545-5971},
  Keywords                 = {belief networks;information technology;optimisation;risk management;security of data;Bayesian attack graphs;Bayesian networks;IT infrastructure;administrator system;attack trees;dynamic security risk management;multiobjective optimization platform;resource constrained environment;security mitigation;Analytical models;Bayesian methods;Computational modeling;Computer crime;Computer hacking;Computer security;Risk management;Bayesian belief networks;Security risk assessment;attack graph.;mitigation analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，Fig. 2 shows the BAG for our test network. We have developed an in-house tool to generate such BAGs. The tool takes as input an initial vulnerability table, generated by a vulnerability scanner, and the network topology. Using a sequence of SQL queries on a vulnerability exposure database, the tool creates consequence attributes for the graph until no further implications can be derived.},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5936075&isnumber=6078484},
  Zone                     = {3Q}
}

@Article{Poolsappas-2012-p61-74b,
  Title                    = {Dynamic Security Risk Management Using Bayesian Attack Graphs},
  Author                   = {Poolsappasit, N. and Dewri, R. and Ray, I.},
  Journal                  = {Dependable and Secure Computing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {61-74},
  Volume                   = {9},

  Abstract                 = {Security risk assessment and mitigation are two vital processes that need to be executed to maintain a productive IT infrastructure. On one hand, models such as attack graphs and attack trees have been proposed to assess the cause-consequence relationships between various network states, while on the other hand, different decision problems have been explored to identify the minimum-cost hardening measures. However, these risk models do not help reason about the causal dependencies between network states. Further, the optimization formulations ignore the issue of resource availability while analyzing a risk model. In this paper, we propose a risk management framework using Bayesian networks that enable a system administrator to quantify the chances of network compromise at various levels. We show how to use this information to develop a security mitigation and management plan. In contrast to other similar models, this risk model lends itself to dynamic analysis during the deployed phase of the network. A multiobjective optimization platform provides the administrator with all trade-off information required to make decisions in a resource constrained environment.},
  Doi                      = {10.1109/TDSC.2011.34},
  File                     = {Published version:Poolsappas-2012-p61-74b.pdf:PDF},
  Impactfactor             = {1.059},
  ISSN                     = {1545-5971},
  Keywords                 = {belief networks;information technology;optimisation;risk management;security of data;Bayesian attack graphs;Bayesian networks;IT infrastructure;administrator system;attack trees;dynamic security risk management;multiobjective optimization platform;resource constrained environment;security mitigation;Analytical models;Bayesian methods;Computational modeling;Computer crime;Computer hacking;Computer security;Risk management;Bayesian belief networks;Security risk assessment;attack graph.;mitigation analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5936075&isnumber=6078484},
  Zone                     = {3Q}
}

@Article{Poolsappas-2012-p61-74c,
  Title                    = {Dynamic Security Risk Management Using Bayesian Attack Graphs},
  Author                   = {Poolsappasit, N. and Dewri, R. and Ray, I.},
  Journal                  = {Dependable and Secure Computing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {61-74},
  Volume                   = {9},

  Doi                      = {10.1109/TDSC.2011.34},
  File                     = {Published version:Poolsappas-2012-p61-74c.pdf:PDF},
  Impactfactor             = {1.059},
  ISSN                     = {1545-5971},
  Keywords                 = {belief networks;information technology;optimisation;risk management;security of data;Bayesian attack graphs;Bayesian networks;IT infrastructure;administrator system;attack trees;dynamic security risk management;multiobjective optimization platform;resource constrained environment;security mitigation;Analytical models;Bayesian methods;Computational modeling;Computer crime;Computer hacking;Computer security;Risk management;Bayesian belief networks;Security risk assessment;attack graph.;mitigation analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5936075&isnumber=6078484},
  Zone                     = {3Q}
}

@Article{Poolsappas-2012-p61-74d,
  Title                    = {Dynamic Security Risk Management Using Bayesian Attack Graphs},
  Author                   = {Poolsappasit, N. and Dewri, R. and Ray, I.},
  Journal                  = {Dependable and Secure Computing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {61-74},
  Volume                   = {9},

  Doi                      = {10.1109/TDSC.2011.34},
  File                     = {Published version:Poolsappas-2012-p61-74d.pdf:PDF},
  Impactfactor             = {1.059},
  ISSN                     = {1545-5971},
  Keywords                 = {belief networks;information technology;optimisation;risk management;security of data;Bayesian attack graphs;Bayesian networks;IT infrastructure;administrator system;attack trees;dynamic security risk management;multiobjective optimization platform;resource constrained environment;security mitigation;Analytical models;Bayesian methods;Computational modeling;Computer crime;Computer hacking;Computer security;Risk management;Bayesian belief networks;Security risk assessment;attack graph.;mitigation analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5936075&isnumber=6078484},
  Zone                     = {3Q}
}

@Article{Poolsappasit2012,
  Title                    = {Dynamic Security Risk Management Using Bayesian Attack Graphs},
  Author                   = {Poolsappasit, N. and Dewri, R. and Ray, I.},
  Journal                  = {Dependable and Secure Computing, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {61-74},
  Volume                   = {9},

  Doi                      = {10.1109/TDSC.2011.34},
  File                     = {Published version:Poolsappas-2012-p61-74d.pdf:PDF},
  Impactfactor             = {1.059},
  ISSN                     = {1545-5971},
  Keywords                 = {belief networks;information technology;optimisation;risk management;security of data;Bayesian attack graphs;Bayesian networks;IT infrastructure;administrator system;attack trees;dynamic security risk management;multiobjective optimization platform;resource constrained environment;security mitigation;Analytical models;Bayesian methods;Computational modeling;Computer crime;Computer hacking;Computer security;Risk management;Bayesian belief networks;Security risk assessment;attack graph.;mitigation analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5936075&isnumber=6078484},
  Zone                     = {3Q}
}

@Article{Pourali-2014-p2883-2890,
  Title                    = {Incorporating Common Cause Failures in Mission-Critical Facilities Reliability Analysis},
  Author                   = {Pourali, M.},
  Journal                  = {Industry Applications, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {2883-2890},
  Volume                   = {50},

  Abstract                 = {Common cause events are those specific groups of dependent events that might adversely affect the operation of a redundant system. Common cause failures (CCF) are considered a subset of dependent failures with a major difference that they cannot be explicitly modeled. A CCF is a single point of failure (SPOF) causing a unit and its “perceived” redundant unit to fail simultaneously. During the first few years of commercial nuclear power plant operations, many probability risk assessment (PRA) studies revealed that CCF have significantly contributed to core damages. Consequently, the U.S. Nuclear Regulation Commission (NRC) initiated various CCF analyses in the 1980s. Since then, CCF analyses have been synonymous with nuclear facilities, and a majority of CCF analysis efforts have rightfully been focused on the safety of nuclear plants. Given that many industrial and commercial mission-critical facilities and high-reliability organizations now require a high degree of availability, therefore, CCF cannot be ignored. The objectives of this paper are to present 1) the importance of considering CCF in reliability, availability, and maintainability (RAM) analysis for industrial and commercial mission-critical facilities and high-reliability organizations, 2) simple quantitative analysis methods of CCF for industrial and commercial mission-critical facilities, and 3) the need for including CCF as part of future failure data collection efforts.},
  Doi                      = {10.1109/TIA.2013.2295472},
  File                     = {Published version:Pourali-2014-p2883-2890.pdf:PDF},
  Impactfactor             = {1.672},
  ISSN                     = {0093-9994},
  Keywords                 = {failure analysis;nuclear power stations;power system reliability;probability;risk management;CCF;NRC;PRA;SPOF;U.S. Nuclear Regulation Commission;commercial nuclear power plant operations;common cause failures;core damages;high-reliability organizations;mission-critical facilities;nuclear facilities;probability risk assessment;redundant system;reliability analysis;simple quantitative analysis;single point of failure;Artificial intelligence;Bismuth;Failure analysis;Maintenance engineering;Mission critical systems;Reliability;Uninterruptible power systems;Common cause failure;Common cause failure (CCF);dependent failure;mission critical facilities;mission-critical facilities;reliability analysis;reliability block diagram;reliability block diagram (RBD)},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6690108&isnumber=6855381},
  Zone                     = {3Q}
}

@InProceedings{Powell-2010-p1-9,
  Title                    = {Risk assessment sensitivities for very low probability events with severe consequences},
  Author                   = {Powell, M.A.},
  Booktitle                = {Aerospace Conference, 2010 IEEE},
  Year                     = {2010},
  Month                    = {March},
  Pages                    = {1-9},

  Abstract                 = {Modern aerospace systems are typically designed to satisfy numerous very stringent performance requirements. The risks posed thus encompass very low probability events with severe consequences. Long-term operational experience with these systems may produce no occurrences of the events, yet the risk of the severe consequence remains and good decision making requires understanding of the risk. With no observed events, classical methods cannot be used to assess these risks quantitatively. Conditional methods may however be used very effectively to produce assessments of the risk when no actual events have been observed, without the use of questionable assumptions. This report develops a quantitative risk assessment for astronaut bone fracture during an extended microgravity exposure using these conditional methods. The microgravity environment experienced by astronauts in space is well known to reduce astronaut bone mineral density, approximating osteopenic and osteoporotic conditions that significantly increases risk of fracture for the elderly here on earth. The risk of bone fracture for astronauts working in a microgravity environment is believed to similarly increase. At this point, it is entirely unknown what the effects of a broken bone would be in a space environment, whether healing could occur or not, and how life threatening a fracture would be in the space environment. To date, no astronaut has ever broken a bone during microgravity space operations. Conditional methods are used to develop a quantitative risk assessment for consideration of extending mission durations for the International Space Station, and for risk assessments for long duration missions to Mars. The effect of a hypothetical bone fracture is further investigated as to how a single event can affect the risk assessment.},
  Doi                      = {10.1109/AERO.2010.5446865},
  File                     = {Published version:Powell-2010-p1-9.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1095-323X},
  Keywords                 = {aerospace engineering;aerospace safety;decision making;risk management;aerospace systems;astronaut bone fracture;bone mineral density;decision making;long-term operational experience;microgravity environment;osteopenic conditions;osteoporotic conditions;performance requirements;quantitative risk assessment;risk assessment sensitivities;severe consequences;space environment;very low probability events;Costs;Humans;Manufacturing;NASA;Risk management;Space missions;Space technology;Supply chain management;Supply chains;Technological innovation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5446865&isnumber=5446655},
  Zone                     = {-}
}

@Article{Pujol-2013-p283-296,
  Title                    = {Potential application of quantitative microbiological risk assessment techniques to an aseptic-UHT process in the food industry },
  Author                   = {Laure Pujol and Isabelle Albert and Nicholas Brian Johnson and Jeanne-Marie Membré},
  Journal                  = {International Journal of Food Microbiology },
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {283 - 296},
  Volume                   = {162},

  Abstract                 = {Abstract Aseptic Ultra-High-Temperature (UHT)-type processed food products (e.g. milk or soup) are ready to eat products which are consumed extensively globally due to a combination of their comparative high quality and long shelf life, with no cold chain or other preservation requirements. Due to the inherent microbial vulnerability of aseptic-UHT product formulations, the safety and stability-related Performance Objectives (POs) required at the end of the manufacturing process are the most demanding found in the food industry. The key determinants to achieving sterility, and which also differentiates aseptic-UHT from in-pack sterilised products, are the challenges associated with the processes of aseptic filling and sealing. This is a complex process that has traditionally been run using deterministic or empirical process settings. Quantifying the risk of microbial contamination and recontamination along the aseptic-UHT process, using the scientifically based process Quantitative Microbial Risk Assessment (QMRA), offers the possibility to improve on the currently tolerable sterility failure rate (i.e. 1 defect per 10,000 units). In addition, benefits of applying \{QMRA\} are i) to implement process settings in a transparent and scientific manner; ii) to develop a uniform common structure whatever the production line, leading to a harmonisation of these process settings, and; iii) to bring elements of a cost-benefit analysis of the management measures. The objective of this article is to explore how \{QMRA\} techniques and risk management metrics may be applied to aseptic-UHT-type processed food products. In particular, the aseptic-UHT process should benefit from a number of novel mathematical and statistical concepts that have been developed in the field of QMRA. Probabilistic techniques such as Monte Carlo simulation, Bayesian inference and sensitivity analysis, should help in assessing the compliance with safety and stability-related \{POs\} set at the end of the manufacturing process. The understanding of aseptic-UHT process contamination will be extended beyond the current “As-Low-As-Reasonably-Achievable” targets to a risk-based framework, through which current sterility performance and future process designs can be optimised. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijfoodmicro.2013.01.021},
  File                     = {Published version:Pujol-2013-p283-296.pdf:PDF},
  Impactfactor             = {3.425},
  ISSN                     = {0168-1605},
  Keywords                 = {Risk-based food safety management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，不是很相关},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0168160513000676},
  Zone                     = {2Q}
}

@Article{Pujol-2013-p283-296a,
  Title                    = {Potential application of quantitative microbiological risk assessment techniques to an aseptic-UHT process in the food industry },
  Author                   = {Laure Pujol and Isabelle Albert and Nicholas Brian Johnson and Jeanne-Marie Membré},
  Journal                  = {International Journal of Food Microbiology },
  Year                     = {2013},
  Number                   = {3},
  Pages                    = {283 - 296},
  Volume                   = {162},

  Abstract                 = {Abstract Aseptic Ultra-High-Temperature (UHT)-type processed food products (e.g. milk or soup) are ready to eat products which are consumed extensively globally due to a combination of their comparative high quality and long shelf life, with no cold chain or other preservation requirements. Due to the inherent microbial vulnerability of aseptic-UHT product formulations, the safety and stability-related Performance Objectives (POs) required at the end of the manufacturing process are the most demanding found in the food industry. The key determinants to achieving sterility, and which also differentiates aseptic-UHT from in-pack sterilised products, are the challenges associated with the processes of aseptic filling and sealing. This is a complex process that has traditionally been run using deterministic or empirical process settings. Quantifying the risk of microbial contamination and recontamination along the aseptic-UHT process, using the scientifically based process Quantitative Microbial Risk Assessment (QMRA), offers the possibility to improve on the currently tolerable sterility failure rate (i.e. 1 defect per 10,000 units). In addition, benefits of applying \{QMRA\} are i) to implement process settings in a transparent and scientific manner; ii) to develop a uniform common structure whatever the production line, leading to a harmonisation of these process settings, and; iii) to bring elements of a cost-benefit analysis of the management measures. The objective of this article is to explore how \{QMRA\} techniques and risk management metrics may be applied to aseptic-UHT-type processed food products. In particular, the aseptic-UHT process should benefit from a number of novel mathematical and statistical concepts that have been developed in the field of QMRA. Probabilistic techniques such as Monte Carlo simulation, Bayesian inference and sensitivity analysis, should help in assessing the compliance with safety and stability-related \{POs\} set at the end of the manufacturing process. The understanding of aseptic-UHT process contamination will be extended beyond the current “As-Low-As-Reasonably-Achievable” targets to a risk-based framework, through which current sterility performance and future process designs can be optimised. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijfoodmicro.2013.01.021},
  File                     = {Published version:Pujol-2013-p283-296a.pdf:PDF},
  Impactfactor             = {3.425},
  ISSN                     = {0168-1605},
  Keywords                 = {Risk-based food safety management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0168160513000676},
  Zone                     = {2Q}
}

@Article{Purb-2014-p21-29,
  Title                    = {A fuzzy-based reliability approach to evaluate basic events of fault tree analysis for nuclear power plant probabilistic safety assessment },
  Author                   = {Julwan Hendry Purba},
  Journal                  = {Annals of Nuclear Energy },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {21 - 29},
  Volume                   = {70},

  Abstract                 = {Abstract Fault tree analysis has been widely utilized as a tool for nuclear power plant probabilistic safety assessment. This analysis can be completed only if all basic events of the system fault tree have their quantitative failure rates or failure probabilities. However, it is difficult to obtain those failure data due to insufficient data, environment changing or new components. This study proposes a fuzzy-based reliability approach to evaluate basic events of system fault trees whose failure precise probability distributions of their lifetime to failures are not available. It applies the concept of failure possibilities to qualitatively evaluate basic events and the concept of fuzzy sets to quantitatively represent the corresponding failure possibilities. To demonstrate the feasibility and the effectiveness of the proposed approach, the actual basic event failure probabilities collected from the operational experiences of the David–Besse design of the Babcock and Wilcox reactor protection system fault tree are used to benchmark the failure probabilities generated by the proposed approach. The results confirm that the proposed fuzzy-based reliability approach arises as a suitable alternative for the conventional probabilistic reliability approach when basic events do not have the corresponding quantitative historical failure data for determining their reliability characteristics. Hence, it overcomes the limitation of the conventional fault tree analysis for nuclear power plant probabilistic safety assessment. },
  Doi                      = {http://dx.doi.org/10.1016/j.anucene.2014.02.022},
  File                     = {Published version:Purb-2014-p21-29.pdf:PDF},
  Impactfactor             = {0.8},
  ISSN                     = {0306-4549},
  Keywords                 = {Reliability},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0306454914001029},
  Zone                     = {4Q}
}

@InProceedings{Qi-2010-p1-4,
  Title                    = {Dynamic Assessment and VaR-Based Quantification of Information Security Risk},
  Author                   = {Wenjing Qi and Xue Liu and Jian Zhang and Weihua Yuan},
  Booktitle                = {e-Business and Information System Security (EBISS), 2010 2nd International Conference on},
  Year                     = {2010},
  Month                    = {May},
  Pages                    = {1-4},

  Abstract                 = {Risk assessment and quantification is crucial to the effectiveness of information security measure deployed in an organization. A dynamic risk assessment process is presented in this paper to cope with the variation and diversity of threats in the information system. To give a clear perspective of the information risk without confusing by the complexity of so many risk factors, an risk quantification model and a VaR-based risk measure are presented, through which, risk can be represented by the prospective maximum daily loss under certain confidence level. We test our risk quantification model and measure in a real network environment.},
  Doi                      = {10.1109/EBISS.2010.5473537},
  File                     = {Published version:Qi-2010-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {risk management;security of data;VaR;dynamic risk assessment process;information security measure;risk factors;risk quantification model;Biomedical measurements;Computer bugs;Computer science;Computer security;Educational institutions;Information security;Information systems;Management information systems;Reactive power;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5473537&isnumber=5473261},
  Zone                     = {-}
}

@Article{Qinqin-2014-p251-260,
  Title                    = {Environmental risk source management system for the petrochemical industry },
  Author                   = {Chen Qinqin and Qian Jia and Zengwei Yuan and Lei Huang},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2014},
  Number                   = {3},
  Pages                    = {251 - 260},
  Volume                   = {92},

  Abstract                 = {We identify environmental risk sources within the petrochemical industry with a bow-tie analysis, evaluate environmental risk sources with an integrated environmental risk assessment index, and classify environmental risk sources considering both environmental consequences and management costs. Furthermore, we develop a routine management system for environmental risk sources based on browser/server model and web-GIS technology. The system has four main functions: petrochemical enterprise registration and declaration, environmental risk source information correction and confirmation, environmental risk source evaluation and classification, and environmental risk source management. The system runs with the following sequential steps. (1) Petrochemical enterprises register and declare their environmental risk source information. (2) The registered environmental risk source information is checked, corrected and confirmed by local environmental officials. (3) The probability and intensity of environmental risk are calculated for all registered petrochemical factories. (4) All environmental risk sources are classified into high, medium and low risk sections based on their potential regional environmental and ecological impacts. (5) The system provides recommendations on the routine risk management based on empirical expert opinions. The software provides an effective tool for safety production of petrochemical enterprises and can be applied by local governments for environmental risk source management. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2013.01.004},
  File                     = {Published version:Qinqin-2014-p251-260.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Petrochemical industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {C，石油工业中的环境风险源管理，利用 bow-tie 分析方法，bow-tie 分析方法很多文章都用到过，本文通过分析事件的因果关系来分析环境的风险源，还给出了环境风险源管理程序的实现。},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957582013000062},
  Zone                     = {3Q}
}

@InProceedings{Quan-2010-p3566-3569,
  Title                    = {A Risk Assessment Model Based-Business-Circle of E-Government Information System},
  Author                   = {Zhou Quan and Fu Gui and Zhilian Zeng and Jiuhao Li and Deqin Xiao},
  Booktitle                = {E-Business and E-Government (ICEE), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {May},
  Pages                    = {3566-3569},

  Abstract                 = {With the development of computer network and information technology rapidly, they are serving every walk of life in China. Chinese government is using widely the information and network technology for improving its work. So, Chinese E-Government information system is developing feetly. However, in fact, E-Government information system has some its characters and some potential hazards in China, which affects E-Government information system's healthy and favorable development. It is insufficient that some secure products and tools are only used for the security system architecture of E-Government information system. To build secure and authentic system architecture of E-Government information system, the risk assessment and management is necessary for secure and reliable E-Government information system. A risk assessment model based-business-circle of E-Government information system is proposed in the article according to the characters of E-Government information system. In the model, some business circles are given according to the importance and features of E-Government information system. Moreover, the risk assessment is done in every business circle. The model introduces the idea of risk assessment, scheme of risk assessment, process of risk assessment and the computational method of risk.},
  Doi                      = {10.1109/ICEE.2010.896},
  File                     = {Published version:Quan-2010-p3566-3569.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {government data processing;risk management;security of data;Chinese e-government information system;authentic system architecture;computer network;information technology;network technology;risk assessment model based-business circle;security system architecture;Computational modeling;Educational institutions;Electronic government;Information security;Risk management;E-Government;information security;risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5591465&isnumber=5590383},
  Zone                     = {-}
}

@Article{Queiroz-2011-p589-597,
  Title                    = {SCADASim—A Framework for Building SCADA Simulations},
  Author                   = {Queiroz, C. and Mahmood, A. and Tari, Z.},
  Journal                  = {Smart Grid, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {589-597},
  Volume                   = {2},

  Abstract                 = {Supervisory Control and Data Acquisition (SCADA) systems control and monitor industrial and critical infrastructure functions, such as electricity, gas, water, waste, railway, and traffic. Recent attacks on SCADA systems highlight the need of stronger SCADA security. It is important to analyze the security risks and develop appropriate security solutions to protect such systems. However, a key problem is the lack of proper modeling tools to evaluate the security of SCADA systems. As widely accepted in academic and industrial communities, it is impractical to conduct security experiments on live systems. A modeling simulation tool would enable the simulation of SCADA systems with the benefit of testing different attack and security solutions. This paper proposes a simulation tool for building SCADA simulations that supports the integration of external devices and applications. A key benefit of this tool is the ability to test the effect of attacks on real devices and applications, even though using a simulated environment. The paper further describes two case studies that demonstrate how the tool can be efficiently used to create SCADA simulations and perform malicious attacks.},
  Doi                      = {10.1109/TSG.2011.2162432},
  File                     = {Published version:Queiroz-2011-p589-597.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1949-3053},
  Keywords                 = {SCADA systems;critical infrastructures;digital simulation;power system security;smart power grids;SCADA security;SCADA simulations;SCADASim;critical infrastructure functions;security risks;smart grids;supervisory control and data acquisition system;Computer security;Logic gates;SCADA systems;Computer security;SCADA systems;computer simulation;smart grids},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6009221&isnumber=6084801},
  Zone                     = {-}
}

@Article{Quigley-2005-p604-611,
  Title                    = {Nonparametric bootstrapping of the reliability function for multiple copies of a repairable item modeled by a birth process},
  Author                   = {Quigley, J. and Walls, L.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2005},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {604-611},
  Volume                   = {54},

  Abstract                 = {Nonparametric bootstrap inference is developed for the reliability function estimated from censored, nonstationary failure time data for multiple copies of repairable items. We assume that each copy has a known, but not necessarily the same, observation period; and upon failure of one copy, design modifications are implemented for all copies operating at that time to prevent further failures arising from the same fault. This implies that, at any point in time, all operating copies will contain the same set of faults. Failures are modeled as a birth process because there is a reduction in the rate of occurrence at each failure. The data structure comprises a mix of deterministic & random censoring mechanisms corresponding to the known observation period of the copy, and the random censoring time of each fault. Hence, bootstrap confidence intervals & regions for the reliability function measure the length of time a fault can remain within the item until realization as failure in one of the copies. Explicit formulae derived for the re-sampling probabilities greatly reduce dependency on Monte-Carlo simulation. Investigations show a small bias arising in re-sampling that can be quantified & corrected. The variability generated by the re-sampling approach approximates the variability in the underlying birth process, and so supports appropriate inference. An illustrative example describes application to a problem, and discusses the validity of modeling assumptions within industrial practice.},
  Doi                      = {10.1109/TR.2005.858097},
  File                     = {Published version:Quigley-2005-p604-611.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Monte Carlo methods;bootstrapping;failure analysis;maintenance engineering;reliability theory;Kaplan-Meier;Monte-Carlo simulation;birth process;censored data;confidence interval;data structure;failure analysis;nonparametric bootstrapping;nonstationary failure time data;random censoring mechanism;reliability function;repairable items;resampling probability;Calendars;Data structures;Fault detection;Fault diagnosis;Helium;Length measurement;Object detection;Probability density function;Spatial databases;Time measurement;Bootstrap;Kaplan-Meier;censored data;confidence intervals;reliability function},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，rate of occurrence of failures 失效的概率},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1546567&isnumber=33008},
  Zone                     = {2Q}
}

@Article{Rachidi-2008-p2489-2496,
  Title                    = {A Review of Current Issues in Lightning Protection of New-Generation Wind-Turbine Blades},
  Author                   = {Rachidi, F. and Rubinstein, M. and Montanya, J. and Bermudez, J.-L. and Rodriguez Sola, R. and Sola, G. and Korovkin, N.},
  Journal                  = {Industrial Electronics, IEEE Transactions on},
  Year                     = {2008},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {2489-2496},
  Volume                   = {55},

  Abstract                 = {The salient issues related to lightning protection of long wind-turbine blades are discussed in this paper. We show that the lightning protection of modern wind turbines presents a number of new challenges due to the geometrical, electrical, and mechanical particularities of the turbines. The risk assessment for the lightning-protection-system design is solely based today on downward flashes. We show in this paper that the majority of the strikes to modern turbines are expected to be upward lightning. Neglecting upward flashes, as implicitly done by the International Electrotechnical Commission, might result in an important underestimation of the actual number of strikes to a tall wind turbine. In addition, we show that the rotation of the blades may have a considerable influence on the number of strikes to modern wind turbines as these may be triggering their own lightning. Because wind turbines are tall structures, the lightning currents that are injected by return strokes into the turbines will be affected by reflections at the top, bottom, and junction of the blades with the static base of the turbine. This is of capital importance when calculating the protection of internal circuitry that may be affected by magnetically induced electromotive forces that depend directly on the characteristics of the current in the turbine. The presence of carbon-reinforced plastics (CRP) in the blades introduces a new set of problems to be dealt with in the design of the turbines' lightning protection system. One problem is the mechanical stress resulting from the energy dissipation in CRP laminates due to the circulation of eddy currents. We evaluate in this paper the dissipated energy and propose recommendations as to the number of down conductors and their orientation with respect to the CRP laminates so that the dissipated energy is minimized. It is also emphasized that the high static fields under thunderclouds might have an influence on the moving carbon-fiber parts. This- - issue needs to be addressed by lightning protection researchers and engineers. Representative full-scale blade tests are still complex because lightning currents from an impulse current generator are conditioned to the electrical characteristics of the element under test and return paths. It is therefore desirable to complement laboratory tests with theoretical and computer modeling for the estimation of fields, currents, and voltages within the blades.},
  Doi                      = {10.1109/TIE.2007.896443},
  File                     = {Published version:Rachidi-2008-p2489-2496.pdf:PDF},
  Impactfactor             = {5.165},
  ISSN                     = {0278-0046},
  Keywords                 = {blades;eddy currents;lightning protection;power generation protection;risk management;stress analysis;wind turbines;International Electrotechnical Commission;carbon-reinforced plastics;eddy currents;energy dissipation;impulse current generator;lightning-protection-system design;magnetically induced electromotive forces;mechanical stress;new-generation wind-turbine blades;risk assessment;Lightning;lightning;lightning to tall structures;wind turbines},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4459254&isnumber=4531695},
  Zone                     = {1Q}
}

@Article{Rahimi-2012-p3-13,
  Title                    = {Analysis of the security of \{VPN\} configurations in industrial control environments },
  Author                   = {Sanaz Rahimi and Mehdi Zargham},
  Journal                  = {International Journal of Critical Infrastructure Protection },
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {3 - 13},
  Volume                   = {5},

  Abstract                 = {Virtual private networks (VPNs) are a popular approach for protecting otherwise insecure industrial control protocols. \{VPNs\} provide confidentiality, integrity and availability, and are often considered to be secure. However, implementation vulnerabilities and protocol flaws expose \{VPN\} weaknesses in many industrial deployments. This paper employs a probabilistic model to evaluate and quantify the security of \{VPN\} configurations. Simulations of the \{VPN\} model are conducted to investigate the trade-offs and parameter dependence in various \{VPN\} configurations. The experimental results provide recommendations for securing \{VPN\} deployments in industrial control environments. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijcip.2012.01.001},
  File                     = {Published version:Rahimi-2012-p3-13.pdf:PDF},
  Impactfactor             = {0.63},
  ISSN                     = {1874-5482},
  Keywords                 = {Control systems},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1874548212000029},
  Zone                     = {4Q}
}

@Article{Ralston-2007-p583-594,
  Title                    = {Cyber security risk assessment for \{SCADA\} and \{DCS\} networks },
  Author                   = {P.A.S. Ralston and J.H. Graham and J.L. Hieb},
  Journal                  = {\{ISA\} Transactions },
  Year                     = {2007},
  Number                   = {4},
  Pages                    = {583 - 594},
  Volume                   = {46},

  Abstract                 = {The growing dependence of critical infrastructures and industrial automation on interconnected physical and cyber-based control systems has resulted in a growing and previously unforeseen cyber security threat to supervisory control and data acquisition (SCADA) and distributed control systems (DCSs). It is critical that engineers and managers understand these issues and know how to locate the information they need. This paper provides a broad overview of cyber security and risk assessment for \{SCADA\} and DCS, introduces the main industry organizations and government groups working in this area, and gives a comprehensive review of the literature to date. Major concepts related to the risk assessment methods are introduced with references cited for more detail. Included are risk assessment methods such as HHM, IIM, and \{RFRM\} which have been applied successfully to \{SCADA\} systems with many interdependencies and have highlighted the need for quantifiable metrics. Presented in broad terms is probability risk analysis (PRA) which includes methods such as FTA, ETA, and FEMA. The paper concludes with a general discussion of two recent methods (one based on compromise graphs and one on augmented vulnerability trees) that quantitatively determine the probability of an attack, the impact of the attack, and the reduction in risk associated with a particular countermeasure. },
  Doi                      = {http://dx.doi.org/10.1016/j.isatra.2007.04.003},
  File                     = {Published version:Ralston-2007-p583-594.pdf:PDF},
  Impactfactor             = {1.626},
  ISSN                     = {0019-0578},
  Keywords                 = {\{SCADA\}},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~，重点看},
  Timestamp                = {2015.03.20},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0019057807000754},
  Zone                     = {2Q}
}

@Article{Ramirez-2015-p119-136,
  Title                    = {Use of dynamic Bayesian networks for life extension assessment of ageing systems },
  Author                   = {Pedro A. Pérez Ramírez and Ingrid Bouwer Utne},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {119 - 136},
  Volume                   = {133},

  Abstract                 = {Abstract Extending the operating lifetime of ageing technical systems is of great interest for industrial applications. Life extension requires identifying and selecting decision alternatives which allow for a safe and economic operation of the system beyond its design lifetime. This article proposes a dynamic Bayesian network for assessing the life extension of ageing repairable systems. The main objective of the model is to provide decision support based on the system performance during a finite time horizon, which is defined by the life extension period. The model has three main applications: (i) assessing and selecting optimal decision alternatives for the life extension at present time, based on historical data; (ii) identifying and minimizing the factors that have a negative impact on the system performance; and (iii) reassessing and optimizing the decision alternatives during operation throughout the life extension period, based on updating the model with new operational data gathered. A case study illustrates the application of the model for life extension of a real firewater pump system in an oil and gas facility. The case study analyzes three decision alternatives, where preventive maintenance and functional test policies are optimized, and the uncertainty involved in each alternative is computed. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2014.09.002},
  File                     = {Published version:Ramirez-2015-p119-136.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Life extension},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832014002129},
  Zone                     = {2Q}
}

@InProceedings{Ramjan-2012-p298-302,
  Title                    = {Flexible security rule-based system on cloud service for e-travel agent},
  Author                   = {Ramjan, S.},
  Booktitle                = {Cloud Computing and Intelligent Systems (CCIS), 2012 IEEE 2nd International Conference on},
  Year                     = {2012},
  Month                    = {Oct},
  Pages                    = {298-302},
  Volume                   = {01},

  Doi                      = {10.1109/CCIS.2012.6664416},
  File                     = {Published version:Ramjan-2012-p298-302.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {cloud computing;fuzzy logic;multi-agent systems;risk management;security of data;travel industry;cloud environment;cloud security infrastructure;cloud security requirement;cloud service;digital channel;e-travel agent;flexible security rule based system;fuzzy logic;network technician;risk management;tourism merchant;Algorithm design and analysis;Analytical models;Cloud computing;Educational institutions;Fuzzy logic;Risk management;Security;Cloud computing;E-travel agent;Fuzzy logic;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6664416&isnumber=6664354},
  Zone                     = {-}
}

@Article{Ramos-2007-p605-610,
  Title                    = {Reliability Analysis of Industrial Electrical Systems Using Bayesian Networks Considering Power Quality and Security Characteristics Applied to the IEEE 493 Standard Network},
  Author                   = {Ramos, G.A. and Torres, A. and Rognon, J.-P.},
  Journal                  = {Latin America Transactions, IEEE (Revista IEEE America Latina)},
  Year                     = {2007},

  Month                    = {Dec},
  Number                   = {8},
  Pages                    = {605-610},
  Volume                   = {5},

  Abstract                 = {The application of power electronics in industrial systems has increased the use of a wide variety of reliability analysis software for commercial and industrial electrical systems. However, there are power quality and security events that are not considered by these tools. This paper describes a methodology for the reliability evaluation of industrial electrical systems using Bayesian networks which incorporates the power quality and security characteristics under events such us short-circuits, random outages and electrical transients. This methodology is applied to the standard network proposed by the IEEE 493 Gold Book to compare the system reliability indices with the ones obtained by others methodologies. This work offers a solid and practical tool for the design of industrial electrical systems},
  Doi                      = {10.1109/T-LA.2007.4445713},
  File                     = {Published version:Ramos-2007-p605-610.pdf:PDF},
  Impactfactor             = {0.218},
  ISSN                     = {1548-0992},
  Keywords                 = {Application software;Bayesian methods;Books;Computer industry;Electronics industry;Gold;Power electronics;Power quality;Power system reliability;Power system security;Availability;Bayes procedures;Industrial power system reliability},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4445713&isnumber=4445704},
  Zone                     = {4Q}
}

@InProceedings{Ramprasad-2010-p213-218,
  Title                    = {State-of-art in regulatory decision making process for a Nuclear Fuel Cycle Facility},
  Author                   = {Ramprasad, K. and Bhattacharya, R.},
  Booktitle                = {Reliability, Safety and Hazard (ICRESH), 2010 2nd International Conference on},
  Year                     = {2010},
  Month                    = {Dec},
  Pages                    = {213-218},

  Abstract                 = {Risk is inherent in every activity taken up by human beings be it potential or obvious, it is accepted or tolerated. Risk posed due to nuclear industry has always been a cause of concern to the individual, society, government and the regulator. The Nuclear Fuel Cycle Faculty (NFCF) encompassing mining to waste management activities has been diligently associated with diversified risks. The risk management for a NFCF right from Siting stage through the De-commissioning stage has been a vital issue in the Consenting process. The regulatory decision at every stage is based on a comprehensive review and assessment of risk and the safety measures available to contain it. It is informed to the management of the NFCF as a part of review process to have a step-by step approach in identification of hazards, assessment of frequency and consequence of events and estimation of risks. This exercise shall facilitate the review and assessment process to arrive at a logical and judicious decision. The Risk based evaluation and review process is a systematic and structured method addressing various regulatory concerns and risk insights, along with other important information, to assist in making decisions. Because there are many facets of NFCF to consider (such as safety, barriers to defense in depth, design margins, techno-economic issues, etc), using a structured process can facilitate in making transparent, comprehensive, and consistent risk informed decisions. The Risk Based Informed Decision Making Process(RBIDMF) is in its evolving stage with respect to regulatory decision making functions of NFCF. The fourstep process involved in the risk information review of NFCF is detailed. The process involves submission of relevant documents and discussions with the NFCF proponents on various safety related issues. An approach model to integrate the risk information with the decision making process is presented. This concept considers the rationale of linking the perception of risk ass- ciated with activities along with the risk agreeable / acceptance criteria of the regulator. The objective of safe assessment and review is achieved through a comprehensive, strucured and systematic study of the risk associated with the activities of NFCF at various stages like Siting, Construction, Commissioning, Operation and De-Commissioning. The prognostic view of established and responsive regulatory framework ensures safe and rational decisions with the consensus and acceptance of all concerned.},
  Doi                      = {10.1109/ICRESH.2010.5779548},
  File                     = {Ramprasad-2010-p213-218.pdf:Ramprasad-2010-p213-218.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {decision making;fission reactor operation;fission reactor safety;nuclear fuel cycle facilities;radioactive waste processing;risk management;RBIDMF;assessment process;decommissioning stage;fission reactor operation;hazard identification;nuclear fuel cycle facility;regulator acceptance criteria;regulatory decision making process;risk management;risk-based informed decision making process;safe assessment;siting stage;waste management activities;Guidelines;Hazards;Licenses;Nuclear measurements;Schedules;Approach;Consenting process;Nuclear Fuel Cycle Facilities (NFCF);Regulatory decision;Risk Based Informed Decision Making Process (RBIDMP);Risk Management;Structured},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5779548&isnumber=5779525},
  Zone                     = {-}
}

@Article{Rastrello-2013-p974-981,
  Title                    = {Thermal Conductivity Detector for Gas Chromatography: Very Wide Gain Range Acquisition System and Experimental Measurements},
  Author                   = {Rastrello, F. and Placidi, P. and Scorzoni, A. and Cozzani, E. and Messina, M. and Elmi, I. and Zampolli, S. and Cardinali, G.C.},
  Journal                  = {Instrumentation and Measurement, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {974-981},
  Volume                   = {62},

  Abstract                 = {The aim of this paper is to present an acquisition system featuring a very wide gain range and experimental measurements of a new micromachined thermal conductivity detector (μ TCD), applied downstream of a gas-chromatography (GC) system. We describe a simple and innovative electronics for μTCD control and data acquisition, outlining its resistance control, native imbalance compensation, and automatic gain control (AGC) algorithm. The acquisition electronics features two parallel amplification stages with programmable gain: a high-gain stage (gain: 70-1280) and a low-gain or attenuating stage (gain: 0.6-30). The resulting detection range turns out to be very wide, with full scales ranging between 3.9 mV and 6.5 V while voltage is acquired with a 10-b-resolution analog-to-digital converter. Measurements and sensitivity tests have been carried out by connecting our μTCD and acquisition system downstream of the microfluidic section and GC column of a commercial GC system. Sensitivity measurements on several toluene masses gave very good results, having observed a system sensitivity of 15.2 ±0.6 μVs/ng. This high sensitivity will enable the μTCD to be used in many portable applications like in-line quality control, and industrial security and safety. We also show the good operation of the AGC algorithm.},
  Doi                      = {10.1109/TIM.2012.2236723},
  File                     = {Published version:Rastrello-2013-p974-981.pdf:PDF},
  Impactfactor             = {1.357},
  ISSN                     = {0018-9456},
  Keywords                 = {analogue-digital conversion;automatic gain control;chromatography;data acquisition;gas sensors;microfluidics;micromachining;microsensors;temperature sensors;thermal conductivity measurement;μTCD;AGC;GC;analog-to-digital converter;automatic gain control;experimental measurement;gas chromatography;in-line quality control;industrial safety;industrial security;microfluidic section;micromachined thermal conductivity detector;native imbalance compensation;parallel amplification stage;programmable gain;resistance control;sensitivity testing;thermal conductivity detector;toluene mass;voltage 3.9 mV to 6.5 mV;wide gain range data acquisition system;word length 10 bit;Bridge circuits;Detectors;Resistance;Resistors;Sensitivity;Temperature sensors;Automatic imbalance compensation;gas chromatography (GC);gas sensor;micromachining;resistance control;thermal conductivity (TC) detector (TCD);wide programmable gain range},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6418029&isnumber=6493455},
  Zone                     = {3Q}
}

@Book{Rausand-2013-p-,
  Title                    = {Risk assessment: Theory, methods, and applications},
  Author                   = {Rausand, Marvin},
  Publisher                = {John Wiley \& Sons},
  Year                     = {2013},
  Volume                   = {115},

  File                     = {Rausand-2013-p-.pdf:Rausand-2013-p-.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.28},
  Zone                     = {-}
}

@InProceedings{Ray-2010-p276-285,
  Title                    = {Smart power grid security: A unified risk management approach},
  Author                   = {Ray, P.D. and Harnoor, R. and Hentea, M.},
  Booktitle                = {Security Technology (ICCST), 2010 IEEE International Carnahan Conference on},
  Year                     = {2010},
  Pages                    = {276--285},

  Doi                      = {10.1109/CCST.2010.5678681},
  File                     = {Published version:Ray-2010-p276-285.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5678681},
  Zone                     = {-}
}

@Article{Reinhold-2015-p282-291,
  Title                    = {Practical tool and procedure for workplace risk assessment: Evidence from \{SMEs\} in Estonia },
  Author                   = {Karin Reinhold and Marina Järvis and Piia Tint},
  Journal                  = {Safety Science },
  Year                     = {2015},
  Note                     = {Managing Safety in Small and Medium Enterprises },
  Number                   = {0},
  Pages                    = {282 - 291},
  Volume                   = {71, Part C},

  Abstract                 = {Abstract This study focuses on a successful risk assessment procedure in small and medium sized enterprises (SMEs) where occupational health and safety resources may be less accessible than in larger companies with more labour force, time, knowledge and technology. An original approach is offered for an easy and unambiguous assessment of occupational hazards (physical, chemical and biological) and evidence from 18 industrial \{SMEs\} from different branches of industries are presented. In order to evaluate employers’ and employees’ attitudes and perceptions towards health and safety, safety interviews in eight enterprises were conducted. The results showed that in most cases participants’ attitudes in the enterprises towards contributing to safety were overall positive: developing safety procedures and practices, written work procedures and safety instructions, providing personal protective equipment, etc. The study identified important safety deficiencies such as the absence of a safety policy; insufficient safety training and unrealistic daily required work load. The measurement results showed that conditions of the working environment varied between companies as well as between the branches of industries. The main identified hazards which exceeded occupational exposure limits were: wood dust in wood processing industry, chemicals and noise in wood processing and mechanical industry, and lighting in mechanical, plastic and printing industry. The authors’ developed flexible risk assessment tool was successfully implemented in all investigated \{SMEs\} and received positive feedback from the enterprises as an applicable and suitable tool for SME’s, where skills and resources are limited. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2014.09.016},
  File                     = {Published version:Reinhold-2015-p282-291.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {Risk assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753514002264},
  Zone                     = {3Q}
}

@Article{Reniers-2008-p336-343,
  Title                    = {Managing domino effect-related security of industrial areas },
  Author                   = {G.L.L. Reniers and W. Dullaert and A. Audenaert and B.J.M. Ale and K. Soudan},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2008},
  Note                     = {Human Factors },
  Number                   = {3},
  Pages                    = {336 - 343},
  Volume                   = {21},

  Abstract                 = {In chemical enterprises, security managers are interested in easy-to-handle and user-friendly decision-support tools, providing them with straightforward information ready for implementation. Therefore, a theoretical conceptualization on how to manage—in a relatively simple way—the prevention and the mitigation of intentionally induced domino effects in a possibly very complex industrial cluster is described in this paper. The theoretical concepts for treating different chemical companies as single industrial area, and how security experts can deal with high-consequence security matters is explained. A case study illustrates a real application of the novel computer-automated tool designed for enhancing security measures at a cluster level. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2007.06.007},
  File                     = {Published version:Reniers-2008-p336-343.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Process security management},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423007001027},
  Zone                     = {3Q}
}

@Article{Reniers-2014-p94-101,
  Title                    = {Resilience of chemical industrial areas through attenuation-based security },
  Author                   = {G.L.L. Reniers and K. Sörensen and F. Khan and P. Amyotte},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {94 - 101},
  Volume                   = {131},

  Abstract                 = {Abstract This paper investigates the possibility of attenuation-based security within chemical industrial areas. Representing chemical industrial areas as mathematical networks, we prove by case-study that the resilience to disaster of such areas may follow a power-law distribution. Furthermore, we examine what happens to the network when highly hazardous installations would be intelligently protected against malicious acts: the network disintegrates into separate smaller networks. Hence, islands are formed with no escalation danger in between. We conclude that it is possible to protect chemical industrial areas in such a way that they are more resilient against terrorism. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2014.05.005},
  File                     = {Published version:Reniers-2014-p94-101.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Attenuation-based security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832014001094},
  Zone                     = {2Q}
}

@Article{Reniers-2014-p583-589,
  Title                    = {Preparing for major terrorist attacks against chemical clusters: Intelligently planning protection measures w.r.t. domino effects },
  Author                   = {Genserik L.L. Reniers and Amaryllis Audenaert},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2014},
  Number                   = {6},
  Pages                    = {583 - 589},
  Volume                   = {92},

  Abstract                 = {Abstract Chemical industrial areas or so-called chemical clusters consist of hundreds, and sometimes thousands, of chemical installations situated next to each other. Such areas can thus be seen as the summation of a large number of structures exhibiting danger to a certain degree for initiating or continuing accident domino effects or knock-on effects. In this article, an approach to investigate in a systemic way the vulnerability of each installation within the larger chemical cluster context, is developed. Our suggested method results in a prioritization of chemical installations with respect to their vulnerability for domino effects. The method can be used for intelligently designed protection of chemical industrial areas against terrorist attacks. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2013.04.002},
  File                     = {Published version:Reniers-2014-p583-589.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Chemical cluster},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095758201300030X},
  Zone                     = {3Q}
}

@Article{Rey-2014-p109-119,
  Title                    = {Infrastructure based approach for roadway departure risk assessment },
  Author                   = {Guillaume Rey and David Clair and Michel Fogli and Frédéric Bernardin},
  Journal                  = {Transportation Research Part C: Emerging Technologies },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {109 - 119},
  Volume                   = {48},

  Abstract                 = {Abstract This paper presents a warning device to prevent the roadway departure of light vehicles while cornering. The proposed risk assessment methodology is based on recent works from the authors (Rey et al., 2011b,a; Rey, 2010). Given the random variability arising from the driver, the vehicle and the infrastructure at the entrance to the curve, a probabilistic strategy is adopted to assess the roadway departure risk. The infrastructure-based methodology enables the real curve characteristics to be considered and an alarm triggering decision to be made. Two safety criteria are tested and the potential safety benefits of the proposed warning device are evaluated. Contrary to existing roadway departure warning systems, the proposed approach does not require extra equipment for vehicles; it only requires that the measuring and warning devices be part of the road infrastructure, which is a great advantage in terms of economic cost. },
  Doi                      = {http://dx.doi.org/10.1016/j.trc.2014.08.022},
  File                     = {Published version:Rey-2014-p109-119.pdf:PDF},
  Impactfactor             = {2.006},
  ISSN                     = {0968-090X},
  Keywords                 = {Roadway departure},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0968090X14002459},
  Zone                     = {3Q}
}

@Article{Ribe-2012-p15-20,
  Title                    = {Applying the Triad method in a risk assessment of a former surface treatment and metal industry site },
  Author                   = {Veronica Ribé and Elisabet Aulenius and Emma Nehrenheim and Ulrika Martell and Monica Odlare},
  Journal                  = {Journal of Hazardous Materials },
  Year                     = {2012},
  Note                     = {Selected papers presented at the 2nd International Conference \{CRETE\} 2010, October 2010 - Industrial and Hazardous Waste Management },
  Number                   = {0},
  Pages                    = {15 - 20},
  Volume                   = {207–208},

  Abstract                 = {With a greater focus on soil protection in the EU, the need for ecological risk assessment tools for cost-effective characterization of site contamination is increasing. One of the challenges in assessing the risk of soil contaminants is to accurately account for changes in mobility of contaminants over time, as a result of ageing. Improved tools for measuring the bioavailable and mobile fraction of contaminants is therefore highly desirable. In this study the Triad method was used to perform a risk characterization of a former surface treatment and metal industry in Eskilstuna, Sweden. The risk assessment confirmed the environmental risk of the most heavily contaminated sample and showed that the toxic effect was most likely caused by high metal concentrations. The assessment of the two soil samples with low to moderate metal contamination levels was more complex, as there was a higher deviation between the results from the three lines of evidence; chemistry, (eco)toxicology and ecology. For the slightly less contaminated sample of the two, a weighting of the results from the ecotoxicological LoE would be recommended in order to accurately determine the risk of the metal contamination at the sampling site as the toxic effect detected in the Microtox® test and Ostracodtoxkit™ test was more likely to be due to oil contamination. The soil sample with higher total metal concentrations requires further ecotoxicological testing, as the integrated risk value indicated an environmental risk from metal contamination. The applied methodology, the Triad method, is considered appropriate for conducting improved environmental risk assessments in order to achieve sustainable remediation processes. },
  Doi                      = {http://dx.doi.org/10.1016/j.jhazmat.2011.07.120},
  File                     = {Published version:Ribe-2012-p15-20.pdf:PDF},
  Impactfactor             = {3.925},
  ISSN                     = {0304-3894},
  Keywords                 = {Ecological risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，利用三元组发进行风险评估，冶金工业，本文也是分层分析风险的，从采样开始，一层一层的分析，有一定的参考意义},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304389411010028},
  Zone                     = {1Q}
}

@Article{Ribe-2012-p15-20a,
  Title                    = {Applying the Triad method in a risk assessment of a former surface treatment and metal industry site },
  Author                   = {Veronica Ribé and Elisabet Aulenius and Emma Nehrenheim and Ulrika Martell and Monica Odlare},
  Journal                  = {Journal of Hazardous Materials },
  Year                     = {2012},
  Note                     = {Selected papers presented at the 2nd International Conference \{CRETE\} 2010, October 2010 - Industrial and Hazardous Waste Management },
  Number                   = {0},
  Pages                    = {15 - 20},
  Volume                   = {207–208},

  Abstract                 = {With a greater focus on soil protection in the EU, the need for ecological risk assessment tools for cost-effective characterization of site contamination is increasing. One of the challenges in assessing the risk of soil contaminants is to accurately account for changes in mobility of contaminants over time, as a result of ageing. Improved tools for measuring the bioavailable and mobile fraction of contaminants is therefore highly desirable. In this study the Triad method was used to perform a risk characterization of a former surface treatment and metal industry in Eskilstuna, Sweden. The risk assessment confirmed the environmental risk of the most heavily contaminated sample and showed that the toxic effect was most likely caused by high metal concentrations. The assessment of the two soil samples with low to moderate metal contamination levels was more complex, as there was a higher deviation between the results from the three lines of evidence; chemistry, (eco)toxicology and ecology. For the slightly less contaminated sample of the two, a weighting of the results from the ecotoxicological LoE would be recommended in order to accurately determine the risk of the metal contamination at the sampling site as the toxic effect detected in the Microtox® test and Ostracodtoxkit™ test was more likely to be due to oil contamination. The soil sample with higher total metal concentrations requires further ecotoxicological testing, as the integrated risk value indicated an environmental risk from metal contamination. The applied methodology, the Triad method, is considered appropriate for conducting improved environmental risk assessments in order to achieve sustainable remediation processes. },
  Doi                      = {http://dx.doi.org/10.1016/j.jhazmat.2011.07.120},
  File                     = {Published version:Ribe-2012-p15-20a.pdf:PDF},
  Impactfactor             = {3.925},
  ISSN                     = {0304-3894},
  Keywords                 = {Ecological risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0304389411010028},
  Zone                     = {1Q}
}

@InProceedings{Rmayti-2014-p7-12,
  Title                    = {Denial of service (DoS) attacks detection in MANETs using Bayesian classifiers},
  Author                   = {Rmayti, M. and Begriche, Y. and Khatoun, R. and Khoukhi, L. and Gaiti, D.},
  Booktitle                = {Communications and Vehicular Technology in the Benelux (SCVT), 2014 IEEE 21st Symposium on},
  Year                     = {2014},
  Month                    = {Nov},
  Pages                    = {7-12},

  Doi                      = {10.1109/SCVT.2014.7046699},
  File                     = {Published version:Rmayti-2014-p7-12.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Bayes methods;computer network security;mobile ad hoc networks;routing protocols;Bayesian classifier;Bayesian filter;Bernoulli filter;DoS attack detection;MANET;blackhole attack;denial of service attack detection;grayhole attack;intrusion detection mechanism;malicious node;mobile ad hoc network;mobile wireless entity;multinomial filter;packet dropping attack detection;reactive routing protocol;received packet drop;route discovery phase;self-organized network;watchdog;wormhole attack;Ad hoc networks;Bayes methods;Computer crime;Mobile computing;Routing;Routing protocols;Vectors},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7046699&isnumber=7046694},
  Zone                     = {-}
}

@Article{Romeis-2014-p1-10,
  Title                    = {From local to global: \{CDPKs\} in systemic defense signaling upon microbial and herbivore attack },
  Author                   = {Tina Romeis and Marco Herde},
  Journal                  = {Current Opinion in Plant Biology },
  Year                     = {2014},
  Note                     = {SI: Biotic interactions },
  Number                   = {0},
  Pages                    = {1 - 10},
  Volume                   = {20},

  Abstract                 = {Calcium-dependent protein kinases (CDPKs) are multifunctional proteins in which a calmodulin-like calcium-sensor and a protein kinase effector domain are combined in one molecule. Not surprisingly, \{CDPKs\} were primarily recognized as signaling mediators, which perceive rapid intracellular changes of Ca2+ ion concentration, for example triggered by environmental stress cues, and relay them into specific phosphorylation events to induce further downstream stress responses. In the context of both, plant exposure to biotrophic pathogens-derived signals as well as plant attack by herbivores and wounding, \{CDPKs\} were shown to undergo rapid biochemical activation within seconds to minutes after stimulation and to induce local defence-responses including respective changes in gene expression patterns. In addition, \{CDPK\} function was correlated with the control of either salicylic acid-mediated or jasmonic acid-mediated phytohormone signaling pathways, mediating long term resistance to either biotrophic bacterial pathogens or herbivores also in distal parts of a plant. It has long been unclear how an individual enzyme can affect both rapid local as well as long-term distal immune responses. Here, we discuss recently raised topics from the field of \{CDPK\} research, in particular with a view on the identification of in vivo phosphorylation targets, which provide first mechanistic insights into the dual role of these enzymes: On the one hand as component of a self-activating circuit responsible for rapid plasma-membrane anchored cell-to-cell signal propagation from local to distal plant sites. On the other hand as nuclear-located regulators of transcription factor activity. Finally, we will highlight the dual function of calcium sensors in plasma-membrane/calcium-mediated signal propagation and in phytohormone signaling-dependent systemic resistance in immune responses to both, bacterial pathogens and herbivores. },
  Doi                      = {http://dx.doi.org/10.1016/j.pbi.2014.03.002},
  File                     = {Published version:Romeis-2014-p1-10.pdf:PDF},
  Impactfactor             = {8.455},
  ISSN                     = {1369-5266},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1369526614000302},
  Zone                     = {1Q}
}

@Article{Ross-2014-p1-1,
  Title                    = {A Socio-Physical Approach to Systemic Risk Reduction in Emergency Response and Preparedness},
  Author                   = {Ross, W. and Gorod, A. and Ulieru, M.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},
  Number                   = {99},
  Pages                    = {1-1},
  Volume                   = {PP},

  Abstract                 = {This paper proposes a socio-physical approach that considers jointly the interaction and integration of the social and physical views of a system to improve emergency response and preparedness. This is accomplished through a reduction of systemic risk, which refers to a risk that could be greater than the sum of the risks of the individual system constituents. Using network analysis, it is shown that the explicit socio-physical approach yields meaningful qualitative and quantitative differences when compared with approaches that focus on the social and physical views in isolation. The benefits of this proposed approach are illustrated on a case study using clustering analysis and a proof-of-concept simulation. This new approach leads to systemic risk reduction by enabling a more informed and coordinated response strategy following an incident and a better identification of possible consequences and preparation strategies prior to an incident.},
  Doi                      = {10.1109/TSMC.2014.2336831},
  File                     = {Published version:Ross-2014-p1-1.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Clustering coefficient;emergency response and preparedness;risk reduction;situational awareness;socio-physical view;systemic risk},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6866197&isnumber=6376248},
  Zone                     = {-}
}

@Article{Rouached-2012-p968-975,
  Title                    = {An Efficient Formal Framework for Intrusion Detection Systems },
  Author                   = {Mohsen Rouached and Hassen Sallay},
  Journal                  = {Procedia Computer Science },
  Year                     = {2012},
  Note                     = {\{ANT\} 2012 and MobiWIS 2012 },
  Number                   = {0},
  Pages                    = {968 - 975},
  Volume                   = {10},

  Abstract                 = {Traffic anomalies and attacks are commonplace in today's networks, and identifying them rapidly and accurately is critical for large network operators. Intrusion detection systems are an important component of defensive measures protecting computer systems and networks from abuse.For an intrusion detection system, it is important to detect previously known attacks with high accuracy. However, detecting previously unseen attacks is equally important in order to minimize the losses as a result of a successful intrusion. It is also equally important to detect attacks at an early stage in order to minimize their impact. To address these challenges, this paper proposes to improve the efficiency of the network intrusion detection process by including an Event Calculus based specification to detect the registered and expected behaviour of the whole network. },
  Doi                      = {http://dx.doi.org/10.1016/j.procs.2012.06.132},
  File                     = {Published version:Rouached-2012-p968-975.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-0509},
  Keywords                 = {Intrusion detection systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877050912004899},
  Zone                     = {-}
}

@Article{Roy-2014-p-,
  Title                    = {Dynamic failure assessment of an ammonia storage unit: A case study },
  Author                   = {Arnab Roy and Prashant Srivastava and Shishir Sinha},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract Chemical Process Industries usually contain a diverse inventory of hazardous chemicals and complex systems required to perform process operations such as storage, separation, reaction, compression etc. The complex interactions between the equipment make them vulnerable to catastrophic accidents. Risk and failure assessment provide engineers with an intuitive tool for decision making in the operation of such plants. Abnormal events and near-miss situations occur regularly during the operation of a system. Accident Sequence Precursors (ASP) can be used to demonstrate the real-time operating condition of a plant. Dynamic Failure Assessment (DFA) methodology is based on Bayesian statistical methods incorporates \{ASP\} data to revise the generic failure probabilities of the systems during its operational lifetime. In this paper, \{DFA\} methodology is applied on an ammonia storage unit in a specialized chemical industry. Ammonia is stored in cold storage tanks as liquefied gas at atmospheric pressure. These tanks are susceptible to failures due to various abnormal conditions arising due process failures. Tank failures due to three such abnormal conditions are considered. Variation of the failure probability of the safety systems is demonstrated. The authors use \{ASP\} data collected from plant specific sources and safety expert judgement. The failure probabilities of some safety systems concerned show considerable deviation from the generic values. The method helps to locate the components which have undergone more degradation over the period and hence must be paid attention to. In addition, a Bayesian predictive model has been used to predict the number of abnormal events in the next time interval. The user-friendly and intuitive nature of the tool makes it appropriate for application in safety assessment reports in process industries.},
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2014.09.004},
  File                     = {Published version:Roy-2014-p-.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Dynamic failure assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957582014001487},
  Zone                     = {3Q}
}

@InProceedings{Roy-2004-p509-512,
  Title                    = {ESRML: a markup language for enterprise security requirement specification},
  Author                   = {Roy, J. and Barik, M.S. and Mazumdar, C.},
  Booktitle                = {India Annual Conference, 2004. Proceedings of the IEEE INDICON 2004. First},
  Year                     = {2004},
  Month                    = {Dec},
  Pages                    = {509-512},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/INDICO.2004.1497807},
  File                     = {Published version:Roy-2004-p509-512.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {ISO standards;XML;authorisation;business communication;computer network management;intranets;specification languages;telecommunication security;ESRML;ISO 17799;XML based structured language;computing system monitoring;enterprise security requirement markup language;information security management;markup language;safeguard enterprise information system;specification language;unauthorized intrusion;Best practices;Guidelines;ISO standards;Information management;Information security;Management information systems;Markup languages;Monitoring;Specification languages;XML},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1497807&isnumber=32114},
  Zone                     = {-}
}

@Article{Rudinskiy-2007-p68-73,
  Title                    = {Fuzzy Knowledge Evaluation Model as a Methodological Basis for Automation of Pedagogical Testing},
  Author                   = {Rudinskiy, I.D.},
  Journal                  = {Education, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {68-73},
  Volume                   = {50},

  Doi                      = {10.1109/TE.2006.888904},
  File                     = {Published version:Rudinskiy-2007-p68-73.pdf:PDF},
  Impactfactor             = {0.95},
  ISSN                     = {0018-9359},
  Keywords                 = {computer aided instruction;fuzzy set theory;testing;educational systems;fuzzy algebra;fuzzy knowledge evaluation models;information technologies;information-oriented society;knowledge evaluation model;mathematical methods;pedagogical testing automation;Algebra;Automatic testing;Automation;Education;Fuzzy logic;Information technology;Inspection;Logic testing;Marine vehicles;System testing;Evaluation;fuzzy logic;knowledge;pedagogical testing},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4084632&isnumber=4084613},
  Zone                     = {4Q}
}

@InProceedings{Rugge-2012-p1-10,
  Title                    = {The case for NATO-EU cooperation in the protection of cyberspace},
  Author                   = {Rugge, F.},
  Booktitle                = {Cybersecurity Summit (WCS), 2012 Third Worldwide},
  Year                     = {2012},
  Month                    = {Oct},
  Pages                    = {1-10},

  Doi                      = {10.1109/WCS.2012.6780880},
  File                     = {Published version:Rugge-2012-p1-10.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {emergency management;organisational aspects;security of data;NATO-EU cooperation;civilian-military capabilities;crisis-management procedures;cyber defence;cyberspace protection;education strategies;international cooperation;international organization cooperation;like-minded actors;minimum security standards requirements;operational cyber capabilities;private sector;situational awareness;training strategies;transatlantic networks;Collaboration;Computer crime;Cyberspace;Europe;Radiation detectors},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6780880&isnumber=6780872},
  Zone                     = {-}
}

@InProceedings{Rutt-2006-p61-70,
  Title                    = {Distributed dynamic event tree generation for reliability and risk assessment},
  Author                   = {Rutt, B. and Catalyurek, U. and Hakobyan, A. and Metzroth, K. and Aldemir, T. and Denning, R. and Dunagan, S. and Kunsman, D.},
  Booktitle                = {Challenges of Large Applications in Distributed Environments, 2006 IEEE},
  Year                     = {2006},
  Pages                    = {61-70},

  Abstract                 = {Level 2 probabilistic risk assessments of nuclear plants (analysis of radionuclide release from containment) may require hundreds of runs of severe accident analysis codes such as MELCOR or RELAP/SCDAP to analyze possible sequences of events (scenarios) that may follow given initiating events. With the advances in computer architectures and ubiquitous networking, it is now possible to utilize multiple computing and storage resources for such computational experiments. This paper presents a system software infrastructure that supports execution and analysis of multiple dynamic event-tree simulations on distributed environments. The infrastructure allow for 1) the testing of event tree completeness, and, 2) the assessment and propagation of uncertainty on the plant state in the quantification of event trees},
  Doi                      = {10.1109/CLADE.2006.1652056},
  File                     = {Published version:Rutt-2006-p61-70.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {fault trees;nuclear engineering computing;nuclear power stations;probability;risk analysis;systems software;ubiquitous computing;computer architecture;distributed database support;distributed dynamic event tree generation;distributed execution;distributed storage resource;nuclear plant;probabilistic risk assessment;reliability;system software infrastructure;ubiquitous networking;Accidents;Analytical models;Computational modeling;Computer architecture;Computer networks;Nuclear power generation;Pervasive computing;Risk analysis;Risk management;System software},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1652056&isnumber=34631},
  Zone                     = {-}
}

@InProceedings{Rysavy-2013-p1435-1440,
  Title                    = {Improving security in SCADA systems through firewall policy analysis},
  Author                   = {Rysavy, O. and Rab, J. and Sveda, M.},
  Booktitle                = {Computer Science and Information Systems (FedCSIS), 2013 Federated Conference on},
  Year                     = {2013},
  Month                    = {Sept},
  Pages                    = {1435-1440},

  File                     = {Published version:Rysavy-2013-p1435-1440.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {IP networks;SCADA systems;computer network security;firewalls;formal specification;formal verification;process control;production engineering computing;IP address;Internet;SAT/SMT tools;SCADA environment;SCADA network;SCADA system security;company enterprise network;critical process control;cyber-security requirements;firewall configuration verification;firewall device;firewall policy analysis;industrial automation system;industrial system;logical formula;network security;security policy specification;security policy violation;straightforward verification method;Abstracts;Arrays;Ports (Computers);Radiation detectors;SCADA systems;Security;Vectors},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6644205&isnumber=6643962},
  Zone                     = {-}
}

@InProceedings{Ryu-2008-p653-658,
  Title                    = {Security Requirement for Cyber Attack Traceback},
  Author                   = {JongHo Ryu and Jungchan Na},
  Booktitle                = {Networked Computing and Advanced Information Management, 2008. NCM '08. Fourth International Conference on},
  Year                     = {2008},
  Month                    = {Sept},
  Pages                    = {653-658},
  Volume                   = {2},

  Doi                      = {10.1109/NCM.2008.188},
  File                     = {Published version:Ryu-2008-p653-658.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {distributed processing;security of data;tracking;account tracking technology;attacking programs;counter-cyber attacks;cyber attack traceback;hacking;information security;networking domain environment;security requirement;traceback technology;viruses;Communication system security;Computer crime;Computer hacking;Computer networks;Computer security;Data security;Information management;Information security;Standards development;Telecommunication computing;Requirement;Traceback},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4624222&isnumber=4624098},
  Zone                     = {-}
}

@Article{Sacha-2014-p1604-1613,
  Title                    = {Knowledge Generation Model for Visual Analytics},
  Author                   = {Sacha, D. and Stoffel, A. and Stoffel, F. and Kwon, B.C. and Ellis, G. and Keim, D.A.},
  Journal                  = {Visualization and Computer Graphics, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {1604-1613},
  Volume                   = {20},

  Doi                      = {10.1109/TVCG.2014.2346481},
  File                     = {Published version:Sacha-2014-p1604-1613.pdf:PDF},
  Impactfactor             = {1.898},
  ISSN                     = {1077-2626},
  Keywords                 = {Analytical models;Computational modeling;Data models;Data visualization;Visual analytics;Interaction;Knowledge Generation;Reasoning;Visual Analytics;Visualization Taxonomies and Models},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6875967&isnumber=6935054},
  Zone                     = {2Q}
}

@Article{Sakti-2007-p1199-1211,
  Title                    = {Incorporating Knowledge Sources Into a Statistical Acoustic Model for Spoken Language Communication Systems},
  Author                   = {Sakti, S. and Markov, K. and Nakamura, S.},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {2007},

  Month                    = {Sept},
  Number                   = {9},
  Pages                    = {1199-1211},
  Volume                   = {56},

  Doi                      = {10.1109/TC.2007.1069},
  File                     = {Published version:Sakti-2007-p1199-1211.pdf:PDF},
  Impactfactor             = {1.379},
  ISSN                     = {0018-9340},
  Keywords                 = {belief networks;hidden Markov models;natural language interfaces;probability;speech recognition;speech-based user interfaces;Bayesian network framework;English speech data;HMM-based statistical acoustic model;graphical model framework;knowledge sources;large-vocabulary continuous speech recognition;probability density function;spoken language communication system;Bayesian methods;Graphical models;Hidden Markov models;Memory management;Natural languages;Performance evaluation;Probability density function;Speech analysis;Speech recognition;Training data;Acoustic modeling;Bayesian network;junction tree;knowledge incorporation;wide-context dependency},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4288087&isnumber=4288079},
  Zone                     = {3Q}
}

@InProceedings{Salini-2012-p1-6,
  Title                    = {Application of Model Oriented Security Requirements Engineering Framework for secure E-Voting},
  Author                   = {Salini, P. and Kanmani, S.},
  Booktitle                = {Software Engineering (CONSEG), 2012 CSI Sixth International Conference on},
  Year                     = {2012},
  Month                    = {Sept},
  Pages                    = {1-6},

  Doi                      = {10.1109/CONSEG.2012.6349489},
  File                     = {Published version:Salini-2012-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Internet;formal specification;formal verification;government data processing;security of data;MOSRE;asset identification;election system;functional requirements;model oriented security requirement engineering framework;requirement phase;secure E-voting;secure electronic Web application;security sensitive processes;threat identification;vulnerability identification;Databases;Electronic voting;Nominations and elections;Object recognition;Security;Unified modeling language},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6349489&isnumber=6349464},
  Zone                     = {-}
}

@Article{Salini-2012-p1785-1797,
  Title                    = {Survey and analysis on Security Requirements Engineering },
  Author                   = {P. Salini and S. Kanmani},
  Journal                  = {Computers \& Electrical Engineering },
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {1785 - 1797},
  Volume                   = {38},

  Abstract                 = {Security Requirements Engineering is a new research area in software engineering, with the realization that security must be analyzed early during the requirements phase. Many researchers are working in this area; however, there is a lack in security requirements treatment. The security requirements are one of the non-functional requirements, which act as constraints on functions of the system. Organizations are depending on information systems for communicating and sharing information. Thus, \{IT\} security is becoming central in fulfilling business goals, to guard assets and to create trustworthy systems. To develop systems with adequate security features, it is essential to capture the security requirements. In this paper, we present a view on Security Requirements, issues, types, Security Requirements Engineering (SRE) and methods. We analyzed and compared different methods and found that \{SQUARE\} and Security Requirements Engineering Process methods cover most of the important activities of SRE. The developers can adopt these \{SRE\} methods and easily identify the security requirements for software systems. },
  Doi                      = {http://dx.doi.org/10.1016/j.compeleceng.2012.08.008},
  File                     = {Published version:Salini-2012-p1785-1797.pdf:PDF},
  Impactfactor             = {0.928},
  ISSN                     = {0045-7906},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.08},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0045790612001644},
  Zone                     = {4Q}
}

@Article{Salvi-2005-p414-422,
  Title                    = {Toward an integrative approach of the industrial risk management process in France },
  Author                   = {Olivier Salvi and Myriam Merad and Nelson Rodrigues},
  Journal                  = {Journal of Loss Prevention in the Process Industries },
  Year                     = {2005},
  Note                     = {Selected Papers Presented at the International Conference on Bhopal Gas Tragedy and its Effects on Process Safety International Conference on Bhopal Gas Tragedy and its Effects on Process Safety },
  Number                   = {4–6},
  Pages                    = {414 - 422},
  Volume                   = {18},

  Abstract                 = {Management in the field of environmental protection and risk prevention has evolved to the increasing participation of all stakeholders in the decision-making process. It certainly results from the development of the Information society and the global increase of knowledge of the population, combined with the concerns of the populations related to a sustainable development of our civilisation. Our ‘risk society’, following the big industrial disasters (Flixborough,Tchernobyl, Bhopal, Challenger, and more recently Toulouse), has also developed a cautious attitude towards the role of the expertise when it comes to assessing risks, along with a question of the ability of science to give definite answers. This has lead in particular to the adoption of the Aarhus convention in 1998 and the evolution of several regulations in the developed countries. For example, in France the new law no. 2003–699 of 30 July 2003 about the ‘prevention of the technological and natural risks and to the compensation for the damages’ has introduced an important innovation into the process of technological risks prevention. This law has enabled the involvement of the stakeholders in the decision-making process related to risk prevention and has urged the development of specific tools to deal with the complexity of risk management issues, in particular for those related to land-use planning. As technical support to decision-makers in risk management from both public and private sectors, \{INERIS\} has played an important role for the evolution of the French risk management system. This paper describes an analysis on the difficulty to control major accident hazards in an evolving context where the industrial systems becomes more and more complex and where the expectations of the civil society has increased. Then, the authors describe how an integrated vision for industrial risk management has emerged in France and is being implemented in a new law adopted after the Toulouse disaster. },
  Doi                      = {http://dx.doi.org/10.1016/j.jlp.2005.06.030},
  File                     = {Published version:Salvi-2005-p414-422.pdf:PDF},
  Impactfactor             = {1.15},
  ISSN                     = {0950-4230},
  Keywords                 = {Risk management},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950423005000975},
  Zone                     = {3Q}
}

@Article{Samejima-2015-p725-733,
  Title                    = {Chance-Constrained Programming Method of IT Risk Countermeasures for Social Consensus Making},
  Author                   = {Samejima, M. and Sasaki, R.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {725-733},
  Volume                   = {45},

  Abstract                 = {The authors address a social consensus making support in discussing countermeasures for information technology risks (IT risks). For supporting stakeholders' discussion on which IT risk countermeasures the stakeholders should implement, experts of the risk management estimate parameter values of the countermeasure, define a goal and constraints, and formulate the decision problem of the countermeasures to be implemented as one of 0-1 integer programming problems. Because parameter values and constraint values are uncertain, the decision problem is reformulated as a chance-constrained programming problem. The sample average approximation method is a well-known method for solving the chance-constrained programming problem. However, the computational time is still so long that the opinion leaders cannot use a solution of the chance-constrained programming problem in their discussion. The authors propose a high-speed chance-constrained programming method by aggregating the constraints that are generated by approximation of the problem in the sample average method. By applying the proposed method to real decision problems, the authors confirmed that computational time is decreased to 1 min while obtaining the same error rate and the same rate of the feasible solutions as a conventional method.},
  Doi                      = {10.1109/TSMC.2014.2376491},
  File                     = {Published version:Samejima-2015-p725-733.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {approximation theory;constraint handling;information systems;information technology;integer programming;parameter estimation;risk management;social sciences;0-1 integer programming problems;IT risk countermeasures;computational time;decision problem;high-speed chance-constrained programming method;information technology risks;parameter value estimation;risk management;sample average approximation method;social consensus making;Approximation methods;Cybernetics;Linear programming;Probability distribution;Programming;Random variables;Vectors;Chance-constrained programming;constraint aggregation;information technology risk (IT risk);sample average approximation method;social consensus making},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6991614&isnumber=7084706},
  Zone                     = {-}
}

@Article{Samet-2013-p3657-3667,
  Title                    = {Incremental learning of privacy-preserving Bayesian networks },
  Author                   = {Saeed Samet and Ali Miri and Eric Granger},
  Journal                  = {Applied Soft Computing },
  Year                     = {2013},
  Number                   = {8},
  Pages                    = {3657 - 3667},
  Volume                   = {13},

  Abstract                 = {Abstract Bayesian Networks (BNs) have received significant attention in various academic and industrial applications, such as modeling knowledge in image processing, engineering, medicine and bio-informatics. Preserving the privacy of sensitive data, owned by different parties, is often a critical issue. However, in many practical applications, \{BNs\} must train from data that gradually becomes available at different period of times, on which the traditional batch learning algorithms are not suitable or applicable. In this paper, an algorithm based on a new and efficient version of Sufficient Statistics is proposed for incremental learning with BNs. The standard K 2  algorithm is also modified to be utilized inside the incremental learning algorithm. Next, some secure building blocks such as secure comparison, and factorial, which are resistant against colluding attacks and could be applied securely over public channels like internet, are presented to be used inside the main protocol. Then a privacy-preserving protocol is proposed for incremental learning of BNs, in which the structure and probabilities are estimated incrementally from homogeneously distributed and gradually available data among two or multi-parties. Finally, security and complexity analysis along with the experimental results are presented to compare with the batch algorithm and to show its performance and applicability in real world applications. },
  Doi                      = {http://dx.doi.org/10.1016/j.asoc.2013.03.011},
  File                     = {Published version:Samet-2013-p3657-3667.pdf:PDF},
  Impactfactor             = {2.14},
  ISSN                     = {1568-4946},
  Keywords                 = {Security and privacy preserving},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S156849461300118X},
  Zone                     = {3Q}
}

@InProceedings{Santos-2014-p1-9,
  Title                    = {A dynamic risk-based access control architecture for cloud computing},
  Author                   = {dos Santos, D.R. and Merkle Westphall, C. and Becker Westphall, C.},
  Booktitle                = {Network Operations and Management Symposium (NOMS), 2014 IEEE},
  Year                     = {2014},
  Month                    = {May},
  Pages                    = {1-9},

  Abstract                 = {Cloud computing is a distributed computing model that still faces problems. New ideas emerge to take advantage of its features and among the research challenges found in the cloud, we can highlight Identity and Access Management. The main problems of the application of access control in the cloud are the necessary flexibility and scalability to support a large number of users and resources in a dynamic and heterogeneous environment, with collaboration and information sharing needs. This paper proposes the use of risk-based dynamic access control for cloud computing. The proposal is presented as an access control model based on an extension of the XACML standard with three new components: the Risk Engine, the Risk Quantification Web Services and the Risk Policies. The risk policies present a method to describe risk metrics and their quantification, using local or remote functions. The risk policies allow users and cloud service providers to define how to handle risk-based access control for their resources, using different quantification and aggregation methods. The model reaches the access decision based on a combination of XACML decisions and risk analysis. A prototype of the model is implemented, showing it has enough expressivity to describe the models of related work. In the experimental results, the prototype takes between 2 and 6 milliseconds to reach access decisions using a risk policy. A discussion on the security aspects of the model is also presented.},
  Doi                      = {10.1109/NOMS.2014.6838319},
  File                     = {Published version:Santos-2014-p1-9.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Web services;authorisation;cloud computing;groupware;risk analysis;software architecture;Web services;XACML standard;cloud computing;collaboration;distributed computing;dynamic environment;dynamic risk-based access control architecture;heterogeneous environment;identity and access management;information sharing needs;risk engine;risk policies;risk quantification;Access control;Cloud computing;Computational modeling;Engines;Measurement;Proposals},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6838319},
  Zone                     = {-}
}

@Article{Santos-2013-p33-41,
  Title                    = {Hydrothermal alteration related to a deep mantle source controlled by a Cambrian intracontinental strike-slip fault: Evidence for the Meruoca felsic intrusion associated with the Transbraziliano Lineament, Northeastern Brazil },
  Author                   = {Roberto Ventura Santos and Claudinei Gouveia de Oliveira and Clóvis Vaz Parente and Maria da Glória Motta Garcia and Elton Luis Dantas},
  Journal                  = {Journal of South American Earth Sciences },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {33 - 41},
  Volume                   = {43},

  Abstract                 = {One of the most prominent geological structures in Borborema Province, northeast Brazil, is the Transbraziliano Lineament that crosscuts most of the South American Platform and was active at least until the Devonian. This continental structure is responsible for the formation of rift and pull-apart basins in Northeastern Brazil, most of which filled with volcanic and continental sedimentary rocks (Parente et al., 2004). In the region of Sobral, Ceará State, this same continental structure controlled the intrusion of the Meruoca pluton and the formation of the Jaibaras Basin, which is bounded by strike-slip shear zones. Hydrothermal alterations seem to have been pervasive in Meruoca, as indicated by disturbances in both the Rb–Sr and U–Pb systems (Sial et al., 1981; Fetter, 1999) and by the large dispersion of anisotropic magnetic susceptibility (AMS) (Archanjo et al., 2009). In this paper, we address the origin of the hydrothermal fluids that affected the borders of the Meruoca batholith and their relationship with the activity of the Transbraziliano Lineament. These fluids were responsible for carbonate veins and Fe–Cu mineral concentrations that are commonly found associated with hydrothermally altered breccias. The carbon and oxygen isotope composition of these carbonate veins suggest that they may be related to CO2-bearing mantle-derived fluids that were channelized by the Transbraziliano Lineament. Based on oxygen isotopes, we argue that Fe–Cu concentrations may have formed in isotope equilibrium with the rhyolitic rocks at temperatures between 500 and 560 °C. This scenario points to magmatism as the main process in the formation of these rocks. We also report a K–Ar age of 530 ± 12 Ma for muscovite associated with the last ductile event that affected the Sobral-Pedro \{II\} Shear Zone and a U–Pb age of 540.8 ± 5.1 Ma for the Meruoca pluton. We further suggest that this granite is a late-kinematic intrusion that is most likely associated with the Parapuí volcanic rocks of the Jaibaras basin (535.6 ± 8.5 Ma, Garcia et al., 2010). },
  Doi                      = {http://dx.doi.org/10.1016/j.jsames.2012.12.005},
  File                     = {Published version:Santos-2013-p33-41.pdf:PDF},
  Impactfactor             = {1.533},
  ISSN                     = {0895-9811},
  Keywords                 = {Transbraziliano Lineament},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0895981112001794},
  Zone                     = {3Q}
}

@InProceedings{Sauciuc-2014-p937-941,
  Title                    = {Event-based use conditions method for thermo-mechanical reliability risk assessment},
  Author                   = {Sauciuc, I. and Goyal, S. and Min Pei and Harirchian, T. and Tse, M. and Kwasnick, R. and Tripathi, S. and Matusevich, A.},
  Booktitle                = {Thermal and Thermomechanical Phenomena in Electronic Systems (ITherm), 2014 IEEE Intersociety Conference on},
  Year                     = {2014},
  Month                    = {May},
  Pages                    = {937-941},

  Abstract                 = {The traditional approach of thermo-mechanical (T-M) reliability modeling is based on power cycle events. This approach is not useful for products which are rarely powered down, because power cycles alone do not capture all the reliability stress from temperature variation over these products' use life. This paper describes a methodology to determine the temperature cycle requirements for products like smartphones and tablets which accounts for the temperature variation associated with usage events, which we call “mini-cycles”. The T-M model is based on the distribution of individual users' histories as a series of events over time, which is then translated into a temperature vs. time trace for each user. These temperature traces are then used as the main inputs to T-M models, for example using the Norris-Landzberg (N-L) acceleration model to evaluate solder damage for each user. Results are summarized in a distribution of T-M damage across all users. This new methodology improves the understanding of thermo-mechanical reliability requirements due to the impact of “mini-cycles”.},
  Doi                      = {10.1109/ITHERM.2014.6892382},
  File                     = {Published version:Sauciuc-2014-p937-941.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1087-9870},
  Keywords                 = {reliability;risk analysis;solders;thermal management (packaging);T-M damage distribution;T-M model;event-based use condition method;mini-cycle;power cycle event;risk assessment;solder damage evaluation;temperature cycle determination;thermomechanical reliability modeling;Entertainment industry;History;Mathematical model;Reliability;Smart phones;Soldering;Temperature measurement;event-based use conditions;knowledge-based qualification;solder joint reliability;thermo-mechanical},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6892382&isnumber=6892251},
  Zone                     = {-}
}

@Article{Sauer-1992-p242-249,
  Title                    = {The engineer as rational man: the problem of imminent danger in a non-rational environment},
  Author                   = {Sauer, B.A.},
  Journal                  = {Professional Communication, IEEE Transactions on},
  Year                     = {1992},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {242-249},
  Volume                   = {35},

  Abstract                 = {US government and industry attitudes toward mine safety and health, articulated in the instruction manuals and training guides published by the Mine Safety and Health Administration, are seen to reflect an engineering perspective based on the concept of a rational man, a perspective that undermines the ability of miners to take responsibility for their own education and ultimately obstructs effective risk management and assessment in the nation's mines. It is argued that to improve miner training and education, technical communicators must understand how underlying gendered assumptions about male rationality influence the construction of knowledge in a large government agency},
  Doi                      = {10.1109/47.180286},
  File                     = {Published version:Sauer-1992-p242-249.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0361-1434},
  Keywords                 = {economic and sociologic effects;mining;safety;technical presentation;training;US government;gendered assumptions;industry attitudes;instruction manuals;mine safety;rational man;risk management;technical communicators;training guides;Government;Health and safety;Industrial training;Law;Legal factors;Management training;Manuals;Mining industry;Railway safety;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=180286&isnumber=4514},
  Zone                     = {-}
}

@Article{Sauer-1992-p242-249a,
  Title                    = {The engineer as rational man: the problem of imminent danger in a non-rational environment},
  Author                   = {Sauer, B.A.},
  Journal                  = {Professional Communication, IEEE Transactions on},
  Year                     = {1992},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {242-249},
  Volume                   = {35},

  Abstract                 = {US government and industry attitudes toward mine safety and health, articulated in the instruction manuals and training guides published by the Mine Safety and Health Administration, are seen to reflect an engineering perspective based on the concept of a rational man, a perspective that undermines the ability of miners to take responsibility for their own education and ultimately obstructs effective risk management and assessment in the nation's mines. It is argued that to improve miner training and education, technical communicators must understand how underlying gendered assumptions about male rationality influence the construction of knowledge in a large government agency},
  Doi                      = {10.1109/47.180286},
  File                     = {Published version:Sauer-1992-p242-249a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0361-1434},
  Keywords                 = {economic and sociologic effects;mining;safety;technical presentation;training;US government;gendered assumptions;industry attitudes;instruction manuals;mine safety;rational man;risk management;technical communicators;training guides;Government;Health and safety;Industrial training;Law;Legal factors;Management training;Manuals;Mining industry;Railway safety;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=180286&isnumber=4514},
  Zone                     = {-}
}

@Article{Schetinin-2013-p5466-5476,
  Title                    = {Prediction of survival probabilities with Bayesian Decision Trees },
  Author                   = {Vitaly Schetinin and Livia Jakaite and Wojtek J. Krzanowski},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2013},
  Number                   = {14},
  Pages                    = {5466 - 5476},
  Volume                   = {40},

  Abstract                 = {Abstract Practitioners use Trauma and Injury Severity Score (TRISS) models for predicting the survival probability of an injured patient. The accuracy of \{TRISS\} predictions is acceptable for patients with up to three typical injuries, but unacceptable for patients with a larger number of injuries or with atypical injuries. Based on a regression model, the \{TRISS\} methodology does not provide the predictive density required for accurate assessment of risk. Moreover, the regression model is difficult to interpret. We therefore consider Bayesian inference for estimating the predictive distribution of survival. The inference is based on decision tree models which recursively split data along explanatory variables, and so practitioners can understand these models. We propose the Bayesian method for estimating the predictive density and show that it outperforms the \{TRISS\} method in terms of both goodness-of-fit and classification accuracy. The developed method has been made available for evaluation purposes as a stand-alone application. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2013.04.009},
  File                     = {Published version:Schetinin-2013-p5466-5476.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Bayesian prediction},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417413002467},
  Zone                     = {3Q}
}

@Article{Scheuer-1969-p30-30,
  Title                    = {Generating Random Variables Having a Specified Failure Rate Function},
  Author                   = {Scheuer, E.M.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1969},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {30-30},
  Volume                   = {R-18},

  Abstract                 = {Not Available},
  Doi                      = {10.1109/TR.1969.5216966},
  File                     = {Published version:Scheuer-1969-p30-30.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Random variables},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5216966&isnumber=5216955},
  Zone                     = {2Q}
}

@InProceedings{Schneeweis-2001-p161-166,
  Title                    = {Petri sub-nets for minpath-based fault trees},
  Author                   = {Schneeweiss, W.G.},
  Booktitle                = {Reliability and Maintainability Symposium, 2001. Proceedings. Annual},
  Year                     = {2001},
  Pages                    = {161-166},

  Abstract                 = {The choice of proper forms of fault tree (FT) and success tree (ST) representations, respectively inside of Petri nets (PNs) in the field of R&M modeling is not a trivial problem, because there is the danger of stray tokens inside of the sub-PNs of those trees which would disturb the system model's proper operation in the long run. The author has been advocating the use of disjunctive normal forms (DNFs=sum-of-products forms). However, typically in the field of graph connectivity problems the initially found FTs usually result from minpaths rather than from mincuts. The Boolean FT functions are therefore initially conjunctive normal forms (CNFs=product-of-sums forms). As the main result of this paper it is shown that for such FTs, sub-PNs can be designed systematically, even though they are not quite as simple as sub-PNs for FTs of DNFs. The main point is to allow for extra FT input places, and to gather all the tokens corresponding to the single variables of the diverse sums once the repairs of the corresponding components are finished. This way no stray tokens remain inside of the FT's sub-PN. As a consequence of the duality between FTs and STs, and since both “trees” are usually inserted in the overall system PN model, it suffices to find a DNF or a CNF of either tree's Boolean function. A CNF or a DNF of the other tree is then readily found via Shonnon's inversion theorem, i.e., it needs no complex Boolean algebra manipulations. The general results are formulated as PN design rules},
  Doi                      = {10.1109/RAMS.2001.902460},
  File                     = {Published version:Schneeweis-2001-p161-166.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0149-144X},
  Keywords                 = {Boolean algebra;Petri nets;fault trees;maintenance engineering;reliability theory;Boolean fault tree functions;Boolean function;Petri sub-nets;Shonnon's inversion theorem;components repair;conjunctive normal forms;disjunctive normal forms;graph connectivity problems;maintainability modeling;mincuts;minpath-based fault trees;product-of-sums forms;reliability modeling;stray tokens;success tree;sum-of-products forms;weighted edges;Boolean algebra;Boolean functions;Delay effects;Fault trees;Monte Carlo methods;Petri nets;Power system modeling;Safety;Sociotechnical systems;Tree graphs},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=902460&isnumber=19492},
  Zone                     = {-}
}

@Article{Schneeweis-1997-p201-207,
  Title                    = {Calculating mean system-failure frequency with prescribed accuracy},
  Author                   = {Schneeweiss, W.G.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1997},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {201-207},
  Volume                   = {46},

  Abstract                 = {To calculate mean system-failure frequency (MSFF) for good engineering designs, an approximation with prescribed accuracy is possible, starting from mincuts, viz, from a sum-of-products form of the fault-tree Boolean-function. Bonferroni-type inequalities, for which a new proof is included, are used. There is a far-reaching similarity between certain kinds of bounds for MSFF (of coherent systems) and for system unavailability. However, this similarity Is not complete. For most real systems, omitting the less-important mincuts yields lower bounds not only for unavailability but also for MSFF. Because an equivalent of the first Bonferroni inequality also holds with MSFF, it is possible to determine an upper bound of the contribution of the deleted mincuts or, the other way around: given a maximum error, determine a set of less-important mincuts, which can be deleted prior to a standard (exact) analysis of the rest. Since 1977 there is a valuable insight that Bonferroni-type inequalities hold also for MSFF (and since 1995 for mean electronic-system life). However, if the difference between the first upper and lower bounds is not small enough, then the investigation of further bounds might be rather cumbersome. However, there is a straight-forward mincut-based approximation to MSFF. As for approximate values of system mean time to failure (MTTF) and mean time to repair (MTTR), upper and lower bounds can be readily found via MSFF=unavailability/MTTR=availability/MTTF using upper and lower bounds for unavailability and MSFF in an obvious way. Of course, these bounds might not be sufficiently tight initially},
  Doi                      = {10.1109/24.589947},
  File                     = {Published version:Schneeweis-1997-p201-207.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Boolean functions;fault trees;reliability theory;Bonferroni-type inequalities;far-reaching similarity;fault-tree Boolean-function;lower bounds;mean electronic-system life;mean system-failure frequency calculation;mean time to failure;mean time to repair;mincuts;prescribed accuracy;sum-of-products;system availability;system unavailability;upper bound determination;Arithmetic;Boolean functions;Counting circuits;Design engineering;Fault trees;Frequency;Random processes;Upper bound},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=589947&isnumber=12968},
  Zone                     = {2Q}
}

@Article{Schneier-1999-p21-29,
  Title                    = {Attack trees},
  Author                   = {Schneier, Bruce},
  Journal                  = {Dr. Dobb’s journal},
  Year                     = {1999},
  Number                   = {12},
  Pages                    = {21--29},
  Volume                   = {24},

  File                     = {Published version:Schneier-1999-p21-29.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.20},
  Url                      = {https://www.schneier.com/attacktrees.pdf},
  Zone                     = {-}
}

@Article{Sendur-2002-p2744-2756,
  Title                    = {Bivariate shrinkage functions for wavelet-based denoising exploiting interscale dependency},
  Author                   = {Sendur, Levent and Selesnick, IW.},
  Journal                  = {Signal Processing, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {2744-2756},
  Volume                   = {50},

  Abstract                 = {Most simple nonlinear thresholding rules for wavelet-based denoising assume that the wavelet coefficients are independent. However, wavelet coefficients of natural images have significant dependencies. We only consider the dependencies between the coefficients and their parents in detail. For this purpose, new non-Gaussian bivariate distributions are proposed, and corresponding nonlinear threshold functions (shrinkage functions) are derived from the models using Bayesian estimation theory. The new shrinkage functions do not assume the independence of wavelet coefficients. We show three image denoising examples in order to show the performance of these new bivariate shrinkage rules. In the second example, a simple subband-dependent data-driven image denoising system is described and compared with effective data-driven techniques in the literature, namely VisuShrink, SureShrink, BayesShrink, and hidden Markov models. In the third example, the same idea is applied to the dual-tree complex wavelet coefficients.},
  Doi                      = {10.1109/TSP.2002.804091},
  File                     = {Published version:Sendur-2002-p2744-2756.pdf:PDF},
  Impactfactor             = {2.813},
  ISSN                     = {1053-587X},
  Keywords                 = {Bayes methods;estimation theory;hidden Markov models;image processing;noise;nonlinear functions;wavelet transforms;BayesShrink;Bayesian estimation theory;HMM;SureShrink;VisuShrink;bivariate shrinkage functions;bivariate shrinkage rules;dual-tree complex wavelet coefficients;hidden Markov models;image denoising;image processing;interscale dependency;nonGaussian bivariate distributions;nonlinear threshold functions;nonlinear thresholding rules;subband-dependent image denoising system;wavelet coefficients;wavelet-based denoising;Bayesian methods;Estimation theory;Hidden Markov models;Image coding;Image denoising;Noise reduction;Probability distribution;Signal processing algorithms;Wavelet coefficients;Wavelet transforms},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1041032&isnumber=22317},
  Zone                     = {2Q}
}

@Article{Sengupta-2013-p161-168,
  Title                    = {Designing of on line intrusion detection system using rough set theory and Q-learning algorithm },
  Author                   = {Nandita Sengupta and Jaydeep Sen and Jaya Sil and Moumita Saha},
  Journal                  = {Neurocomputing },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {161 - 168},
  Volume                   = {111},

  Abstract                 = {Development of an efficient real time intrusion detection system (IDS) has been proposed in the paper by integrating Q-learning algorithm and rough set theory (RST). The objective of the work is to achieve maximum classification accuracy while detecting intrusions by classifying NSL-KDD network traffic data either ‘normal’ or ‘anomaly’. Since \{RST\} processes discrete data only, by applying cut operation attributes in training data are discretized. Using indiscernibility concept of RST, reduced attribute sets, called reducts are obtained and among the reducts a single reduct is chosen which provides highest classification accuracy. However, for the test data the same reduct would not provide highest classification accuracy due to change of discretized attribute values. Therefore, to overcome the problem discretization and feature selection processes are dealt in a comprehensive and systematic way in the paper using machine learning approach. The Q-learning algorithm has been modified to learn optimum cut value for different attributes so that corresponding reduct produces maximum classification accuracy while classifying network traffic data. Since, not all attributes but reduct only take part to detect intrusions, the proposed algorithm is faster than Q-learning and reduces complexity of the IDS. Classification accuracy with 98% success rate has been obtained using real time data, which demonstrates superior performance compared to other classifiers. },
  Doi                      = {http://dx.doi.org/10.1016/j.neucom.2012.12.023},
  File                     = {Published version:Sengupta-2013-p161-168.pdf:PDF},
  Impactfactor             = {1.634},
  ISSN                     = {0925-2312},
  Keywords                 = {Q-learning algorithm},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S092523121300060X},
  Zone                     = {3Q}
}

@Article{Seo-2013-p366-385,
  Title                    = {APFS: Adaptive Probabilistic Filter Scheduling against distributed denial-of-service attacks },
  Author                   = {Dongwon Seo and Heejo Lee and Adrian Perrig},
  Journal                  = {Computers \& Security },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {366 - 385},
  Volume                   = {39, Part B},

  Abstract                 = {Abstract Distributed denial-of-service (DDoS) attacks are considered to be among the most crucial security challenges in current networks because they significantly disrupt the availability of a service by consuming extreme amount of resource and/or by creating link congestions. One type of countermeasure against \{DDoS\} attacks is a filter-based approach where filter-based intermediate routers within the network coordinate with each other to filter undesired flows. The key to success for this approach is effective filter propagation and management techniques. However, existing filter-based approaches do not consider effective filter propagation and management. In this paper, we define three necessary properties for a viable \{DDoS\} solution: how to practically propagate filters, how to place filters to effective filter routers, and how to manage filters to maximize the efficacy of the defense. We propose a novel mechanism, called Adaptive Probabilistic Filter Scheduling (APFS), that effectively defends against \{DDoS\} attacks and also satisfies the three necessary properties. In APFS, a filter router adaptively calculates its own marking probability based on three factors: 1) hop count from a sender, 2) the filter router's resource availability, and 3) the filter router's link degree. That is, a filter router that is closer to attackers, has more available resources, or has more connections to neighbors inserts its marking with a higher probability. These three factors lead a victim to receive more markings from more effective filter routers, and thus, filters are quickly distributed to effective filter routers. Moreover, each filter router manages multiple filters using a filter scheduling policy that allows it to selectively keep the most effective filters depending on attack situations. Experimental results show that \{APFS\} has a faster filter propagation and a higher attack blocking ratio than existing approaches that use fixed marking probability. In addition, \{APFS\} has a 44% higher defense effectiveness than existing filter-based approaches that do not use a filter scheduling policy. },
  Doi                      = {http://dx.doi.org/10.1016/j.cose.2013.09.002},
  File                     = {Published version:Seo-2013-p366-385.pdf:PDF},
  Impactfactor             = {1.158},
  ISSN                     = {0167-4048},
  Keywords                 = {\{DDoS\} attack defense},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167404813001314},
  Zone                     = {3Q}
}

@Article{Sh-1994-p12-13,
  Title                    = {Industrial computing. A grand challenge},
  Author                   = {Lui Sha},
  Journal                  = {Computer},
  Year                     = {1994},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {12-13},
  Volume                   = {27},

  Abstract                 = {To make a truly significant impact, integrated industrial computing technologies must be actively transitioned into practice. First, practitioner needs must be clearly articulated at the beginning of RandD projects and throughout the development process. Second, the new knowledge must be codified into engineering procedures for ready use by the industry. Finally. truly significant new technologies should be supported by open standards so that they can be used cost-effectively. Industrial computing touches upon almost every aspect of our daily life and affects our nation's competitiveness and security. The creation of a renewable industrial computing architecture and transitioning it into practice is a Grand Challenge for computing in the nineties. The author introduces a series of papers which address a ubiquitous aspect of industrial computing: meeting real-time and fault-tolerant requirements.<>},
  Doi                      = {10.1109/2.248872},
  File                     = {Published version:Sh-1994-p12-13.pdf:PDF},
  Impactfactor             = {1.675},
  ISSN                     = {0018-9162},
  Keywords                 = {fault tolerant computing;manufacturing data processing;real-time systems;technological forecasting;Grand Challenge;RandD projects;competitiveness;engineering procedures;fault-tolerant;industrial computing;practitioner needs;real-time;Application software;Computer industry;Computer networks;Defense industry;Electrical equipment industry;Industrial control;Manufacturing industries;Pervasive computing;Research and development;Traffic control},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=248872&isnumber=6372},
  Zone                     = {2Q}
}

@Article{Shaikh-2012-p447-464,
  Title                    = {Dynamic risk-based decision methods for access control systems },
  Author                   = {Riaz Ahmed Shaikh and Kamel Adi and Luigi Logrippo},
  Journal                  = {Computers \& Security },
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {447 - 464},
  Volume                   = {31},

  Abstract                 = {In traditional multi-level security systems, trust and risk values are pre-computed. Any change in these values requires manual intervention of an administrator. In many dynamic environments, however, these values should be auto-adaptive, and auto-tunable according to the usage history of the users. Moreover, occasional exceptions on resource needs, which are common in dynamic environments like healthcare, should be allowed if the subjects show a positive record of use toward resources they acquired in the past. Conversely, access of authorized users, who have negative record, should be restricted. These requirements are not taken into consideration in existing risk-based access control systems. In order to overcome these shortcomings and to meet different sensitivity requirements of various applications, we propose two dynamic risk-based decision methods for access control systems. We provide theoretical and simulation-based analysis and evaluation of both schemes. Also, we analytically prove that the proposed methods, not only allow exceptions under certain controlled conditions, but uniquely restrict legitimate access of bad authorized users. },
  Doi                      = {http://dx.doi.org/10.1016/j.cose.2012.02.006},
  File                     = {Published version:Shaikh-2012-p447-464.pdf:PDF},
  Impactfactor             = {1.158},
  ISSN                     = {0167-4048},
  Keywords                 = {Access control},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167404812000399},
  Zone                     = {3Q}
}

@Article{Shamala-2013-p45-52,
  Title                    = {A conceptual framework of info structure for information security risk assessment (ISRA) },
  Author                   = {Palaniappan Shamala and Rabiah Ahmad and Mariana Yusoff},
  Journal                  = {Journal of Information Security and Applications },
  Year                     = {2013},
  Note                     = {SETOP'2012 and FPS'2012 Special Issue },
  Number                   = {1},
  Pages                    = {45 - 52},
  Volume                   = {18},

  __markedentry            = {[Qiqi:5]},
  Abstract                 = {Abstract Information security has become a vital entity to most organizations today due to current trends in information transfer through a borderless and vulnerable world. The concern and interest in information security is mainly due to the fact that information security risk assessment (ISRA) is a vital method to not only to identify and prioritize information assets but also to identify and monitor the specific threats that an organization induces; especially the chances of these threats occurring and their impact on the respective businesses. However, organizations wanting to conduct risk assessment may face problems in selecting suitable methods that would augur well in meeting their needs. This is due to the existence of numerous methodologies that are readily available. However, there is a lack in agreed reference benchmarking as well as in the comparative framework for evaluating these \{ISRA\} methods to access the information security risk. Generally, organizations will choose the most appropriate \{ISRA\} method by carrying out a comparative study between the available methodologies in detail before a suitable method is selected to conduct the risk assessment. This paper suggests a conceptual framework of info-structure for \{ISRA\} that was developed by comparing and analysing six methodologies which are currently available. The info-structure for \{ISRA\} aims to assist organizations in getting a general view of \{ISRA\} flow, gathering information on the requirements to be met before risk assessment can be conducted successfully. This info-structure can be conveniently used by organizations to complete all the required planning as well as the selection of suitable methods to complete the ISRA. },
  Doi                      = {http://dx.doi.org/10.1016/j.jisa.2013.07.002},
  File                     = {Published version:Shamala-2013-p45-52.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2214-2126},
  Keywords                 = {Info-structure},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，无仿真},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S221421261300029X},
  Zone                     = {-}
}

@Article{Shahaboddin-1,
  Title                    = {D-FICCA: A density-based fuzzy imperialist competitive clustering algorithm for intrusion detection in wireless sensor networks},
  Author                   = {Shahaboddin Shamshirband and Amineh Amini and Nor Badrul Anuar and Miss Laiha Mat Kiah and Ying Wah Teh and Steven Furnell},
  Journal                  = {Measurement },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {212 - 226},
  Volume                   = {55},

  Abstract                 = {Abstract Owing to the scattered nature of Denial-of-Service attacks, it is tremendously challenging to detect such malicious behavior using traditional intrusion detection systems in Wireless Sensor Networks (WSNs). In the current paper, a hybrid clustering method is introduced, namely a density-based fuzzy imperialist competitive clustering algorithm (D-FICCA). Hereby, the imperialist competitive algorithm (ICA) is modified with a density-based algorithm and fuzzy logic for optimum clustering&#xa0;in WSNs. A density-based clustering algorithm helps improve the imperialist competitive algorithm for the formation of arbitrary cluster shapes as well as handling noise. The fuzzy logic controller (FLC) assimilates to imperialistic competition by adjusting the fuzzy rules to avoid possible errors of the worst imperialist action selection strategy. The proposed method aims to enhance the accuracy of malicious detection. D-FICCA is evaluated on a publicly available dataset consisting of real measurements collected from sensors deployed at the Intel Berkeley Research Lab. Its performance is compared against existing empirical methods, such as K-MICA, K-mean, and DBSCAN. The results demonstrate that the proposed framework achieves higher detection accuracy 87% and clustering quality 0.99 compared to existing approaches. },
  Doi                      = {http://dx.doi.org/10.1016/j.measurement.2014.04.034},
  File                     = {Published version:Shahaboddin-1.pdf:PDF},
  Impactfactor             = {1.13},
  ISSN                     = {0263-2241},
  Keywords                 = {Imperialist competitive algorithm},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0263224114001924},
  Zone                     = {3Q}
}

@Article{Sheen-2013-p527-536,
  Title                    = {Kinetics of H atom attack on unsaturated hydrocarbons using spectral uncertainty propagation and minimization techniques },
  Author                   = {David A. Sheen and Claudette M. Rosado-Reyes and Wing Tsang},
  Journal                  = {Proceedings of the Combustion Institute },
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {527 - 536},
  Volume                   = {34},

  Abstract                 = {Unsaturated hydrocarbons are an important component of hydrocarbon fuels and intermediates in their oxidation. Under rich conditions, H atom attack is one of the principal pathways of the decomposition of these unsaturated compounds. Consequently, it is critical to understand the H atom attack mechanisms as part of chemical model development. Previous studies have examined the kinetics of H atom attack on various unsaturated hydrocarbons in single pulse shock-tubes. These studies have noted that there are multiple pathways by which H atom attack can proceed, so it is straightforward to measure relative rates but absolute rates are more difficult to estimate. In addition, there is a confounding influence from secondary chemistry. A multiparameter optimization and uncertainty minimization technique is used to constrain a chemical model for the oxidation of H2/CO/C1–C4 hydrocarbons against a range of measurements of the H atom attack process on toluene, trimethylbenzene (TMB), propyne, and propene. The recommended rate constant expressions, with 2σ uncertainties, are as follows: k ( \{CH\} 4 + H ↔ \{CH\} 3 + H 2 ) = 10 8.81 ± 0.21 s - 1 cm 3 mol - 1 K 1.6 T 1.6 exp ( - 5400 ± 500 K / T ) k ( C 2 H 2 + \{CH\} 3 ↔ p - C 3 H 4 + H ) = 10 11.17 ± 0.18 s - 1 cm 3 mol - 1 K 0.6 T 0.6 exp ( - 7200 ± 400 K / T ) k ( p - C 3 H 4 + H ↔ a - C 3 H 4 + H ) = 10 18.20 ± 0.21 s - 1 cm 3 mol - 1 K T - 1 exp ( - 6100 ± 500 K / T ) k ( C 3 H 6 + H ↔ C 2 H 4 + \{CH\} 3 ) = 10 24.80 ± 0.20 s - 1 cm 3 mol - 1 K 3 T - 3 exp ( - 7400 ± 700 K / T ) k ( C 3 H 6 + H ↔ a - C 3 H 5 + H 2 ) = 10 5.14 ± 0.19 s - 1 cm 3 mol - 1 K - 2.5 T 2.5 exp ( - 1300 ± 200 K / T ) k ( C 6 H 5 \{CH\} 3 + H ↔ C 6 H 6 + \{CH\} 3 ) = 10 6.23 ± 0.19 s - 1 cm 3 mol - 1 K - 2.2 T 2.2 exp ( - 2000 ± 400 K / T ) k ( C 6 H 5 \{CH\} 3 + H ↔ C 6 H 5 \{CH\} 2 + H 2 ) = 10 14.17 ± 0.25 s - 1 cm 3 mol - 1 exp ( - 4500 ± 500 K / T ) k ( \{TMB\} + H ↔ m -xylene + \{CH\} 3 ) = 10 14.07 ± 0.18 s - 1 cm 3 mol - 1 exp ( - 3900 ± 400 K / T ) k ( \{TMB\} + H ↔ ( \{CH\} 3 ) 2 ) = 10 14.52 ± 0.25 s - 1 cm 3 mol - 1 exp ( - 4300 ± 600 K / T ) In addition, we quantify the effect of secondary chemistry on these rate estimates and the contribution to their uncertainty. Furthermore, we demonstrate how the detailed measurements constrain the model’s predictions of global properties such as ignition delay time in propene oxidation. },
  Doi                      = {http://dx.doi.org/10.1016/j.proci.2012.06.062},
  File                     = {Published version:Sheen-2013-p527-536.pdf:PDF},
  Impactfactor             = {2.374},
  ISSN                     = {1540-7489},
  Keywords                 = {Combustion kinetics},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1540748912001708},
  Zone                     = {3Q}
}

@Article{Shen-2013-p266-276,
  Title                    = {The Coordination of Fashion Supply Chains With a Risk-Averse Supplier Under the Markdown Money Policy},
  Author                   = {Bin Shen and Tsan-Ming Choi and Yulan Wang and Lo, C.K.Y.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {266-276},
  Volume                   = {43},

  Abstract                 = {Motivated by the popular markdown money policy (MMP) in the textiles and clothing (TC) industry, in this paper, we explore how this policy performs in a two-stage TC/fashion supply chain with an upstream risk-averse manufacturer (supplier) and a downstream risk-neutral retailer. Specifically, we investigate both the optimal decisions of the risk-averse supplier with respect to the MMP contract parameters and the optimal ordering decision of the risk-neutral retailer so that the whole supply chain can be coordinated (i.e., optimized). We then conduct a numerical study with the real data from two companies to explore the performance of the optimal MMP proposed in our paper. Important insights and specific implications to the industry practitioners are discussed.},
  Doi                      = {10.1109/TSMCA.2012.2204739},
  File                     = {Published version:Shen-2013-p266-276.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {clothing industry;contracts;numerical analysis;risk management;supply chain management;textile industry;MMP;MMP contract parameter;TC industry;downstream risk-neutral retailer;fashion supply chain coordination;markdown money policy;optimal ordering decision;risk-averse supplier;textiles-and-clothing industry;two-stage TC supply chain;upstream risk-averse manufacturer;Analytical models;Clothing;Companies;Contracts;Industries;Supply chains;Textiles;Apparel supply chain;markdown money policy (MMP);mean-variance (MV) analysis;risk averse},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6408205&isnumber=6461142},
  Zone                     = {-}
}

@InProceedings{Shi-2010-p132-136,
  Title                    = {Design and Implementation of Inference Engine in Safety Risk Assessment Expert System in Petrochemical Industry Based on Fault Tree},
  Author                   = {Huifang Shi and Wenhua Song and Qinggong Li},
  Booktitle                = {Data Storage and Data Engineering (DSDE), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {Feb},
  Pages                    = {132-136},

  Abstract                 = {The project in petrochemical industry is complex and risky. For this feature, we established a safety risk assessment (SRA) expert system based on fault tree in petrochemical industry. In this paper, we studied the design and implementation of infer engine in SRA expert system. we adopted the method of fault tree analysis (FTA) to acquire expert knowledge and the fault tree established is to be the basis of inference. The knowledge in petrochemical industry was divided into shallow knowledge and deep knowledge and the method of KR(knowledge representation) adopted in this paper is production rule combined with the framework. On the basis of good representation and organization of knowledge, we adopted infer control strategy of forward reason combined with depth-first search which improve the validity and accuracy of the SRA expert system to a certain extent.},
  Doi                      = {10.1109/DSDE.2010.8},
  File                     = {Published version:Shi-2010-p132-136.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {expert systems;fault trees;inference mechanisms;knowledge representation;petrochemicals;production engineering computing;risk management;safety;SRA;expert knowledge;fault tree;inference engine design;inference engine implementation;knowledge representation;petrochemical industry;safety risk assessment expert system;Chemical industry;Electrical equipment industry;Engines;Expert systems;Fault trees;Petrochemicals;Production;Risk management;Safety;US Department of Transportation;expert system;fault tree;inference engine;knowledge representation;safety risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5452617&isnumber=5452484},
  Zone                     = {-}
}

@Article{Shi-2011-p272-277,
  Title                    = {Security Risk Assessment about Enterprise Networks on the Base of Simulated Attacks},
  Author                   = {Jiaoli Shi},
  Journal                  = {Procedia Engineering },
  Year                     = {2011},
  Note                     = {International Conference on Advances in Engineering 2011 },
  Number                   = {0},
  Pages                    = {272 - 277},
  Volume                   = {24},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {To assess the security risk of an enterprise network, a security risk assessment model was designed based on simulated attack. The vulnerability of single computers and the threat of network attacks were both considered in this model. And then, experiments verify that this method from the model can effectively enhance the accuracy of network security risk assessment; so the method affords a more accurate the assessment of network risk status and better risk control strategy. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2011.11.2640},
  File                     = {Published version:Shi-2011-p272-277.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {Security of an Enterprise Network},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，Case Study，随着主机的增多，风险值增多，还能看出来他提出的方法更加精准，我反正是没看出来},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705811054920},
  Zone                     = {-}
}

@Article{Shi-2011-p272-277a,
  Title                    = {Security Risk Assessment about Enterprise Networks on the Base of Simulated Attacks },
  Author                   = {Jiaoli Shi},
  Journal                  = {Procedia Engineering },
  Year                     = {2011},
  Note                     = {International Conference on Advances in Engineering 2011 },
  Number                   = {0},
  Pages                    = {272 - 277},
  Volume                   = {24},

  Abstract                 = {To assess the security risk of an enterprise network, a security risk assessment model was designed based on simulated attack. The vulnerability of single computers and the threat of network attacks were both considered in this model. And then, experiments verify that this method from the model can effectively enhance the accuracy of network security risk assessment; so the method affords a more accurate the assessment of network risk status and better risk control strategy. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2011.11.2640},
  File                     = {Shi-2011-p272-277a.pdf:Shi-2011-p272-277a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {Security of an Enterprise Network},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.02.28},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705811054920},
  Zone                     = {-}
}

@Article{Shin-2015-p208-217,
  Title                    = {Development of a cyber security risk model using Bayesian networks },
  Author                   = {Jinsoo Shin and Hanseong Son and Rahman Khalil ur and Gyunyoung Heo},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {208 - 217},
  Volume                   = {134},

  Abstract                 = {Abstract Cyber security is an emerging safety issue in the nuclear industry, especially in the instrumentation and control (I&amp;C) field. To address the cyber security issue systematically, a model that can be used for cyber security evaluation is required. In this work, a cyber security risk model based on a Bayesian network is suggested for evaluating cyber security for nuclear facilities in an integrated manner. The suggested model enables the evaluation of both the procedural and technical aspects of cyber security, which are related to compliance with regulatory guides and system architectures, respectively. The activity-quality analysis model was developed to evaluate how well people and/or organizations comply with the regulatory guidance associated with cyber security. The architecture analysis model was created to evaluate vulnerabilities and mitigation measures with respect to their effect on cyber security. The two models are integrated into a single model, which is called the cyber security risk model, so that cyber security can be evaluated from procedural and technical viewpoints at the same time. The model was applied to evaluate the cyber security risk of the reactor protection system (RPS) of a research reactor and to demonstrate its usefulness and feasibility. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2014.10.006},
  File                     = {Published version:Shin-2015-p208-217.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Cyber security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832014002464},
  Zone                     = {2Q}
}

@Article{Shin-2015-p208-217a,
  Title                    = {Development of a cyber security risk model using Bayesian networks },
  Author                   = {Jinsoo Shin and Hanseong Son and Rahman Khalil ur and Gyunyoung Heo},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {208 - 217},
  Volume                   = {134},

  Abstract                 = {Abstract Cyber security is an emerging safety issue in the nuclear industry, especially in the instrumentation and control (I&amp;C) field. To address the cyber security issue systematically, a model that can be used for cyber security evaluation is required. In this work, a cyber security risk model based on a Bayesian network is suggested for evaluating cyber security for nuclear facilities in an integrated manner. The suggested model enables the evaluation of both the procedural and technical aspects of cyber security, which are related to compliance with regulatory guides and system architectures, respectively. The activity-quality analysis model was developed to evaluate how well people and/or organizations comply with the regulatory guidance associated with cyber security. The architecture analysis model was created to evaluate vulnerabilities and mitigation measures with respect to their effect on cyber security. The two models are integrated into a single model, which is called the cyber security risk model, so that cyber security can be evaluated from procedural and technical viewpoints at the same time. The model was applied to evaluate the cyber security risk of the reactor protection system (RPS) of a research reactor and to demonstrate its usefulness and feasibility. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2014.10.006},
  File                     = {Published version:Shin-2015-p208-217a.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Cyber security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832014002464},
  Zone                     = {2Q}
}

@Article{Shin-2015-p208-217b,
  Title                    = {Development of a cyber security risk model using Bayesian networks },
  Author                   = {Jinsoo Shin and Hanseong Son and Rahman Khalil ur and Gyunyoung Heo},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {208 - 217},
  Volume                   = {134},

  Abstract                 = {Abstract Cyber security is an emerging safety issue in the nuclear industry, especially in the instrumentation and control (I&amp;C) field. To address the cyber security issue systematically, a model that can be used for cyber security evaluation is required. In this work, a cyber security risk model based on a Bayesian network is suggested for evaluating cyber security for nuclear facilities in an integrated manner. The suggested model enables the evaluation of both the procedural and technical aspects of cyber security, which are related to compliance with regulatory guides and system architectures, respectively. The activity-quality analysis model was developed to evaluate how well people and/or organizations comply with the regulatory guidance associated with cyber security. The architecture analysis model was created to evaluate vulnerabilities and mitigation measures with respect to their effect on cyber security. The two models are integrated into a single model, which is called the cyber security risk model, so that cyber security can be evaluated from procedural and technical viewpoints at the same time. The model was applied to evaluate the cyber security risk of the reactor protection system (RPS) of a research reactor and to demonstrate its usefulness and feasibility. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2014.10.006},
  File                     = {Published version:Shin-2015-p208-217b.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Cyber security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832014002464},
  Zone                     = {2Q}
}

@Article{Shin-2010-p744-757,
  Title                    = {An Experimental Study of Hierarchical Intrusion Detection for Wireless Industrial Sensor Networks},
  Author                   = {Sooyeon Shin and Taekyoung Kwon and Gil-Yong Jo and Youngman Park and Rhy, H.},
  Journal                  = {Industrial Informatics, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {744-757},
  Volume                   = {6},

  Abstract                 = {Wireless industrial sensor networks are necessary for industrial applications, so that wireless sensor nodes sense around themselves and detect anomaly events in the harsh industrial environments. Due to the harshness, anomaly events such as adversarial intrusions may result in harmful and disastrous situations for industrial applications but it is difficult to detect them over wireless medium. Intrusion detection is an essential requirement for security, but as far as we know, there have not been such studies for wireless industrial sensor networks in the literature. The previous intrusion detection methods proposed for wireless sensor networks consider networks rather in general senses and restrict capabilities to specific attacks only. In this paper, we first study intrusion detection for wireless industrial sensor networks, through various experiments and design of a hierarchical framework. We classify and select better methodologies against various intrusions. Subsequently, we find novel results on the previous methodologies. We also propose a new hierarchical framework for intrusion detection as well as data processing. Throughout the experiments on the proposed framework, we stress the significance of one-hop clustering, which was neglected in the previous studies. Finally, we construct required logical protocols in the hierarchical framework; hierarchical intrusion detection and prevention protocols.},
  Doi                      = {10.1109/TII.2010.2051556},
  File                     = {Published version:Shin-2010-p744-757.pdf:PDF},
  Impactfactor             = {3.381},
  ISSN                     = {1551-3203},
  Keywords                 = {security of data;telecommunication security;wireless sensor networks;hierarchical intrusion detection;intrusion prevention;wireless industrial sensor network;Communication system security;Industry applications;Intrusion detection;Monitoring;Protocols;Wireless communication;Wireless sensor networks;Clustering;industrial applications;intrusion detection;intrusion prevention;wireless industrial sensor network},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5560697&isnumber=5605856},
  Zone                     = {2Q}
}

@Article{Shin-2013-p315-322,
  Title                    = {Advanced probabilistic approach for network intrusion forecasting and detection },
  Author                   = {Seongjun Shin and Seungmin Lee and Hyunwoo Kim and Sehun Kim},
  Journal                  = {Expert Systems with Applications },
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {315 - 322},
  Volume                   = {40},

  Abstract                 = {Recently, as damage caused by Internet threats has increased significantly, one of the major challenges is to accurately predict the period and severity of threats. In this study, a novel probabilistic approach is proposed effectively to forecast and detect network intrusions. It uses a Markov chain for probabilistic modeling of abnormal events in network systems. First, to define the network states, we perform K-means clustering, and then we introduce the concept of an outlier factor. Based on the defined states, the degree of abnormality of the incoming data is stochastically measured in real-time. The performance of the proposed approach is evaluated through experiments using the well-known \{DARPA\} 2000 data set and further analyzes. The proposed approach achieves high detection performance while representing the level of attacks in stages. In particular, our approach is shown to be very robust to training data sets and the number of states in the Markov model. },
  Doi                      = {http://dx.doi.org/10.1016/j.eswa.2012.07.057},
  File                     = {Published version:Shin-2013-p315-322.pdf:PDF},
  Impactfactor             = {1.854},
  ISSN                     = {0957-4174},
  Keywords                 = {Intrusion forecasting},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417412009128},
  Zone                     = {3Q}
}

@Article{Shore-1997-p116-121,
  Title                    = {A general formula for the failure-rate function when distribution information is partially specified},
  Author                   = {Shore, H.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1997},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {116-121},
  Volume                   = {46},

  Doi                      = {10.1109/24.589936},
  File                     = {Published version:Shore-1997-p116-121.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {failure analysis;maximum likelihood estimation;reliability theory;Cdf;algebraic simplicity;censored data;failure-rate function formula;four-parameter distribution family;maximum likelihood estimates;partially-specified distribution information;reliability estimation;routine distribution fitting;Artificial intelligence;Data mining;Error analysis;Failure analysis;Fitting;Hazards;Maximum likelihood estimation;Sampling methods;Statistical distributions},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=589936&isnumber=12967},
  Zone                     = {2Q}
}

@Article{Shore-1997-p116-121a,
  Title                    = {A general formula for the failure-rate function when distribution information is partially specified},
  Author                   = {Shore, H.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1997},

  Month                    = {Mar},
  Number                   = {1},
  Pages                    = {116-121},
  Volume                   = {46},

  Abstract                 = {This paper presents a new formula for the failure-rate function (FRF), derived from a recently introduced 4-parameter family of distributions. The new formula can be expressed in terms of its Cdf, is characterized by algebraic simplicity, and can replace more complex hazard functions by using routine distribution fitting. When the actual Cdf is unknown and partial distribution-information is available (or can be extracted from sample data), new fitting procedures that use only first-degree or first- and second-degree moments are used to approximate the unknown FRF. This new approach is demonstrated for some commonly used Cdfs and shown to yield highly accurate values for the FRF. Relative to current practice, the new FRF has four major advantages: it does not require specification of an exact distribution thus avoiding errors incurred by the use of a wrong model; since estimates of only low-degree (at most first- or second-degree) moments are required to determine the parameters of the FRF, the associated mean-square-deviations are relatively small; the new FRF can be easily adapted for use with censored data; and simple maximum likelihood estimates can be developed},
  Doi                      = {10.1109/24.589936},
  File                     = {Published version:Shore-1997-p116-121a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {failure analysis;maximum likelihood estimation;reliability theory;Cdf;algebraic simplicity;censored data;failure-rate function formula;four-parameter distribution family;maximum likelihood estimates;partially-specified distribution information;reliability estimation;routine distribution fitting;Artificial intelligence;Data mining;Error analysis;Failure analysis;Fitting;Hazards;Maximum likelihood estimation;Sampling methods;Statistical distributions},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Function 表示函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=589936&isnumber=12967},
  Zone                     = {2Q}
}

@InProceedings{Shuangchen-2009-p431-434,
  Title                    = {Dynamic Bayesian Network Model for the Enterprise Financial Risk Warning},
  Author                   = {Wang Shuangcheng and Shao Ruiqing and Leng Cuiping},
  Booktitle                = {Electronic Commerce and Business Intelligence, 2009. ECBI 2009. International Conference on},
  Year                     = {2009},
  Month                    = {June},
  Pages                    = {431-434},

  Abstract                 = {At present, the methods of enterprise financial risk warning emphasize static function dependency or dynamic propagation of time series, which results in a unconsistent combination of the static and dynamic information. In this paper, a dynamic hierarchical naive Bayesian network model is developed for enterprise financial risk warning. The process of using the model and the methods of analyzing contribution on risk level prediction are presented. This model features universality and can be widely used in other risk warning domains.},
  Doi                      = {10.1109/ECBI.2009.79},
  File                     = {Published version:Shuangchen-2009-p431-434.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {belief networks;financial management;risk management;time series;dynamic hierarchical naive Bayesian network model;dynamic information;dynamic propagation;enterprise financial risk warning;risk level prediction;static function dependency;static information;time series;Bayesian methods;Business;Companies;Electronic commerce;Electronic mail;Financial management;Gaussian distribution;Intelligent networks;Mathematical model;Mathematics;Bayesian network;classifier;risk warning},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5190491&isnumber=5189471},
  Zone                     = {-}
}

@Article{Silva-2014-p67-68,
  Title                    = {Industrial Wireless Sensor Networks: Applications, Protocols, and Standards [Book News]},
  Author                   = {Silva, F.},
  Journal                  = {Industrial Electronics Magazine, IEEE},
  Year                     = {2014},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {67-68},
  Volume                   = {8},

  Abstract                 = {This book offers an in-depth state of the art in sensor network technologies for industry applications while also dealing with emerging and already deployed industrial WSN applications and technologies. It is divided into 16 chapters: Applications of Industrial Wireless Sensor Networks; Machine Condition Monitoring with Industrial Wireless Sensor Networks; Wireless Sensor Networks for Intelligent Transportation Applications; Design Challenges and Objectives in Industrial Wireless Sensor Networks; Resource Management and Scheduling in WSNs Powered by Ambient Energy Harvesting; Energy Harvesting Techniques for Industrial Wireless Sensor Networks; Fault Tolerant Industrial Wireless Sensor Networks; Network Architectures for Delay Critical Industrial Wireless Sensor Networks; Network Synchronization in Industrial Wireless Sensor Networks; Wireless Control Networks with Real-Time Constraints; Medium Access Control and Routing in Industrial Wireless Sensor Networks; QoS-Aware Routing for Industrial Wireless Sensor Networks; Reliable and Robust Communications in Industrial Wireless Sensor Networks; and Network Security in Industrial Wireless Sensor Networks.},
  Doi                      = {10.1109/MIE.2014.2361239},
  File                     = {Published version:Silva-2014-p67-68.pdf:PDF},
  Impactfactor             = {3.758},
  ISSN                     = {1932-4529},
  Keywords                 = {Book reviews;Computer applications;Sensors;Wireless sensor networks},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6982373&isnumber=6982272},
  Zone                     = {1Q}
}

@Article{Singh-2014-p1-15,
  Title                    = {Undisclosed chemicals — implications for risk assessment: A case study from the mining industry },
  Author                   = {Khareen Singh and Christopher Oates and Jane Plant and Nikolaos Voulvoulis},
  Journal                  = {Environment International },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {1 - 15},
  Volume                   = {68},

  Abstract                 = {Abstract Many of the chemicals used in industry can be hazardous to human health and the environment, and some formulations can have undisclosed ingredients and hazards, increasing the uncertainty of the risks posed by their use. The need for a better understanding of the extent of undisclosed information in chemicals arose from collecting data on the hazards and exposures of chemicals used in typical mining operations (copper, platinum and coal). Four main categories of undisclosed chemicals were defined (incomplete disclosure; chemicals with unspecific identities; relative quantities of ingredients not stated; and trade secret ingredients) by reviewing material safety data sheet (MSDS) omissions in previous studies. A significant number of chemicals (20% of 957 different chemicals) across the three sites had a range of undisclosed information, with majority of the chemicals (39%) having unspecific identities. The majority of undisclosed information was found in commercially available motor oils followed by cleaning products and mechanical maintenance products, as opposed to reagents critical to the main mining processes. All three types of chemicals had trade secrets, unspecific chemical identities and incomplete disclosures. These types of undisclosed information pose a hindrance to a full understanding of the hazards, which is made worse when combined with additional \{MSDS\} omissions such as acute toxicity endpoints (LD50) and/or acute aquatic toxicity endpoints (LC50), as well as inadequate hazard classifications of ingredients. The communication of the hazard information in the \{MSDSs\} varied according to the chemical type, the manufacturer and the regulations governing the MSDSs. Undisclosed information can undermine occupational health protection, compromise the safety of workers in industry, hinder risk assessment procedures and cause uncertainty about future health. It comes down to the duty of care that industries have towards their employees. With a wide range of chemicals increasingly used, there is a balance that needs to be reached between disclosure requirements, trade secret provisions and definitions of hazardous ingredients for market needs, and the information required to protect the health of their workers.},
  Doi                      = {http://dx.doi.org/10.1016/j.envint.2014.02.012},
  File                     = {Published version:Singh-2014-p1-15.pdf:PDF},
  Impactfactor             = {6.248},
  ISSN                     = {0160-4120},
  Keywords                 = {Mining chemicals},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，采矿业的风险评估，偏向化学},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0160412014000622},
  Zone                     = {1Q}
}

@Article{Singh-2014-p1-15a,
  Title                    = {Undisclosed chemicals — implications for risk assessment: A case study from the mining industry },
  Author                   = {Khareen Singh and Christopher Oates and Jane Plant and Nikolaos Voulvoulis},
  Journal                  = {Environment International },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {1 - 15},
  Volume                   = {68},

  Abstract                 = {Abstract Many of the chemicals used in industry can be hazardous to human health and the environment, and some formulations can have undisclosed ingredients and hazards, increasing the uncertainty of the risks posed by their use. The need for a better understanding of the extent of undisclosed information in chemicals arose from collecting data on the hazards and exposures of chemicals used in typical mining operations (copper, platinum and coal). Four main categories of undisclosed chemicals were defined (incomplete disclosure; chemicals with unspecific identities; relative quantities of ingredients not stated; and trade secret ingredients) by reviewing material safety data sheet (MSDS) omissions in previous studies. A significant number of chemicals (20% of 957 different chemicals) across the three sites had a range of undisclosed information, with majority of the chemicals (39%) having unspecific identities. The majority of undisclosed information was found in commercially available motor oils followed by cleaning products and mechanical maintenance products, as opposed to reagents critical to the main mining processes. All three types of chemicals had trade secrets, unspecific chemical identities and incomplete disclosures. These types of undisclosed information pose a hindrance to a full understanding of the hazards, which is made worse when combined with additional \{MSDS\} omissions such as acute toxicity endpoints (LD50) and/or acute aquatic toxicity endpoints (LC50), as well as inadequate hazard classifications of ingredients. The communication of the hazard information in the \{MSDSs\} varied according to the chemical type, the manufacturer and the regulations governing the MSDSs. Undisclosed information can undermine occupational health protection, compromise the safety of workers in industry, hinder risk assessment procedures and cause uncertainty about future health. It comes down to the duty of care that industries have towards their employees. With a wide range of chemicals increasingly used, there is a balance that needs to be reached between disclosure requirements, trade secret provisions and definitions of hazardous ingredients for market needs, and the information required to protect the health of their workers. },
  Doi                      = {http://dx.doi.org/10.1016/j.envint.2014.02.012},
  File                     = {Published version:Singh-2014-p1-15a.pdf:PDF},
  Impactfactor             = {6.248},
  ISSN                     = {0160-4120},
  Keywords                 = {Mining chemicals},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0160412014000622},
  Zone                     = {1Q}
}

@Article{Sinha-2012-p37-41,
  Title                    = {Understanding Industrial Espionage for Greater Technological and Economic Security},
  Author                   = {Sinha, S.},
  Journal                  = {Potentials, IEEE},
  Year                     = {2012},

  Month                    = {May},
  Number                   = {3},
  Pages                    = {37-41},
  Volume                   = {31},

  Abstract                 = {Large and highly successful companies all over the world have to deal with the problem of industrial espionage at one time or another. Encyclopedia Britannica defines Industrial Espionage as “acquisition of trade secrets from business competitors” and goes on to state that “... industrial espionage is a reaction to the efforts of many business to keep secret their designs, formulas, manufacturing processes, research and future plans in order to protect or expand their shares of the market.” Thus we can say that companies spy on other companies to obtain information related to trade secrets and intellectual property that can bring financial payoffs, market leadership, economic growth and, in some cases, political clout to the spying companies. It should be well understood that spying is an illegal and covert activity in almost every country in the world, where laws to deal with it have been enacted. Thus, industrial espionage qualifies as an illegal activity by virtue of its nature itself. Nevertheless, organizations and governments still engage in it because of the benefits it can bring and the fact that legal proceedings are extremely complicated and time consuming. Where specific laws do not exist, legal proceedings can still be initiated by framing charges of theft and unauthorized access. The United States enacted the Industrial Espionage Act of 1996, also called the Economic Espionage Act (EEA) of 1996 to deal with such espionage.},
  Doi                      = {10.1109/MPOT.2012.2187118},
  File                     = {Published version:Sinha-2012-p37-41.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0278-6648},
  Keywords                 = {government policies;industrial economics;industrial property;organisational aspects;security;EEA;Economic Espionage Act;Industrial Espionage Act of 1996;business competitor;company;economic growth;economic security;financial payoffs;government;illegal activity;industrial espionage;intellectual property;law;legal proceedings;market leadership;organization;political clout;spying;technological security;Computer hacking;Computer security;Economics;Government pollicies;Legal aspects;Radiation detectors;Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6193307&isnumber=6193277},
  Zone                     = {-}
}

@Article{Sirounian-1995-p82-105,
  Title                    = {A knowledge model for unifying deductive and non-deductive heterogeneous databases},
  Author                   = {Sirounian, L. and Grosky, William I.},
  Journal                  = {Knowledge and Data Engineering, IEEE Transactions on},
  Year                     = {1995},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {82-105},
  Volume                   = {7},

  Doi                      = {10.1109/69.368516},
  File                     = {Published version:Sirounian-1995-p82-105.pdf:PDF},
  Impactfactor             = {1.892},
  ISSN                     = {1041-4347},
  Keywords                 = {data structures;deductive databases;knowledge based systems;autonomous components;autonomous sources;common representation;complex forms;deductive heterogeneous databases;diverse data models;domain interdependencies;functional units;future shared environments;heterogeneous knowledge;independently developed deductive rules;integration operators;knowledge model;knowledge representation;non-deductive heterogeneous databases;subsumption;Computer science;Data models;Database systems;Deductive databases;Expert systems;Gratings;Indexes;Manufacturing},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=368516&isnumber=8440},
  Zone                     = {3Q}
}

@Article{Sivulka-2009-p174-185,
  Title                    = {Reconstruction of historical exposures in the \{US\} nickel alloy industry and the implications for carcinogenic hazard and risk assessments },
  Author                   = {Donna J. Sivulka and Steven K. Seilkop},
  Journal                  = {Regulatory Toxicology and Pharmacology },
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {174 - 185},
  Volume                   = {53},

  Abstract                 = {Recently, various regulatory authorities have been reexamining the potential carcinogenic hazards and risks associated with exposures to nickel and certain nickel compounds. In making their assessments, the authorities have focused on occupational cohorts at facilities where nickel-containing sulfidic ores were processed and where increased lung and nasal cancer risks were found in specific groups of workers. Little attention, however, has been paid to the vast number of workers in nickel-using industries, where no excess respiratory cancer risks have been observed. In this paper, the historical exposures of one such group of workers engaged in the production of nickel alloys are reconstructed, and the implications for cancer risk assessments are analyzed. The results indicate that nickel alloy workers were exposed to insoluble oxidic and metallic nickel species at levels comparable to those found in certain nickel processing cohorts; yet they experienced no increase in respiratory cancer risks. This suggests that extrapolating risks from certain primary nickel producers to other nickel industry sectors may not be appropriate. },
  Doi                      = {http://dx.doi.org/10.1016/j.yrtph.2009.01.006},
  File                     = {Published version:Sivulka-2009-p174-185.pdf:PDF},
  Impactfactor             = {2.132},
  ISSN                     = {0273-2300},
  Keywords                 = {Nickel alloy industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {C，镍合金工业的风险评估，偏向化学},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0273230009000105},
  Zone                     = {3Q}
}

@Article{Sivulka-2009-p174-185a,
  Title                    = {Reconstruction of historical exposures in the \{US\} nickel alloy industry and the implications for carcinogenic hazard and risk assessments },
  Author                   = {Donna J. Sivulka and Steven K. Seilkop},
  Journal                  = {Regulatory Toxicology and Pharmacology },
  Year                     = {2009},
  Number                   = {3},
  Pages                    = {174 - 185},
  Volume                   = {53},

  Abstract                 = {Recently, various regulatory authorities have been reexamining the potential carcinogenic hazards and risks associated with exposures to nickel and certain nickel compounds. In making their assessments, the authorities have focused on occupational cohorts at facilities where nickel-containing sulfidic ores were processed and where increased lung and nasal cancer risks were found in specific groups of workers. Little attention, however, has been paid to the vast number of workers in nickel-using industries, where no excess respiratory cancer risks have been observed. In this paper, the historical exposures of one such group of workers engaged in the production of nickel alloys are reconstructed, and the implications for cancer risk assessments are analyzed. The results indicate that nickel alloy workers were exposed to insoluble oxidic and metallic nickel species at levels comparable to those found in certain nickel processing cohorts; yet they experienced no increase in respiratory cancer risks. This suggests that extrapolating risks from certain primary nickel producers to other nickel industry sectors may not be appropriate. },
  Doi                      = {http://dx.doi.org/10.1016/j.yrtph.2009.01.006},
  File                     = {Published version:Sivulka-2009-p174-185a.pdf:PDF},
  Impactfactor             = {2.132},
  ISSN                     = {0273-2300},
  Keywords                 = {Nickel alloy industry},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0273230009000105},
  Zone                     = {3Q}
}

@Article{Skorin-Kap-2012-p418-429,
  Title                    = {Wavelength assignment for reducing in-band crosstalk attack propagation in optical networks: \{ILP\} formulations and heuristic algorithms },
  Author                   = {Nina Skorin-Kapov and Marija Furdek and Ramon Aparicio Pardo and Pablo Pavón Mariño},
  Journal                  = {European Journal of Operational Research },
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {418 - 429},
  Volume                   = {222},

  Abstract                 = {Today’s Transparent Optical Networks (TONs) are highly vulnerable to various physical-layer attacks, such as high-power jamming, which can cause severe service disruption or even service denial. The transparency of \{TONs\} enables certain attacks to propagate through the network, not only increasing their damage proportions, but also making source identification and attack localization more difficult. High-power jamming attacks causing in-band crosstalk in switches are amongst the most malicious of such attacks. In this paper, we propose a wavelength assignment scheme to reduce their damage assuming limited attack propagation capabilities. This complements our previous work in Furdek et al. (M. Furdek, N. Skorin-Kapov, M. Grbac, Attack-aware wavelength assignment for localization of in-band crosstalk attack propagation, IEEE/OSA Journal of Optical Communications and Networking 2 (11) (2010) 1000–1009) where we investigated infinite jamming attack propagation to find an upper bound on the network vulnerability to such attacks. Here, we consider a more realistic scenario where crosstalk attacks can spread only via primary and/or secondary attackers and define new objective criteria for wavelength assignment, called the \{PAR\} (Primary Attack Radius) and \{SAR\} (Secondary Attack Radius), accordingly. We formulate the problem variants as integer linear programs (ILPs) with the objectives of minimizing the \{PAR\} and \{SAR\} values. Due to the intractability of the \{ILP\} formulations, for larger instances we propose \{GRASP\} (Greedy Randomized Adaptive Search Procedure) heuristic algorithms to find suboptimal solutions in reasonable time. Results show that these approaches can obtain solutions using the same number of wavelengths as classical wavelength assignment, while significantly reducing jamming attack damage proportions in optical networks. },
  Doi                      = {http://dx.doi.org/10.1016/j.ejor.2012.05.022},
  File                     = {Published version:Skorin-Kap-2012-p418-429.pdf:PDF},
  Impactfactor             = {2.038},
  ISSN                     = {0377-2217},
  Keywords                 = {\{OR\} in telecommunications},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0377221712003712},
  Zone                     = {2Q}
}

@InCollection{Slay-2008-p73-82,
  Title                    = {Lessons Learned from the Maroochy Water Breach},
  Author                   = {Slay, Jill and Miller, Michael},
  Booktitle                = {Critical Infrastructure Protection},
  Publisher                = {Springer US},
  Year                     = {2008},
  Editor                   = {Goetz, Eric and Shenoi, Sujeet},
  Pages                    = {73-82},
  Series                   = {IFIP International Federation for Information Processing},
  Volume                   = {253},

  Doi                      = {10.1007/978-0-387-75462-8_6},
  File                     = {Published version:Slay-2008-p73-82.pdf:PDF},
  Impactfactor             = {-},
  ISBN                     = {978-0-387-75461-1},
  Language                 = {English},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://dx.doi.org/10.1007/978-0-387-75462-8_6},
  Zone                     = {-}
}

@Article{Smeitink-1990-p71-75,
  Title                    = {A simple approximation to the renewal function [reliability theory]},
  Author                   = {Smeitink, E. and Dekker, R.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1990},

  Month                    = {Apr},
  Number                   = {1},
  Pages                    = {71-75},
  Volume                   = {39},

  Abstract                 = {The authors present a simple, easy-to-understand approximation to the renewal function that is easy to implement on a personal computer. The key idea is that, for small values of time, the renewal function is almost equal to the cumulative distribution function of the interrenewal time, whereas for larger values of time an asymptotic expansion depending only on the first and second moment of the interrenewal time can be used. The relative error is typically smaller than a few percent for Weibull interrenewal times. The simple approximation methods works very well with one term if not too much accuracy is required (e.g. in the block replacement problem) or if the interrenewal (failure) distribution is not exactly known (e.g. only the first two moments are known). Although the accuracy of the simple approximation can be improved by increasing the number of terms, this strategy is not advocated since speed and simplicity are lost. If high accuracy is required, it is better to use another approximating method (e.g. power series expansion or cubic splines method)},
  Doi                      = {10.1109/24.52614},
  File                     = {Published version:Smeitink-1990-p71-75.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {function approximation;reliability theory;Weibull interrenewal times;block replacement problem;cumulative distribution function;interrenewal time;personal computer;reliability theory;renewal function;simple approximation;Approximation methods;Convolution;Microcomputers;Power generation;Reliability theory;Weibull distribution},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=52614&isnumber=1894},
  Zone                     = {2Q}
}

@InProceedings{Sobh-1996-p33-36,
  Title                    = {Altered cardiorespiratory control in patients with severe congestive heart failure: a transfer function analysis approach},
  Author                   = {Sobh, J.F. and Lucas, C. and Stevenson, L.W. and Saul, J.P.},
  Booktitle                = {Computers in Cardiology, 1996},
  Year                     = {1996},
  Month                    = {Sept},
  Pages                    = {33-36},

  Doi                      = {10.1109/CIC.1996.542466},
  File                     = {Published version:Sobh-1996-p33-36.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0276-6547},
  Keywords                 = {biocontrol;cardiology;haemodynamics;transfer functions;0.05 Hz;altered cardiorespiratory control;arterial baroreflex gain;cardiac output;central venous return;periodic breathing;random interval breathing;respiration mechanical influence;respiratory sinus arrthymia;severe congestive heart failure patients;spontaneous interval breathing;systolic blood pressure oscillations;therapy;transfer function analysis approach;Baroreflex;Cardiology;Centralized control;Frequency;Heart rate;Medical treatment;Performance analysis;Performance gain;Pressure control;Transfer functions},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=542466&isnumber=11932},
  Zone                     = {-}
}

@Article{Soler-2008-p282-289,
  Title                    = {IDEAS07: An approach based on i* for security requirement analysis in data warehouses},
  Author                   = {Soler, E. and Stefanov, V. and Mazon, J.-N. and Trujillo, J. and Fernandez-Medina, E. and Piattini, M.},
  Journal                  = {Latin America Transactions, IEEE (Revista IEEE America Latina)},
  Year                     = {2008},

  Month                    = {July},
  Number                   = {3},
  Pages                    = {282-289},
  Volume                   = {6},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/TLA.2008.4653859},
  File                     = {Published version:Soler-2008-p282-289.pdf:PDF},
  Impactfactor             = {0.218},
  ISSN                     = {1548-0992},
  Keywords                 = {data warehouses;security of data;IDEAS07;data warehouses;information needs;model driven architecture;quality-of-service requirements;security requirement analysis;Data analysis;Data security;Data warehouses;Multidimensional systems;Silicon compounds;Visualization;Data warehouses;requirements;security},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4653859&isnumber=4653851},
  Zone                     = {4Q}
}

@Article{Soler-2008-p282-289a,
  Title                    = {IDEAS07: An approach based on i* for security requirement analysis in data warehouses},
  Author                   = {Soler, E. and Stefanov, V. and Mazon, J.-N. and Trujillo, J. and Fernandez-Medina, E. and Piattini, M.},
  Journal                  = {Latin America Transactions, IEEE (Revista IEEE America Latina)},
  Year                     = {2008},

  Month                    = {July},
  Number                   = {3},
  Pages                    = {282-289},
  Volume                   = {6},

  Abstract                 = {Requirement analysis approaches for data warehouse (DW) systems are only focused on information needs of decision makers, without considering other kind of requirement such as security or performance. However, modeling these issues in the early stages of the development is a cornerstone for building a DW that satisfies user expectations. In this paper, we define the two kinds of requirements for DWing as information and quality-of-service requirements. These requirements are defined within the MDA (Model Driven Architecture) framework, which allows their traceability towards the subsequent conceptual and logical design phases. It is worth mentioning that this paper focuses on proposing a requirement model for security (as a concrete kind of quality-of-service requirement), and a three-step process for modeling together information and security requirements.},
  Doi                      = {10.1109/TLA.2008.4653859},
  File                     = {Published version:Soler-2008-p282-289a.pdf:PDF},
  Impactfactor             = {0.218},
  ISSN                     = {1548-0992},
  Keywords                 = {data warehouses;security of data;IDEAS07;data warehouses;information needs;model driven architecture;quality-of-service requirements;security requirement analysis;Data analysis;Data security;Data warehouses;Multidimensional systems;Silicon compounds;Visualization;Data warehouses;requirements;security},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4653859&isnumber=4653851},
  Zone                     = {4Q}
}

@Article{Soellner-2014-p84-96,
  Title                    = {Single-process versus multiple-strategy models of decision making: Evidence from an information intrusion paradigm },
  Author                   = {Anke Söllner and Arndt Bröder and Andreas Glöckner and Tilmann Betsch},
  Journal                  = {Acta Psychologica },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {84 - 96},
  Volume                   = {146},

  Abstract                 = {Abstract When decision makers are confronted with different problems and situations, do they use a uniform mechanism as assumed by single-process models (SPMs) or do they choose adaptively from a set of available decision strategies as multiple-strategy models (MSMs) imply? Both frameworks of decision making have gathered a lot of support, but only rarely have they been contrasted with each other. Employing an information intrusion paradigm for multi-attribute decisions from givens, \{SPM\} and \{MSM\} predictions on information search, decision outcomes, attention, and confidence judgments were derived and tested against each other in two experiments. The results consistently support the \{SPM\} view: Participants seemingly using a “take-the-best” (TTB) strategy do not ignore TTB-irrelevant information as \{MSMs\} would predict, but adapt the amount of information searched, choose alternative choice options, and show varying confidence judgments contingent on the quality of the “irrelevant” information. The uniformity of these findings underlines the adequacy of the novel information intrusion paradigm and comprehensively promotes the notion of a uniform decision making mechanism as assumed by single-process models. },
  Doi                      = {http://dx.doi.org/10.1016/j.actpsy.2013.12.007},
  File                     = {Published version:Soellner-2014-p84-96.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0001-6918},
  Keywords                 = {Decision making},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0001691813002692},
  Zone                     = {-}
}

@InProceedings{Sommestad-2009-p1-10,
  Title                    = {Cyber Security Risks Assessment with Bayesian Defense Graphs and Architectural Models},
  Author                   = {Sommestad, T. and Ekstedt, M. and Johnson, P.},
  Booktitle                = {System Sciences, 2009. HICSS '09. 42nd Hawaii International Conference on},
  Year                     = {2009},
  Month                    = {Jan},
  Pages                    = {1-10},

  Abstract                 = {To facilitate rational decision making regarding cyber security investments, decision makers need to be able to assess expected losses before and after potential investments. This paper presents a model based assessment framework for analyzing the cyber security provided by different architectural scenarios. The framework uses the Bayesian statistics based extended influence diagrams to express attack graphs and related countermeasures. In this paper it is demonstrated how this structure can be captured in an abstract model to support analysis based on architectural models. The approach allows calculating the probability that attacks will succeed and the expected loss of these given the instantiated architectural scenario. Moreover, the framework can handle the uncertainties that are accompanied to the analyses. In architectural analysis there are uncertainties acquainted both to the scenario and its properties, as well as to the analysis framework that stipulates how security countermeasures contribute to cyber security.},
  Doi                      = {10.1109/HICSS.2009.141},
  File                     = {Published version:Sommestad-2009-p1-10.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1530-1605},
  Keywords                 = {Bayes methods;decision making;graph theory;investment;probability;risk management;security of data;Bayesian defense graph;Bayesian statistics based extended influence diagram;architectural model;cyber security investment;cyber security risk assessment management;decision maker;probability;Bayesian methods;Computer architecture;Computer security;Decision making;Information security;Investments;Management information systems;Risk management;Tree graphs;Uncertainty},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4755419&isnumber=4755314},
  Zone                     = {-}
}

@InProceedings{Song-2009-p167-170,
  Title                    = {Application of Fault Tree Knowledge in Reasoning of Safety Risk Assessment Expert System in Petrochemical Industry},
  Author                   = {Wenhua Song and Huifang Shi and Qinggong Li},
  Booktitle                = {Knowledge Engineering and Software Engineering, 2009. KESE '09. Pacific-Asia Conference on},
  Year                     = {2009},
  Month                    = {Dec},
  Pages                    = {167-170},

  Abstract                 = {In studying the safety risk assessment expert system in petrochemical industry, the conception of fault tree based on binary decision diagrams (BDD) is proposed for fault tree analysis. Taking the boiler fouling explosion for example, the fault tree model and BDD model are built. On the basis of the fault tree model and BDD model, the stress is put on the knowledge representation (KR) of BDD based fault tree in the safety risk assessment expert system and the fault tree knowledge based mechanism of reasoning in this paper.},
  Doi                      = {10.1109/KESE.2009.51},
  File                     = {Published version:Song-2009-p167-170.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {binary decision diagrams;boilers;expert systems;fault trees;inference mechanisms;knowledge representation;maintenance engineering;petrochemicals;production engineering computing;risk management;safety systems;BDD model;binary decision diagram;boiler fouling explosion;fault tree analysis;fault tree knowledge;knowledge representation;petrochemical industry;reasoning;safety risk assessment expert system;Binary decision diagrams;Boolean functions;Chemical industry;Data structures;Expert systems;Fault trees;Petrochemicals;Risk analysis;Risk management;Safety;binary decision diagrams;expert system;fault tree knowledge;knowledge representation;reasoning},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5383592&isnumber=5383567},
  Zone                     = {-}
}

@InProceedings{Song-2009-p213-216,
  Title                    = {Study on Safety Risk Assessment Expert System in Petrochemical Industry Based on Fault Tree},
  Author                   = {Wenhua Song and Huifang Shi and Qinggong Li},
  Booktitle                = {Knowledge Acquisition and Modeling, 2009. KAM '09. Second International Symposium on},
  Year                     = {2009},
  Month                    = {Nov},
  Pages                    = {213-216},
  Volume                   = {3},

  Abstract                 = {In this paper, we design a safety risk assessment expert system (SRAES) in petrochemical industry based on fault tree (FT). The expert knowledge is acquired through fault tree analysis (FTA) and stored in a knowledge base (KB) of the expert system. We adopt a rule and framework based reasoning (RFBR) method which is intuitive and conforms to the human thinking customary. This system will figure out two main difficulties, that is, hard to acquire system knowledge and how to construct a high efficient reasoning mechanism. It can realize the safety risk assessment (SRA) of dangerous and harmful factors of the engineering in the petrochemical industry in theory, and improve efficiency and accuracy of the SRA.},
  Doi                      = {10.1109/KAM.2009.255},
  File                     = {Song-2009-p213-216.pdf:Song-2009-p213-216.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {expert systems;fault trees;inference mechanisms;petrochemicals;production engineering computing;risk management;safety;fault tree;fault tree analysis;knowledge base;petrochemical industry;rule and framework based reasoning;safety risk assessment expert system;Chemical industry;Expert systems;Fault trees;Fuel storage;Industrial accidents;Knowledge acquisition;Petrochemicals;Risk management;Safety;US Department of Transportation;fault tree analysis;knowledge acquisition;rule and framework based reasoning;safety risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5362374&isnumber=5361827},
  Zone                     = {-}
}

@InProceedings{Song-2013-p1-6,
  Title                    = {Risk-based dynamic security assessment under typhoon weather for power transmission system},
  Author                   = {Xiaozhe Song and Zhen Wang and Huanhai Xin and Deqiang Gan},
  Booktitle                = {Power and Energy Engineering Conference (APPEEC), 2013 IEEE PES Asia-Pacific},
  Year                     = {2013},
  Month                    = {Dec},
  Pages                    = {1-6},

  Abstract                 = {Risk-based method, which takes into account the probability and severity of contingencies, can be used to quantitatively assess the security level of power system, especially during severe weather. In this paper, a risk-based method to assess the dynamic security level of power system under typhoon weather is presented. The impact of typhoon on operational reliability of transmission lines is modeled firstly. An emergency control optimization problem is then addressed to measure the severity of each contingency, in which the cost of generation/load shedding is minimized and the transient stability constraint and post-contingency overload constraint are considered simultaneously. Case study on the New England system is conducted to demonstrate the effectiveness of the proposed method and the resultant emergency control scheme can provide feasible decision making against potential contingencies.},
  Doi                      = {10.1109/APPEEC.2013.6837129},
  File                     = {Published version:Song-2013-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {costing;decision making;load shedding;optimisation;power generation economics;power generation reliability;power system security;power system stability;power transmission economics;power transmission lines;power transmission reliability;risk management;New England system;decision making;emergency control optimization problem;generation-load shedding costing;operational reliability;post-contingency overload constraint;power system security;power transmission line system;probability;risk-based dynamic security assessment;transient stability constraint;typhoon weather;Meteorology;Power system stability;Power transmission lines;Reliability;Security;Transient analysis;Tropical cyclones;dynamic security;emergency control;operational reliability;risk assessment;typhoon weather},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6837129&isnumber=6837104},
  Zone                     = {-}
}

@Article{Stecher-1986-p51-58,
  Title                    = {Evaluation of Large Fault-Trees with Repeated Events Using an Efficient Bottom-Up Algorithm},
  Author                   = {Stecher, Karl},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1986},

  Month                    = {April},
  Number                   = {1},
  Pages                    = {51-58},
  Volume                   = {35},

  Abstract                 = {In current fault-tree analysis of systems, the usual algorithms for evaluation of fault-trees with repeated events apply the method of minimal cuts. Since the number of minimal cuts increases exponentially with the number of system components, truncation as well as optimization techniques have to be performed for the evaluation of large fault-trees. This paper presents an algorithm which combines the bottom-up algorithm for fault-trees without repeated events with the bottom-up algorithm for establishing the structure function of a fault-tree with repeated events. It allows convenient modeling and evaluation of large fault-trees with repeated events and it computes the exact reliability characteristics, such as steady-state and time-dependent system unavailability and failure rate for repairable and nonrepairable systems. This algorithm has been implemented by a computer program and has been in use at the Power Engineering and Automation Group of Siemens AG. The program includes many features for the description and modular structuring of fault-trees, eg, 1. Definition of fault-tree modules, which can also be declared as repeated transmission trees. 2. A subtree can be used several times as a transmission tree, representing many modules of the system. 3. Repeated events with respect to a subtree level. A great advantage is that the user of the program need not concern himself with a repeated event which appears in different subtrees. The structure of the system can be carried over into a system of subsidiary fault-trees which can be mutually dependent, analogous to the system features.},
  Doi                      = {10.1109/TR.1986.4335344},
  File                     = {Published version:Stecher-1986-p51-58.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Algorithm design and analysis;Automation;Performance evaluation;Power engineering;Power engineering computing;Power system modeling;Power system reliability;Reliability theory;Set theory;Steady-state},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4335344&isnumber=4335320},
  Zone                     = {2Q}
}

@Article{Stewart-1991-p1000-1007,
  Title                    = {Probabilistic risk assessment of quality control and quality assurance measures in structural design},
  Author                   = {Stewart, M.G.},
  Journal                  = {Systems, Man and Cybernetics, IEEE Transactions on},
  Year                     = {1991},

  Month                    = {Sep},
  Number                   = {5},
  Pages                    = {1000-1007},
  Volume                   = {21},

  Doi                      = {10.1109/21.120052},
  File                     = {Published version:Stewart-1991-p1000-1007.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0018-9472},
  Keywords                 = {Monte Carlo methods;human factors;quality control;structural engineering;trees (mathematics);Monte Carlo simulation analysis;event-tree methodology;probabilistic risk assessment;quality assurance measures;quality control measure efficiency;safe load tables;steel I-beam;structural engineering design task;Bridges;Error correction;Guidelines;Humans;Power system modeling;Quality assurance;Quality control;Risk management;Steel;Structural engineering},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，用到了事件树以及蒙特卡洛仿真},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=120052&isnumber=3428},
  Zone                     = {-}
}

@Article{stouffer2011guide,
  Title                    = {Guide to industrial control systems (ICS) security},
  Author                   = {Stouffer, Keith and Falco, Joe and Scarfone, Karen},
  Journal                  = {NIST special publication},
  Year                     = {2011},
  Pages                    = {800--82},

  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Publisher                = {Citeseer},
  Timestamp                = {2015.02.28},
  Zone                     = {-}
}

@Article{Stouffer-2011-p82-800,
  Title                    = {Guide to industrial control systems (ICS) security},
  Author                   = {Stouffer, Keith and Falco, Joe and Scarfone, Karen},
  Journal                  = {NIST special publication},
  Year                     = {2011},
  Pages                    = {800--82},

  File                     = {Stouffer-2011-p82-800.pdf:Stouffer-2011-p82-800.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Publisher                = {Citeseer},
  Timestamp                = {2015.02.28},
  Zone                     = {-}
}

@Article{Stroeve-2013-p133-149,
  Title                    = {Contrasting safety assessments of a runway incursion scenario: Event sequence analysis versus multi-agent dynamic risk modelling },
  Author                   = {Sybert H. Stroeve and Henk A.P. Blom and G.J. (Bert) Bakker},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {133 - 149},
  Volume                   = {109},

  Abstract                 = {In the safety literature it has been argued, that in a complex socio-technical system safety cannot be well analysed by event sequence based approaches, but requires to capture the complex interactions and performance variability of the socio-technical system. In order to evaluate the quantitative and practical consequences of these arguments, this study compares two approaches to assess accident risk of an example safety critical sociotechnical system. It contrasts an event sequence based assessment with a multi-agent dynamic risk model (MA-DRM) based assessment, both of which are performed for a particular runway incursion scenario. The event sequence analysis uses the well-known event tree modelling formalism and the MA-DRM based approach combines agent based modelling, hybrid Petri nets and rare event Monte Carlo simulation. The comparison addresses qualitative and quantitative differences in the methods, attained risk levels, and in the prime factors influencing the safety of the operation. The assessments show considerable differences in the accident risk implications of the performance of human operators and technical systems in the runway incursion scenario. In contrast with the event sequence based results, the MA-DRM based results show that the accident risk is not manifest from the performance of and relations between individual human operators and technical systems. Instead, the safety risk emerges from the totality of the performance and interactions in the agent based model of the safety critical operation considered, which coincides very well with the argumentation in the safety literature. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2012.07.002},
  File                     = {Published version:Stroeve-2013-p133-149.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Event tree},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832012001433},
  Zone                     = {2Q}
}

@Article{Stroeve-2013-p133-149a,
  Title                    = {Contrasting safety assessments of a runway incursion scenario: Event sequence analysis versus multi-agent dynamic risk modelling },
  Author                   = {Sybert H. Stroeve and Henk A.P. Blom and G.J. (Bert) Bakker},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {133 - 149},
  Volume                   = {109},

  Abstract                 = {In the safety literature it has been argued, that in a complex socio-technical system safety cannot be well analysed by event sequence based approaches, but requires to capture the complex interactions and performance variability of the socio-technical system. In order to evaluate the quantitative and practical consequences of these arguments, this study compares two approaches to assess accident risk of an example safety critical sociotechnical system. It contrasts an event sequence based assessment with a multi-agent dynamic risk model (MA-DRM) based assessment, both of which are performed for a particular runway incursion scenario. The event sequence analysis uses the well-known event tree modelling formalism and the MA-DRM based approach combines agent based modelling, hybrid Petri nets and rare event Monte Carlo simulation. The comparison addresses qualitative and quantitative differences in the methods, attained risk levels, and in the prime factors influencing the safety of the operation. The assessments show considerable differences in the accident risk implications of the performance of human operators and technical systems in the runway incursion scenario. In contrast with the event sequence based results, the MA-DRM based results show that the accident risk is not manifest from the performance of and relations between individual human operators and technical systems. Instead, the safety risk emerges from the totality of the performance and interactions in the agent based model of the safety critical operation considered, which coincides very well with the argumentation in the safety literature. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2012.07.002},
  File                     = {Published version:Stroeve-2013-p133-149a.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Event tree},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，好好看看},
  Timestamp                = {2015.01.14},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832012001433},
  Zone                     = {2Q}
}

@Article{Suleiman-2014-p-,
  Title                    = {Integrated smart grid systems security threat model },
  Author                   = {Husam Suleiman and Israa Alqassem and Ali Diabat and Edin Arnautovic and Davor Svetinovic},
  Journal                  = {Information Systems },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract The Smart Grid (SG) integrates the power grid and the Information and Communication Technology (ICT) with the aim of achieving more reliable and safe power transmission and distribution to the customers. Integrating the power grid with the \{ICT\} exposes the \{SG\} to systems security threats and vulnerabilities that could be compromised by malicious users and attackers. This paper presents a \{SG\} systems threats analysis and integrated \{SG\} Systems Security Threat Model (SSTM). The reference architecture of the SG, with its components and communication interfaces used to exchange the energy-related information, is integrated with the results of \{SG\} systems security threat analysis to produce a comprehensive, integrated \{SG\} SSTM. The \{SG\} \{SSTM\} in this paper helps better depict and understand the vulnerabilities exploited by attackers to compromise the components and communication links of the SG. The \{SG\} \{SSTM\} provides a reference of the systems security threats for industrial security practitioners, and can be used for design and implementation of \{SG\} systems security controls and countermeasures. },
  Doi                      = {http://dx.doi.org/10.1016/j.is.2014.12.002},
  File                     = {Published version:Suleiman-2014-p-.pdf:PDF},
  Impactfactor             = {1.768},
  ISSN                     = {0306-4379},
  Keywords                 = {Systems security threats},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0306437914001896},
  Zone                     = {2Q}
}

@Article{Sun-2013-p801-813,
  Title                    = {Concept Drift-Oriented Adaptive and Dynamic Support Vector Machine Ensemble With Time Window in Corporate Financial Risk Prediction},
  Author                   = {Jie Sun and Hui Li and Adeli, H.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {801-813},
  Volume                   = {43},

  Abstract                 = {This paper proposes a novel method of corporate financial risk prediction (FRP) modeling called the adaptive and dynamic ensemble (ADE) of support vector machine (SVM) (ADE-SVM), which integrates the inflow of new data batches for FRP with the process of time. Namely, the characteristic change of corporate financial distress hidden in the data flow is considered as the concept drift of financial distress, and it is handled by ADE-SVM that keeps updating in time. Using the criteria of predictive ability and classifier diversity, the SVM ensemble is dynamically constructed by adaptively selecting the current base SVMs from candidate ones. The candidate SVMs are incrementally updated by considering the newest data batch at each new current time point. The results of the base SVMs are dynamically weighted by their validation accuracies on the latest data batch to generate the final prediction. Experiments were carried out on real-world data sets with current data for training and future data for testing. The results show that ADE-SVM overall outperforms the other three traditional dynamic modeling methods, particularly for harder FRP task with more insufficient information and more obvious concept drift.},
  Doi                      = {10.1109/TSMCA.2012.2224338},
  File                     = {Published version:Sun-2013-p801-813.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {batch processing (computers);corporate modelling;data integration;economic indicators;pattern classification;risk analysis;support vector machines;ADE;FRP;SVM ensemble;adaptive and dynamic ensemble;classifier diversity;corporate financial distress;corporate financial risk prediction;data batch inflow integration;data testing;data training;dynamic modeling method;support vector machine;time window;Accuracy;Adaptation models;Companies;Data models;Predictive models;Support vector machines;Training data;Adaptive and dynamic ensemble (ADE);concept drift;early warning;financial risk prediction (FRP);support vector machine (SVM)},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6425552&isnumber=6530659},
  Zone                     = {-}
}

@Article{Sun-2014-p83-95,
  Title                    = {Neogene igneous intrusions in the northern South China Sea: Evidence from high-resolution three dimensional seismic data },
  Author                   = {Qiliang Sun and Shiguo Wu and Joe Cartwright and Shenghao Wang and Yintao Lu and Duanxin Chen and Dongdong Dong},
  Journal                  = {Marine and Petroleum Geology },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {83 - 95},
  Volume                   = {54},

  Abstract                 = {Abstract Igneous intrusions emplaced within the prospective intervals of sedimentary basins can exert a significant impact on petroleum systems and hence are of considerable interest particularly when risking exploration plays. A number of discordant high amplitude seismic anomalies (DSAs) with a range of geometries are documented in high resolution 3D seismic data in the northern South China Sea. Their distribution and seismic characteristics are analysed and compared with similar seismic anomalies documented within sedimentary basins in the Atlantic margins and other locations. The \{DSAs\} occur mainly within Early Miocene strata and are interpreted as igneous intrusions that were emplaced close to the palaeo-seabed and are dated as early Middle Miocene using seismic-stratigraphic methods. A number of vents are also identified above the intrusions within early Middle Miocene strata. Several geometrical forms are observed, referred to here as saucer-shaped, lensoid-shaped, stacked and composite intrusions. The seismic expression of these is increasing complex towards the palaeo-seabed, with a tendency for the saucer-shaped sills to be the deepest intrusive forms. The igneous intrusions observed in this study only could be identified using 3D seismic data and they are important for the future evaluations of petroleum systems, basin evolution and tectonic analysis in the Pearl River Mouth Basin. },
  Doi                      = {http://dx.doi.org/10.1016/j.marpetgeo.2014.02.014},
  File                     = {Published version:Sun-2014-p83-95.pdf:PDF},
  Impactfactor             = {2.111},
  ISSN                     = {0264-8172},
  Keywords                 = {Igneous intrusion},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0264817214000439},
  Zone                     = {3Q}
}

@InProceedings{Sun-2010-p2021-2025,
  Title                    = {The model of credit risk assessment in power industry base on RS-SVM},
  Author                   = {Wei Sun and Qiu-Shi Du and Bo Cui},
  Booktitle                = {Machine Learning and Cybernetics (ICMLC), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {July},
  Pages                    = {2021-2025},
  Volume                   = {4},

  Abstract                 = {In this paper, according to the situation of credit risk assessment in power industry, index system of risk assessment was established. Credit risk assessment models based on rough set and support vector machines (RSSVM) were proposed for the characteristic of more indicator numbers. Through introducing actual data of a power industry to the empirical analysis, this method was testified that it can classify the data in a high accuracy. The research illustrates that the model mentioned above has good results, and the method is practical and feasible.},
  Doi                      = {10.1109/ICMLC.2010.5580511},
  File                     = {Sun-2010-p2021-2025.pdf:Sun-2010-p2021-2025.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {economic indicators;electricity supply industry;finance;pattern classification;power plants;risk management;rough set theory;support vector machines;credit risk assessment;data classification;empirical analysis;power industry;risk assessment index system;rough set;support vector machine;Accuracy;Indexes;Power industry;Risk management;Support vector machine classification;Training;Credit Risk Assessment;Power Industry;Rough Set;Support Vector Machine},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5580511&isnumber=5580479},
  Zone                     = {-}
}

@Article{Syrus-2001-p339-350,
  Title                    = {Part assessment guidelines and criteria for parts selection and management},
  Author                   = {Syrus, T. and Pecht, M.G. and Humphrey, D.},
  Journal                  = {Electronics Packaging Manufacturing, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {339-350},
  Volume                   = {24},

  Abstract                 = {Parts selection and management is a process designed to evaluate the risks inherent in the use of an electronic part (e.g., a resistor, diode, or integrated circuit), and then facilitate informed decisions regarding its selection and future management activities. One step in the process is part assessment, which evaluates a part's quality and integrity. This involves comparing data acquired for the part with predetermined criteria to determine if the part will function acceptably in an equipment manufacturer's product. This paper presents the part assessment process, along with the criteria developed for assessment. The process and criteria were developed by analyzing industry standards and existing company methods, consulting with industry and academia experts, and conducting a case study of 113 electronic parts. The results of the case study are also provided},
  Doi                      = {10.1109/6104.980044},
  File                     = {Published version:Syrus-2001-p339-350.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1521-334X},
  Keywords                 = {assembling;electronics industry;equipment selection;quality control;standards;academia experts;assembly guidelines;company methods;electronics industry;industry experts;industry standards;integrity;part assessment guidelines;parts management;parts selection;quality;Diodes;Electronic equipment manufacture;Electronics industry;Guidelines;Industrial electronics;Manufacturing industries;Process design;Resistors;Risk management;Standards development},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=980044&isnumber=21107},
  Zone                     = {-}
}

@Article{Syrus-2001-p339-350a,
  Title                    = {Part assessment guidelines and criteria for parts selection and management},
  Author                   = {Syrus, T. and Pecht, M.G. and Humphrey, D.},
  Journal                  = {Electronics Packaging Manufacturing, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {339-350},
  Volume                   = {24},

  Abstract                 = {Parts selection and management is a process designed to evaluate the risks inherent in the use of an electronic part (e.g., a resistor, diode, or integrated circuit), and then facilitate informed decisions regarding its selection and future management activities. One step in the process is part assessment, which evaluates a part's quality and integrity. This involves comparing data acquired for the part with predetermined criteria to determine if the part will function acceptably in an equipment manufacturer's product. This paper presents the part assessment process, along with the criteria developed for assessment. The process and criteria were developed by analyzing industry standards and existing company methods, consulting with industry and academia experts, and conducting a case study of 113 electronic parts. The results of the case study are also provided},
  Doi                      = {10.1109/6104.980044},
  File                     = {Published version:Syrus-2001-p339-350a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1521-334X},
  Keywords                 = {assembling;electronics industry;equipment selection;quality control;standards;academia experts;assembly guidelines;company methods;electronics industry;industry experts;industry standards;integrity;part assessment guidelines;parts management;parts selection;quality;Diodes;Electronic equipment manufacture;Electronics industry;Guidelines;Industrial electronics;Manufacturing industries;Process design;Resistors;Risk management;Standards development},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=980044&isnumber=21107},
  Zone                     = {-}
}

@Article{Syrus-2001-p351-358,
  Title                    = {Manufacturer assessment procedure and criteria for parts selection and management},
  Author                   = {Syrus, T. and Pecht, M.G. and Uppalapati, R.},
  Journal                  = {Electronics Packaging Manufacturing, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {351-358},
  Volume                   = {24},

  Abstract                 = {Parts selection and management is a process designed to evaluate the risks inherent in the use of an electronic part (e.g., a resistor, diode, or integrated circuit), and then facilitate informed decisions regarding its selection and future management activities. One step in the process is the assessment of the part manufacturer, which involves comparing data acquired for the manufacturer with predetermined criteria to determine if the manufacturer's policies and procedures are consistent with producing quality and reliable parts. This paper presents the manufacturer assessment process, along with the criteria developed for assessment. The process and criteria were developed by analyzing industry standards and existing company methods, consulting with industry and academia experts, and conducting a case study of 36 electronic part manufacturers. The results of the case study are also provided},
  Doi                      = {10.1109/6104.980045},
  File                     = {Published version:Syrus-2001-p351-358.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1521-334X},
  Keywords                 = {assembling;electronic equipment manufacture;electronics industry;equipment selection;quality control;standards;company methods;electronic part;electronics industry;industry standards;manufacturer assessment procedure;manufacturer assessment process;part manufacturer;parts management;parts selection;policies;procedures;quality parts;traceability;Diodes;Electronics industry;Industrial electronics;Manufacturing industries;Manufacturing processes;Process design;Pulp manufacturing;Resistors;Risk management;Standards development},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=980045&isnumber=21107},
  Zone                     = {-}
}

@Article{Syrus-2001-p351-358a,
  Title                    = {Manufacturer assessment procedure and criteria for parts selection and management},
  Author                   = {Syrus, T. and Pecht, M.G. and Uppalapati, R.},
  Journal                  = {Electronics Packaging Manufacturing, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Oct},
  Number                   = {4},
  Pages                    = {351-358},
  Volume                   = {24},

  Abstract                 = {Parts selection and management is a process designed to evaluate the risks inherent in the use of an electronic part (e.g., a resistor, diode, or integrated circuit), and then facilitate informed decisions regarding its selection and future management activities. One step in the process is the assessment of the part manufacturer, which involves comparing data acquired for the manufacturer with predetermined criteria to determine if the manufacturer's policies and procedures are consistent with producing quality and reliable parts. This paper presents the manufacturer assessment process, along with the criteria developed for assessment. The process and criteria were developed by analyzing industry standards and existing company methods, consulting with industry and academia experts, and conducting a case study of 36 electronic part manufacturers. The results of the case study are also provided},
  Doi                      = {10.1109/6104.980045},
  File                     = {Published version:Syrus-2001-p351-358a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1521-334X},
  Keywords                 = {assembling;electronic equipment manufacture;electronics industry;equipment selection;quality control;standards;company methods;electronic part;electronics industry;industry standards;manufacturer assessment procedure;manufacturer assessment process;part manufacturer;parts management;parts selection;policies;procedures;quality parts;traceability;Diodes;Electronics industry;Industrial electronics;Manufacturing industries;Manufacturing processes;Process design;Pulp manufacturing;Resistors;Risk management;Standards development},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=980045&isnumber=21107},
  Zone                     = {-}
}

@Article{Takaoka-1971-p536-542,
  Title                    = {N-Fail-Safe Logical Systems},
  Author                   = {Takaoka, T. and Mine, Hisashi},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1971},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {536-542},
  Volume                   = {C-20},

  Abstract                 = {The analysis and synthesis of the fail-safe logical systems have been studied by many authors. They utilize logical elements with asymmetric failure. It is shown that by duplicating two identical systems in parallel it is possible to raise the reliability of the system considerably. If, for example, 0 is the safe value, the duplicate output (0, 1) or (1, 0) is regarded as 1. The output (0, 0) is regarded as 0, which cannot, however, be judged to be correct or to be incorrect.},
  Doi                      = {10.1109/T-C.1971.223287},
  File                     = {Published version:Takaoka-1971-p536-542.pdf:PDF},
  Impactfactor             = {1.379},
  ISSN                     = {0018-9340},
  Keywords                 = {Asymmetric failure, complementary duplicate system, erasure error, fail-safe logical system, failure detecting circuit, N-fail-safe function, partially ordered set, prime implicant expansion.;Circuits;Error correction;Failure analysis;Mathematical analysis;Asymmetric failure, complementary duplicate system, erasure error, fail-safe logical system, failure detecting circuit, N-fail-safe function, partially ordered set, prime implicant expansion.},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，没有类似表达},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1671880&isnumber=35058},
  Zone                     = {3Q}
}

@Article{Tamasi-2011-p892-899,
  Title                    = {Risk assessment techniques for civil aviation security},
  Author                   = {Galileo Tamasi and Micaela Demichela},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2011},
  Number                   = {8},
  Pages                    = {892 - 899},
  Volume                   = {96},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {Following the 9/11 terrorists attacks in New York a strong economical effort was made to improve and adapt aviation security, both in infrastructures as in airplanes. National and international guidelines were promptly developed with the objective of creating a security management system able to supervise the identification of risks and the definition and optimization of control measures. Risk assessment techniques are thus crucial in the above process, since an incorrect risk identification and quantification can strongly affect both the security level as the investments needed to reach it. The paper proposes a set of methodologies to qualitatively and quantitatively assess the risk in the security of civil aviation and the risk assessment process based on the threats, criticality and vulnerabilities concepts, highlighting their correlation in determining the level of risk. \{RAMS\} techniques are applied to the airport security system in order to analyze the protection equipment for critical facilities located in air-side, allowing also the estimation of the importance of the security improving measures vs. their effectiveness. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2011.03.009},
  File                     = {Published version:Tamasi-2011-p892-899.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Civil aviation security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A， airport security system，通过Case描述过程，没有仿真},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832011000378},
  Zone                     = {2Q}
}

@Article{Tamasi-2011-p892-899a,
  Title                    = {Risk assessment techniques for civil aviation security},
  Author                   = {Galileo Tamasi and Micaela Demichela},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2011},
  Number                   = {8},
  Pages                    = {892 - 899},
  Volume                   = {96},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {Following the 9/11 terrorists attacks in New York a strong economical effort was made to improve and adapt aviation security, both in infrastructures as in airplanes. National and international guidelines were promptly developed with the objective of creating a security management system able to supervise the identification of risks and the definition and optimization of control measures. Risk assessment techniques are thus crucial in the above process, since an incorrect risk identification and quantification can strongly affect both the security level as the investments needed to reach it. The paper proposes a set of methodologies to qualitatively and quantitatively assess the risk in the security of civil aviation and the risk assessment process based on the threats, criticality and vulnerabilities concepts, highlighting their correlation in determining the level of risk. \{RAMS\} techniques are applied to the airport security system in order to analyze the protection equipment for critical facilities located in air-side, allowing also the estimation of the importance of the security improving measures vs. their effectiveness. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2011.03.009},
  File                     = {Published version:Tamasi-2011-p892-899.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Civil aviation security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A， airport security system，通过Case描述过程，没有仿真},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832011000378},
  Zone                     = {2Q}
}

@Article{Taylor-1962-p13-17,
  Title                    = {Weapon System Design and Supportability, A Function of Failure Prediction},
  Author                   = {Taylor, Vernon J.},
  Journal                  = {Reliability and Quality Control, IRE Transactions on},
  Year                     = {1962},

  Month                    = {July},
  Number                   = {2},
  Pages                    = {13-17},
  Volume                   = {RQC-11},

  Abstract                 = {Not Available},
  Doi                      = {10.1109/IRE-PGRQC.1962.5009599},
  File                     = {Published version:Taylor-1962-p13-17.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0097-4552},
  Keywords                 = {Acceleration;Availability;Cost function;Frequency;Logistics;Personnel;Quality control;Security;System testing;Weapons},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5009599&isnumber=5009590},
  Zone                     = {-}
}

@Article{Team-2013-p-,
  Title                    = {ICS-CERT Year in Review},
  Author                   = {Industrial Control Systems Cyber Emergency Response Team},
  Year                     = {2013},

  File                     = {Published version:Team-2013-p-.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.19},
  Url                      = {https://ics-cert.us-cert.gov/sites/default/files/documents/Year_In_Review_FY2013_Final.pdf},
  Zone                     = {-}
}

@InProceedings{Teixeira-2010-p5991-5998,
  Title                    = {Cyber security analysis of state estimators in electric power systems},
  Author                   = {Teixeira, A. and Amin, S. and Sandberg, H. and Johansson, K.H. and Sastry, S.S.},
  Booktitle                = {Decision and Control (CDC), 2010 49th IEEE Conference on},
  Year                     = {2010},
  Month                    = {Dec},
  Pages                    = {5991-5998},

  Abstract                 = {In this paper, we analyze the cyber security of state estimators in Supervisory Control and Data Acquisition (SCADA) systems operating in power grids. Safe and reliable operation of these critical infrastructure systems is a major concern in our society. In current state estimation algorithms there are bad data detection (BDD) schemes to detect random outliers in the measurement data. Such schemes are based on high measurement redundancy. Although such methods may detect a set of very basic cyber attacks, they may fail in the presence of a more intelligent attacker. We explore the latter by considering scenarios where deception attacks are performed, sending false information to the control center. Similar attacks have been studied before for linear state estimators, assuming the attacker has perfect model knowledge. Here we instead assume the attacker only possesses a perturbed model. Such a model may correspond to a partial model of the true system, or even an out-dated model. We characterize the attacker by a set of objectives, and propose policies to synthesize stealthy deceptions attacks, both in the case of linear and nonlinear estimators. We show that the more accurate model the attacker has access to, the larger deception attack he can perform undetected. Specifically, we quantify trade-offs between model accuracy and possible attack impact for different BDD schemes. The developed tools can be used to further strengthen and protect the critical state-estimation component in SCADA systems.},
  Doi                      = {10.1109/CDC.2010.5717318},
  File                     = {Published version:Teixeira-2010-p5991-5998.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0743-1546},
  Keywords                 = {SCADA systems;nonlinear estimation;power engineering computing;power grids;power system control;power system reliability;power system security;security of data;state estimation;BDD scheme;SCADA system;bad data detection;critical infrastructure system;critical state-estimation component;cyber attack;cyber security analysis;deception attack;electric power system;intelligent attacker;linear state estimator;measurement redundancy;nonlinear estimator;power grid;random outlier detection;reliable operation;safe operation;supervisory control and data acquisition system;Boolean functions;Convergence;Data structures;Equations;Mathematical model;Measurement uncertainty;SCADA systems},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.21},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5717318&isnumber=5716927},
  Zone                     = {-}
}

@Article{Theoharido-2007-p64-67,
  Title                    = {Common Body of Knowledge for Information Security},
  Author                   = {Theoharidou, M. and Gritzalis, D.},
  Journal                  = {Security Privacy, IEEE},
  Year                     = {2007},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {64-67},
  Volume                   = {5},

  Abstract                 = {The need for skilled information security professionals has led various academic, governmental, and industrial organizations to work to develop a common body of knowledge (CBK) for the security domain. A CBK is a framework and collection of information that provides a basis for understanding terms and concepts in a particular knowledge area. It defines the basic information that people who work in that area are expected to know. The International Information Systems Security Certification Consortium ([ISC]; www.isc2.org) defines a CBK as a taxonomy of topics relevant to professionals around the world. Information security is a multidisciplinary endeavor. In practice, professionals need knowledge and experience from fields such as management, business administration, ethics, sociology, and political science. Yet, existing CBKs focus on specific information security subdomains and thus offer limited understanding and narrow perceptions of the overall domain. Our aim is to identify and define an InfoSec CBK to serve as a tool for developing an information security curriculum},
  Doi                      = {10.1109/MSP.2007.32},
  File                     = {Published version:Theoharido-2007-p64-67.pdf:PDF},
  Impactfactor             = {0.962},
  ISSN                     = {1540-7993},
  Keywords                 = {computer science education;educational courses;security of data;common body of knowledge;information security curriculum;secure software assurance;secure software engineering;Communication system security;Computer security;Cryptography;Data security;Design engineering;Information security;Project management;Signal design;Software testing;System testing;CBK;InfoSec;common body of knowledge;curriculum development;degree programs;information security},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4140992&isnumber=4140976},
  Zone                     = {4Q}
}

@InProceedings{Thuraising-2009-p2-3,
  Title                    = {Keynote: Security Engineering: Developments and Directions},
  Author                   = {Thuraisingham, Bhavani},
  Booktitle                = {Secure Software Integration and Reliability Improvement, 2009. SSIRI 2009. Third IEEE International Conference on},
  Year                     = {2009},
  Month                    = {July},
  Pages                    = {2-3},

  Doi                      = {10.1109/SSIRI.2009.74},
  File                     = {Published version:Thuraising-2009-p2-3.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {military computing;program testing;program verification;security of data;software metrics;software reusability;CNCI cyber plan;CONOPS system;DoD policy;GIG mission assurance;complex system;database system;flexible system;global information grid;large system;mission critical system;operating system;operation concept;secure system development;security engineering methodology;security policy;security requirement;security validation;security verification;supply chain risk management;system evaluation;system metric;system usability;war fighting capability;Accreditation;Certification;Data security;Database systems;Design engineering;Mission critical systems;Operating systems;Supply chains;System testing;Systems engineering and theory},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5325402&isnumber=5325011},
  Zone                     = {-}
}

@InProceedings{Tianshui-2014-p233-238,
  Title                    = {A New Security and Privacy Risk Assessment Model for Information System Considering Influence Relation of Risk Elements},
  Author                   = {Wu Tianshui and Zhao Gang},
  Booktitle                = {Broadband and Wireless Computing, Communication and Applications (BWCCA), 2014 Ninth International Conference on},
  Year                     = {2014},
  Month                    = {Nov},
  Pages                    = {233-238},

  Abstract                 = {Considering the influence relations among risk assessment elements and the uncertainty generated in the security and privacy risk assessment process, this paper proposes a new security and privacy risk assessment model for information system which is based on DEMATEL-ANP combined with grey system theory. On the basis of risk assessment standard process, this model utilizes the DEMATEL method to identify risk assessment elements and evaluate comprehensive influence relations. Further, the model combines with ANP to solve the weight distribution ratio of the subordinate element of each evaluation elements. Finally the paper uses grey system theory to obtain grey evaluation matrix, and computes final security and privacy risk level. Examples simulation demonstrates that it is an effective method for information system on security and privacy risk assessment, which the model not only weighs up the association influence among the various evaluation factors in practical evaluation system, reduces the subjective evaluation, but also can effectively mitigate the uncertainty of expert evaluation.},
  Doi                      = {10.1109/BWCCA.2014.76},
  File                     = {Published version:Tianshui-2014-p233-238.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data privacy;decision making;grey systems;information systems;risk management;security of data;DEMATEL-ANP;analytic network process;decision making trial-and-evaluation laboratory;final security;grey evaluation matrix;grey system theory;information system;privacy risk assessment model;privacy risk level;security risk assessment model;weight distribution ratio;Computational modeling;Indexes;Information security;Privacy;Risk management;analytic network process (ANP);decision making trial and evaluation laboratory (DEMATEL);grey system theory;risk assessment;security and privacy},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7016074&isnumber=7015998},
  Zone                     = {-}
}

@Article{Tombros-2014-p-,
  Title                    = {Origin of a Barite-Sulfide Ore Deposit in the Mykonos Intrusion, Cyclades: Trace Element, Isotopic, Fluid Inclusion And Raman Spectroscopy Evidence },
  Author                   = {Stylianos F. Tombros and Karen St. Seymour and Anthony E. Williams-Jones and Degao Zhai and Jiajun Liu},
  Journal                  = {Ore Geology Reviews },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract The polymetallic Mykonos vein system in the Cyclades, Greece, consists of 15 tension-gashes filled with barite, quartz, pyrite, sphalerite, chalcopyrite and galena in the ca. 13.5&#xa0;Ma, I-type, Mykonos monzogranite. Zones of silica and chlorite-muscovite alteration are associated with the veins and overprint pervasive silicification, phyllic and argillic alteration that affected large parts of the monzogranite. The mineralization cements breccias and consists of an early barite-silica-pyrite-sphalerite-chalcopyrite assemblage followed by later argentiferous galena. A combination of fluid inclusion and stable isotope data suggests that the barite and associated mineralization were deposited from fluids containing 2 to 17 wt. % NaCl equivalent, at temperatures of&#xa0;~&#xa0;225° to 370&#xa0;°C, under a hydrostatic pressure of&#xa0;≤&#xa0;100 bars. The mineralizing fluids boiled and were saturated in \{H2S\} and SO2. Calculated δ18OH2O and δDH2O, initial 87Sr/86Sr isotope compositions and the trace and \{REEs\} elements contents are consistent with a model in which the mineralizing fluids were derived during alteration of the Mykonos intrusion and subsequently mixed with Miocene seawater. Heterogeneities in the calculated δ34SSO4-&#xa0;2 and δ34SH2S compositions of the ore fluids indicate two distinct sources for sulfur, namely of magmatic and seawater origin, and precipitation due to reduction of the SO4-&#xa0;2 during fluid mixing. The physicochemical conditions of the fluids were pH&#xa0;=&#xa0;5.0 to 6.2, logfS2&#xa0;=&#xa0;-13.8 to -12.5, logfO2&#xa0;=&#xa0;-31.9 to -30.9, logfH2S(g)&#xa0;=&#xa0;-1.9 to -1.7, logfTe2&#xa0;=&#xa0;-7.9 and logα(SO4-&#xa0;2(aq)/H2S(aq))&#xa0;=&#xa0;+&#xa0;2.6 to +&#xa0;5.5. We propose that retrograde mesothermal hydrothermal alteration of the Mykonos monzogranite released barium and silica from the alkali felspars. Barite was precipitated due to mixing of SO4-&#xa0;2-rich Miocene seawater with the ascending Ba-rich magmatic fluid venting upwards in the pluton. },
  Doi                      = {http://dx.doi.org/10.1016/j.oregeorev.2014.11.016},
  File                     = {Published version:Tombros-2014-p-.pdf:PDF},
  Impactfactor             = {2.417},
  ISSN                     = {0169-1368},
  Keywords                 = {Mykonos monzogranite},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0169136814003436},
  Zone                     = {2Q}
}

@Article{Toniutti-2010-p3074-3083,
  Title                    = {Failure of the Scalar Dielectric Function Approach for the Screening Modeling in Double-Gate SOI MOSFETs and in FinFETs},
  Author                   = {Toniutti, P. and Esseni, D. and Palestri, P.},
  Journal                  = {Electron Devices, IEEE Transactions on},
  Year                     = {2010},

  Month                    = {Nov},
  Number                   = {11},
  Pages                    = {3074-3083},
  Volume                   = {57},

  Abstract                 = {This paper shows that modeling of the screening effect based on the scalar dielectric function (SDF) fails in double-gate (DG) MOS transistors and in FinFETs. This leads to simulation results inconsistent with the experiments, especially at high channel inversion densities where the mobility is limited by the surface roughness scattering. These results suggest that one should not use the SDF to model transport in DG silicon-on-insulator MOSFETs or FinFETs, but rather resort to the full tensorial dielectric function. This paper clearly identifies, using multi-subband Monte Carlo simulations as well as analytical derivations for the screened matrix elements of the surface roughness scattering, the simplifying assumptions in the derivation of the SDF that do not hold in a DG MOSFET.},
  Doi                      = {10.1109/TED.2010.2068990},
  File                     = {Published version:Toniutti-2010-p3074-3083.pdf:PDF},
  Impactfactor             = {2.062},
  ISSN                     = {0018-9383},
  Keywords                 = {MOSFET;Monte Carlo methods;dielectric properties;silicon-on-insulator;surface roughness;FinFET;double-gate SOI MOSFET;multisubband Monte Carlo simulation;scalar dielectric function;screening effect;surface roughness scattering;Dielectrics;FinFETs;MOSFETs;Monte Carlo methods;Silicon;Dielectric function;Monte Carlo;electron transport;multi-gate structures;screening modelling},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5570941&isnumber=5607250},
  Zone                     = {2Q}
}

@Article{Topuz-2011-p393-403,
  Title                    = {Integration of environmental and human health risk assessment for industries using hazardous materials: A quantitative multi criteria approach for environmental decision makers },
  Author                   = {E. Topuz and I. Talinli and E. Aydin},
  Journal                  = {Environment International },
  Year                     = {2011},
  Number                   = {2},
  Pages                    = {393 - 403},
  Volume                   = {37},

  Abstract                 = {Environmental management, for which environmental and human health risk assessment is the first stage, is a requirement for industries both before construction and during operation in order to sustain improved quality of life in the ecosystem. Therefore, the aim of this study is to propose an approach that integrates environmental and human health risk assessment for industries using hazardous materials in order to support environmental decision makers with quantitative and directive results. Analytic hierarchy process and fuzzy logic are used as tools to handle problems caused by complexity of environment and uncertain data. When the proposed approach is implemented to a scenario, it was concluded that it is possible to define risk sources with their risk classes and related membership degrees in that classes which enable the decision maker to decide which risk source has priority. In addition, they can easily point out and rank the factors contributing those risk sources owing to priority weights of them. As a result, environmental decision makers can use this approach while they are developing management alternatives for unfounded and on-going industrial plants using hazardous materials. },
  Doi                      = {http://dx.doi.org/10.1016/j.envint.2010.10.013},
  File                     = {Published version:Topuz-2011-p393-403.pdf:PDF},
  Impactfactor             = {6.248},
  ISSN                     = {0160-4120},
  Keywords                 = {Environmental risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，本文主要讲的是工业中的有害物质对环境和人的影响，定量分析，利用 Factor Index 方法，以及模糊方法。重点要看的文章},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0160412010002291},
  Zone                     = {1Q}
}

@Article{Topuz-2011-p393-403a,
  Title                    = {Integration of environmental and human health risk assessment for industries using hazardous materials: A quantitative multi criteria approach for environmental decision makers },
  Author                   = {E. Topuz and I. Talinli and E. Aydin},
  Journal                  = {Environment International },
  Year                     = {2011},
  Number                   = {2},
  Pages                    = {393 - 403},
  Volume                   = {37},

  Abstract                 = {Environmental management, for which environmental and human health risk assessment is the first stage, is a requirement for industries both before construction and during operation in order to sustain improved quality of life in the ecosystem. Therefore, the aim of this study is to propose an approach that integrates environmental and human health risk assessment for industries using hazardous materials in order to support environmental decision makers with quantitative and directive results. Analytic hierarchy process and fuzzy logic are used as tools to handle problems caused by complexity of environment and uncertain data. When the proposed approach is implemented to a scenario, it was concluded that it is possible to define risk sources with their risk classes and related membership degrees in that classes which enable the decision maker to decide which risk source has priority. In addition, they can easily point out and rank the factors contributing those risk sources owing to priority weights of them. As a result, environmental decision makers can use this approach while they are developing management alternatives for unfounded and on-going industrial plants using hazardous materials. },
  Doi                      = {http://dx.doi.org/10.1016/j.envint.2010.10.013},
  File                     = {Published version:Topuz-2011-p393-403a.pdf:PDF},
  Impactfactor             = {6.248},
  ISSN                     = {0160-4120},
  Keywords                 = {Environmental risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0160412010002291},
  Zone                     = {1Q}
}

@InProceedings{Toyota-1999-p886-891,
  Title                    = {Failure detection and diagnosis of rotating machinery by orthogonal expansion of density function of vibration signal},
  Author                   = {Toyota, T. and Niho, T. and Peng Chen},
  Booktitle                = {Environmentally Conscious Design and Inverse Manufacturing, 1999. Proceedings. EcoDesign '99: First International Symposium On},
  Year                     = {1999},
  Month                    = {Feb},
  Pages                    = {886-891},

  Doi                      = {10.1109/ECODIM.1999.747733},
  File                     = {Published version:Toyota-1999-p886-891.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {failure analysis;fault diagnosis;machine testing;machine theory;normal distribution;probability;reliability;failure detection;failure diagnosis;normal distribution;orthogonal expansion;probability density function;rotating machinery;time domain;vibration characteristics;vibration signal;Density functional theory;Feature extraction;Gaussian distribution;Machinery;Probability density function;Rolling bearings;Rotating machines;Signal analysis;Signal processing;Time domain analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=747733&isnumber=16131},
  Zone                     = {-}
}

@InProceedings{Tsai-2010-p485-490,
  Title                    = {Earthquake risk assessment information for tourism industry},
  Author                   = {Chung-Hung Tsai},
  Booktitle                = {Information Sciences and Interaction Sciences (ICIS), 2010 3rd International Conference on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {485-490},

  Abstract                 = {In this study we provide a practical mechanism for earthquake disaster risk assessment and management in the tourism industry, especially focusing on insurance and prevention. The primary methodology is to integrate the information sciences, earth sciences, civil engineering and insurance with the concepts of disaster risk management. There are four primary components within this system: stochastic event generator, hazard analysis procedure, vulnerability analysis procedure, financial analysis procedure. Based on the analysis results of the above generator and procedures, the model can produce annual exceeding probability curve, average annual loss, and probable maximum loss, which are very useful information for earthquake risk control strategy and management. Several scenario data are taken into consideration with the characteristics and sources of earthquake-related damage.},
  Doi                      = {10.1109/ICICIS.2010.5534779},
  File                     = {Tsai-2010-p485-490.pdf:Tsai-2010-p485-490.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Civil engineering;Disaster management;Earthquakes;Geoscience;Hazards;Information analysis;Insurance;Risk analysis;Risk management;Stochastic systems;Disaster management;Earthquake risk;Tourism industry},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5534779&isnumber=5534673},
  Zone                     = {-}
}

@Article{Tsai-2011-p158-171,
  Title                    = {The establishment of a rapid natural disaster risk assessment model for the tourism industry },
  Author                   = {Chung-Hung Tsai and Cheng-Wu Chen},
  Journal                  = {Tourism Management },
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {158 - 171},
  Volume                   = {32},

  Abstract                 = {At the current time, the Taiwan government is aggressively promoting projects, such as the so-called “Double Tourist Plan”, designed to encourage the development of the sightseeing related business. Operators in the tourist industry hope to construct facilities at or near scenic areas, which, given their special geography, are often adjacent to the mountains or the ocean. Unfortunately these are also the areas that most often experience natural disasters. This has a negative impact on the tourism industry. The centralization of tourist facilities, leading to the gathering of large numbers of visitors during the holiday seasons, can place people in danger. In other words, tourism operators in the Taiwan region face a high risk of natural disasters. It is difficult to assess such risks. Most of the existing models for catastrophe risk assessment consume huge amounts of time and are costly to use, so are more commonly applied for assessment at high-value facilities (such science-based industrial parks), rather than by the tourism industry, where economic factors are of greater concern. It is necessary to develop a simple and rapid assessment method that will allow ordinary business owners to carry out comprehensive risk analysis of tourist facilities. Thus, in this study, we explore various theories related to different kinds of natural disaster risk analysis mechanisms, with the goal of establishing a rapid risk assessment model suited to the tourism industry that can be used to quickly analyze disaster-forming characteristics and risk weaknesses in local regions. Furthermore, we incorporate an expert weighting process for assigning weightings for natural disaster risk index assessment. This method can help tourism asset owners prepare for the worst, and be capable of responding appropriately if and when such an event occurs. Good planning can effectively reduce the loss and risks associated with natural disasters and allow recovery work to commence sooner. },
  Doi                      = {http://dx.doi.org/10.1016/j.tourman.2010.05.015},
  File                     = {Published version:Tsai-2011-p158-171.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0261-5177},
  Keywords                 = {Tourism industry},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，旅游业自然灾害风险评估模型，不是很相关},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0261517710000993},
  Zone                     = {-}
}

@Article{Tsai-2011-p158-171a,
  Title                    = {The establishment of a rapid natural disaster risk assessment model for the tourism industry },
  Author                   = {Chung-Hung Tsai and Cheng-Wu Chen},
  Journal                  = {Tourism Management },
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {158 - 171},
  Volume                   = {32},

  Abstract                 = {At the current time, the Taiwan government is aggressively promoting projects, such as the so-called “Double Tourist Plan”, designed to encourage the development of the sightseeing related business. Operators in the tourist industry hope to construct facilities at or near scenic areas, which, given their special geography, are often adjacent to the mountains or the ocean. Unfortunately these are also the areas that most often experience natural disasters. This has a negative impact on the tourism industry. The centralization of tourist facilities, leading to the gathering of large numbers of visitors during the holiday seasons, can place people in danger. In other words, tourism operators in the Taiwan region face a high risk of natural disasters. It is difficult to assess such risks. Most of the existing models for catastrophe risk assessment consume huge amounts of time and are costly to use, so are more commonly applied for assessment at high-value facilities (such science-based industrial parks), rather than by the tourism industry, where economic factors are of greater concern. It is necessary to develop a simple and rapid assessment method that will allow ordinary business owners to carry out comprehensive risk analysis of tourist facilities. Thus, in this study, we explore various theories related to different kinds of natural disaster risk analysis mechanisms, with the goal of establishing a rapid risk assessment model suited to the tourism industry that can be used to quickly analyze disaster-forming characteristics and risk weaknesses in local regions. Furthermore, we incorporate an expert weighting process for assigning weightings for natural disaster risk index assessment. This method can help tourism asset owners prepare for the worst, and be capable of responding appropriately if and when such an event occurs. Good planning can effectively reduce the loss and risks associated with natural disasters and allow recovery work to commence sooner. },
  Doi                      = {http://dx.doi.org/10.1016/j.tourman.2010.05.015},
  File                     = {Published version:Tsai-2011-p158-171a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0261-5177},
  Keywords                 = {Tourism industry},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0261517710000993},
  Zone                     = {-}
}

@Article{Tsai-2010-p470-481,
  Title                    = {An earthquake disaster management mechanism based on risk assessment information for the tourism industry-a case study from the island of Taiwan },
  Author                   = {Chung-Hung Tsai and Cheng-Wu Chen},
  Journal                  = {Tourism Management },
  Year                     = {2010},
  Number                   = {4},
  Pages                    = {470 - 481},
  Volume                   = {31},

  Abstract                 = {Although progress has been made in assessing risks in Taiwan, there remains a need to undertake risk assessment for the Taiwanese tourism industry. Although the conventional rules of thumb usually adopted by the insurance industry for damage assessment may be adequate in normal circumstances, this is not feasible for rare incidents for which there is very little reference material. One such example is earthquake damage. The average period of occurrence of major earthquake in Taiwan is about a hundred years, which makes it impossible to precisely assess average losses. In addition, earthquakes are not like some disasters (such as typhoons or epidemics) for which one can receive early warning. Thus, in this study, we seek to provide a practical mechanism for earthquake disaster risk assessment and management for the tourism industry, focusing on insurance and prevention. The primary objective is to integrate the relevant earth sciences, engineering and insurance data with the concepts of disaster risk management, taking into consideration the characteristics and sources of earthquake-related risks. In short, we develop an earthquake disaster assessment model to apply to risk management in the tourism industry. The practicality of the model is demonstrated with some examples related to the hotel industry in Hualien, Taiwan, an area plagued by frequent earthquakes. The results clearly show the efficacy of the proposed disaster assessment models for the tourism industry. },
  Doi                      = {http://dx.doi.org/10.1016/j.tourman.2009.05.008},
  File                     = {Published version:Tsai-2010-p470-481.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0261-5177},
  Keywords                 = {Earthquake risk},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0261517709001009},
  Zone                     = {-}
}

@Article{Tsai-2011-p801-816,
  Title                    = {An Analytic Hierarchy Process-Based Risk Assessment Method for Wireless Networks},
  Author                   = {Hsin-Yi Tsai and Yu-Lun Huang},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {801-816},
  Volume                   = {60},

  Abstract                 = {This paper presents a wireless risk assessment method to help an administrator manage wireless network security. The assessment method consists of a risk model and an assessment measure. The risk model is in charge of modeling the wireless network risk. Security requirements, wireless attacks, and system configurations are considered in the model. The assessment measure is an algorithm which determines the risk value of the wireless network according to the risk model. Our risk model is developed upon an extended analytic hierarchy process, which contains the 4 layers: the risk layer, the requirement layer, the attack layer, and the configuration layer. The separate layers of the risk model are helpful in dealing with the dynamics of a wireless network because only the related layers are introduced to the assessment measure when changes of the network are detected. Based on the risk model per device, our assessment measure evaluates the wireless network risk in consideration of the relations between devices, attacks, and configurations. Hence, our risk assessment method, composed of the risk model and the assessment measure, can determine the wireless network risk efficiently while considering the dependencies in the wireless network. Two examples are introduced in this paper to examine the feasibility of our method. In the first example, we demonstrate that the risk values derived by our method meet the ground truth by performing practical experiments. The second example shows that our method can evaluate the risk of a changing wireless network with efficiency, and can distinguish disparities in different wireless networks.},
  Doi                      = {10.1109/TR.2011.2170117},
  File                     = {Published version:Tsai-2011-p801-816.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {radio networks;telecommunication security;analytic hierarchy process;security requirement;system configuration;wireless attack;wireless network security;wireless risk assessment method;Analytical models;Communication system security;Risk management;Security;Wireless networks;Analytic hierarchy process (AHP);risk assessment;wireless security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6043900&isnumber=6087327},
  Zone                     = {2Q}
}

@Article{Tsai-2011-p801-816a,
  Title                    = {An Analytic Hierarchy Process-Based Risk Assessment Method for Wireless Networks},
  Author                   = {Hsin-Yi Tsai and Yu-Lun Huang},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {801-816},
  Volume                   = {60},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {This paper presents a wireless risk assessment method to help an administrator manage wireless network security. The assessment method consists of a risk model and an assessment measure. The risk model is in charge of modeling the wireless network risk. Security requirements, wireless attacks, and system configurations are considered in the model. The assessment measure is an algorithm which determines the risk value of the wireless network according to the risk model. Our risk model is developed upon an extended analytic hierarchy process, which contains the 4 layers: the risk layer, the requirement layer, the attack layer, and the configuration layer. The separate layers of the risk model are helpful in dealing with the dynamics of a wireless network because only the related layers are introduced to the assessment measure when changes of the network are detected. Based on the risk model per device, our assessment measure evaluates the wireless network risk in consideration of the relations between devices, attacks, and configurations. Hence, our risk assessment method, composed of the risk model and the assessment measure, can determine the wireless network risk efficiently while considering the dependencies in the wireless network. Two examples are introduced in this paper to examine the feasibility of our method. In the first example, we demonstrate that the risk values derived by our method meet the ground truth by performing practical experiments. The second example shows that our method can evaluate the risk of a changing wireless network with efficiency, and can distinguish disparities in different wireless networks.},
  Doi                      = {10.1109/TR.2011.2170117},
  File                     = {Published version:Tsai-2011-p801-816a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {radio networks;telecommunication security;analytic hierarchy process;security requirement;system configuration;wireless attack;wireless network security;wireless risk assessment method;Analytical models;Communication system security;Risk management;Security;Wireless networks;Analytic hierarchy process (AHP);risk assessment;wireless security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，无仿真，Case Study，两个实验，，一自己组建的小网络，个通过对两个对象进行评估，体现方法的适应性，第二个是通过对网络不通时刻的评估，体现动态性},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6043900&isnumber=6087327},
  Zone                     = {2Q}
}

@Article{Tsai-2011-p801-816b,
  Title                    = {An Analytic Hierarchy Process-Based Risk Assessment Method for Wireless Networks},
  Author                   = {Hsin-Yi Tsai and Yu-Lun Huang},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {801-816},
  Volume                   = {60},

  Abstract                 = {This paper presents a wireless risk assessment method to help an administrator manage wireless network security. The assessment method consists of a risk model and an assessment measure. The risk model is in charge of modeling the wireless network risk. Security requirements, wireless attacks, and system configurations are considered in the model. The assessment measure is an algorithm which determines the risk value of the wireless network according to the risk model. Our risk model is developed upon an extended analytic hierarchy process, which contains the 4 layers: the risk layer, the requirement layer, the attack layer, and the configuration layer. The separate layers of the risk model are helpful in dealing with the dynamics of a wireless network because only the related layers are introduced to the assessment measure when changes of the network are detected. Based on the risk model per device, our assessment measure evaluates the wireless network risk in consideration of the relations between devices, attacks, and configurations. Hence, our risk assessment method, composed of the risk model and the assessment measure, can determine the wireless network risk efficiently while considering the dependencies in the wireless network. Two examples are introduced in this paper to examine the feasibility of our method. In the first example, we demonstrate that the risk values derived by our method meet the ground truth by performing practical experiments. The second example shows that our method can evaluate the risk of a changing wireless network with efficiency, and can distinguish disparities in different wireless networks.},
  Doi                      = {10.1109/TR.2011.2170117},
  File                     = {Published version:Tsai-2011-p801-816b.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {radio networks;telecommunication security;analytic hierarchy process;security requirement;system configuration;wireless attack;wireless network security;wireless risk assessment method;Analytical models;Communication system security;Risk management;Security;Wireless networks;Analytic hierarchy process (AHP);risk assessment;wireless security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6043900&isnumber=6087327},
  Zone                     = {2Q}
}

@Article{Valenzano-2014-p6-17,
  Title                    = {Industrial Cybersecurity: Improving Security Through Access Control Policy Models},
  Author                   = {Valenzano, A.},
  Journal                  = {Industrial Electronics Magazine, IEEE},
  Year                     = {2014},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {6-17},
  Volume                   = {8},

  Abstract                 = {The protection of a networked industrial control system (ICS) against cyberattacks and malicious behavior is a process that should be taken into account since the very beginning of the system's conception. This is true, in particular, for the design and verification of access control policies that build up the core of any protection scheme. The aim of this article is to assess the general framework and shed some light on the research activities concerning the analysis and verification of access policies in ICSs, which are currently being carried out in our laboratory.},
  Doi                      = {10.1109/MIE.2014.2311313},
  File                     = {Published version:Valenzano-2014-p6-17.pdf:PDF},
  Impactfactor             = {3.758},
  ISSN                     = {1932-4529},
  Keywords                 = {authorisation;industrial control;ICS;access control policy design;access control policy verification;cyberattacks;malicious behavior;networked industrial control system protection;Access control;Analytical models;Computer crime;Computer security;Control systems;IEC standards;Industrial plants;Integrated circuit modeling;Network security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6839138&isnumber=6839098},
  Zone                     = {1Q}
}

@Article{Vandenhove-2015-p14-23,
  Title                    = {Potential radiological impact of the phosphate industry on wildlife },
  Author                   = {Hildegarde Vandenhove and Jordi Vives i Batlle and Lieve Sweeck},
  Journal                  = {Journal of Environmental Radioactivity },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {14 - 23},
  Volume                   = {141},

  Abstract                 = {Abstract The activities of the phosphate industry may lead to enhanced levels of naturally occurring radioactivity in terrestrial and aquatic ecosystems. We performed a preliminary environmental risk assessment (ERA) of environmental contamination resulting from the activities of 5 phosphate fertiliser plants (located in Belgium, Spain, Syria, Egypt, Brazil), a phosphate-mine and a phosphate-export platform in a harbour (both located in Syria). These sites were selected because of the availability of information on concentrations of naturally occurring radionuclides in the surrounding environments. Assessments were generally performed considering highest environmental concentrations reported in the studies. The \{ERICA\} Tool, operating in a Tier 2 assessment mode, was used to predict radiation dose rates and associated risk to the selected reference organisms using the \{ERICA\} default parameter setting. Reference organisms were those assigned as default by the \{ERICA\} Tool. Potential impact is expressed as a best estimate risk quotient (RQ) based on a radiation screening value of 10 μGy h−1. If RQ ≤ 1, the environment is considered unlikely to be at risk and further radiological assessment is not deemed necessary. Except for one of the cases assessed, the best estimate \{RQ\} exceeded 1 for at least one of the reference organisms. Internal exposure covered for 90–100 % of the total dose. 226Ra or 210Po were generally the highest contributors to the dose. The aquatic ecosystems in the vicinity of the phosphate fertiliser plants in Tessenderlo (Belgium), Huelva (Spain), Goiás (Brazil) and the terrestrial environment around the phosphate mine in Palmyra (Syria) are the ecosystems predicted to be potentially most at risk. },
  Doi                      = {http://dx.doi.org/10.1016/j.jenvrad.2014.11.001},
  File                     = {Published version:Vandenhove-2015-p14-23.pdf:PDF},
  Impactfactor             = {2.119},
  ISSN                     = {0265-931X},
  Keywords                 = {Environmental radiological impact assessment},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0265931X1400321X},
  Zone                     = {3Q}
}

@InProceedings{Vasquez-2004-p4-3936,
  Title                    = {Motion prediction for moving objects: a statistical approach},
  Author                   = {Vasquez, D. and Fraichard, T.},
  Booktitle                = {Robotics and Automation, 2004. Proceedings. ICRA '04. 2004 IEEE International Conference on},
  Year                     = {2004},
  Month                    = {April},
  Pages                    = {3931-3936 Vol.4},
  Volume                   = {4},

  Abstract                 = {This paper proposes a technique to obtain long term estimates of the motion of a moving object in a structured environment. Objects moving in such environments often participate in typical motion patterns which can be observed consistently. Our technique learns those patterns by observing the environment and clustering the observed trajectories using any pairwise clustering algorithm. We have implemented our technique using both simulated and real data coming from a vision system. The results show that the technique is general, produces long-term predictions and is fast enough for its use in real time applications.},
  Doi                      = {10.1109/ROBOT.2004.1308883},
  File                     = {Published version:Vasquez-2004-p4-3936.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1050-4729},
  Keywords                 = {learning (artificial intelligence);motion estimation;pattern clustering;robot vision;statistical analysis;learning algorithms;motion estimation;motion patterns;motion prediction;moving objects;pairwise clustering algorithm;statistical analysis;vision system;Animals;Clustering algorithms;Collision avoidance;Machine vision;Motion control;Motion estimation;Navigation;State estimation;Trajectory;Video surveillance},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1308883&isnumber=29027},
  Zone                     = {-}
}

@InProceedings{Vasyliuk-2011-p222-223,
  Title                    = {The principle of“function-structure” dependency in MEMS ontology},
  Author                   = {Vasyliuk, I and Teslyuk, V. and Hemich, N. and Zelinskyy, A},
  Booktitle                = {Perspective Technologies and Methods in MEMS Design (MEMSTECH), 2011 Proceedings of VIIth International Conference on},
  Year                     = {2011},
  Month                    = {May},
  Pages                    = {222-223},

  Abstract                 = {In the paper it is reviewed the classification of MEMS ontologies and the significance of the function-structure coupling in MEMS ontology.},
  File                     = {Published version:Vasyliuk-2011-p222-223.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {microfabrication;micromechanical devices;ontologies (artificial intelligence);MEMS ontology;function structure dependency;Couplings;Fabrication;Frequency measurement;Materials;Micromechanical devices;Ontologies;Suspensions;MEMS;fabrication process;function-structure coupling;ontology},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5960371&isnumber=5960241},
  Zone                     = {-}
}

@InProceedings{Vega-2013-p735-738,
  Title                    = {Dynamic risk assessment of the onset of Paroxysmal Atrial Fibrillation},
  Author                   = {Vega, C. and Garcia, I. and Jimenez, J.},
  Booktitle                = {Computing in Cardiology Conference (CinC), 2013},
  Year                     = {2013},
  Month                    = {Sept},
  Pages                    = {735-738},

  Abstract                 = {We propose a computational methodology for evaluating the temporal evolution of the risk of onset of Paroxysmal Atrial Fibrillation (PAF) episodes. Firstly, we obtained 75 records of hour long ambulatory electrocardiograms (Holter monitoring) from healthy volunteers. From these we constructed a catalog of normal heart rate behavior patterns. For each record, patterns were defined as the standardized sequences of 5 consecutive values of RR intervals and the catalog was made up of all possible patterns contained in the 75 records. Secondly, 25 records of RR intervals corresponding to one hour long electrocardiograms ending with a PAF episode (group 1) and 25 additional records from healthy volunteers (group 2) were compiled. We then implemented a numerical procedure to compare the patterns belonging to these two groups with those of the catalog: using mobile windows of 6 consecutive minutes, we tested the null hypothesis that the patterns contained in windows of data taken from groups 1 and 2 were statistically indistinguishable from different subsets of patterns within the catalog. Our results demonstrated that the power at which this hypothesis is rejected indicates increased fluctuations in the patterns as we approach arrhythmia. This enabled us to propose an early warning system for the onset of PAF episodes with a specificity of 74% and sensitivity of 88%.},
  File                     = {Published version:Vega-2013-p735-738.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2325-8861},
  Keywords                 = {bioelectric potentials;diseases;electrocardiography;medical signal detection;medical signal processing;numerical analysis;statistical analysis;Holter monitoring;RR intervals;ambulatory electrocardiograms;arrhythmia;computational methodology;dynamic risk assessment;mobile windows;normal heart rate behavior patterns;numerical procedure;paroxysmal atrial fibrillation episodes;temporal evolution;time 1 hour;Atrial fibrillation;Catalogs;Electrocardiography;Heart rate variability;Protocols;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6713482&isnumber=6712387},
  Zone                     = {-}
}

@Article{Vellaithur-2015-p566-575,
  Title                    = {CPIndex: Cyber-Physical Vulnerability Assessment for Power-Grid Infrastructures},
  Author                   = {Vellaithurai, C. and Srivastava, A. and Zonouz, S. and Berthier, R.},
  Journal                  = {Smart Grid, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {566-575},
  Volume                   = {6},

  Doi                      = {10.1109/TSG.2014.2372315},
  File                     = {Published version:Vellaithur-2015-p566-575.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1949-3053},
  Keywords                 = {Bayes methods;graph theory;power engineering computing;power grids;power system control;power system security;risk management;stochastic processes;CPIndex;cyber-physical critical infrastructures;cyber-physical security indices;cyber-physical vulnerability assessment;cyber-side instrumentation probes;graph-theoretic power system indexing algorithm;interprocess communications;numerical indices;operating system assets;power network configuration;power operators;power-grid Infrastructures;power-grid control networks;security assessment techniques;security-oriented stochastic risk management technique;stochastic Bayesian network models;Generators;Indexes;Power measurement;Security;Smart grids;Cyber-physical security metrics;cyber-physical systems;intrusion detection systems;situational awareness},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6979242&isnumber=7042857},
  Zone                     = {-}
}

@Article{Vellaithurai2015,
  Title                    = {CPIndex: Cyber-Physical Vulnerability Assessment for Power-Grid Infrastructures},
  Author                   = {Vellaithurai, C. and Srivastava, A. and Zonouz, S. and Berthier, R.},
  Journal                  = {Smart Grid, IEEE Transactions on},
  Year                     = {2015},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {566-575},
  Volume                   = {6},

  Doi                      = {10.1109/TSG.2014.2372315},
  File                     = {Published version:Vellaithur-2015-p566-575.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1949-3053},
  Keywords                 = {Bayes methods;graph theory;power engineering computing;power grids;power system control;power system security;risk management;stochastic processes;CPIndex;cyber-physical critical infrastructures;cyber-physical security indices;cyber-physical vulnerability assessment;cyber-side instrumentation probes;graph-theoretic power system indexing algorithm;interprocess communications;numerical indices;operating system assets;power network configuration;power operators;power-grid Infrastructures;power-grid control networks;security assessment techniques;security-oriented stochastic risk management technique;stochastic Bayesian network models;Generators;Indexes;Power measurement;Security;Smart grids;Cyber-physical security metrics;cyber-physical systems;intrusion detection systems;situational awareness},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6979242&isnumber=7042857},
  Zone                     = {-}
}

@InProceedings{VenkataSwa-2011-p1-6,
  Title                    = {VSM: A visual tool for the design and deployment of security requirements},
  Author                   = {VenkataSwamy, M. and Mohanty, H. and Ramaswamy, S.},
  Booktitle                = {India Conference (INDICON), 2011 Annual IEEE},
  Year                     = {2011},
  Month                    = {Dec},
  Pages                    = {1-6},

  Doi                      = {10.1109/INDCON.2011.6139347},
  File                     = {Published version:VenkataSwa-2011-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Linux;safety-critical software;security of data;software engineering;augment reliability;automate policy deployment;model driven approach;rule based active security system;rule extraction process;sandbox Linux kernel;sandbox critical system component;security component;software development life cycle;software security requirement deployment;structural relation;visual security model;visual tool;Kernel;Load modeling;Mathematical model;Object oriented modeling;Security;Unified modeling language;XML;Policy Extraction;Security Policy;Security Requirements;Visual Security Model},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6139347&isnumber=6139311},
  Zone                     = {-}
}

@Article{Vezzaro-2014-p1707-1716,
  Title                    = {Water Quality-based Real Time Control of Integrated Urban Drainage Systems: A Preliminary Study from Copenhagen, Denmark },
  Author                   = {L. Vezzaro and M.L. Christensen and C. Thirsing and M. Grum and P.S. Mikkelsen},
  Journal                  = {Procedia Engineering },
  Year                     = {2014},
  Note                     = {12th International Conference on Computing and Control for the Water Industry, \{CCWI2013\} },
  Number                   = {0},
  Pages                    = {1707 - 1716},
  Volume                   = {70},

  Abstract                 = {Abstract Global Real Time Control (RTC) of urban drainage systems is increasingly seen as cost-effective solution for responding to increasing performance demands. This study investigated the potential for including water-quality based \{RTC\} into the global control strategy which is under implementation in the Lynetten catchment (Copenhagen, Denmark). Two different strategies were simulated, considering: (i) water quality at the wastewater treatment plant (WWTP) inlet and (ii) pollution discharge to the bathing areas. These strategies were included in the Dynamic Overflow Risk Assessment (DORA) \{RTC\} strategy, which allows for prioritization of the discharge points in the systems according to their sensitivity. A conceptual hydrological model was used to assess the performance of the integrated control strategy over an entire year. The simulation results showed the benefits of the proposed approaches in reducing Combined Sewer Overflow (CSO) loads at the \{WWTP\} inlet and in an upstream location discharging to sensitive bathing waters for medium \{CSO\} events (i.e. those with greater potential for control). Furthermore, when looking at the overall performance across the entire catchment during the simulation period, no significant changes were observed. These preliminary results require further analysis by including detailed water quality measurements and simulations. Nevertheless, the potential for including water-quality \{RTC\} in global \{RTC\} schemes was unveiled, providing a further option to urban water managers to improve the performance of their systems. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2014.02.188},
  File                     = {Published version:Vezzaro-2014-p1707-1716.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {Global Real Time Control},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.10},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705814001908},
  Zone                     = {-}
}

@Article{Vezzaro-2014-p292-303,
  Title                    = {A generalised Dynamic Overflow Risk Assessment (DORA) for Real Time Control of urban drainage systems },
  Author                   = {Luca Vezzaro and Morten Grum},
  Journal                  = {Journal of Hydrology },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {292 - 303},
  Volume                   = {515},

  Abstract                 = {Summary An innovative and generalised approach to the integrated Real Time Control of urban drainage systems is presented. The Dynamic Overflow Risk Assessment (DORA) strategy aims to minimise the expected Combined Sewer Overflow (CSO) risk by considering (i) the water volume presently stored in the drainage network, (ii) the expected runoff volume (calculated by radar-based nowcast models) and 鈥? most important 鈥? (iii) the estimated uncertainty of the runoff forecasts. The inclusion of uncertainty allows for a more confident use of Real Time Control (RTC). Overflow risk is calculated by a flexible function which allows for the prioritisation of the discharge points according to their sensitivity and intended use. \{DORA\} was tested on a hypothetical example inspired by the main catchment in the city of Aarhus (Denmark). An analysis of DORA鈥檚 performance over a range of events with different return periods, using a simple conceptual model, is presented. Compared to a traditional local control approach, \{DORA\} contributed to reduce \{CSO\} volumes from the most sensitive points while reducing total \{CSO\} volumes discharged from the catchment. Additionally, the results show that the inclusion of forecasts and their uncertainty contributed to further improving the performance of drainage systems. The results of this paper will contribute to the wider usage of global \{RTC\} methods in the management of urban drainage networks. },
  Doi                      = {http://dx.doi.org/10.1016/j.jhydrol.2014.05.019},
  File                     = {Published version:Vezzaro-2014-p292-303.pdf:PDF},
  Impactfactor             = {2.964},
  ISSN                     = {0022-1694},
  Keywords                 = {Integrated urban water management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0022169414003837},
  Zone                     = {1Q}
}

@Article{Vezzaro-2014-p292-303a,
  Title                    = {A generalised Dynamic Overflow Risk Assessment (DORA) for Real Time Control of urban drainage systems},
  Author                   = {Luca Vezzaro and Morten Grum},
  Journal                  = {Journal of Hydrology},
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {292 - 303},
  Volume                   = {515},

  Abstract                 = {Summary An innovative and generalised approach to the integrated Real Time Control of urban drainage systems is presented. The Dynamic Overflow Risk Assessment (DORA) strategy aims to minimise the expected Combined Sewer Overflow (CSO) risk by considering (i) the water volume presently stored in the drainage network, (ii) the expected runoff volume (calculated by radar-based nowcast models) and – most important – (iii) the estimated uncertainty of the runoff forecasts. The inclusion of uncertainty allows for a more confident use of Real Time Control (RTC). Overflow risk is calculated by a flexible function which allows for the prioritisation of the discharge points according to their sensitivity and intended use. \{DORA\} was tested on a hypothetical example inspired by the main catchment in the city of Aarhus (Denmark). An analysis of DORA’s performance over a range of events with different return periods, using a simple conceptual model, is presented. Compared to a traditional local control approach, \{DORA\} contributed to reduce \{CSO\} volumes from the most sensitive points while reducing total \{CSO\} volumes discharged from the catchment. Additionally, the results show that the inclusion of forecasts and their uncertainty contributed to further improving the performance of drainage systems. The results of this paper will contribute to the wider usage of global \{RTC\} methods in the management of urban drainage networks. },
  Doi                      = {http://dx.doi.org/10.1016/j.jhydrol.2014.05.019},
  File                     = {Published version:Vezzaro-2014-p292-303a.pdf:PDF},
  Impactfactor             = {2.964},
  ISSN                     = {0022-1694},
  Keywords                 = {Integrated urban water management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0022169414003837},
  Zone                     = {1Q}
}

@Article{Vezzaro-2014-p292-303b,
  Title                    = {A generalised Dynamic Overflow Risk Assessment (DORA) for Real Time Control of urban drainage systems },
  Author                   = {Luca Vezzaro and Morten Grum},
  Journal                  = {Journal of Hydrology },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {292 - 303},
  Volume                   = {515},

  Abstract                 = {Summary An innovative and generalised approach to the integrated Real Time Control of urban drainage systems is presented. The Dynamic Overflow Risk Assessment (DORA) strategy aims to minimise the expected Combined Sewer Overflow (CSO) risk by considering (i) the water volume presently stored in the drainage network, (ii) the expected runoff volume (calculated by radar-based nowcast models) and – most important – (iii) the estimated uncertainty of the runoff forecasts. The inclusion of uncertainty allows for a more confident use of Real Time Control (RTC). Overflow risk is calculated by a flexible function which allows for the prioritisation of the discharge points according to their sensitivity and intended use. \{DORA\} was tested on a hypothetical example inspired by the main catchment in the city of Aarhus (Denmark). An analysis of DORA’s performance over a range of events with different return periods, using a simple conceptual model, is presented. Compared to a traditional local control approach, \{DORA\} contributed to reduce \{CSO\} volumes from the most sensitive points while reducing total \{CSO\} volumes discharged from the catchment. Additionally, the results show that the inclusion of forecasts and their uncertainty contributed to further improving the performance of drainage systems. The results of this paper will contribute to the wider usage of global \{RTC\} methods in the management of urban drainage networks. },
  Doi                      = {http://dx.doi.org/10.1016/j.jhydrol.2014.05.019},
  File                     = {Published version:Vezzaro-2014-p292-303b.pdf:PDF},
  Impactfactor             = {2.964},
  ISSN                     = {0022-1694},
  Keywords                 = {Integrated urban water management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.10},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0022169414003837},
  Zone                     = {1Q}
}

@Article{Vierow-2014-p320-328,
  Title                    = {Application of dynamic probabilistic risk assessment techniques for uncertainty quantification in generation \{IV\} reactors },
  Author                   = {K. Vierow and K. Hogan and K. Metzroth and T. Aldemir},
  Journal                  = {Progress in Nuclear Energy },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {320 - 328},
  Volume                   = {77},

  Abstract                 = {Abstract Demonstration of a practical approach is presented by which dynamic probabilistic risk/safety assessment (PRA/PSA) techniques are used to augment the uncertainty quantification process for Generation \{IV\} reactors. While the traditional \{PRA\} approach using event- and fault-trees is mostly concerned with aleatory uncertainties, dynamic PRA/PSA techniques allow simultaneous consideration of both epistemic and aleatory uncertainties in a phenomenologically and stochastically consistent manner. Epistemic uncertainties are expected to be particularly important in the PRA/PSA of Generation \{IV\} reactors. A dynamic PRA/PSA methodology is coupled with a reactor safety code to enable a more efficient and yet accurate evaluation of epistemic uncertainties which appear in the analysis of Generation \{IV\} reactors. Key uncertainties are identified for a postulated Loss of Forced Circulation event in a Pebble Bed Modular Reactor. The effects of these uncertainties on a selected key Figure of Merit, the stored energy in the core materials, are quantified. As a key result, the analyses show that the shape of the cumulative distribution function for each event tree is influenced by the aleatory uncertainties while the variations in the magnitudes of the cumulative distribution functions are determined by the epistemic uncertainties. },
  Doi                      = {http://dx.doi.org/10.1016/j.pnucene.2014.04.012},
  File                     = {Published version:Vierow-2014-p320-328.pdf:PDF},
  Impactfactor             = {0.704},
  ISSN                     = {0149-1970},
  Keywords                 = {Generation \{IV\} reactor},
  Level                    = {C},
  Owner                    = {Qiqi},
  Review                   = {C},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0149197014001012},
  Zone                     = {4Q}
}

@InProceedings{Vijaya-2010-p226-229,
  Title                    = {Risk assessment of operational events methodology for Indian nuclear power plants},
  Author                   = {Vijaya, A.K. and Guptan, R. and Dharne, S.P. and Ghadge, S.G.},
  Booktitle                = {Reliability, Safety and Hazard (ICRESH), 2010 2nd International Conference on},
  Year                     = {2010},
  Month                    = {Dec},
  Pages                    = {226-229},

  Abstract                 = {Operational Event is any event which happened at a nuclear power plant leading to an unplanned transient or significantly affecting the plant response characteristics. Such events may be caused by operator errors or equipment failures. The range of events could be from an apparently insignificant event to a transient that leads to reactor trip and subsequent actuation of designed safety systems or operator action. The efficient feedback of operating experience is a valuable source of information for improving the safety of nuclear power plants (NPPs). Among the various efforts to improve operational safety of nuclear installations, the systematic collection, evaluation and feedback of operational events (OE) are considered valuable and effective. This may be achieved by establishing a system for the effective feedback of operating experience. It is important that the assessment of events is carried out to provide confidence that the safety consequences have been fully understood, the causes have been correctly established and appropriate corrective actions identified. The traditional ways of investigating operational events have been predominantly qualitative. A method established upon probabilistic safety assessment (PSA) called probabilistic precursor event analysis is being increasingly used as it allows for a quantitative estimation of the safety significance of operational events. It is crucial that no events that are relevant to plant safety are screened out. The PSA based view helps to ensure that safety related aspects of an event are identified and do not get missed out. The purpose of this paper is to outline this method that makes more effective use of event information by mapping it with the plant specific PSA. A pilot study on PSA based Event Analysis was conducted for few events of an Indian PHWR, to establish a methodology for Event Analysis in Indian Nuclear Power Plants. The risk measure suitable for the purpose of operational events analysis is th- Conditional Core Damage Probability (CCDP). This paper presents a case study bringing out the methodology for estimation of exposure times and CCDP.},
  Doi                      = {10.1109/ICRESH.2010.5779551},
  File                     = {Published version:Vijaya-2010-p226-229.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {fission reactor core control;fission reactor operation;fission reactor safety;heavy water reactors;nuclear power stations;risk management;CCDP;Conditional Core Damage Probability;Indian PHWR;Indian nuclear power plants;nuclear installations;operational events methodology;probabilistic precursor event analysis;probabilistic safety assessment;risk assessment;safety systems;Analytical models;Generators;Core Damage Frequency;Core Damage Probability;Exposure time;Operational Events;Precursor Analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5779551&isnumber=5779525},
  Zone                     = {-}
}

@Article{Villacourt1994,
  Title                    = {Designing a reliability demonstration test on a lithography expose tool using Bayesian techniques},
  Author                   = {Villacourt, M. and Mahaney, M.},
  Journal                  = {Components, Packaging, and Manufacturing Technology, Part A, IEEE Transactions on},
  Year                     = {1994},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {458-462},
  Volume                   = {17},

  Doi                      = {10.1109/95.311756},
  File                     = {Published version:Villacourt-1994-p458-462.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1070-9886},
  Keywords                 = {Bayes methods;lithography;reliability;semiconductor process modelling;statistical analysis;Bayesian techniques;SEMATECH Qualification Plan;confidence interval;equipment reliability;lithography expose tool;past history;process stability;reliability demonstration test;statistical methods;test costs;tool development;wafer fabrication processing tool;Bayesian methods;Costs;Fabrication;History;Lithography;Qualifications;Stability;State estimation;Statistical analysis;System testing},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=311756&isnumber=7559},
  Zone                     = {-}
}

@Article{Villacourt-1994-p458-462,
  Title                    = {Designing a reliability demonstration test on a lithography expose tool using Bayesian techniques},
  Author                   = {Villacourt, M. and Mahaney, M.},
  Journal                  = {Components, Packaging, and Manufacturing Technology, Part A, IEEE Transactions on},
  Year                     = {1994},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {458-462},
  Volume                   = {17},

  Doi                      = {10.1109/95.311756},
  File                     = {Published version:Villacourt-1994-p458-462.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1070-9886},
  Keywords                 = {Bayes methods;lithography;reliability;semiconductor process modelling;statistical analysis;Bayesian techniques;SEMATECH Qualification Plan;confidence interval;equipment reliability;lithography expose tool;past history;process stability;reliability demonstration test;statistical methods;test costs;tool development;wafer fabrication processing tool;Bayesian methods;Costs;Fabrication;History;Lithography;Qualifications;Stability;State estimation;Statistical analysis;System testing},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=311756&isnumber=7559},
  Zone                     = {-}
}

@Article{Villemant-2014-p247-277,
  Title                    = {Evidence for a new shallow magma intrusion at La Soufrière of Guadeloupe (Lesser Antilles): Insights from long-term geochemical monitoring of halogen-rich hydrothermal fluids },
  Author                   = {B. Villemant and J.C. Komorowski and C. Dessert and A. Michel and O. Crispi and G. Hammouya and F. Beauducel and J.B. De Chabalier},
  Journal                  = {Journal of Volcanology and Geothermal Research },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {247 - 277},
  Volume                   = {285},

  Abstract                 = {Abstract More than three decades of geochemical monitoring of hot springs and fumaroles of La Soufrière of Guadeloupe allows the construction of a working model of the shallow hydrothermal system. This system is delimited by the nested caldera structures inherited from the repeated flank collapse events and the present dome built during the last magmatic eruption (1530&#xa0;AD) and which has been highly fractured by the subsequent phreatic or phreatomagmatic eruptions. Because it is confined into the low volume, highly compartmented and partially sealed upper edifice structure, the hydrothermal system is highly reactive to perturbations in the volcanic activity (input of deep magmatic fluids), the edifice structure (sealing and fracturing) and meteorology (wet tropical regime). The current unrest, which began with a mild reactivation of fumarolic activity in 1990, increased markedly in 1992 with seismic swarms and an increase of degassing from the summit of the dome. In 1997 seismic activity increased further and was accompanied by a sudden high-flux HCl-rich gas from summit fumaroles. We focus on the interpretation of the time series of the chemistry and temperature of fumarolic gases and hot springs as well as the relative behaviours of halogens (F, Cl, Br and I). This extensive geochemical time series shows that the deep magmatic fluids have undergone large changes in composition due to condensation and chemical interaction with shallow groundwater (scrubbing). It is possible to trace back these processes and the potential contribution of a deep magmatic source using a limited set of geochemical time series: T, \{CO2\} and total S content in fumaroles, T and Cl− in hot springs and the relative fractionations between F, Cl, Br and I in both fluids. Coupling 35&#xa0;years of geochemical data with meteorological rainfall data and models of ion transport in the hydrothermal aquifers has allowed us to identify a series of magmatic gas pulses into the hydrothermal system since the 1976–1977 crisis. The contrasting behaviours of S- and Cl-bearing species in fumarolic gas and in thermal springs suggest that the current activity is the result of a new magma intrusion which was progressively emplaced at shallow depth since ~&#xa0;1992. Although it might still be evolving, the characteristics of this new intrusion indicate that it has already reached a magnitude similar to the intrusion that was emplaced during the 1976–1977 eruptive crisis. The assessment of potential hazards associated with evolution of the current unrest must consider the implications of recurrent intrusion and further pressurisation of the hydrothermal system on the likelihood of renewed phreatic explosive activity. Moreover, the role of hydrothermal pressurisation on the basal friction along low-strength layers within the upper part of the edifice must be evaluated with regards to partial flank collapse. At this stage enhanced monitoring, research, and data analysis are required to quantify the uncertainties related to future scenarios of renewed eruptive activity and magmatic evolution. },
  Doi                      = {http://dx.doi.org/10.1016/j.jvolgeores.2014.08.002},
  File                     = {Published version:Villemant-2014-p247-277.pdf:PDF},
  Impactfactor             = {2.193},
  ISSN                     = {0377-0273},
  Keywords                 = {Hydrothermal system},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S037702731400242X},
  Zone                     = {3Q}
}

@InProceedings{Visser-2008-p1371-1379,
  Title                    = {Risk assessment modelling for the south african construction industry},
  Author                   = {Visser, K. and Joubert, P.},
  Booktitle                = {Management of Engineering Technology, 2008. PICMET 2008. Portland International Conference on},
  Year                     = {2008},
  Month                    = {July},
  Pages                    = {1371-1379},

  Abstract                 = {The construction project environment is fraught with risks of every conceivable nature and high levels of uncertainty. A research project was therefore initiated to define the most important construction risks from the insurance stakeholders' point of view. The risk management functionality in terms of the formal risk culture, risk framework and risk practices within the participating construction organisations were tested through a comprehensive questionnaire. Analysis of the completed questionnaires indicated that the results were consistent and repeatable. It was found that construction companies generally have weak risk management cultures, frameworks and practices, even though risk management awareness was relatively high at the construction project level. This seemed to emanate from the separation of project- and enterprise-related risks. The most important risks in the new proposed model are (1) the loss of key employees and business intelligence, (2) contractual related failure, (3) unfavourable financial market conditions and (4) failures of key contractors and clients.},
  Doi                      = {10.1109/PICMET.2008.4599749},
  File                     = {Visser-2008-p1371-1379.pdf:Visser-2008-p1371-1379.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {construction industry;insurance;organisational aspects;personnel;project management;risk management;South African construction industry;business intelligence;construction organisations;construction project environment;construction risks;employees;insurance stakeholders;research project;risk assessment modelling;Africa;Construction industry;Costs;Databases;Disaster management;Hazards;Insurance;Project management;Risk management;Uncertainty},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4599749&isnumber=4599598},
  Zone                     = {-}
}

@Article{Vogel-2009-p882-889,
  Title                    = {In-Ear Vital Signs Monitoring Using a Novel Microoptic Reflective Sensor},
  Author                   = {Vogel, S. and Hulsbusch, M. and Hennig, T. and Blazek, V. and Leonhardt, S.},
  Journal                  = {Information Technology in Biomedicine, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {882-889},
  Volume                   = {13},

  Abstract                 = {Cardiovascular diseases are among the most common causes of death in industrial countries. In order to take preventive actions, it is of great interest, to both physicians and patients, to determine cardiovascular risk factors early. To address this problem, a wearable in-ear measuring system (IN-MONIT) for 24/7 monitoring of vital parameters has been developed. The central component is a microoptic reflective sensor located inside the auditory canal. From the measured photoplethysmographic curves, heart activity and heart rate can be derived. In this paper, we describe the optoelectronic sensor concept and the autonomous design of the IN-MONIT measurement system. For the assessment of heart rate, different algorithms are introduced and the performance of the developed sensor system is evaluated in relation to conventional systems. In addition, the robustness to external artifacts is evaluated and artifact reduction strategies are considered.},
  Doi                      = {10.1109/TITB.2009.2033268},
  File                     = {Published version:Vogel-2009-p882-889.pdf:PDF},
  Impactfactor             = {1.978},
  ISSN                     = {1089-7771},
  Keywords                 = {biosensors;cardiovascular system;diseases;ear;medical signal detection;medical signal processing;optoelectronic devices;plethysmography;IN-MONIT;artifact reduction strategies;auditory canal;cardiovascular diseases;heart activity;heart rate;in-ear vital signs monitoring;microoptic reflective sensor;optoelectronic sensor;photoplethysmographic curves;wearable in-ear measuring system;Artifact analysis;Monte Carlo methods;heart rate;microsensors;photo plethysmography;Algorithms;Artifacts;Computer Simulation;Ear, External;Equipment Design;Equipment Failure Analysis;Heart Rate;Humans;Mastication;Miniaturization;Monitoring, Ambulatory;Monte Carlo Method;Photoplethysmography;Signal Processing, Computer-Assisted;Telemetry},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5290134&isnumber=5300653},
  Zone                     = {2Q}
}

@InProceedings{Vogelsang-2012-p8-14,
  Title                    = {Extent and characteristics of dependencies between vehicle functions in automotive software systems},
  Author                   = {Vogelsang, A and Teuchert, S. and Girard, J.},
  Booktitle                = {Modeling in Software Engineering (MISE), 2012 ICSE Workshop on},
  Year                     = {2012},
  Month                    = {June},
  Pages                    = {8-14},

  Abstract                 = {Functional dependencies and feature interactions are a major source of erroneous and unwanted behavior in software-intensive systems. To overcome these problems, many approaches exist that focus on modeling these functional dependencies in advance, i.e., in the specification or the design of a system. However, there is little empirical data on the amount of such interactions between system functions in realistic systems. In this paper, we analyze structural models of a modern realistic automotive vehicle system with the aim to assess the extent and characteristics of interactions between system functions. Our results show that at least 69% of the analyzed system functions depend on each other or influence each other. These dependencies stretch all over the system whereby single system functions have dependencies to up to 40% of all system functions. These results challenge the current development methods and processes that treat system functions more or less as independent units of functionality.},
  Doi                      = {10.1109/MISE.2012.6226020},
  File                     = {Published version:Vogelsang-2012-p8-14.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2156-788},
  Keywords                 = {automobiles;automotive engineering;mechanical engineering computing;automotive software systems;feature interactions;functional dependencies;realistic automotive vehicle system;software-intensive systems;structural model analysis;system design;system functions;system specification;vehicle functions;Analytical models;Automotive engineering;Computer architecture;Context;Software systems;Unified modeling language;Vehicles;automotive;empirical studies;feature interaction;functional specifications;model-based development},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6226020&isnumber=6226004},
  Zone                     = {-}
}

@Article{Volkanovsk-2009-p1116-1127,
  Title                    = {Application of the fault tree analysis for assessment of power system reliability },
  Author                   = {Andrija Volkanovski and Marko Čepin and Borut Mavko},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2009},
  Number                   = {6},
  Pages                    = {1116 - 1127},
  Volume                   = {94},

  Abstract                 = {A new method for power system reliability analysis using the fault tree analysis approach is developed. The method is based on fault trees generated for each load point of the power system. The fault trees are related to disruption of energy delivery from generators to the specific load points. Quantitative evaluation of the fault trees, which represents a standpoint for assessment of reliability of power delivery, enables identification of the most important elements in the power system. The algorithm of the computer code, which facilitates the application of the method, has been applied to the \{IEEE\} test system. The power system reliability was assessed and the main contributors to power system reliability have been identified, both qualitatively and quantitatively. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2009.01.004},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Fault tree},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.21},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832009000210},
  Zone                     = {2Q}
}

@Article{Vollmer-2014-p1337-1347,
  Title                    = {Cyber-Physical System Security With Deceptive Virtual Hosts for Industrial Control Networks},
  Author                   = {Vollmer, T. and Manic, M.},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {1337--1347},
  Volume                   = {10},

  Doi                      = {10.1109/TII.2014.2304633},
  File                     = {Published version:Vollmer-2014-p1337-1347.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.22},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6750078},
  Zone                     = {-}
}

@Article{Vollmer-2014-p1337-1347a,
  Title                    = {Cyber-Physical System Security With Deceptive Virtual Hosts for Industrial Control Networks},
  Author                   = {Vollmer, T. and Manic, M.},
  Journal                  = {Industrial Informatics, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {May},
  Number                   = {2},
  Pages                    = {1337-1347},
  Volume                   = {10},

  Abstract                 = {A challenge facing industrial control network administrators is protecting the typically large number of connected assets for which they are responsible. These cyber devices may be tightly coupled with the physical processes they control and human induced failures risk dire real-world consequences. Dynamic virtual honeypots are effective tools for observing and attracting network intruder activity. This paper presents a design and implementation for self-configuring honeypots that passively examine control system network traffic and actively adapt to the observed environment. In contrast to prior work in the field, six tools were analyzed for suitability of network entity information gathering. Ettercap, an established network security tool not commonly used in this capacity, outperformed the other tools and was chosen for implementation. Utilizing Ettercap XML output, a novel four-step algorithm was developed for autonomous creation and update of a Honeyd configuration. This algorithm was tested on an existing small campus grid and sensor network by execution of a collaborative usage scenario. Automatically created virtual hosts were deployed in concert with an anomaly behavior (AB) system in an attack scenario. Virtual hosts were automatically configured with unique emulated network stack behaviors for 92% of the targeted devices. The AB system alerted on 100% of the monitored emulated devices.},
  Doi                      = {10.1109/TII.2014.2304633},
  File                     = {Published version:Vollmer-2014-p1337-1347a.pdf:PDF},
  Impactfactor             = {3.381},
  ISSN                     = {1551-3203},
  Keywords                 = {XML;control engineering computing;security of data;sensors;AB;anomaly behavior system;campus grid;collaborative usage scenario;cyber-physical system security;deceptive virtual hosts;ettercap XML output;honeyd configuration;human induced failures risk;industrial control network administrators;network entity information gathering;network intruder activity;network stack behaviors;physical processes;real-world consequences;self-configuring honeypots;sensor network;Control systems;Emulation;Monitoring;Operating systems;Ports (Computers);Security;XML;Industrial control;intrusion detection;network security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6750078&isnumber=6809862},
  Zone                     = {2Q}
}

@Article{Vosough-2014-p4722-4725,
  Title                    = {Performance analysis of integer linear programming optical monitor placement for crosstalk attack detection in different network topologies },
  Author                   = {H. Vosough and S.C. Tan},
  Journal                  = {Optik - International Journal for Light and Electron Optics },
  Year                     = {2014},
  Number                   = {17},
  Pages                    = {4722 - 4725},
  Volume                   = {125},

  Abstract                 = {Abstract In this paper, an optimum optical power monitor placement is formulated in integer linear programming for crosstalk attack detection by exploiting monitoring information from test connections. The experimental results showed that equipping 6 out of 14, 8 out of 15 and 8 out of 16 nodes with optical power monitoring devices in NSF, \{MESH\} and \{GRID\} topologies are enough to detect the nodes that are traversed by the crosstalk attack in the networks. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijleo.2014.05.020},
  File                     = {Published version:Vosough-2014-p4722-4725.pdf:PDF},
  Impactfactor             = {0.524},
  ISSN                     = {0030-4026},
  Keywords                 = {Optical monitor placement},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0030402614005932},
  Zone                     = {4Q}
}

@InProceedings{Vucovich-2007-p405-412,
  Title                    = {Risk Assessment in Early Software Design Based on the Software Function-Failure Design Method},
  Author                   = {Vucovich, J.P. and Stone, R.B. and Liu, X.F. and Tumer, I.Y.},
  Booktitle                = {Computer Software and Applications Conference, 2007. COMPSAC 2007. 31st Annual International},
  Year                     = {2007},
  Month                    = {July},
  Pages                    = {405-412},
  Volume                   = {1},

  Doi                      = {10.1109/COMPSAC.2007.184},
  File                     = {Published version:Vucovich-2007-p405-412.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0730-3157},
  Keywords                 = {risk management;software development management;software reliability;NASA Mars Exploratory Rover;early design technique;risk assessment;risk management program;software design;software development process;software failure database;software failures;software function-failure design method;Application software;Design engineering;Design methodology;Mars;NASA;Programming;Risk analysis;Risk management;Software design;Software safety},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4291032&isnumber=4290963},
  Zone                     = {-}
}

@InProceedings{Wagner-2012-p1-4,
  Title                    = {Smart grid cyber security: A German perspective},
  Author                   = {Wagner, M. and Kuba, M. and Oeder, A},
  Booktitle                = {Smart Grid Technology, Economics and Policies (SG-TEP), 2012 International Conference on},
  Year                     = {2012},
  Month                    = {Dec},
  Pages                    = {1-4},

  Doi                      = {10.1109/SG-TEP.2012.6642389},
  File                     = {Published version:Wagner-2012-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data privacy;power system security;smart power grids;German power grid;data privacy;energy chain;low voltage segment;security-attacks;smart grid communication network;smart grid cyber security;smart grid technology;supply stability;Computer security;Encryption;Internet;Logic gates;Smart grids;Wide area networks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6642389&isnumber=6642357},
  Zone                     = {-}
}

@InProceedings{Wahlstrom-2014-p3132-3137,
  Title                    = {Risk assessment of vehicle cornering events in GNSS data driven insurance telematics},
  Author                   = {Wahlstrom, J. and Skog, I. and Handel, P.},
  Booktitle                = {Intelligent Transportation Systems (ITSC), 2014 IEEE 17th International Conference on},
  Year                     = {2014},
  Month                    = {Oct},
  Pages                    = {3132-3137},

  Abstract                 = {We propose a framework for the detection of dangerous vehicle cornering events, based on test statistics related to the no-sliding and no-rollover conditions. The input variables are estimated using circle fitting of position measurements (giving the instantaneous radius of the driving trajectory), and a Rauch-Tung-Striebel smoother applied to a state-space model describing the vehicle dynamics. The accuracy of the resulting test statistic is evaluated in a simulation study. In addition, we present the results of a field experiment where three smartphones were used as measurement probes. Since the estimation only uses position and speed data from global navigation satellite system receivers, the framework is particularly well-suited for smartphone-based insurance telematics applications, aiming to avoid the logistic and monetary costs associated with e.g., on-board-diagnostics or black-box dependent solutions. The design of the estimation algorithm allows for instant feedback to be given to the driver, and hence, supports the inclusion of real time value added services in usage-based-insurance programs.},
  Doi                      = {10.1109/ITSC.2014.6958194},
  File                     = {Published version:Wahlstrom-2014-p3132-3137.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {curve fitting;insurance;position measurement;risk management;satellite navigation;smart phones;traffic engineering computing;vehicle dynamics;GNSS data driven insurance telematics;Global Navigation Satellite System receivers;Rauch-Tung-Striebel smoother;circle fitting;real time value added services;risk assessment;smartphones;state-space model;usage based insurance programs;vehicle cornering events;vehicle dynamics;Force;Global Positioning System;Position measurement;Smart phones;Tires;Trajectory;Vehicles},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6958194&isnumber=6957655},
  Zone                     = {-}
}

@Article{Walker-1977-p494-497,
  Title                    = {Nickel-cadmium rechargeable batteries},
  Author                   = {Walker, W.D.C.},
  Journal                  = {Electronics and Power},
  Year                     = {1977},

  Month                    = {June},
  Number                   = {6},
  Pages                    = {494-497},
  Volume                   = {23},

  Abstract                 = {The versatility, convenience and freedom from maintenance of nickel-cadmium cells means that they are being relied on more and more for powering items from small domestic electrical appliances to industrial security systems. What do engineers need to know to design nickel-cadmium cells into these systems?},
  Doi                      = {10.1049/ep.1977.0261},
  File                     = {Published version:Walker-1977-p494-497.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0013-5127},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5184639&isnumber=5184637},
  Zone                     = {-}
}

@Article{Wan-2000-p1247-1254,
  Title                    = {Risk based voltage security assessment},
  Author                   = {Wan, H. and McCalley, J.D. and Vittal, V.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Nov},
  Number                   = {4},
  Pages                    = {1247-1254},
  Volume                   = {15},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {The objective of this paper is to provide a risk-based approach to security assessment for a voltage stability constrained power system. It is motivated by a perception that today's deterministic reliability criteria to voltage stability assessment often results in operating restrictions that are not commensurate with the corresponding level of risk. The risk calculation provided in this paper accounts for both the future uncertainties on the system and the consequences associated with voltage collapse and violation of limits. Although the main purpose of this paper is to provide a method of evaluation, they also give an introduction to show how this reliability “leading indicator” penetrates the traditional rigid reliability boundary and how it may be used to price reliability in order to make a trade off between reliability and economics},
  Doi                      = {10.1109/59.898097},
  File                     = {Published version:Wan-2000-p1247-1254.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {load (electric);power system dynamic stability;power system reliability;power system security;risk management;deterministic reliability criteria;limits violation;operating restrictions;power system voltage stability;reliability leading indicator;reliability pricing;risk calculation;risk-based voltage security assessment;uncertainties;voltage collapse;voltage stability assessment;Power generation economics;Power system economics;Power system reliability;Power system security;Power system stability;Reactive power;Safety;Stability criteria;Uncertainty;Voltage},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，举例 IEEE Reliability Test System，电网，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=898097&isnumber=19443},
  Zone                     = {2Q}
}

@InProceedings{Wang-2012-p1-4,
  Title                    = {Switch Function Model with the Effects of Commutation Failure for Converter},
  Author                   = {Chao Wang and Chongru Liu},
  Booktitle                = {Power and Energy Engineering Conference (APPEEC), 2012 Asia-Pacific},
  Year                     = {2012},
  Month                    = {March},
  Pages                    = {1-4},

  Doi                      = {10.1109/APPEEC.2012.6307281},
  File                     = {Published version:Wang-2012-p1-4.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2157-4839},
  Keywords                 = {HVDC power transmission;commutation;failure analysis;power convertors;DC system;commutation failure;converter;dynamic phasor model;normal operation;normal switch function;physical process;three-phase switch function;Bridge circuits;HVDC transmission;Inverters;Mathematical model;Power system dynamics;Switches;Valves},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6307281&isnumber=6306868},
  Zone                     = {-}
}

@InProceedings{Wang-2008-p1-6,
  Title                    = {Dynamic Security Risk Assessment and Optimization of Power Transmission System},
  Author                   = {Dongtao Wang and Yixin Yu},
  Booktitle                = {Probabilistic Methods Applied to Power Systems, 2008. PMAPS '08. Proceedings of the 10th International Conference on},
  Year                     = {2008},
  Month                    = {May},
  Pages                    = {1-6},

  Abstract                 = {The paper presents a practical dynamic security region (PDSR) based dynamic security risk assessment and optimization model for power transmission system. The cost of comprehensive security control and the influence of uncertainties of power injections are considered in the model of dynamic security risk assessment. The transient stability constraints and uncertainties of power injections can be considered easily by PDSR in form of hyper-box. A method to define and classify contingency set is presented, and a risk control optimization model is given which takes total dynamic insecurity risk as the objective function for a dominant contingency set. An optimal solution of dynamic insecurity risk is obtained by optimizing preventive and emergency control cost and contingency set decomposition. The effectiveness of this model has been proved by test results on the New England 10-generator 39-bus system.},
  File                     = {Published version:Wang-2008-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {power system security;power transmission;risk management;England;comprehensive security control;dynamic security risk assessment;power injections uncertainties;power transmission system;risk control optimization model;transient stability constraints;Cost function;Optimal control;Optimization methods;Power system modeling;Power system security;Power transmission;Risk management;Stability;System testing;Uncertainty},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4912612&isnumber=4912596},
  Zone                     = {-}
}

@InProceedings{Wang-2007-p1051-1054,
  Title                    = {Design and Implementation of a Missile Fault Diagnosis System Based on Fault-Tree Analysis},
  Author                   = {Fan Wang and Duo-Sheng Wu},
  Booktitle                = {Machine Learning and Cybernetics, 2007 International Conference on},
  Year                     = {2007},
  Month                    = {Aug},
  Pages                    = {1051-1054},
  Volume                   = {2},

  Abstract                 = {The paper briefly introduces the basic theory of fault tree analysis and rule-based expert system, and combines the fault tree analysis with rule-based expert system. Connecting fault tree with diagnosis expert system knowledge by cut set, we can express expert knowledge totally, systematically, and logically by building fault tree. It realizes the knowledge automatic acquisition and insure the consistency and integrality that rule-based diagnosis expert system knowledge be made automatic by the fault tree, By using program language Borland C++ Builder6.0 under Windows XP, applying the diagnostic method of artificial intelligence and the object-oriented programming design technology, we made the a missile fault diagnosis system, that has the good interface convenient for man-machine interacted and explain function.},
  Doi                      = {10.1109/ICMLC.2007.4370298},
  File                     = {Published version:Wang-2007-p1051-1054.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {C++ language;expert systems;fault diagnosis;fault trees;knowledge acquisition;missiles;object-oriented programming;user interfaces;Borland C++ Builder6.0;artificial intelligence;cut set;fault-tree analysis;knowledge automatic acquisition;man-machine interaction;missile fault diagnosis system;object-oriented programming;rule-based expert system;Artificial intelligence;Buildings;Diagnostic expert systems;Fault diagnosis;Fault trees;Joining processes;Man machine systems;Missiles;Object oriented programming;Windows;Expert System;Fault Diagnosis;Fault-tree Analysis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4370298&isnumber=4370218},
  Zone                     = {-}
}

@Article{Wang-1997-p2333-2343,
  Title                    = {Knowledge-based neural models for microwave design},
  Author                   = {Wang, Fang and Qi-Jun Zhang},
  Journal                  = {Microwave Theory and Techniques, IEEE Transactions on},
  Year                     = {1997},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {2333-2343},
  Volume                   = {45},

  Doi                      = {10.1109/22.643839},
  File                     = {Published version:Wang-1997-p2333-2343.pdf:PDF},
  Impactfactor             = {2.229},
  ISSN                     = {0018-9480},
  Keywords                 = {circuit CAD;circuit optimisation;microwave circuits;neural nets;waveguide components;active microwave components;internal structure;knowledge-based neural models;microwave design;microwave empirical information;model development cost;neural model;passive microwave components;semi-analytical information;training scheme;Costs;Design automation;Design optimization;Microstrip components;Microwave devices;Microwave theory and techniques;Neural networks;Physics;Signal design;Training data},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=643839&isnumber=14039},
  Zone                     = {2Q}
}

@Article{Wang-2015-p897-904,
  Title                    = {Stability analysis of an e-SEIAR model with point-to-group worm propagation },
  Author                   = {Fangwei Wang and Yunkai Zhang and Changguang Wang and Jianfeng Ma},
  Journal                  = {Communications in Nonlinear Science and Numerical Simulation },
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {897 - 904},
  Volume                   = {20},

  Abstract                 = {Abstract Internet worms have drawn significant attention due to their enormous threats to the Internet. The main goal of this paper is to explore the interaction dynamics between a malicious worm and an benign worm, using a mathematical model, namely e-SEIAR. The e-SEIAR model takes two important network environment factors into consideration: point-to-group worm propagation mode and benign worms. Furthermore, some related dynamics properties are studied, along with the analysis of how to combat the worm prevalence based on the stability of equilibria. Simulation results show that the performance of our proposed models is effective in combating such worms, in terms of decreasing the number of hosts infected by the malicious worm and reducing the malicious worm propagation speed. Based on our simulations, we believe there is great potential for an effective method to use benign worms to combat malicious worms in some point-to-group applications. },
  Doi                      = {http://dx.doi.org/10.1016/j.cnsns.2014.03.032},
  File                     = {Published version:Wang-2015-p897-904.pdf:PDF},
  Impactfactor             = {2.773},
  ISSN                     = {1007-5704},
  Keywords                 = {Internet worm},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1007570414001580},
  Zone                     = {1Q}
}

@Article{Wang-2014-p155-163,
  Title                    = {Modelling and simulating worm propagation in static and dynamic traffic},
  Author                   = {Jian Wang and Yanheng Liu and Kevin Deng},
  Journal                  = {Intelligent Transport Systems, IET},
  Year                     = {2014},

  Month                    = {March},
  Number                   = {2},
  Pages                    = {155-163},
  Volume                   = {8},

  Abstract                 = {Vehicular ad hoc networks (VANETs) have no fixed infrastructure and instead relies on the vehicles themselves to provide network functionality. An attack scenario with potentially catastrophic consequences is the outbreak of mobile worm epidemic in these networks. This paper analyses the snapshot spreading results under an urban scenario with equilibrium traffic through modelling the mobility pattern, the communication channel, the medium access control (MAC) mechanism and the worm propagation process. The extensive Monte Carlo simulations uncovered the effects of the transmission range (from a typical minimum to a maximum), the minimum velocity and the maximum velocity (from the free flow to the congested traffic), the vehicle density (from a sparse topology to a dense spatial relation) and the MAC mechanism (from presence to absence) on epidemic spreading of such worms in VANETs. Furthermore, the authors simulate the wireless worm propagation in dynamic traffic with the same scenario as the static traffic by using a network simulation tool. The authors discuss the correlation between snapshot results and evolutive outcome, also analyse the reasons resulting in the local differences and finally uncover the interrelations between the affected rate and network parameters. The results are expected to help engineers design intelligent and automatic detection prevention strategies for VANETs.},
  Doi                      = {10.1049/iet-its.2012.0070},
  File                     = {Published version:Wang-2014-p155-163.pdf:PDF},
  Impactfactor             = {0.959},
  ISSN                     = {1751-956X},
  Keywords                 = {Monte Carlo methods;authorisation;computer network security;traffic engineering computing;vehicular ad hoc networks;MAC mechanism;VANET;attack scenario;automatic detection prevention strategies;communication channel;dynamic traffic;epidemic spreading;equilibrium traffic;extensive Monte Carlo simulations;intelligent detection prevention strategies;medium access control mechanism;mobile worm epidemic;mobility pattern;network functionality;network parameters;network simulation tool;onboard computer infection;potentially catastrophic consequences;static traffic;vehicle density;vehicle to vehicle transmission;vehicular ad hoc networks;wired networks;wireless worm propagation;worm propagation modelling;worm propagation process;worm propagation simulation},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6766935&isnumber=6820886},
  Zone                     = {4Q}
}

@Article{Wang-2006-p2917-2933,
  Title                    = {Using attack graphs for correlating, hypothesizing, and predicting intrusion alerts },
  Author                   = {Lingyu Wang and Anyi Liu and Sushil Jajodia},
  Journal                  = {Computer Communications },
  Year                     = {2006},
  Note                     = {Computer Communications Internet Communications Security },
  Number                   = {15},
  Pages                    = {2917 - 2933},
  Volume                   = {29},

  Abstract                 = {To defend against multi-step intrusions in high-speed networks, efficient algorithms are needed to correlate isolated alerts into attack scenarios. Existing correlation methods usually employ an in-memory index for fast searches among received alerts. With finite memory, the index can only be built on a limited number of alerts inside a sliding window. Knowing this fact, an attacker can prevent two attack steps from both falling into the sliding window by either passively delaying the second step or actively injecting bogus alerts between the two steps. In either case, the correlation effort is defeated. In this paper, we first address the above issue with a novel queue graph (QG) approach. Instead of searching all the received alerts for those that prepare for a new alert, we only search for the latest alert of each type. The correlation between the new alert and other alerts is implicitly represented using the temporal order between alerts. Consequently, our approach can correlate alerts that are arbitrarily far away, and it has a linear (in the number of alert types) time complexity and quadratic memory requirement. Then, we extend the basic \{QG\} approach to a unified method to hypothesize missing alerts and to predict future alerts. Finally, we propose a compact representation for the result of alert correlation. Empirical results show that our method can fulfill correlation tasks faster than an \{IDS\} can report alerts. Hence, the method is a promising solution for administrators to monitor and predict the progress of intrusions and thus to take appropriate countermeasures in a timely manner. },
  Doi                      = {http://dx.doi.org/10.1016/j.comcom.2006.04.001},
  File                     = {Published version:Wang-2006-p2917-2933.pdf:PDF},
  Impactfactor             = {1.079},
  ISSN                     = {0140-3664},
  Keywords                 = {Intrusion detection},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S014036640600137X},
  Zone                     = {3Q}
}

@Article{Wang-2006-p46-59,
  Title                    = {Implementation of online security assessment},
  Author                   = {Wang, L. and Morison, K.},
  Journal                  = {Power and Energy Magazine, IEEE},
  Year                     = {2006},

  Month                    = {Sept},
  Number                   = {5},
  Pages                    = {46-59},
  Volume                   = {4},

  Abstract                 = {The implementation of online dynamic security assessment (DSA) systems is growing worldwide, and the deployment of this advanced technology is expected to improve the real-time security and, hence, the reliability of power systems. While not insignificant, the cost and efforts required to install online DSA tools are minor compared to the benefits of reducing the volume of offline studies required and, more importantly, the benefits of identifying and avoiding potential security problems in the systems to reduce the risk of blackouts. Based on practical experience, a process of DSA system integration is presented that can assist utilities and grid operators in addressing key issues during the specification, development, and installation of such tools. A number of successful online DSA projects are discussed to illustrate the viability and practicality of such applications, even for large, complex power systems. The penetration of online DSA tools is expected to continue to grow as operators seek timely and cost-effective approaches to enhance system performance. In the meantime, work is continuing on new methods of online analysis, advanced preventive and corrective control tools, and improved hardware architectures},
  Doi                      = {10.1109/MPAE.2006.1687817},
  File                     = {Published version:Wang-2006-p46-59.pdf:PDF},
  Impactfactor             = {1.582},
  ISSN                     = {1540-7977},
  Keywords                 = {power engineering computing;power grids;power system faults;power system reliability;power system security;blackout risk reduction;corrective control tools;cost-effective approaches;grid operators;hardware architectures;online dynamic security assessment;potential security problems;power system reliability;system performance enhancement;Customer service;Electric variables measurement;Frequency measurement;Maintenance;Power measurement;Power system measurements;Power system reliability;Power system security;Power system simulation;Time measurement},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1687817&isnumber=35600},
  Zone                     = {3Q}
}

@Article{Wang-2003-p551-554,
  Title                    = {The evaluation of manual FOUP handling in 300-mm wafer fab},
  Author                   = {Wang, M.-J.J. and Hsiu-Chen Chung and Hsin-Chieh Wu},
  Journal                  = {Semiconductor Manufacturing, IEEE Transactions on},
  Year                     = {2003},

  Month                    = {Aug},
  Number                   = {3},
  Pages                    = {551-554},
  Volume                   = {16},

  Abstract                 = {Semiconductor manufacturing industry is moving into the production of 300-mm wafers. To solve the increased workload problem in manual wafer handling, some personal guided vehicles (PGVs) have been developed to help in the transfer of front opening unified pods (FOUP). This study compares two kinds of PGVs with a traditional cart and evaluates the feasibility of using them for manual FOUP handling tasks. Manual FOUP handling capability was assessed. The results indicate that there is no obvious advantage in using any of the two evaluated PGV's over the manual cart. There is potential risk of causing musculoskeletal disorders for female operators to handle the 300-mm FOUP manually. Since the development of a fully automated intrabay FOUP handling system is a project of high technical difficulty, a combination of manual and automated handling is the current approach. To enhance the operator's health, safety and productivity, selection and training of operators, adequate design of handling tools and machine interface, assessment and balancing of workload are necessary.},
  Doi                      = {10.1109/TSM.2003.815207},
  File                     = {Published version:Wang-2003-p551-554.pdf:PDF},
  Impactfactor             = {0.862},
  ISSN                     = {0894-6507},
  Keywords                 = {materials handling;semiconductor device manufacture;300 mm;automated handling;cart;female operator;front opening unified pod;manual handling;musculoskeletal disorder;occupational health and safety;personal guided vehicle;productivity;semiconductor manufacturing;wafer fab;Industrial engineering;Injuries;Manuals;Manufacturing automation;Manufacturing industries;Manufacturing processes;Musculoskeletal system;Production;Transportation;Vehicles},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1219503&isnumber=27403},
  Zone                     = {4Q}
}

@Article{Wang-2015-p53-70,
  Title                    = {Petrogenesis of Dongguashan skarn-porphyry Cu-Au deposit related intrusion in the Tongling district, eastern China: Geochronological, mineralogical, geochemical and Hf isotopic evidence },
  Author                   = {Shi-Wei Wang and Tao-Fa Zhou and Feng Yuan and Yu Fan and Le-Jun Zhang and Yu-Long Song},
  Journal                  = {Ore Geology Reviews },
  Year                     = {2015},
  Number                   = {0},
  Pages                    = {53 - 70},
  Volume                   = {64},

  Abstract                 = {Abstract The Dongguashan skarn-porphyry Cu-Au deposit, located in the Tongling district of the Middle-Lower Yangtze River Valley metallogenic belt (MLYB), consists of skarn ore bodies in the upper part and porphyry ore bodies in the lower part, both of which are hosted in quartz diorite and quartz monzodiorite. Zircon U-Pb age and geochemical studies show that the quartz diorite of the Dongguashan intrusion formed at 140.3&#xa0;±&#xa0;2.0&#xa0;Ma (MSWD&#xa0;=&#xa0;0.19) and belongs to the high potassium calc-alkaline series. It is enriched in large ion lithophile elements (LILE) and light rare earth elements (LREE), depleted in high field-strength elements (HFSE) and heavy rare earth elements (HREE), and has a slightly negative Eu anomaly. 176Hf/177Hf values of the rims of zircons show a variable range (0.282087–0.282391), corresponding with calculated εHf(t) values of −&#xa0;10.72 to −&#xa0;21.46. Plagioclases in the quartz diorite have unbalanced structure characterized by bright andesine and labradorite (An&#xa0;=&#xa0;37.0–65.5) cores with higher contents of Fe and Sr and are corroded by dark oligoclase (An&#xa0;=&#xa0;13.8–27.6) rim. Major elements, trace elements, Hf isotope, and the composition of plagioclases indicate that the parental magma of the Dongguashan intrusion was produced by the mixing of underplating mafic magma and felsic magma formed by remelting of Paleoproterozoic and Neoarchean crustal rocks, Neoproterozoic crust may also provide some material to the felsic magma. Mafic magma played a key role and made the parental magma rich in water, sulfur, metals (Cu, Au) and gave it a high oxygen fugacity. During its magmatic evolution, the parental magma underwent fractional crystallization of hornblende, apatite, sphene and other mafic minerals. Some quartz diorite and quartz monzodiorite samples that show adakitic signatures, may result from injection of mafic magma. Some inherited zircons of the quartz diorite in the Dongguashan intrusion gave ages of 2.40–2.50&#xa0;Ga, 1.95–2.05&#xa0;Ga and 0.74–0.81&#xa0;Ga, coming from ultramafic, mafic and andesitic igneous rocks, and this indicates that there may have been three periods (2.4, 2.0, and 0.8&#xa0;Ga) of magmatic activity in the Tongling district. },
  Doi                      = {http://dx.doi.org/10.1016/j.oregeorev.2014.06.012},
  File                     = {Published version:Wang-2015-p53-70.pdf:PDF},
  Impactfactor             = {2.417},
  ISSN                     = {0169-1368},
  Keywords                 = {Zircon U-Pb dating},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0169136814001450},
  Zone                     = {2Q}
}

@Article{Wang-2013-p2194-2200,
  Title                    = {Research on Risk Control of Logistics Park Construction Project Based on Reliability Theory },
  Author                   = {Yuzhou Wang},
  Journal                  = {Procedia - Social and Behavioral Sciences },
  Year                     = {2013},
  Note                     = {Intelligent and Integrated Sustainable Multimodal Transportation Systems Proceedings from the 13th \{COTA\} International Conference of Transportation Professionals (CICTP2013) },
  Number                   = {0},
  Pages                    = {2194 - 2200},
  Volume                   = {96},

  Abstract                 = {Abstract Logistics park construction may face high risks for many uncertainty factors. Logistics park construction project risk control should not only take measures to avoid or reduce risks and improve the reliability of the project, but also take the most effective measures to control risks with a minimum total investment. Based on reliability theory, the paper constructs series- parallel model of risk control and reliability distribution model of logistics park construction project, and studies the validity of method empirically. },
  Doi                      = {http://dx.doi.org/10.1016/j.sbspro.2013.08.248},
  File                     = {Published version:Wang-2013-p2194-2200.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-0428},
  Keywords                 = {logistics park},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877042813023744},
  Zone                     = {-}
}

@Article{Wang-2006-p93-100,
  Title                    = {A latent class modeling approach to detect network intrusion },
  Author                   = {Yun Wang and Inyoung Kim and Gaston Mbateng and Shih-Yieh Ho},
  Journal                  = {Computer Communications },
  Year                     = {2006},
  Number                   = {1},
  Pages                    = {93 - 100},
  Volume                   = {30},

  Abstract                 = {This study presents a latent class modeling approach to examine network traffic data when labeled abnormal events are absent in training data, or such events are insufficient to fit a conventional regression model. Using six anomaly-associated risk factors identified from previous studies, the latent class model based on an unlabeled sample yielded acceptable classification results compared with a logistic regression model based on a labeled sample (correctly classified: 0.95 vs. 0.98, sensitivity: 0.99 vs. 0.99, and specificity: 0.77 vs. 0.97). The study demonstrates a great potency for using the latent class modeling technique to analyze network traffic data. },
  Doi                      = {http://dx.doi.org/10.1016/j.comcom.2006.07.018},
  File                     = {Published version:Wang-2006-p93-100.pdf:PDF},
  Impactfactor             = {1.079},
  ISSN                     = {0140-3664},
  Keywords                 = {Intrusion detection},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0140366406002891},
  Zone                     = {3Q}
}

@InProceedings{Wang-2009-p1-4,
  Title                    = {Fault Diagnosis Expert System Based on Integration of Fault-Tree and Neural Network},
  Author                   = {Yingying Wang and Li Li and Ming Chang and Hongwei Chen and Xiaoming Dong and Yueou Ren and Qiuju Li and Dan Liu},
  Booktitle                = {Computational Intelligence and Software Engineering, 2009. CiSE 2009. International Conference on},
  Year                     = {2009},
  Month                    = {Dec},
  Pages                    = {1-4},

  Abstract                 = {The traditional fault diagnosis expert system is dependent on knowledge acquisition of the experts. Knowledge acquisition is recognized as the "bottleneck" problem of expert system. In addition, there are also some limitations of adaptive capacity, learning ability and real-time. And artificial neural network with good fault-tolerance and associative memory function, as well as very strong self-adaptive, self-learning ability, just can make up for the limitations of traditional expert system. This paper will construct a new expert system with the artificial neural network into and fault tree. Besides fault tree and neural network, this article mainly introduces the system model of fault diagnosis of the fire control computer and sensor subsystem, the method and process of fault diagnosis. In this expert system, we use the object-oriented production rule to represent the knowledge, which solves the bottleneck problem of the diagnostic knowledge acquisition effectively. The inferential process begins with the abnormal event and finally finds all of the possible faults and the faulty component. For some possible faulty components, which have large number of fault samples, the neural network model can be used to diagnose. The training network of fault samples employs the BP neural network. Finally, simulation training results show that the fault diagnosis expert system based on the combination of fault tree and neural network is rational and effective in fault diagnosis of the fire control system, realizes perfectly the combination of new knowledge and old one, and can grasp the state of systems dynamically.},
  Doi                      = {10.1109/CISE.2009.5365615},
  File                     = {Published version:Wang-2009-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {expert systems;fault diagnosis;fault trees;knowledge acquisition;neural nets;object-oriented methods;artificial neural network;associative memory function;fault diagnosis expert system;fault tree;fire control computer;knowledge acquisition;object-oriented production rule;sensor subsystem;Artificial neural networks;Associative memory;Diagnostic expert systems;Fault diagnosis;Fault tolerant systems;Fault trees;Fires;Knowledge acquisition;Neural networks;Object oriented modeling},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5365615&isnumber=5362501},
  Zone                     = {-}
}

@InProceedings{Wang-2012-p1-5,
  Title                    = {Research on application and security protection of Internet of Things in Smart Grid},
  Author                   = {Wang, Y.F. and Lin, W.M. and Zhang, T. and Ma, Y.Y.},
  Booktitle                = {Information Science and Control Engineering 2012 (ICISCE 2012), IET International Conference on},
  Year                     = {2012},
  Month                    = {Dec},
  Pages                    = {1-5},

  Abstract                 = {The Internet of Things (IoTs) has been considered one of the very promising technologies for the implementation of Smart Grid. First, main features of information and communication systems in Smart Grid are abstracted; the applications of IoTs in Smart Grid are analyzed and classified based on the different function. Then, based on the characters of smart grid systems, reference architecture for Smart Grid IoTs is introduced. Second, main security issues and information security protection requirements of Smart Gird IoTs are well studied. Considering the function of the IoTs application, IoTs in smart grid are classified into three areas with different importance, including electrical industrial control IoTs; gird information collection and process IoTs, and IoTs for power consumer interaction. And classified protection framework of internet of things security is described. The proposed framework can guide the security deployment for Smart Grid IoTs applications. Finally, trusted secure access control system for IoTs terminals is designed for the security protection of Internet of Things in Smart Grid.},
  Doi                      = {10.1049/cp.2012.2311},
  File                     = {Published version:Wang-2012-p1-5.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Internet of Things;power engineering computing;security of data;smart power grids;Internet of Things;IoT;electrical industrial control;gird information collection;information security protection requirements;power consumer interaction;smart grid systems;Classified Protection;Industrial Control System;Information Security Risk;Internet of Things;Reference Architecture;Smart Grid},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6755690&isnumber=6755634},
  Zone                     = {-}
}

@InProceedings{Wardzinski-2008-p1-4,
  Title                    = {Dynamic risk assessment in autonomous vehicles motion planning},
  Author                   = {Wardzinski, A.},
  Booktitle                = {Information Technology, 2008. IT 2008. 1st International Conference on},
  Year                     = {2008},
  Month                    = {May},
  Pages                    = {1-4},

  Abstract                 = {Arguing that an autonomous mobile system is sufficiently safe to operate in presence of other vehicles and objects is an important element in development of such systems. Traditional approach to assure safety is to distinguish between safe and unsafe area and prevent the autonomous vehicle from entering the unsafe area. The paper presents a model of autonomous vehicle control system which uses risk assessment of the current and foreseen situations to plan its movement. The approach is discussed for two examples of simulation scenarios. The problem of risk assessment uncertainty and the need for cooperation between vehicles is addressed.},
  Doi                      = {10.1109/INFTECH.2008.4621607},
  File                     = {Published version:Wardzinski-2008-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {mobile robots;path planning;remotely operated vehicles;risk analysis;autonomous mobile robot;autonomous vehicle control system model;autonomous vehicle motion planning;dynamic risk assessment;Airports;Control system synthesis;Control systems;Kinematics;Mobile robots;Remotely operated vehicles;Risk management;Road safety;Vehicle dynamics;Vehicle safety},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4621607&isnumber=4621577},
  Zone                     = {-}
}

@InProceedings{Warschofsk-2010-p43-50,
  Title                    = {Transformation and Aggregation of Web Service Security Requirements},
  Author                   = {Warschofsky, R. and Menzel, M. and Meinel, C.},
  Booktitle                = {Web Services (ECOWS), 2010 IEEE 8th European Conference on},
  Year                     = {2010},
  Month                    = {Dec},
  Pages                    = {43-50},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/ECOWS.2010.13},
  File                     = {Published version:Warschofsk-2010-p43-50.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Web services;authorisation;formal specification;formal verification;service-oriented architecture;Web services;policy negotiation;security mechanism;security policy;security requirement;service-oriented architecture;Policy Generation;SOA Security;Service-oriented Architectures;WS-Security Policy},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5693243&isnumber=5693237},
  Zone                     = {-}
}

@InProceedings{Wei-2010-p15-22,
  Title                    = {Resilient industrial control system (RICS): Concepts, formulation, metrics, and insights},
  Author                   = {Dong Wei and Kun Ji},
  Booktitle                = {Resilient Control Systems (ISRCS), 2010 3rd International Symposium on},
  Year                     = {2010},
  Month                    = {Aug},
  Pages                    = {15-22},

  Abstract                 = {To monitor and control industrial machinery and processes, industrial control systems play an important role in daily life. Resilient control is a new topic of control technologies which studies how to maintain acceptable level of operation or service in face of undesirable incidents. This emerging technology, applied to industrial control systems, can provide a certain degree of protection for critical infrastructure, such as electric power generation, transmission and distribution, oil and gas production, water treatment. This paper proposes a 3-layer system model and resilience curve, discusses the concept and presents definition, properties and some insights of resilient industrial control systems. The metrics to estimate resilience quantitatively is disclosed. The general approaches to build, operate and improve a resilience industrial control system are proposed as well. Cyber attack resilient power grid automation system is discussed to illustrate the proposed approaches.},
  Doi                      = {10.1109/ISRCS.2010.5603480},
  File                     = {Published version:Wei-2010-p15-22.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {industrial control;machinery;power grids;power system control;power system security;cyber attack;industrial machinery control;power grid automation system;resilience curve;resilient industrial control system;Automation;Control systems;Degradation;Power grids;Process control;Resilience;Fault-tolerance;Flexibility;Industrial Control Systems;Reliability;Resilience;Robustness;Security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {老板说的风险曲线就是出自这篇文章},
  Timestamp                = {2015.04.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5603480&isnumber=5602019},
  Zone                     = {-}
}

@InProceedings{Wei-2010-p1-5,
  Title                    = {Research on Risk Assessment and Optimization Control of Information Systems Development: Based on Improved Fuzzy Comprehensive Evaluation and Dynamic Programming Algorithm},
  Author                   = {Guangfu Wei and Xin Zhang and Xinlan Zhang},
  Booktitle                = {Wireless Communications Networking and Mobile Computing (WiCOM), 2010 6th International Conference on},
  Year                     = {2010},
  Month                    = {Sept},
  Pages                    = {1-5},

  Abstract                 = {With the development of the information technology, risk control is becoming more and more important during the risk management process of Information System Development (ISD). The risk assessment and optimization control have become the key duty for the managers. At the same time, they are also the key factor to decide the success of ISD. The paper firstly builds a risk assessment model, and adopts improved fuzzy comprehensive evaluation and entropy weight coefficient to assess the influence of different risk factors during the process of ISD. Then we establish the risk optimal control model and use the dynamic programming algorithm to solve the problem. Finally, the risk assessment model and optimal control method are applied to solve a practical problem, the effectiveness is verified. This research provides a strong support for effective risk manage and optimal control of ISD, thus it will help a lot for the decision maker to manage risk and improve the success rate of ISD.},
  Doi                      = {10.1109/WICOM.2010.5601410},
  File                     = {Published version:Wei-2010-p1-5.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {dynamic programming;entropy;fuzzy set theory;information systems;optimal control;risk management;systems analysis;dynamic programming;entropy weight coefficient;fuzzy comprehensive evaluation;information systems development;information technology;optimization control;risk assessment;risk factor;risk management;risk optimal control;Dynamic programming;Entropy;Heuristic algorithms;Investments;Optimal control;Optimization;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5601410&isnumber=5600089},
  Zone                     = {-}
}

@InProceedings{Weiss-2008-p169-172,
  Title                    = {Selecting Security Patterns that Fulfill Security Requirements},
  Author                   = {Weiss, M. and Mouratidis, H.},
  Booktitle                = {International Requirements Engineering, 2008. RE '08. 16th IEEE},
  Year                     = {2008},
  Month                    = {Sept},
  Pages                    = {169-172},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/RE.2008.32},
  File                     = {Published version:Weiss-2008-p169-172.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1090-705X},
  Keywords                 = {formal specification;formal verification;security of data;systems analysis;security pattern selection;security requirement;Clouds;Computer industry;Computer security;Guidelines;Navigation;Pattern recognition;Search engines;Software systems;Systems engineering and theory;Visualization;Secure Tropos;Security Patterns;Security Requirements},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4685666&isnumber=4685636},
  Zone                     = {-}
}

@InProceedings{Wesley-2002-p7-3389,
  Title                    = {Dynamic risk assessment of CFD codes [aerospace design]},
  Author                   = {Wesley, L.P. and Childs, R.E.},
  Booktitle                = {Aerospace Conference Proceedings, 2002. IEEE},
  Year                     = {2002},
  Pages                    = {7-3369-7-3389 vol.7},
  Volume                   = {7},

  Abstract                 = {The results of an innovative effort to dynamically extract risk measures that can be factored into the knowledge-base closed-loop control of flow calculations is presented. Example control decisions include changing flow-solving parameters or restarting from previous checkpoints to reduce the need for manual intervention. One significance of the work is a formal, rigorous, yet practical KB means to dynamically control the execution of CFD codes by interpreting solution metrics within the context of larger project-related factors. Such factors include the desired solution fidelity, resource limitations such as budget and time, and related previously completed computational fluid dynamics (CFD) analyses. A measure of the risk associated with continuing, changing, or stopping flow calculations is developed in a manner that can be considered when deliberating if, how, and when to change flow calculations. We demonstrate the benefits of extracting and factoring in measures of risk when controlling the execution of CFD codes.},
  Doi                      = {10.1109/AERO.2002.1035315},
  File                     = {Published version:Wesley-2002-p7-3389.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {CAD;aerospace computing;closed loop systems;computational fluid dynamics;risk management;aerospace community;aerospace design;computational fluid dynamics;dynamic risk assessment;flow-solving parameters;knowledge-base closed-loop control;project-related factors;resource limitations;risk measures;solution fidelity;solution metrics;Aerodynamics;Computational fluid dynamics;Educational institutions;Fluid flow control;Fluid flow measurement;Humans;Knowledge engineering;Risk management;Software systems;Technological innovation},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1035315&isnumber=22224},
  Zone                     = {-}
}

@Article{Whist,
  Title                    = {Designing a risk-based surveillance program for Mycobacterium avium ssp. paratuberculosis in Norwegian dairy herds using multivariate statistical process control analysis},
  Author                   = {A.C. Whist and K.H. Liland and M.E. Jonsson and S. Sæbø and S. Sviland and O. Østerås and M. Norström and P. Hopp},
  Journal                  = {Journal of Dairy Science},
  Year                     = {2014},
  Number                   = {11},
  Pages                    = {6835 - 6849},
  Volume                   = {97},

  Abstract                 = {Abstract Surveillance programs for animal diseases are critical to early disease detection and risk estimation and to documenting a population’s disease status at a given time. The aim of this study was to describe a risk-based surveillance program for detecting Mycobacterium avium ssp. paratuberculosis (MAP) infection in Norwegian dairy cattle. The included risk factors for detecting \{MAP\} were purchase of cattle, combined cattle and goat farming, and location of the cattle farm in counties containing goats with MAP. The risk indicators included production data [culling of animals &gt;3&#xa0;yr of age, carcass conformation of animals &gt;3&#xa0;yr of age, milk production decrease in older lactating cows (lactations 3, 4, and 5)], and clinical data (diarrhea, enteritis, or both, in animals &gt;3&#xa0;yr of age). Except for combined cattle and goat farming and cattle farm location, all data were collected at the cow level and summarized at the herd level. Predefined risk factors and risk indicators were extracted from different national databases and combined in a multivariate statistical process control to obtain a risk assessment for each herd. The ordinary Hotelling’s \{T2\} statistic was applied as a multivariate, standardized measure of difference between the current observed state and the average state of the risk factors for a given herd. To make the analysis more robust and adapt it to the slowly developing nature of MAP, monthly risk calculations were based on data accumulated during a 24-mo period. Monitoring of these variables was performed to identify outliers that may indicate deviance in one or more of the underlying processes. The highest-ranked herds were scattered all over Norway and clustered in high-density dairy cattle farm areas. The resulting rankings of herds are being used in the national surveillance program for \{MAP\} in 2014 to increase the sensitivity of the ongoing surveillance program in which 5 fecal samples for bacteriological examination are collected from 25 dairy herds. The use of multivariate statistical process control for selection of herds will be beneficial when a diagnostic test suitable for mass screening is available and validated on the Norwegian cattle population, thus making it possible to increase the number of sampled herds. },
  Doi                      = {http://dx.doi.org/10.3168/jds.2013-6821},
  File                     = {Published version:Whist.pdf:PDF},
  Impactfactor             = {2.566},
  ISSN                     = {0022-0302},
  Keywords                 = {risk-based surveillance},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0022030214005876},
  Zone                     = {2Q}
}

@InProceedings{Williams-2009-p1-28,
  Title                    = {Taking care of business a novel process to account for failure functions},
  Author                   = {Williams, A.},
  Booktitle                = {SIL Determination, 2009 5th IET Seminar on},
  Year                     = {2009},
  Month                    = {Dec},
  Pages                    = {1-28},

  File                     = {Published version:Williams-2009-p1-28.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {aerospace safety;failure analysis;risk management;SIL;aerospace safety;failure function severity assessment;risk class;safety integrity level},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5393341&isnumber=5393328},
  Zone                     = {-}
}

@Article{Wilson-2000-p127-130,
  Title                    = {Problems with correlated data},
  Author                   = {Wilson, J.R. and Hunt, R.C.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2000},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {127-130},
  Volume                   = {49},

  Doi                      = {10.1109/24.877326},
  File                     = {Published version:Wilson-2000-p127-130.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Monte Carlo methods;probability;risk management;Monte Carlo methods;common data-distribution;correlated data;failure frequency overestimation;failure rate;probabilistic risk assessment;Data engineering;Databases;Engineering drawings;Environmental management;Fault trees;Mathematics;Monte Carlo methods;Risk management;Testing;US Department of Energy},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=877326&isnumber=18992},
  Zone                     = {2Q}
}

@Article{Worrall-2012-p32-44,
  Title                    = {A Context-Based Approach to Vehicle Behavior Prediction},
  Author                   = {Worrall, S. and Agamennoni, G. and Nieto, J. and Nebot, E.},
  Journal                  = {Intelligent Transportation Systems Magazine, IEEE},
  Year                     = {2012},

  Month                    = {Fall},
  Number                   = {3},
  Pages                    = {32-44},
  Volume                   = {4},

  Abstract                 = {Despite the best efforts of research and development carried out in the automotive industry, accidents continue to occur resulting in many deaths and injuries each year. It has been shown that the vast majority of accidents occur as a result (at least in part) of human error. This paper introduces the model for the Intelligent Systems for Risk Assessment (ISRA) project which has the goal of eliminating accidents by detecting risk, alerting the operators when appropriate, and ultimately removing some control of the vehicle from the operator when the risk is deemed unacceptable. The underlying premise is that vehicle dynamic information without contextual information is insufficient to understand the situation well enough to enable the analysis of risk. This paper defines the contextual information required to analyze the situation and shows how location context information can be derived using collected vehicle data. The process to infer high level vehicle state information using context information is also presented. The experimental results demonstrate the context based inference process using data collected from a fleet of mining vehicles during normal operation. The systems developed for the mining industry can later be extended to include more complex traffic scenarios that exist in the domain of ITS.},
  Doi                      = {10.1109/MITS.2012.2203230},
  File                     = {Published version:Worrall-2012-p32-44.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1939-1390},
  Keywords                 = {Accidents;Automotive engineering;Injuries;Intelligent vehicles;Research and development;Road transportation;Road vehicles},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6261615&isnumber=6261595},
  Zone                     = {-}
}

@Article{Worrall-2012-p32-44a,
  Title                    = {A Context-Based Approach to Vehicle Behavior Prediction},
  Author                   = {Worrall, S. and Agamennoni, G. and Nieto, J. and Nebot, E.},
  Journal                  = {Intelligent Transportation Systems Magazine, IEEE},
  Year                     = {2012},

  Month                    = {Fall},
  Number                   = {3},
  Pages                    = {32-44},
  Volume                   = {4},

  Abstract                 = {Despite the best efforts of research and development carried out in the automotive industry, accidents continue to occur resulting in many deaths and injuries each year. It has been shown that the vast majority of accidents occur as a result (at least in part) of human error. This paper introduces the model for the Intelligent Systems for Risk Assessment (ISRA) project which has the goal of eliminating accidents by detecting risk, alerting the operators when appropriate, and ultimately removing some control of the vehicle from the operator when the risk is deemed unacceptable. The underlying premise is that vehicle dynamic information without contextual information is insufficient to understand the situation well enough to enable the analysis of risk. This paper defines the contextual information required to analyze the situation and shows how location context information can be derived using collected vehicle data. The process to infer high level vehicle state information using context information is also presented. The experimental results demonstrate the context based inference process using data collected from a fleet of mining vehicles during normal operation. The systems developed for the mining industry can later be extended to include more complex traffic scenarios that exist in the domain of ITS.},
  Doi                      = {10.1109/MITS.2012.2203230},
  File                     = {Published version:Worrall-2012-p32-44a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1939-1390},
  Keywords                 = {Accidents;Automotive engineering;Injuries;Intelligent vehicles;Research and development;Road transportation;Road vehicles},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6261615&isnumber=6261595},
  Zone                     = {-}
}

@Article{Wu-2014-p1624-1632a,
  Title                    = {A Decision Support Approach for Accounts Receivable Risk Management},
  Author                   = {Wu, D.D. and Olson, D.L. and Cuicui Luo},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Dec},
  Number                   = {12},
  Pages                    = {1624-1632},
  Volume                   = {44},

  Abstract                 = {Financial disasters in private firms led to increased emphasis on various forms of risk management, to include market risk management, operational risk management, and credit risk management. Financial institutions are motivated by the need to meet increased regulatory requirements for risk measurement and capital reserves. This paper describes and demonstrates a model to support risk management of accounts receivable. We present a decision support model for a large bank enabling assessment of risk of default on the part of loan recipients. A credit scoring model is presented to assess account creditworthiness. Alternative methods of risk measurement for fault detection are compared, and a logistic regression model selected to analyze accounts receivable risk. Accuracy results of this model are presented, enabling accounts receivable managers to confidently apply statistical analysis through data mining to manage their risk.},
  Doi                      = {10.1109/TSMC.2014.2318020},
  File                     = {Published version:Wu-2014-p1624-1632a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {bank data processing;data mining;decision support systems;regression analysis;risk management;account receivable risk management;capital reserves;credit risk management;credit scoring model;creditworthiness;data mining;decision support model;fault detection;financial disasters;financial institutions;loan recipients;logistic regression model;market risk management;operational risk management;private firms;risk measurement;Data mining;Decision support systems;Neural networks;Risk management;Accounts receivable;data mining;decision support system;enterprise risk management;risk measures},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6861450&isnumber=6955885},
  Zone                     = {-}
}

@Article{Wu-2014-p1077-1087,
  Title                    = {A Decision Support Approach for Online Stock Forum Sentiment Analysis},
  Author                   = {Wu, D.D. and Lijuan Zheng and Olson, D.L.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Aug},
  Number                   = {8},
  Pages                    = {1077-1087},
  Volume                   = {44},

  Abstract                 = {The Internet provides the opportunity for investors to post online opinions that they share with fellow investors. Sentiment analysis of online opinion posts can facilitate both investors' investment decision making and stock companies' risk perception. This paper develops a novel sentiment ontology to conduct context-sensitive sentiment analysis of online opinion posts in stock markets. The methodology integrates popular sentiment analysis into machine learning approaches based on support vector machine and generalized autoregressive conditional heteroskedasticity modeling. A typical financial website called Sina Finance has been selected as an experimental platform where a corpus of financial review data was collected. Empirical results suggest solid correlations between stock price volatility trends and stock forum sentiment. Computational results show that the statistical machine learning approach has a higher classification accuracy than that of the semantic approach. Results also imply that investor sentiment has a particularly strong effect for value stocks relative to growth stocks.},
  Doi                      = {10.1109/TSMC.2013.2295353},
  File                     = {Published version:Wu-2014-p1077-1087.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Internet;Web sites;autoregressive processes;decision making;investment;learning (artificial intelligence);ontologies (artificial intelligence);pattern classification;statistical analysis;stock markets;support vector machines;text analysis;Internet;Sina Finance;classification accuracy;context-sensitive sentiment analysis;decision support approach;financial Website;financial review data;generalized autoregressive conditional heteroskedasticity modeling;growth stocks;investor investment decision making;investor sentiment;online opinion posts;online stock forum sentiment analysis;semantic approach;sentiment ontology;statistical machine learning approach;stock company risk perception;stock markets;stock price volatility trend;support vector machine;value stocks;Indexes;Internet;Predictive models;Stock markets;Support vector machines;Tagging;Training;Decision support;generalized autoregressive conditional heteroskedasticity (GARCH);sentiment analysis;stock price;support vector machine (SVM);volatility},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6705664&isnumber=6856252},
  Zone                     = {-}
}

@Article{Wu-2011-p548-555,
  Title                    = {A novel FBG-based security fence enabling to detect extremely weak intrusion signals from nonequivalent sensor nodes },
  Author                   = {Huijuan Wu and Yunjiang Rao and Cheng Tang and Yu Wu and Yuan Gong},
  Journal                  = {Sensors and Actuators A: Physical},
  Year                     = {2011},
  Note                     = {Solid-State Sensors, Actuators and Microsystems Workshop },
  Number                   = {2},
  Pages                    = {548 - 555},
  Volume                   = {167},

  Abstract                 = {An intrusion detection method is proposed based on the autocorrelation characteristics of the signal with and without disturbances in an FBG-based fiber-optic fence. This technique is very effective to detect the extremely weak intrusion signal while it need not critical equivalent noise power or the same sensitivity or performance for all of sensor nodes in a large sensing network. And it can be easily used for a long perimeter with several fences of different materials and structures or a large security system inspecting multiple remote separated areas. The experimental results show the probability of detection (PD) of the fiber fence using this method for the single and multiple event detection can be both higher than 99.5%. },
  Doi                      = {http://dx.doi.org/10.1016/j.sna.2011.02.046},
  File                     = {Published version:Wu-2011-p548-555.pdf:PDF},
  Impactfactor             = {1.841},
  ISSN                     = {0924-4247},
  Keywords                 = {Weak intrusion detection},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0924424711001191},
  Zone                     = {3Q}
}

@InProceedings{Wu-2012-p730-731,
  Title                    = {Cyber Attacks Prediction Model Based on Bayesian Network},
  Author                   = {Jinyu Wu and Lihua Yin and Yunchuan Guo},
  Booktitle                = {Parallel and Distributed Systems (ICPADS), 2012 IEEE 18th International Conference on},
  Year                     = {2012},
  Month                    = {Dec},
  Pages                    = {730-731},

  Abstract                 = {Cyber attacks prediction is an important part of risk management. Existing cyber attacks prediction methods did not fully consider the specific environment factors of the target network, which may make the results deviate from the true situation. In this paper, we propose a cyber attacks prediction model based on Bayesian network. We use attack graphs to represent all the vulnerabilities and possible attack paths. Then we capture the using environment factors using Bayesian network model. Cyber attacks predictions are performed on the constructed Bayesian network. Experimental analysis shows that our method gets more accurate results.},
  Doi                      = {10.1109/ICPADS.2012.117},
  File                     = {Published version:Wu-2012-p730-731.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1521-9097},
  Keywords                 = {belief networks;graph theory;risk management;security of data;Bayesian network;attack graphs;cyber attacks prediction model;environment factors;risk management;target network;Bayesian methods;Computational modeling;History;Predictive models;Security;Web servers;Bayesian network;attack graph;cyber attacks prediction;network security;quantitative assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6413615&isnumber=6413550},
  Zone                     = {-}
}

@InProceedings{Wu-2012-p730-731a,
  Title                    = {Cyber Attacks Prediction Model Based on Bayesian Network},
  Author                   = {Jinyu Wu and Lihua Yin and Yunchuan Guo},
  Booktitle                = {Parallel and Distributed Systems (ICPADS), 2012 IEEE 18th International Conference on},
  Year                     = {2012},
  Month                    = {Dec},
  Pages                    = {730-731},

  Doi                      = {10.1109/ICPADS.2012.117},
  File                     = {Published version:Wu-2012-p730-731a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1521-9097},
  Keywords                 = {belief networks;graph theory;risk management;security of data;Bayesian network;attack graphs;cyber attacks prediction model;environment factors;risk management;target network;Bayesian methods;Computational modeling;History;Predictive models;Security;Web servers;Bayesian network;attack graph;cyber attacks prediction;network security;quantitative assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6413615&isnumber=6413550},
  Zone                     = {-}
}

@Article{Wu-2011-p3433-3438,
  Title                    = {Intrusion Intention Identification Methods Based on Dynamic Bayesian Networks },
  Author                   = {Qingtao Wu and Ruijuan Zheng and Guanfeng Li and Juwei Zhang},
  Journal                  = {Procedia Engineering },
  Year                     = {2011},
  Note                     = {\{CEIS\} 2011 },
  Number                   = {0},
  Pages                    = {3433 - 3438},
  Volume                   = {15},

  Abstract                 = {It is difficult to detect the intention of an intruder, identify semantics of attacks and predict further attacks effectively using intrusion detection methods in the construction of high-level attack scene and disposal of sophisticated attack. An intrusion intention identification method based on dynamic Bayesian network is proposed for indeterminate problems that occur during sophisticated network attacks. This method applies dynamic Bayesian directed acyclic graphs to give real-time formulation of incidence among attack behaviors, intentions and attacks. It also applies probabilistic reasoning method to predict further attacks by an intruder. The result reflects varying histories of the intention of an intruder and demonstrates the effectiveness of the method. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2011.08.643},
  File                     = {Published version:Wu-2011-p3433-3438.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {dynamic Bayesian network},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705811021448},
  Zone                     = {-}
}

@Article{Wu-2011-p3433-3438a,
  Title                    = {Intrusion Intention Identification Methods Based on Dynamic Bayesian Networks },
  Author                   = {Qingtao Wu and Ruijuan Zheng and Guanfeng Li and Juwei Zhang},
  Journal                  = {Procedia Engineering },
  Year                     = {2011},
  Note                     = {\{CEIS\} 2011 },
  Number                   = {0},
  Pages                    = {3433 - 3438},
  Volume                   = {15},

  Abstract                 = {It is difficult to detect the intention of an intruder, identify semantics of attacks and predict further attacks effectively using intrusion detection methods in the construction of high-level attack scene and disposal of sophisticated attack. An intrusion intention identification method based on dynamic Bayesian network is proposed for indeterminate problems that occur during sophisticated network attacks. This method applies dynamic Bayesian directed acyclic graphs to give real-time formulation of incidence among attack behaviors, intentions and attacks. It also applies probabilistic reasoning method to predict further attacks by an intruder. The result reflects varying histories of the intention of an intruder and demonstrates the effectiveness of the method. },
  Doi                      = {http://dx.doi.org/10.1016/j.proeng.2011.08.643},
  File                     = {Published version:Wu-2011-p3433-3438a.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1877-7058},
  Keywords                 = {dynamic Bayesian network},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1877705811021448},
  Zone                     = {-}
}

@InProceedings{Wu-2011-p2516-2519,
  Title                    = {Study about environmental risk assessment and management of chemical industry park},
  Author                   = {Xiaobo Wu and Chaofeng Shao and Meiting Ju},
  Booktitle                = {Remote Sensing, Environment and Transportation Engineering (RSETE), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {June},
  Pages                    = {2516-2519},

  Abstract                 = {Chemical industrial park is the role driving force for China's social and economic development, but also an important region with lots of risk sources, belongs to the environmental pollution accidents high-risk areas. According to the characters of chemical industry, environmental risk identification was carried out in chemical industry parks, and the main risk factors were confirmed. Building on the experience of risk analysis in projects and insurance, the theory and system of regional environmental risk analysis has gradually evolved and matured. Taking the goals of environmental safety and requirements of environmental protection into account, the core research contents of environmental risk management were discussed, covering areas such as emergency response to the sudden environmental pollution accidents, Regional safety control planning, and risk sources layout optimization. The paper also highlights the key scientific problems, and ways to address them for carrying out environmental risk assessment and management of chemical industry park.},
  Doi                      = {10.1109/RSETE.2011.5964825},
  File                     = {Wu-2011-p2516-2519.pdf:Wu-2011-p2516-2519.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {chemical industry;environmental management;risk management;chemical industrial park;chemical industry park management;environmental pollution accident;environmental risk assessment;environmental risk identification;environmental risk management;environmental safety;regional environmental risk analysis;regional safety control planning;risk source layout optimization;Chemical industry;Environmentally friendly manufacturing techniques;Pollution;Presses;Risk management;Safety;chemical industry park;environmental risk assessment;risk management;risk sourc},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5964825&isnumber=5963913},
  Zone                     = {-}
}

@Article{Wu-2014-p33-42,
  Title                    = {A Tag Encoding Scheme against Pollution Attack to Linear Network Coding},
  Author                   = {Xiaohu Wu and Yinlong Xu and Chau Yuen and Liping Xiang},
  Journal                  = {Parallel and Distributed Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {33-42},
  Volume                   = {25},

  Abstract                 = {Network coding allows intermediate nodes to encode data packets to improve network throughput and robustness. However, it increases the propagation speed of polluted data packets if a malicious node injects fake data packets into the network, which degrades the bandwidth efficiency greatly and leads to incorrect decoding at sinks. In this paper, insights on new mathematical relations in linear network coding are presented and a key predistribution-based tag encoding scheme KEPTE is proposed, which enables all intermediate nodes and sinks to detect the correctness of the received data packets. Furthermore, the security of KEPTE with regard to pollution attack and tag pollution attack is quantitatively analyzed. The performance of KEPTE is competitive in terms of: low computational complexity; the ability that all intermediate nodes and sinks detect pollution attack; the ability that all intermediate nodes and sinks detect tag pollution attack; and high fault-tolerance ability. To the best of our knowledge, the existing key predistribution-based schemes aiming at pollution detection can only achieve at most three points as described above. Finally, discussions on the application of KEPTE to practical network coding are also presented.},
  Doi                      = {10.1109/TPDS.2013.24},
  File                     = {Published version:Wu-2014-p33-42.pdf:PDF},
  Impactfactor             = {1.796},
  ISSN                     = {1045-9219},
  Keywords                 = {computational complexity;data privacy;decoding;fault tolerance;linear codes;mathematical analysis;network coding;KEPTE;computational complexity;data packet encoding;decoding;fault-tolerance ability;key predistribution-based tag encoding scheme;linear network coding;malicious node injection;mathematical relation;security;tag pollution data packet attack;Educational institutions;Encoding;Equations;Network coding;Pollution;Security;Vectors;Byzantine attack;Network coding;compromised nodes;pollution attack;security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6463388&isnumber=6674937},
  Zone                     = {2Q}
}

@InProceedings{Wu-2010-p3552-3555,
  Title                    = {A Practical Approach for Risk Assessment of Data Leakage Prevention in Telecommunication Industry},
  Author                   = {Yumei Wu},
  Booktitle                = {E-Business and E-Government (ICEE), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {May},
  Pages                    = {3552-3555},

  Abstract                 = {Recently more incidents caused by improper data leakage prevention have occurred in many organizations which have been suffering a huge loss. The study of data leakage prevention is receiving considerable attention for reasons of privacy protection of personally identifiable information, preservation of an organization's brand and reputation, retention of intellectual capital. It is very challenging for senior management in telecommunication industry to identify, analyze and properly treat the data leakage risks due to the complexity and diversities of business process and supportive information systems. In this paper a practical method of risk assessment aimed to analyze and evaluate the risks of data leakage protection related to customer information in telecommunication company is presented, and the distribution view and transition view have been defined to describe and analyze the customer information which provides the basis for the risk assessment. A group of technical tests are proposed to find and verify the risky vulnerabilities which have been identified based on the method.},
  Doi                      = {10.1109/ICEE.2010.893},
  File                     = {Published version:Wu-2010-p3552-3555.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data privacy;risk management;telecommunication industry;customer information;data leakage prevention;information systems;intellectual capital retention;organization brand preservation;organization reputation preservation;personally identifiable information;privacy protection;risk assessment;telecommunication industry;Communications technology;Companies;ISO standards;Industries;Information security;Risk management;customer information view;data leakage prevention;penetration testing;risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5591461&isnumber=5590383},
  Zone                     = {-}
}

@InProceedings{Xiangwei-2008-p1-4,
  Title                    = {Research on the Technological Innovation Risk Management in Promoting Enterprise Sustainable Development},
  Author                   = {Wang Xiangwei},
  Booktitle                = {Wireless Communications, Networking and Mobile Computing, 2008. WiCOM '08. 4th International Conference on},
  Year                     = {2008},
  Month                    = {Oct},
  Pages                    = {1-4},

  Abstract                 = {The survival and sustainable development capability (SSDC) of enterprises can enable enterprises to adapt to the changes of environment in their process of formation and development. Recurring to a good managerial structure and system, the SSDC of enterprises can also enable enterprises to keep competitive advantages in a number of industries. Then, SSDC is the basic conditions for enterprises to keep continuing survive, growth and sustainable development. It is similar to the existence and development of the nature, the phenomenon of life cycle also exists for enterprises. Every enterprise has it's life cycle of come into being, maturation, aging, and death. In many countries, 40% of the new enterprises can not able to survive for 10 years. The problem of survival difficulty and short-time life for enterprises has attracted widespread concern. However, as long as the proper management and suitable environment for enterprises, their survival, longevity and sustainable development is possible. As an important content of enterprise management, enterprise risk management comes from the needs of enterprise and socio-economic development. Enterprise risk management helps enterprises to reduce or eliminate various types of risk, thus conducive for enterprises to get expected profits. While, as an important content of enterprise risk management, the enterprise technological innovation risk management enables enterprises to minimize the loss of technological innovation risk, and it can reduce or eliminate the damage to the entire economy, thus facilitating the sustainable development of enterprises. The technological innovation risk management capability of enterprise has become one of the key elements that can determine the survival and sustainable development of enterprises. The main task of the paper is to enable enterprise to become a sustainable development organization. Based on the analysis for the necessary of strengthening enterprise technological innovation risk - - management, the paper analyzes the technological innovation risks. It includes the impact factors of technological innovation risk and the characteristics of technological innovation risk. Furthermore, the paper uses qualitative and quantitative method to illustrate how to make risk identification and risk assessment.},
  Doi                      = {10.1109/WiCom.2008.2489},
  File                     = {Xiangwei-2008-p1-4.pdf:Xiangwei-2008-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {innovation management;risk management;socio-economic effects;sustainable development;technology management;enterprise risk management;life cycle;risk assessment;risk identification;socio-economic development;survival and sustainable development capability;technological innovation risk management;Aging;Content management;Environmental management;Industrial accidents;Innovation management;Risk analysis;Risk management;Sustainable development;Technological innovation;Technology management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4680678&isnumber=4677909},
  Zone                     = {-}
}

@Article{Xiao-2009-p78-85,
  Title                    = {Power System Risk Assessment and Control in a Multiobjective Framework},
  Author                   = {Fei Xiao and McCalley, J.D.},
  Journal                  = {Power Systems, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {Feb},
  Number                   = {1},
  Pages                    = {78-85},
  Volume                   = {24},

  __markedentry            = {[Qiqi:3]},
  Abstract                 = {Traditional online security assessment determines whether the system is secure or not, but how secure or insecure is not explicitly indicated. This paper develops probabilistic indices, risk, to assess real-time power system security level. Risk captures not only event likelihood, but also consequence. System security level associated with low voltage and overload can be optimally controlled, using the NSGA multiobjective optimization method. A security diagram is used to visualize operating conditions in a way that enables both risk-based and traditional deterministic views. An index for cascading overloads is used to evaluate the Pareto optimal solutions. This paper shows that the multiobjective approach results in less risky and less costly operating conditions, and it provides a practical algorithm for implementation. The IEEE 24-bus RTS-1996 system is analyzed to show that risk-based system security control results in lower risk, lower cost, and less exposure to cascading outages.},
  Doi                      = {10.1109/TPWRS.2008.2004823},
  File                     = {Published version:Xiao-2009-p78-85.pdf:PDF},
  Impactfactor             = {2.921},
  ISSN                     = {0885-8950},
  Keywords                 = {power system control;power system security;probability;multiobjective optimization;online security assessment;power system control;power system risk assessment;power system security;probabilistic indices;Decision making;evolutionary algorithm;nonlinear multiobjective optimization;reliability;risk;security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A， The IEEE 24-bus RTS-1996 system，仿真},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4682627&isnumber=4762160},
  Zone                     = {2Q}
}

@InProceedings{Xiaolin-2008-p1057-1061,
  Title                    = {A Markov Game Theory-Based Risk Assessment Model for Network Information System},
  Author                   = {Cui Xiaolin and Tan Xiaobin and Zhang Yong and Xi Hongsheng},
  Booktitle                = {Computer Science and Software Engineering, 2008 International Conference on},
  Year                     = {2008},
  Month                    = {Dec},
  Pages                    = {1057-1061},
  Volume                   = {3},

  Abstract                 = {Risk assessment is a very important tool to acquire a present and future security status of the network information system. Many risk assessment approaches consider the present system security status, while the future security status, which also has an impact on assessing the system risk, is not taken into consideration. In this paper we propose a novel risk assessment model based on Markov game theory. In this model, all of the possible risk in the future will impact on the present risk assessment. The farther away from now, the smaller impact on the risk assessment it has. After acquiring the system security status, we proposed an automatic generated reinforcement scheme which will provide a great convenience to the system administrator. A software tool is developed to demonstrate the performance of the risk assessment of a network information system and a simulation example shows the effectiveness of the proposed model.},
  Doi                      = {10.1109/CSSE.2008.949},
  File                     = {Published version:Xiaolin-2008-p1057-1061.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Internet;Markov processes;game theory;information systems;risk management;security of data;Internet;Markov game theory;automatic generated reinforcement scheme;network information system security;risk assessment model;software tool;system administrator;Automation;Computer science;Game theory;IEC standards;ISO standards;Information security;Information systems;National security;Risk management;Software engineering;Markov game theory;risk assessment;threat transmission},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4722524&isnumber=4722268},
  Zone                     = {-}
}

@Article{Xiaopeng-2014-p7-12,
  Title                    = {A hybrid attack on ‘double images encryption method with resistance against the specific attack based on an asymmetric algorithm’ },
  Author                   = {Deng Xiaopeng},
  Journal                  = {Optics Communications },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {7 - 12},
  Volume                   = {317},

  Abstract                 = {Abstract A hybrid attack method, which is based on an iteration process and the decisive role of phase in the propagation process of light, is proposed to break the double image encryption system based on a nonlinear algorithm. The whole attack process contains three steps. First, an approximate value of the encoded image is achieved by using the phase retrieval algorithm when the encryption key, placed in the Fourier spectrum plane, and the ciphertext are used as the two constraints. Then, an approximate value of the joint power spectrum (JPS) is obtained based on the result of the first step. Finally, two approximate values of the original images are obtained by the use of the approximate value of the \{JPS\} and other two encryption keys. The simulation results show that the hybrid attack is valid and the cryptosystem is vulnerable to this attack. },
  Doi                      = {http://dx.doi.org/10.1016/j.optcom.2013.11.055},
  File                     = {Published version:Xiaopeng-2014-p7-12.pdf:PDF},
  Impactfactor             = {1.438},
  ISSN                     = {0030-4018},
  Keywords                 = {Optical information processing},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0030401813011334},
  Zone                     = {3Q}
}

@InProceedings{Xiaoqian-2013-p102-107,
  Title                    = {Correlative study on TCM syndromes of congestive heart failure, cardiac function and brain natriuretic peptide},
  Author                   = {Li Xiaoqian and He Jiancheng and Fu Deyu and Shen Lin and Hu Dongpei and Cao Xuebin},
  Booktitle                = {Bioinformatics and Biomedicine (BIBM), 2013 IEEE International Conference on},
  Year                     = {2013},
  Month                    = {Dec},
  Pages                    = {102-107},

  Doi                      = {10.1109/BIBM.2013.6732648},
  File                     = {Published version:Xiaoqian-2013-p102-107.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {brain;cardiology;haemodynamics;medical disorders;neurophysiology;patient diagnosis;CHF syndromes;Qi-stagnation-blood-stasis;TCM syndromes;brain natriuretic peptide;cardiac function;cardiac function level;cardiac functional grading;congestive heart failure;diagnostic criteria;heart blood stasis;phlegm blocking heart vessel syndromes;phlegm-stagnation;qi-deficiency-blood-stasis;qi-yin deficiency;yang-deficiency-water-flooding syndrome;Blood;Correlation;Heart;Hospitals;Medical diagnostic imaging;Peptides;Standardization;Brain Natriuretic Peptide;Cardiac Function;Congestive Heart Failure;TCM Syndromes},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6732648&isnumber=6732442},
  Zone                     = {-}
}

@InProceedings{Xie-2010-p211-220,
  Title                    = {Using Bayesian networks for cyber security analysis},
  Author                   = {Peng Xie and Li, Jason H. and Xinming Ou and Peng Liu and Levy, R.},
  Booktitle                = {Dependable Systems and Networks (DSN), 2010 IEEE/IFIP International Conference on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {211-220},

  Abstract                 = {Capturing the uncertain aspects in cyber security is important for security analysis in enterprise networks. However, there has been insufficient effort in studying what modeling approaches correctly capture such uncertainty, and how to construct the models to make them useful in practice. In this paper, we present our work on justifying uncertainty modeling for cyber security, and initial evidence indicating that it is a useful approach. Our work is centered around near real-time security analysis such as intrusion response. We need to know what is really happening, the scope and severity level, possible consequences, and potential countermeasures. We report our current efforts on identifying the important types of uncertainty and on using Bayesian networks to capture them for enhanced security analysis. We build an example Bayesian network based on a current security graph model, justify our modeling approach through attack semantics and experimental study, and show that the resulting Bayesian network is not sensitive to parameter perturbation.},
  Doi                      = {10.1109/DSN.2010.5544924},
  File                     = {Published version:Xie-2010-p211-220.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {belief networks;business communication;computer network security;uncertainty handling;Bayesian networks;cyber security analysis;enterprise networks;intrusion response;parameter perturbation;uncertainty modeling;Access protocols;Bayesian methods;Computer security;File servers;Graphical models;Intrusion detection;USA Councils;Uncertainty;Web server;Workstations},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5544924&isnumber=5544245},
  Zone                     = {-}
}

@Article{Xiong-2001-p282-293,
  Title                    = {On the defense of the distributed denial of service attacks: an on-off feedback control approach},
  Author                   = {Yong Xiong and Liu, S. and Sun, P.},
  Journal                  = {Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {Jul},
  Number                   = {4},
  Pages                    = {282-293},
  Volume                   = {31},

  Abstract                 = {Proposes a coordinated defense scheme of distributed denial of service (DDoS) network attacks, based on the backward-propagation, on-off control strategy. When a DDoS attack is in effect, a high concentration of malicious packet streams are routed to the victim in a short time, making it a hot spot. A similar problem has been observed in multiprocessor systems, where a hot spot is formed when a large number of processors access simultaneously shared variables in the same memory module. Despite the similar terminologies used here, solutions for multiprocessor hot spot problems cannot be applied to that in the Internet, because the hot traffic in DDoS may only represent a small fraction of the Internet traffic, and the attack strategies on the Internet are far more sophisticated than that in the multiprocessor systems. The performance impact on the hot spot is related to the total hot packet rate that can be tolerated by the victim. We present a backward pressure propagation, feedback control scheme to defend DDoS attacks. We use a generic network model to analyze the dynamics of network traffic, and develop the algorithms for rate-based and queue-length-based feedback control. We show a simple design to implement our control scheme on a practical switch queue architecture},
  Doi                      = {10.1109/3468.935045},
  File                     = {Published version:Xiong-2001-p282-293.pdf:PDF},
  Impactfactor             = {2.183},
  ISSN                     = {1083-4427},
  Keywords                 = {Internet;asynchronous transfer mode;feedback;on-off control;queueing theory;telecommunication congestion control;telecommunication network routing;telecommunication security;telecommunication traffic;variable structure systems;Internet traffic;backward pressure propagation feedback control scheme;coordinated defense scheme;distributed denial of service attacks;malicious packet streams;network traffic;on-off feedback control approach;performance impact;queue-length-based feedback control;rate-based feedback control;switch queue architecture;Algorithm design and analysis;Communication system traffic control;Computer crime;Feedback control;Internet;Multiprocessing systems;Queueing analysis;Switches;Terminology;Traffic control},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=935045&isnumber=20237},
  Zone                     = {2Q}
}

@InProceedings{Xu-2006-p79-82,
  Title                    = {An Aspect-Oriented Approach to Security Requirements Analysis},
  Author                   = {Dianxiang Xu and Goel, V. and Nygard, K.},
  Booktitle                = {Computer Software and Applications Conference, 2006. COMPSAC '06. 30th Annual International},
  Year                     = {2006},
  Month                    = {Sept},
  Pages                    = {79-82},
  Volume                   = {2},

  Abstract                 = {This paper presents an aspect-oriented approach to integrated elicitation of functional and security requirements based on use case-driven development. We identify security threats with respect to use cases and adopt threat mitigations for preventing or reducing security threats. To capture crosscutting nature of threats and mitigations, we specify them as aspects that encapsulate pointcuts and advice. A threat (mitigation) pointcut is a collection of join points in use cases at which the use cases are threatened (secured); whereas threat/mitigation advice describes how a threat can become an attack (can be mitigated). Eliciting threats and mitigations as aspects provides a structured way for separating functional and security concerns},
  Doi                      = {10.1109/COMPSAC.2006.109},
  File                     = {Published version:Xu-2006-p79-82.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0730-3157},
  Keywords                 = {formal specification;object-oriented programming;security of data;aspect-oriented approach;aspect-oriented software development;functional requirements;security requirement analysis;security requirements;security threats;threat mitigations;threat pointcut;use case-driven development;Application software;Computer applications;Computer science;Computer security;Neodymium;Programming;Security requirements;aspect-oriented;mitigation.;software development;threats;use case},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4020145&isnumber=4020118},
  Zone                     = {-}
}

@InProceedings{Xu-2007-p532-534,
  Title                    = {A CAI Tool for the Theory of Relation Normalization},
  Author                   = {Fangjie Xu and Huichuan Duan},
  Booktitle                = {Information Technologies and Applications in Education, 2007. ISITAE '07. First IEEE International Symposium on},
  Year                     = {2007},
  Month                    = {Nov},
  Pages                    = {532-534},

  Abstract                 = {Teaching and learning the theory of relation normalization in a database course is a nontrivial work. Although this theory and the related concepts and algorithms have been clearly stated in authoritative texts, suitable CAIs have not been found to our best knowledge. This paper reports a CAI tool, RDBNorm, the authors designed and implemented to facilitate both teachers' classroom teaching and students' home practicing of the theory of relation normalization. RDBNorm demonstrates relation normalization associated algorithms step by step to make them easy to understand, as well as a theory tutorial presenting the key points of the theory.},
  Doi                      = {10.1109/ISITAE.2007.4409342},
  File                     = {Published version:Xu-2007-p532-534.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer aided instruction;computer science education;educational courses;relational databases;teaching;CAI tool;classroom teaching;relation normalization associated algorithm;relation normalization theory;relational database course;Books;Computer aided instruction;Computer science;Data engineering;Database systems;Education;Information science;Manuals;Noise measurement;Relational databases;CAI tool;closure of attributes set;function dependency;normal form;theory of relation normalization},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4409342&isnumber=4409225},
  Zone                     = {-}
}

@InProceedings{Xu-2004-p214-219,
  Title                    = {Combining dynamic fault trees and event trees for probabilistic risk assessment},
  Author                   = {Hong Xu and Bechta Dugan, J.},
  Booktitle                = {Reliability and Maintainability, 2004 Annual Symposium - RAMS},
  Year                     = {2004},
  Month                    = {Jan},
  Pages                    = {214-219},

  Abstract                 = {As system analysis methodologies, both event tree analysis (ETA) and fault tree analysis (FTA) are used in probabilistic risk assessment (PRA), especially in identifying system interrelationships due to shared events. Although there are differences between them, ETA and FTA, are so closely linked that fault trees (FT) are often used to quantify system events that are part of event tree (ET) sequences (J.D. Andrew et al., 2000). The logical processes employed to evaluate ET sequences and quantify the consequences are the same as those used in FTA. Although much work has been done to combine FT and ET, traditional methods only concentrate on combining static fault trees (SFT) and ET. Our main concern is considering how to combine dynamic fault trees (DFT) and ET. We proposed a reasonable approach in this paper, which is illustrated through a hypothetical example. Because of the complexity of dynamic systems, including the huge size and complicated dependencies, there may exist contradictions among different dynamic subsystems. The key benefit of our approach is that we avoid the generation of such contradictions in our model. Another benefit is that efficiency may be improved through modularization.},
  Doi                      = {10.1109/RAMS.2004.1285450},
  File                     = {Published version:Xu-2004-p214-219.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Markov processes;fault trees;probability;risk management;Markov chain;dynamic fault trees analysis;dynamic subsystems;dynamic systems complexity;event tree analysis;event trees;probabilistic risk assessment;system analysis methodologies;Event detection;Fault trees;Hydrogen;Logic;Risk analysis;Risk management;Time of arrival estimation;Tree graphs;US Department of Transportation;Valves},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1285450&isnumber=28680},
  Zone                     = {-}
}

@InProceedings{Xu-2004-p214-219a,
  Title                    = {Combining dynamic fault trees and event trees for probabilistic risk assessment},
  Author                   = {Hong Xu and Bechta Dugan, J.},
  Booktitle                = {Reliability and Maintainability, 2004 Annual Symposium - RAMS},
  Year                     = {2004},
  Month                    = {Jan},
  Pages                    = {214-219},

  Abstract                 = {As system analysis methodologies, both event tree analysis (ETA) and fault tree analysis (FTA) are used in probabilistic risk assessment (PRA), especially in identifying system interrelationships due to shared events. Although there are differences between them, ETA and FTA, are so closely linked that fault trees (FT) are often used to quantify system events that are part of event tree (ET) sequences (J.D. Andrew et al., 2000). The logical processes employed to evaluate ET sequences and quantify the consequences are the same as those used in FTA. Although much work has been done to combine FT and ET, traditional methods only concentrate on combining static fault trees (SFT) and ET. Our main concern is considering how to combine dynamic fault trees (DFT) and ET. We proposed a reasonable approach in this paper, which is illustrated through a hypothetical example. Because of the complexity of dynamic systems, including the huge size and complicated dependencies, there may exist contradictions among different dynamic subsystems. The key benefit of our approach is that we avoid the generation of such contradictions in our model. Another benefit is that efficiency may be improved through modularization.},
  Doi                      = {10.1109/RAMS.2004.1285450},
  File                     = {Published version:Xu-2004-p214-219a.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Markov processes;fault trees;probability;risk management;Markov chain;dynamic fault trees analysis;dynamic subsystems;dynamic systems complexity;event tree analysis;event trees;probabilistic risk assessment;system analysis methodologies;Event detection;Fault trees;Hydrogen;Logic;Risk analysis;Risk management;Time of arrival estimation;Tree graphs;US Department of Transportation;Valves},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，故障树和事件树联合使用},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1285450&isnumber=28680},
  Zone                     = {-}
}

@Article{Xu-2014-p59-67,
  Title                    = {Financial ratio selection for business failure prediction using soft set theory },
  Author                   = {Wei Xu and Zhi Xiao and Xin Dang and Daoli Yang and Xianglei Yang},
  Journal                  = {Knowledge-Based Systems },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {59 - 67},
  Volume                   = {63},

  Abstract                 = {Abstract This paper presents a novel parameter reduction method guided by soft set theory (NSS) to select financial ratios for business failure prediction (BFP). The proposed method integrates statistical logistic regression into soft set decision theory, hence takes advantages of two approaches. The procedure is applied to real data sets from Chinese listed firms. From the financial analysis statement category set and the financial ratio set considered by the previous literatures, our proposed method selects nine significant financial ratios. Among them, four ratios are newly recognized as important variables for BFP. For comparison, principal component analysis, traditional soft set theory, and rough set theory are reduction methods included in the study. The predictive ability of the selected ratios by each reduction method along with the ratios commonly used in the prior literature is evaluated by three forecasting tools support vector machine, neural network, and logistic regression. The results demonstrate superior forecasting performance of the proposed method in terms of accuracy and stability. },
  Doi                      = {http://dx.doi.org/10.1016/j.knosys.2014.03.007},
  File                     = {Published version:Xu-2014-p59-67.pdf:PDF},
  Impactfactor             = {4.104},
  ISSN                     = {0950-7051},
  Keywords                 = {Business failure prediction},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950705114000902},
  Zone                     = {2Q}
}

@InProceedings{Xu-2010-p1-7,
  Title                    = {Earlier detection of risk of blackout by real-time dynamic security assessment based on Extreme Learning Machines},
  Author                   = {Xu, Y. and Dong, Z.Y. and Meng, K. and Xu, Z. and Zhang, R. and Wu, A.Y. and Wong, K.P.},
  Booktitle                = {Power System Technology (POWERCON), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {Oct},
  Pages                    = {1-7},

  Abstract                 = {The lack of real-time tools capable of detecting risk of blackouts is one of the contribution factors to the recent large blackouts occurred around the world. In terms of dynamic security assessment (DSA), artificial intelligence and data mining techniques have been widely applied to facilitate very fast DSA for enhanced situational awareness of insecurity. However, many of the current state-of-the-art models usually suffer from excessive training time and complex parameters tuning problems, leading to their inefficiency for real-time implementation. In this paper, a new DSA method using Extreme Learning Machine (ELM) is proposed, which has significantly improved the learning speed and can therefore provide earlier detection of the risk of blackout. The proposed method is examined on the New England 39-bus test system, and compared with other state-of-the-art methods in terms of computation time and accuracy. The simulation results show that the ELM-based DSA method possesses superior computation speed and acceptably high accuracy.},
  Doi                      = {10.1109/POWERCON.2010.5666055},
  File                     = {Published version:Xu-2010-p1-7.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {artificial intelligence;data mining;power system reliability;power system security;real-time systems;risk management;New England 39-bus test system;artificial intelligence;blackout detection;data mining;dynamic security assessment;extreme learning machines;real time tools;risk detection;Accuracy;Robustness;Support vector machines;Valves;blackout prevention;dynamic security assessment;extreme learning machine (ELM);intelligent system},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5666055&isnumber=5666013},
  Zone                     = {-}
}

@Article{Xu-2011-p314-322,
  Title                    = {Real-time transient stability assessment model using extreme learning machine},
  Author                   = {Xu, Y. and Dong, Z.Y. and Meng, K. and Zhang, R. and Wong, K.P.},
  Journal                  = {Generation, Transmission Distribution, IET},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {314-322},
  Volume                   = {5},

  Abstract                 = {In recent years, computational intelligence and machine learning techniques have gained popularity to facilitate very fast dynamic security assessment for earlier detection of the risk of blackouts. However, many of the current state-of-the-art models usually suffer from excessive training time and complex parameters tuning problems, leading to inefficiency for real-time implementation and on-line model updating. In this study, a new transient stability assessment model using the increasingly prevalent extreme learning machine theory is developed. It has significantly improved the learning speed and can enable effective on-line updating. The proposed model is examined on the New England 39-bus test system, and compared with some state-of-the-art methods in terms of computation time and prediction accuracy. The simulation results show that the proposed model possesses significant superior computation speed and competitively high accuracy.},
  Doi                      = {10.1049/iet-gtd.2010.0355},
  File                     = {Published version:Xu-2011-p314-322.pdf:PDF},
  Impactfactor             = {1.414},
  ISSN                     = {1751-8687},
  Keywords                 = {learning (artificial intelligence);power engineering computing;power system transient stability;39-bus test system;England;computational intelligence;extreme learning machine;fast dynamic security assessment;machine learning techniques;parameters tuning problems;power systems;real-time transient stability assessment model},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5714771&isnumber=5714767},
  Zone                     = {3Q}
}

@Article{Xu-2011-p314-322a,
  Title                    = {Real-time transient stability assessment model using extreme learning machine},
  Author                   = {Xu, Y. and Dong, Z.Y. and Meng, K. and Zhang, R. and Wong, K.P.},
  Journal                  = {Generation, Transmission Distribution, IET},
  Year                     = {2011},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {314-322},
  Volume                   = {5},

  Abstract                 = {In recent years, computational intelligence and machine learning techniques have gained popularity to facilitate very fast dynamic security assessment for earlier detection of the risk of blackouts. However, many of the current state-of-the-art models usually suffer from excessive training time and complex parameters tuning problems, leading to inefficiency for real-time implementation and on-line model updating. In this study, a new transient stability assessment model using the increasingly prevalent extreme learning machine theory is developed. It has significantly improved the learning speed and can enable effective on-line updating. The proposed model is examined on the New England 39-bus test system, and compared with some state-of-the-art methods in terms of computation time and prediction accuracy. The simulation results show that the proposed model possesses significant superior computation speed and competitively high accuracy.},
  Doi                      = {10.1049/iet-gtd.2010.0355},
  File                     = {Published version:Xu-2011-p314-322a.pdf:PDF},
  Impactfactor             = {1.414},
  ISSN                     = {1751-8687},
  Keywords                 = {learning (artificial intelligence);power engineering computing;power system transient stability;39-bus test system;England;computational intelligence;extreme learning machine;fast dynamic security assessment;machine learning techniques;parameters tuning problems;power systems;real-time transient stability assessment model},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5714771&isnumber=5714767},
  Zone                     = {3Q}
}

@Article{Yacoub-2002-p529-547,
  Title                    = {A methodology for architecture-level reliability risk analysis},
  Author                   = {Yacoub, S.M. and Ammar, H.H.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {Jun},
  Number                   = {6},
  Pages                    = {529-547},
  Volume                   = {28},

  Abstract                 = {The paper presents a methodology for reliability risk assessment at the early stages of the development lifecycle, namely, the architecture level. We describe a heuristic risk assessment methodology that is based on dynamic metrics. The methodology uses dynamic complexity and dynamic coupling metrics to define complexity factors for the architecture elements (components and connectors). Severity analysis is performed using Failure Mode and Effect Analysis (FMEA) as applied to architecture models. We combine severity and complexity factors to develop heuristic risk factors for the architecture components and connectors. Based on analysis scenarios, we develop a risk assessment model that represents components, connectors, component risk factors, connector risk factors, and probabilities of component interactions. We also develop a risk analysis algorithm that aggregates risk factors of components and connectors to the architectural level. Using the risk aggregation and the risk analysis model, we show how to analyze the overall risk factor of the architecture as the function of the risk factors of its constituting components and connectors. A case study of a pacemaker architecture is used to illustrate the application of the methodology. The methodology is used to identify critical components and connectors and to investigate the sensitivity of the architecture risk factor to changes in the heuristic risk factors of the architecture elements},
  Doi                      = {10.1109/TSE.2002.1010058},
  File                     = {Published version:Yacoub-2002-p529-547.pdf:PDF},
  Impactfactor             = {2.588},
  ISSN                     = {0098-5589},
  Keywords                 = {risk management;software architecture;software metrics;software reliability;FMEA;Failure Mode and Effect Analysis;analysis scenarios;architectural level;architecture components;architecture elements;architecture level;architecture models;architecture risk factor;architecture-level reliability risk analysis methodology;complexity factors;component interactions;component risk factors;connector risk factors;development lifecycle;domain experts;dynamic complexity;dynamic coupling metrics;dynamic metrics;heuristic risk assessment methodology;heuristic risk factors;pacemaker architecture;product attributes;product metrics;reliability risk analysis;reliability risk assessment;risk aggregation;risk assessment model;risk factors;severity analysis;severity factors;software architecture;software risk management plan;subjective judgement;subjective risk assessment techniques;Computer Society;Computer architecture;Computer errors;Connectors;Failure analysis;Frequency;Humans;Performance analysis;Risk analysis;Risk management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1010058&isnumber=21774},
  Zone                     = {1Q}
}

@Article{Yacoub-2002-p529-547a,
  Title                    = {A methodology for architecture-level reliability risk analysis},
  Author                   = {Yacoub, S.M. and Ammar, H.H.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {Jun},
  Number                   = {6},
  Pages                    = {529-547},
  Volume                   = {28},

  Abstract                 = {The paper presents a methodology for reliability risk assessment at the early stages of the development lifecycle, namely, the architecture level. We describe a heuristic risk assessment methodology that is based on dynamic metrics. The methodology uses dynamic complexity and dynamic coupling metrics to define complexity factors for the architecture elements (components and connectors). Severity analysis is performed using Failure Mode and Effect Analysis (FMEA) as applied to architecture models. We combine severity and complexity factors to develop heuristic risk factors for the architecture components and connectors. Based on analysis scenarios, we develop a risk assessment model that represents components, connectors, component risk factors, connector risk factors, and probabilities of component interactions. We also develop a risk analysis algorithm that aggregates risk factors of components and connectors to the architectural level. Using the risk aggregation and the risk analysis model, we show how to analyze the overall risk factor of the architecture as the function of the risk factors of its constituting components and connectors. A case study of a pacemaker architecture is used to illustrate the application of the methodology. The methodology is used to identify critical components and connectors and to investigate the sensitivity of the architecture risk factor to changes in the heuristic risk factors of the architecture elements},
  Doi                      = {10.1109/TSE.2002.1010058},
  File                     = {Published version:Yacoub-2002-p529-547a.pdf:PDF},
  Impactfactor             = {2.588},
  ISSN                     = {0098-5589},
  Keywords                 = {risk management;software architecture;software metrics;software reliability;FMEA;Failure Mode and Effect Analysis;analysis scenarios;architectural level;architecture components;architecture elements;architecture level;architecture models;architecture risk factor;architecture-level reliability risk analysis methodology;complexity factors;component interactions;component risk factors;connector risk factors;development lifecycle;domain experts;dynamic complexity;dynamic coupling metrics;dynamic metrics;heuristic risk assessment methodology;heuristic risk factors;pacemaker architecture;product attributes;product metrics;reliability risk analysis;reliability risk assessment;risk aggregation;risk assessment model;risk factors;severity analysis;severity factors;software architecture;software risk management plan;subjective judgement;subjective risk assessment techniques;Computer Society;Computer architecture;Computer errors;Connectors;Failure analysis;Frequency;Humans;Performance analysis;Risk analysis;Risk management},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.12.06},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1010058&isnumber=21774},
  Zone                     = {1Q}
}

@InProceedings{Yacoub-2000-p210-221,
  Title                    = {A methodology for architectural-level risk assessment using dynamic metrics},
  Author                   = {Yacoub, S.M. and Ammar, H.H. and Robinson, T.},
  Booktitle                = {Software Reliability Engineering, 2000. ISSRE 2000. Proceedings. 11th International Symposium on},
  Year                     = {2000},
  Pages                    = {210-221},

  Abstract                 = {Risk assessment is an essential process of every software risk management plan. Several risk assessment techniques are based on the subjective judgement of domain experts. Subjective risk assessment techniques are human-intensive and error-prone. Risk assessment should be based on product attributes that we can quantitatively measure using product metrics. This paper presents a methodology for risk assessment at the early stages of the development lifecycle, namely the architecture level. We describe a heuristic risk assessment methodology that is based on dynamic metrics obtained from UML specifications. The methodology uses dynamic complexity and dynamic coupling metrics to define complexity factors for the architecture elements (components and connectors). Severity analysis is performed using FMEA (failure mode and effect analysis), as applied to architecture simulation models. We combine severity and complexity factors to develop heuristic risk factors for the architecture components and connectors. Based on component dependency graphs that were developed earlier for reliability analysis, and using analysis scenarios, we develop a risk assessment model and a risk analysis algorithm that aggregates the risk factors of components and connectors to the architectural level. We show how to analyze the overall risk factor of the architecture as the function of the risk factors of its constituting components and connectors. A case study of a pacemaker is used to illustrate the application of the methodology},
  Doi                      = {10.1109/ISSRE.2000.885873},
  File                     = {Published version:Yacoub-2000-p210-221.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1071-9458},
  Keywords                 = {graphs;risk management;software architecture;software development management;software metrics;software reliability;FMEA;UML specifications;analysis scenarios;architectural-level risk assessment;architecture simulation models;case study;complexity factors;component dependency graphs;components;connectors;coupling metrics;dynamic complexity;dynamic metrics;failure mode and effect analysis;heuristic risk assessment methodology;pacemaker;product attributes;product metrics;reliability analysis;risk analysis algorithm;risk factors;severity analysis;software development lifecycle;software risk management plans;subjective judgement;Aggregates;Algorithm design and analysis;Analytical models;Connectors;Failure analysis;Pacemakers;Performance analysis;Risk analysis;Risk management;Unified modeling language},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=885873&isnumber=19143},
  Zone                     = {-}
}

@InProceedings{Yahya-2013-p190-194,
  Title                    = {A review on tool supports for security requirements engineering},
  Author                   = {Yahya, S. and Kamalrudin, M. and Sidek, S.},
  Booktitle                = {Open Systems (ICOS), 2013 IEEE Conference on},
  Year                     = {2013},
  Month                    = {Dec},
  Pages                    = {190-194},

  Doi                      = {10.1109/ICOS.2013.6735072},
  File                     = {Published version:Yahya-2013-p190-194.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {formal specification;formal verification;safety-critical software;security of data;software tools;security requirement analyses;security requirement engineering tools;security software;software development failure;Analytical models;Business;Conferences;Open systems;Security;Software;Standards;capturing security requirements;essential use case (euc);security requirements elicitation;tools- security requirements},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6735072&isnumber=6735032},
  Zone                     = {-}
}

@Article{Yamamoto-1992-p28-35,
  Title                    = {P-functions-ternary logic functions capable of correcting input failures and suitable for treating ambiguities},
  Author                   = {Yamamoto, Y. and Mukaidono, M.},
  Journal                  = {Computers, IEEE Transactions on},
  Year                     = {1992},

  Month                    = {Jan},
  Number                   = {1},
  Pages                    = {28-35},
  Volume                   = {41},

  Abstract                 = {The authors consider a new class of special ternary logic functions: the P-functions capable of correcting input failures, based on the regular ternary logic. There are three major results. First, the P-functions are a special subset of the regular functions with no information loss. Second a ternary logic function defined by the all-prime-implicant disjoint of any Boolean connective is a P-function. Third, a P-function may be identified from the irredundant disjunctive form. The relationship between P-functions and other meaningful ternary logic functions is discussed},
  Doi                      = {10.1109/12.123380},
  File                     = {Published version:Yamamoto-1992-p28-35.pdf:PDF},
  Impactfactor             = {1.379},
  ISSN                     = {0018-9340},
  Keywords                 = {Boolean functions;ternary logic;Boolean connective;P-functions;all-prime-implicant disjoint;ambiguities;input failures;irredundant disjunctive form;regular ternary logic;ternary logic functions;Arithmetic;Boolean functions;Circuit faults;Computer science;Hazards;Logic circuits;Logic functions;Multivalued logic;Safety;Sufficient conditions},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=123380&isnumber=3494},
  Zone                     = {3Q}
}

@Article{Yampolskiy-2014-p-,
  Title                    = {A language for describing attacks on cyber-physical systems },
  Author                   = {Mark Yampolskiy and Péter Horváth and Xenofon D. Koutsoukos and Yuan Xue and Janos Sztipanovits},
  Journal                  = {International Journal of Critical Infrastructure Protection },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = { - },

  Abstract                 = {Abstract The security of cyber-physical systems is of paramount importance because of their pervasiveness in the critical infrastructure. Protecting cyber-physical systems greatly depends on a deep understanding of the possible attacks and their properties. The prerequisite for quantitative and qualitative analyses of attacks is a knowledge base containing attack descriptions. The structure of the attack descriptions is the indispensable foundation of the knowledge base. This paper introduces the Cyber-Physical Attack Description Language (CP-ADL), which lays a cornerstone for the structured description of attacks on cyber-physical systems. The core of the language is a taxonomy of attacks on cyber-physical systems. The taxonomy specifies the semantically distinct aspects of attacks on cyber-physical systems that should be described. CP-ADL extends the taxonomy with the means to describe relationships between semantically distinct aspects, despite the complex relationships that exist for attacks on cyber-physical systems. The language is capable of expressing relationships between attack descriptions, including the links between attack steps and the folding of attack details. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijcip.2014.09.003},
  File                     = {Published version:Yampolskiy-2014-p-.pdf:PDF},
  Impactfactor             = {0.63},
  ISSN                     = {1874-5482},
  Keywords                 = {Cyber-physical systems},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1874548214000602},
  Zone                     = {4Q}
}

@Article{Yampolskiy-2012-p48-58,
  Title                    = {Artimetrics: Biometrics for Artificial Entities},
  Author                   = {Yampolskiy, R.V. and Gavrilova, M.L.},
  Journal                  = {Robotics Automation Magazine, IEEE},
  Year                     = {2012},

  Month                    = {Dec},
  Number                   = {4},
  Pages                    = {48-58},
  Volume                   = {19},

  Abstract                 = {Domestic and industrial robots, intelligent software agents, virtual-world avatars, and other artificial entities are being created and deployed in our society for various routine and hazardous tasks, as well as for entertainment and companionship. Over the past ten years or so, primarily in response to the growing security threats and financial fraud, it has become necessary to accurately authenticate the identities of human beings using biometrics. For similar reasons, it may become essential to determine the identities of nonbiological entities. Trust and security issues associated with the large-scale deployment of military soldier-robots [55], robot museum guides [22], software office assistants [24], human like biped robots [67], office robots [5], domestic and industrial androids [93], [76], bots [85], robots with humanlike faces [60], virtual-world avatars [109], and thousands of other man-made entities require the development of methods for a decentralized, affordable, automatic, fast, secure, reliable, and accurate means of authenticating these artificial agents. The approach has to be decentralized to allow authority-free authentication important for open-source and collaborative societies. To address these concerns, we proposed [117], [120], [119], [38] the concept of artimetricsa field of study that identifies, classifies, and authenticates robots, software, and virtual reality agents. In this article, unless otherwise qualified, the term robot refers to both embodied robots (industrial, mobile, tele, personal, military, and service) and virtual robots or avatars, focusing specifically on those that have a human morphology.},
  Doi                      = {10.1109/MRA.2012.2201574},
  File                     = {Published version:Yampolskiy-2012-p48-58.pdf:PDF},
  Impactfactor             = {2.484},
  ISSN                     = {1070-9932},
  Keywords                 = {biometrics (access control);security of data;artificial entities;artimetrics;biometrics;domestic robots;financial fraud;industrial robots;intelligent software agents;military soldier robots;robot museum;security threats;virtual-world avatars;Artificial intelligence;Avatars;Biometrics;Industrial robots;Service robots;Software agents},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6377475&isnumber=6377433},
  Zone                     = {2Q}
}

@InProceedings{Yan-2011-p1757-1760,
  Title                    = {The assessment research of operational risk in the road freight industry undertake cold chain logistics services},
  Author                   = {Ping Yan and Huanhuan Qin},
  Booktitle                = {Remote Sensing, Environment and Transportation Engineering (RSETE), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {June},
  Pages                    = {1757-1760},

  Abstract                 = {Road freight industry undertaking cold chain logistics, shows enterprises an opportunity for enabling a rapid promotion to modern logistics industries, expanding the scope of commercial services, and benefiting their developments. For the special requirements of cold chain logistics, road freight transport companies must know and recognize the risks (existing or potential one) of cold chain logistics operations, and consequently take effective risk management techniques to solve their derived problems in advance. Based on the constructed criteria for evaluating the operational risks of undertaking cold chain logistics services, this report adopts the “fuzzy comprehensive decision-making” to carry out quantifiable evaluations of the operational risks in the road freight industry undertake cold chain logistics services.},
  Doi                      = {10.1109/RSETE.2011.5964635},
  File                     = {Yan-2011-p1757-1760.pdf:Yan-2011-p1757-1760.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {decision making;freight handling;fuzzy set theory;service industries;supply chains;cold chain logistics services;commercial services;fuzzy comprehensive decision making;logistics industries;operational risk assessment;risk management techniques;road freight industry;Industrial economics;Industries;Inventory management;Logistics;Presses;Risk management;Roads;cold chain logistics;risk evaluatio;road freight},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5964635&isnumber=5963913},
  Zone                     = {-}
}

@InProceedings{Yan-2009-p317-320,
  Title                    = {Risk Assessment Model of Automobile Defect Based on Gray Theory},
  Author                   = {Wang Yan and Wang Yun-Song and Zhang Jin-Huan},
  Booktitle                = {Measuring Technology and Mechatronics Automation, 2009. ICMTMA '09. International Conference on},
  Year                     = {2009},
  Month                    = {April},
  Pages                    = {317-320},
  Volume                   = {2},

  Abstract                 = {Risk assessment of automobile defect is classifying safety related risk level of manufacturing or designing defect. To carry out equivalent study between defect risk and failure defect, a risk flow route of automobile defect is developed introducing ETA method. A SPN function for risk assessment is setup based on Characteristics of automobile defect risk, and 3D matrix graph is introduced to describe overall risk. According to the scattered and fluctuant characteristics of automobile defect data, a risk forecast method based on gray theory is discussed. A risk assessment model of automobile defect is built; forecast of risk possibility is based on failure data. The model is corrected by residual discrimination. It is found that on gathering actual failure data from after-sales service, the gray model has a favorable applicability for forecast of risk possibility.},
  Doi                      = {10.1109/ICMTMA.2009.621},
  File                     = {Published version:Yan-2009-p317-320.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {automobile manufacture;graph theory;grey systems;matrix algebra;risk management;safety;3D matrix graph;ETA method;automobile defect;gray theory;risk assessment model;risk flow route;safety;Automobile manufacture;Automotive engineering;Government;Injuries;Predictive models;Risk analysis;Risk management;Road accidents;Time of arrival estimation;Vehicle safety;applicability;automobile;defect;model;risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5203437&isnumber=5203355},
  Zone                     = {-}
}

@Article{Yang-2007-p479-484,
  Title                    = {Mining TCP/IP packets to detect stepping-stone intrusion},
  Author                   = {Jianhua Yang and Shou-Hsuan Stephen Huang},
  Journal                  = {Computers \& Security },
  Year                     = {2007},
  Number                   = {7–8},
  Pages                    = {479 - 484},
  Volume                   = {26},

  Abstract                 = {An effective approach of detecting stepping-stone intrusion is to estimate the number of hosts compromised through estimating the length of a connection chain. This can be done by studying the changes in \{TCP\} packet round-trip time. In this paper, we propose a new algorithm by using data mining method to find the round-trip time from the timestamps of \{TCP\} send and echo packets. Previous algorithms produce either good packet matches on very few packets, or poor matches on many packets. This method gives us better round-trip time and more matched packets than other algorithms proposed in the past. It can estimate the length of a connection more accurate than other methods and has largely decreased false positive error and false negative error in detecting stepping-stone intrusion comparing with existing methods. },
  Doi                      = {http://dx.doi.org/10.1016/j.cose.2007.07.001},
  File                     = {Published version:Yang-2007-p479-484.pdf:PDF},
  Impactfactor             = {1.158},
  ISSN                     = {0167-4048},
  Keywords                 = {Network security},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167404807000934},
  Zone                     = {3Q}
}

@Article{Yang-2011-p538-546,
  Title                    = {Correlating TCP/IP Packet contexts to detect stepping-stone intrusion},
  Author                   = {Jianhua Yang and David Woolbright},
  Journal                  = {Computers \& Security},
  Year                     = {2011},
  Number                   = {6–7},
  Pages                    = {538 - 546},
  Volume                   = {30},

  Abstract                 = {Stepping-stone intrusion is one of the most popular techniques for attacking other computers, and detecting this form of intrusion and resisting intruders’ evasion are critical security issues. In this paper, we propose a new approach to this problem by introducing packet context to help detect stepping-stone intrusion. Pearson product-moment correlation coefficient is introduced to correlate packet context. The proposed approach does not need a threshold, and it is easily implemented. The experimental results show that the proposed approach can detect stepping-stone intrusion and resist intruders’ time-jittering and chaff-perturbation manipulation to an extent. },
  Doi                      = {http://dx.doi.org/10.1016/j.cose.2011.06.003},
  File                     = {Published version:Yang-2011-p538-546.pdf:PDF},
  Impactfactor             = {1.158},
  ISSN                     = {0167-4048},
  Keywords                 = {Network security},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0167404811000721},
  Zone                     = {3Q}
}

@Article{Yang-2013-p333-342,
  Title                    = {Precursor-based hierarchical Bayesian approach for rare event frequency estimation: A case of oil spill accidents },
  Author                   = {Ming Yang and Faisal I. Khan and Leonard Lye},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2013},
  Number                   = {5},
  Pages                    = {333 - 342},
  Volume                   = {91},

  Abstract                 = {Due to a scarcity of data, the estimate of the frequency of a rare event is a consistently challenging problem in probabilistic risk assessment (PRA). However, the use of precursor data has been shown to help in obtaining more accurate estimates. Moreover, the use of hyper-priors to represent prior parameters in the hierarchical Bayesian approach (HBA) generates more consistent results in comparison to the conventional Bayesian method. This study proposes a framework that uses a precursor-based \{HBA\} for rare event frequency estimation. The proposed method is demonstrated using the recent \{BP\} Deepwater Horizon accident in the Gulf of Mexico. The conventional Bayesian method is also applied to the same case study. The results show that the proposed approach is more effective with regards to the following perspectives: (a) using the \{HBA\} in the proposed framework provides an opportunity to take full advantage of the sparse data available and add information from indirect but relevant data; (b) the \{HBA\} is more sensitive to changes in precursor data than the conventional Bayesian method; and (c) using hyper-priors to represent prior parameters, the \{HBA\} is able to model the variability that can exist among different sources of data. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2012.07.006},
  File                     = {Published version:Yang-2013-p333-342.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Rare event frequency estimation},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.23},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957582012000894},
  Zone                     = {3Q}
}

@InProceedings{Yang-2011-p1-7,
  Title                    = {Impact of cyber-security issues on Smart Grid},
  Author                   = {Yang, Y. and Littler, T. and Sezer, S. and McLaughlin, K. and Wang, H.F.},
  Booktitle                = {Innovative Smart Grid Technologies (ISGT Europe), 2011 2nd IEEE PES International Conference and Exhibition on},
  Year                     = {2011},
  Month                    = {Dec},
  Pages                    = {1-7},

  Doi                      = {10.1109/ISGTEurope.2011.6162722},
  File                     = {Published version:Yang-2011-p1-7.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2165-4816},
  Keywords                 = {load management;power system security;security of data;smart power grids;cyber attack;data integrity;intrusion detection;load management;prevention technology;real power system;smart grid cyber security issues;smart grid operation;Computer security;IEC standards;Intrusion detection;Protocols;Smart grids;Cyber attack;Cyber-security;Smart Grid},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6162722&isnumber=6162607},
  Zone                     = {-}
}

@Article{Yang-2014-p72-90,
  Title                    = {A new risk quantification approach in port facility security assessment},
  Author                   = {Zaili Yang and Adolf K.Y. Ng and Jin Wang},
  Journal                  = {Transportation Research Part A: Policy and Practice},
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {72 - 90},
  Volume                   = {59},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {Abstract Terrorist attacks in the past decade had raised concern that terrorists capable of the suicide hijackings of airplanes could readily adapt such capabilities to maritime targets like ports. Although a large number of port security control measures have been proposed which have greatly enhanced security performance, the voice of requiring further justification on their effectiveness from various maritime stakeholders remains strong. Indeed, different ports around the world still have very diversified practices and standards regarding “secure” facilities, with a generally accepted assessment methodology found wanting. Despite the existence of previous research works, few have been done to address this issue, which clearly exposes a significant research gap. Understanding such deficiency, this paper introduces a novel fuzzy evidential reasoning approach to facilitate the quantitative analysis of port facility security assessment (PFSA). To achieve it, the major key security performance indicators (KSPIs) used by designated authorities in port facility security plan are identified; the current \{PFSA\} practices are reviewed with particular attention to the grades used by port operators when assessing the KSPIs; and a fuzzy evidential reasoning approach is applied to quantify port facility security risks and to conduct the cost benefit analysis of the associated security control measures.},
  Doi                      = {http://dx.doi.org/10.1016/j.tra.2013.10.025},
  File                     = {Published version:Yang-2014-p72-90.pdf:PDF},
  Impactfactor             = {2.725},
  ISSN                     = {0965-8564},
  Keywords                 = {Port security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，以集装箱港口为例说明风险评估过程，用到了模糊理论，Case},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0965856413002139},
  Zone                     = {2Q}
}

@Article{Yang-2014-p72-90a,
  Title                    = {A new risk quantification approach in port facility security assessment},
  Author                   = {Zaili Yang and Adolf K.Y. Ng and Jin Wang},
  Journal                  = {Transportation Research Part A: Policy and Practice},
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {72 - 90},
  Volume                   = {59},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {Abstract Terrorist attacks in the past decade had raised concern that terrorists capable of the suicide hijackings of airplanes could readily adapt such capabilities to maritime targets like ports. Although a large number of port security control measures have been proposed which have greatly enhanced security performance, the voice of requiring further justification on their effectiveness from various maritime stakeholders remains strong. Indeed, different ports around the world still have very diversified practices and standards regarding “secure” facilities, with a generally accepted assessment methodology found wanting. Despite the existence of previous research works, few have been done to address this issue, which clearly exposes a significant research gap. Understanding such deficiency, this paper introduces a novel fuzzy evidential reasoning approach to facilitate the quantitative analysis of port facility security assessment (PFSA). To achieve it, the major key security performance indicators (KSPIs) used by designated authorities in port facility security plan are identified; the current \{PFSA\} practices are reviewed with particular attention to the grades used by port operators when assessing the KSPIs; and a fuzzy evidential reasoning approach is applied to quantify port facility security risks and to conduct the cost benefit analysis of the associated security control measures.},
  Doi                      = {http://dx.doi.org/10.1016/j.tra.2013.10.025},
  File                     = {Published version:Yang-2014-p72-90.pdf:PDF},
  Impactfactor             = {2.725},
  ISSN                     = {0965-8564},
  Keywords                 = {Port security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，以集装箱港口为例说明风险评估过程，用到了模糊理论，Case},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0965856413002139},
  Zone                     = {2Q}
}

@InProceedings{Yanliang-2010-p1-6,
  Title                    = {Research of electric power information security protection on cloud security},
  Author                   = {Wang Yanliang and Deng Song and Lin Wei-min and Zhang Tao and Yu Yong},
  Booktitle                = {Power System Technology (POWERCON), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {Oct},
  Pages                    = {1-6},

  Abstract                 = {Informatization is an important feature of smart grid. With the construction of smart grid and the constant depth development of electric power information, the information networks of electric power have been continued to invade by all kinds of network threats. Traditional information security protection system for electric power had been unable to meet the security threats demand of ever changing and growing. Moreover, the cost of electrical safe operation was increased. Meanwhile, with the further application of electric power information engineering, operation capacity of electric power is required to be fully improved so that safe and stable operation is essential to electric power information system. Cloud security is service form that cloud computing provides for users. In view of the advantages of cloud computing and cloud security, architecture of electric power information security protection based on cloud security (AEPISP-CS) is put forward which combines with the current power information security status. Meanwhile, anti-virus system, spam mail filtering, threat detection based on cloud security are described in detail.},
  Doi                      = {10.1109/POWERCON.2010.5666728},
  File                     = {Published version:Yanliang-2010-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {power engineering;power system security;smart power grids;AEPISP-CS;anti-virus system;architecture of electric power information security protection;cloud security;electric power information engineering;electric power information security protection;electric power information system;information networks;protection system;smart grid;spam mail filtering;Biomedical monitoring;Fires;Information filters;Lead;Middleware;Monitoring;Security;cloud computing;cloud security;electric power information security;sender reputation;smart grid;spam mail filtering;threat detection},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5666728&isnumber=5666013},
  Zone                     = {-}
}

@Article{Yeh-2012-p702-709,
  Title                    = {A Modified Universal Generating Function Algorithm for the Acyclic Binary-State Network Reliability},
  Author                   = {Wei-Chang Yeh},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {702-709},
  Volume                   = {61},

  Abstract                 = {Network reliability is an important part of planning, designing, and controlling networks. Now, the most general binary-state network (BSN) reliability evaluation methods are based on Minimal Paths (MPs), or Minimal Cuts (MCs). The universal generating function method (UGFM) is a novel, efficient scheme for determining network reliability. Because the current best-known UGFM can only search for all MPs, it needs to be coupled with another routine such as Sum-of-Disjoint-Product method to calculate the final flow network reliability in terms of obtained MPs. In this study, a straightforward, novel UGFM is presented for calculating the acyclic BSN (ABSN) reliability between the source node and the sink node (i.e. one-to-one reliability). The proposed method is the first UGFM for the ABSN reliability problem without searching for all MPs in advance, which can reduce computational complexity. The computational complexity of the proposed algorithm is analysed, and its efficiency is well illustrated by a numerical example.},
  Doi                      = {10.1109/TR.2012.2207574},
  File                     = {Published version:Yeh-2012-p702-709.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {network theory (graphs);reliability theory;BSN;MC;MP;UGFM;acyclic binary state network reliability;computational complexity;minimal cuts;minimal paths;modified universal generating function algorithm;sum-of-disjoint-product method;Algorithm design and analysis;Complexity theory;Computer network reliability;Reliability engineering;Reliability theory;State-space methods;Binary-state;network reliability;universal generating function method},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6239642&isnumber=6289428},
  Zone                     = {2Q}
}

@Article{Yeh-2009-p476-484,
  Title                    = {A Convolution Universal Generating Function Method for Evaluating the Symbolic One-to-All-Target-Subset Reliability Function of Acyclic Multi-State Information Networks},
  Author                   = {Wei-Chang Yeh},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {476-484},
  Volume                   = {58},

  Abstract                 = {The acyclic multi-state information network (AMIN) is an extension of the multi-state network without having to satisfy the flow conservation law. A very straightforward convolution universal generating function method (CUGFM) is developed to find the exact symbolic one-to-all-target-subset reliability function of AMIN. The correctness and computational complexity of the proposed algorithm will be proven. Two illustrative examples demonstrate the power of the proposed CUGFM to solve the exact symbolic reliability functions of the one-to-all-target-subset AMIN problem more efficiently than the best-known UGFM.},
  Doi                      = {10.1109/TR.2009.2026688},
  File                     = {Published version:Yeh-2009-p476-484.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {computational complexity;computer network reliability;acyclic multistate information network;computational complexity;convolution universal generating function method;flow conservation law;symbolic one-to-all-target-subset reliability function;One-to-all-target-subset;symbolic network reliability function;universal generating function},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {~},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5191074&isnumber=5230375},
  Zone                     = {2Q}
}

@Article{Yeh-2009-p374-382,
  Title                    = {A Squeeze Response Surface Methodology for Finding Symbolic Network Reliability Functions},
  Author                   = {Wei-Chang Yeh and Chien-Hsing Lin},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {374-382},
  Volume                   = {58},

  Abstract                 = {A new response surface methodology (RSM) called the squeeze response surface methodology (SRSM) is proposed to gain the approximate symbolic network reliability function (SNRF). The proposed SRSM can be used to solve not only complicated system configurations, but also help decision makers gain greater understanding for the structure of the system. The response value is the value of the Bonferroni bounds (using by-products of cellular automata (CA) Monte Carlo simulation (MCS), and min-cuts) minus the simulation value (obtained from CA-MCS). SRSM squeezes the range of response values to improve solution quality. Our results compare favorably with previously developed algorithms in the literature from the experiment of the benchmark example.},
  Doi                      = {10.1109/TR.2009.2020121},
  File                     = {Published version:Yeh-2009-p374-382.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Monte Carlo methods;cellular automata;reliability theory;Bonferroni bounds;Monte Carlo simulation;approximate symbolic network reliability function;cellular automata;complicated system configuration;response value;simulation value;squeeze response surface methodology;symbolic network reliability function;Bonferroni bound;Box-Behnken design;Monde Carlo simulation;cellular automata;min-cuts;reliability function;response surface methodology},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A， If the simulation result is failure in a repetition ...},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4967913&isnumber=5062546},
  Zone                     = {2Q}
}

@Article{Yeh-2011-p404-414,
  Title                    = {Approximate Reliability Function Based on Wavelet Latin Hypercube Sampling and Bee Recurrent Neural Network},
  Author                   = {Wei-Chang Yeh and Su, J.C.P. and Tsung-Jung Hsieh and Mingchang Chih and Sin-Long Liu},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {June},
  Number                   = {2},
  Pages                    = {404-414},
  Volume                   = {60},

  Abstract                 = {This work combines a Bee Recurrent Neural Network (BRNN) optimized by the Artificial Bee Colony (ABC) algorithm with Monte Carlo Simulation (MCS) to generate a novel approximate model for predicting network reliability. We utilize the Wavelet Transform (WT)-based Latin Hypercube Sampling (LHS) (WLHS) to select input training data, and open the black box of neural networks by constructing a limited space reliability function from neural network parameters. Furthermore, the proposed method compares favorably with existing methods in literature based on experimental results for a benchmark example. The result reveals that the novel WLHS-MCS based on BRNN (WLHS-BRNN-MCS for short) is an excellent estimator of the reliability function.},
  Doi                      = {10.1109/TR.2011.2134190},
  File                     = {Published version:Yeh-2011-p404-414.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {Monte Carlo methods;hypercube networks;recurrent neural nets;reliability;wavelet transforms;ABC;BRNN;LHS;MCS;Monte Carlo simulation;WT;Wavelet transform;artificial bee colony;bee recurrent neural network;black box;network reliability;reliability function approximation;wavelet Latin hypercube sampling;Artificial neural networks;Hypercubes;Recurrent neural networks;Telecommunication network reliability;Wavelet transforms;Artificial bee colony algorithm;Monte Carlo simulation;bee recurrent neural network;wavelet latin hypercube sampling;wavelet transform},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，Each arc, and the system, is either operative, or failed.},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5746639&isnumber=5779932},
  Zone                     = {2Q}
}

@InProceedings{Yi-2009-p264-268,
  Title                    = {Fault Tree Data Structure Based on XML and the Conversion Method to BDD},
  Author                   = {Ren Yi and Liu Linlin and Zeng Shengkui},
  Booktitle                = {Computer Science and Information Engineering, 2009 WRI World Congress on},
  Year                     = {2009},
  Month                    = {March},
  Pages                    = {264-268},
  Volume                   = {2},

  Abstract                 = {In this paper we propose an XML based approach called XFTOM (extensible fault tree object model) to describe the data structure of fault tree. The model can directly describe the data structure, and has the excellent trait to share data through network, and can satisfy the requirement of the computer aided fault tree analysis in the environment of the internet/intranet in the future. The traditional fault tree representation encodes a failure logic function which does not lend itself to the mathematical manipulation, a more convenient form for the logic function from the mathematical viewpoint is the binary decision diagram (BDD), the paper also introduces a direct solution Based on XML regularity for building binary decision diagrams (BDD) from fault trees. Sequence reading and handling a criterion fault tree in an XFTOM form is introduced here; a well-structured and well-formatted BDD XML document corresponding to the fault tree can be obtained smoothly.},
  Doi                      = {10.1109/CSIE.2009.730},
  File                     = {Published version:Yi-2009-p264-268.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {XML;binary decision diagrams;fault trees;tree data structures;XFTOM;XML;binary decision diagram;computer aided fault tree analysis;extensible fault tree object model;fault tree data structure;logic function;mathematical manipulation;Binary decision diagrams;Boolean functions;Computer networks;Data structures;Document handling;Fault trees;IP networks;Logic functions;Tree data structures;XML;Binary Decision Diagram;Conversion;Fault Tree;Object Model;XML},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5171340&isnumber=5171282},
  Zone                     = {-}
}

@InProceedings{Yifeng-2012-p1-6,
  Title                    = {Distribution asset risk dynamic warning and coordination management},
  Author                   = {Wu Yifeng and Zhou Hua and Lu Zhihao and Hu Longsheng and Li Jufang and Xu Ping},
  Booktitle                = {Electricity Distribution (CICED), 2012 China International Conference on},
  Year                     = {2012},
  Month                    = {Sept},
  Pages                    = {1-6},

  Abstract                 = {In 2011, "7.23" Yong-Wenline high-speed rail accident resulted in a large loss of life and economy. Significant social impact brings a problem: high speed of development seen as 'success'? Power industry, similar with railway industry, is growing fast in recent years. It is important for Power Company to strengthen the distribution asset risk management. Qingpu electric power Distribution Company establishes a quantitative risk assessment model based on risk management standards of the International Electrotechnical Commission( (IEC3 101 0 risk management risk assessment, technical standards) ) and the research of the State Grid. The result of risk assessment can be applied to distribution network life cycle management, which includes designing, planning, procurement, construction, operation and maintenance,disposal , according to the framework of PAS55. Thus, an overall risk management system is established. Risk assessment model includes risk identification, analysis and assessment. Each potential event can be identified by analyzing comprehensive information of climate, operation environment, load, asset condition, construction/maintenance/operation plans , etc. The output of risk assessment model is a quantitative result. After defining scope of risk and determining level of risk, a dynamic warning will be provided. Qingpu electric power Distribution Company optimizes the risk management strategy. A cross-sector risk control mechanism is set out, involved several departments including maintenance,operation,planning, construction,procurement. Different risk control measures are taken to different level risk. Risk management system help company to enhance the overall capacity of distribution asset risks control and improve asset management performance.},
  Doi                      = {10.1109/CICED.2012.6508497},
  File                     = {Yifeng-2012-p1-6.pdf:Yifeng-2012-p1-6.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2161-7481},
  Keywords                 = {alarm systems;railway engineering;railway industry;risk management;International Electrotechnical Commission;Qingpu electric power distribution company;Yong-Wenline high-speed rail accident;asset management performance;coordination management;distribution asset risk dynamic warning;distribution asset risks control;quantitative risk assessment model;railway industry;risk management standards;Cross-department management and control mechanism;Distribution network assets;PAS 55;Risk dynamic warning},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6508497&isnumber=6508401},
  Zone                     = {-}
}

@InProceedings{Yihua-2012-p1-6,
  Title                    = {Research on risk control system in regional power grid},
  Author                   = {Liu Yihua and Gu Haoliang},
  Booktitle                = {Electricity Distribution (CICED), 2012 China International Conference on},
  Year                     = {2012},
  Month                    = {Sept},
  Pages                    = {1-6},

  Abstract                 = {Today's society increasingly high demand on the supply level of service, and quality service based on the safe and stable operation of power grids, regional power grid face the majority of users directly, while the active power itself can be modulated by a relatively small grid operation security risk is always there, in order to effectively control the risk of grid when it runs and improve ability to resist risks, therefor it is very important to build the system of risk control system of regional power grid. Based on the characteristics of regional power grid itself, described the contents of the regional power grid risks tube control system's design principle, the system software and hardware architecture, which is mainly introduced the regional power grid risk management and control systems: historical data read, grid risk identification and assessment, grid risk comprehensive assessment, grid risk controlling and decision-making, intelligent early warning function, and realize the function of this five respects the need to study the content and technical policy. Regional power grid can effectively analyse, induce and recognise the various potential or inherent risk factors in the regional grid security and stability through the establishment of risk management and control system. at the same time the probability of occurrence of various risk factors and the severity of the impact on the regional power grid are quantified, with the formation of different risk indicators and unified the various indicators of risk, it determines the level of risk ultimately, power companies establish appropriate contingency plans and emergency response programs according to the level of risk, which reduce the risk of the grid operation and improve the supply service levels and the enterprise economic efficiency.},
  Doi                      = {10.1109/CICED.2012.6508468},
  File                     = {Published version:Yihua-2012-p1-6.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2161-7481},
  Keywords                 = {decision making;power grids;power system control;power system security;quality of service;risk management;active power;control system design principle;decision-making;emergency response programs;enterprise economic efficiency;grid risk comprehensive assessment;grid risk controlling;grid risk identification;hardware architecture;intelligent early warning function;probability of occurrence;quality of service;regional power grid;regional power grid risk management;risk control system;risk indicators;small grid operation security risk;supply service levels;system software;Risk control;Risk index;economic efficiency;regional power grid},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6508468&isnumber=6508401},
  Zone                     = {-}
}

@InProceedings{Yingzhuo-2010-p3822-3827,
  Title                    = {Research of Visualization Intelligent Decision Support System for Drilling Risk Control},
  Author                   = {Xu Yingzhuo and Wang Liupeng},
  Booktitle                = {Control and Decision Conference (CCDC), 2010 Chinese},
  Year                     = {2010},
  Month                    = {May},
  Pages                    = {3822-3827},

  Abstract                 = {There were many lacks in traditional risk control systems,for example,the systems were mostly based on static information from ground, thereby difficult to accurately distinguish the actual situation “down hole” in real time,they could only solve a sole factor risk control problem,and moreover,their information was not intuitively revealed and could not be shared. So this paper proposes a Visualization Intelligent Decision Support System for Drilling Risk Control. A risk visualization analysis GIS system and intelligent decision-making models of risk control are built by use of Web GIS,case-based reasoning and network communication techniques. And an information-sharing and visualization decision-making support platform is provided for multi-domain experts and technicians in different locations to make cooperative decisions. The platform, which makes full use of all types of information as well as a variety of existing risk control methods,techniques,cases,multi-domain experts' experience and knowledge,can intuitively reveal the well location and distribution regularity of various types of potential risk in exploration area. In addition,it can achieve decision-making control of all risks in drilling engineering. This can improve risk decision-making and control in terms of science, accuracy and real-time. Details are provided about the architecture'design and key techniques of the system in this paper, including the building of intelligent decision-making models, asynchronous communication between the client and server,visualization of risk analysis, speedy loading and display of the map, and so forth.},
  Doi                      = {10.1109/CCDC.2010.5498486},
  File                     = {Published version:Yingzhuo-2010-p3822-3827.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Internet;case-based reasoning;control engineering computing;decision support systems;geographic information systems;mining;oil drilling;risk analysis;GIS system;Web GIS;case-based reasoning;down hole;drilling engineering;drilling risk control;intelligent decision-making models;network communication techniques;risk visualization analysis;static information;visualization intelligent decision support system;Communication system control;Control systems;Decision making;Decision support systems;Drilling;Geographic Information Systems;Intelligent networks;Intelligent systems;Risk analysis;Visualization;Case-Based Reasoning;Drilling Risk;Intelligent Decision;Risk Control;Visualization},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5498486&isnumber=5498064},
  Zone                     = {-}
}

@Article{Yoon-1999-p1067-1071,
  Title                    = {Estimation of system reliability for uncooled optical transmitters using system reliability function},
  Author                   = {Hyun-Jae Yoon and Nack-Jin Chung and Min-Ho Choi and In-Shik Park and Jichai Jeong},
  Journal                  = {Lightwave Technology, Journal of},
  Year                     = {1999},

  Month                    = {Jun},
  Number                   = {6},
  Pages                    = {1067-1071},
  Volume                   = {17},

  Abstract                 = {This paper investigates the system reliability for 155 Mb/s optical transmitters by deriving a system reliability function from reliability data of each component for transmitters, laser diode, photodiode, optical assembly, and driver IC. The reliability data for each component reliability function have been obtained from accelerated aging test. The reliability parameters such as failure rate, mean time-to-failure (MTTF), standard deviation are obtained from a probability plotting method. From the system reliability function, the MTTF of the optical transmitter at 65°C was estimated to be 47000 h with 95% confidence. In this estimation, we introduced modified lifetime of laser diodes and reliability function of optical assembly},
  Doi                      = {10.1109/50.769309},
  File                     = {Published version:Yoon-1999-p1067-1071.pdf:PDF},
  Impactfactor             = {2.555},
  ISSN                     = {0733-8724},
  Keywords                 = {ageing;life testing;modules;optical testing;optical transmitters;photodiodes;reliability;semiconductor device testing;semiconductor lasers;65 C;Mb/s optical transmitters;accelerated aging test;component reliability function;driver IC;failure rate;laser diode;mean time-to-failure;optical assembly;photodiode;probability plotting method;reliability data;standard deviation;system reliability;system reliability function;uncooled optical transmitters;Accelerated aging;Assembly systems;Diode lasers;Integrated circuit testing;Optical devices;Optical transmitters;Photodiodes;Photonic integrated circuits;Reliability;System testing},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，component failures 组件失效
里面还有 Fault Tree Analysis
失效概率：The random failure rate which reflects the probability of failures not associated with wear-out mech- anism can be calculated from total device hours for given confidence level using (5).},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=769309&isnumber=16675},
  Zone                     = {2Q}
}

@Article{You-2011-p2062-2074,
  Title                    = {A study of Electrical Security Risk Assessment System based on Electricity Regulation},
  Author                   = {A. Dahai You and B. QingQian Chen and C. Xianggen Yin and D. Bo Wang},
  Journal                  = {Energy Policy},
  Year                     = {2011},
  Number                   = {4},
  Pages                    = {2062 - 2074},
  Volume                   = {39},

  __markedentry            = {[Qiqi:1]},
  Abstract                 = {With the expansion of China's power system, there are more and more factors contributing to Large-area Power Outages. The snow disaster that hit Southern China in 2008 caused serious damages to the power systems, which made Chinese government realize that they cannot solely depend on power enterprises to effectively control the risk involved in power system. The Chinese government needs to collaborate with power enterprises in strengthening Electricity Regulation and taking countermeasures to reduce the risk of Large-area Power Outages. This paper first proposes an objective, practical and adaptive Electrical Security Risk Assessment System based on Electricity Regulation. The system consists of an assessment process with 278 indices, which are used to calculate the risk of Large-area Power Outages by the use of Analytic Hierarchy Process and the Delphi Method. Then, an example of Guangdong power system serves to illustrate the detailed implementation procedures of the proposed assessment system and the results show that Guangdong power system faces a moderate risk of Large-area Power Outages. Finally, some countermeasures are proposed to overcome the defects in the existing power system.},
  Doi                      = {http://dx.doi.org/10.1016/j.enpol.2011.01.050},
  File                     = {Published version:You-2011-p2062-2074.pdf:PDF},
  Impactfactor             = {2.743},
  ISSN                     = {0301-4215},
  Keywords                 = {Large-area power outages},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，以 Guangdong power system 为例，无仿真对比},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0301421511000619},
  Zone                     = {3Q}
}

@Article{You-2014-p133-145,
  Title                    = {Quantifying the Influence of Climate Change to Priorities for Infrastructure Projects},
  Author                   = {Haowen You and Lambert, J.H. and Clarens, A.F. and McFarlane, B.J.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Feb},
  Number                   = {2},
  Pages                    = {133-145},
  Volume                   = {44},

  Abstract                 = {Uncertainties of climate change and other nonprobabilistic and structural uncertainties need to be addressed in strategic planning and priority setting for infrastructure systems. Traditional economic analysis and risk analysis of particular uncertainties can be prohibitive due to sparse data, complex models, and unforeseen interactions of climate change with other stressors. Nevertheless, planners need to proceed in the near term and may be asked to allocate resources to these deep uncertainties. This paper identifies and quantifies the influence of climate change combining with other sources of uncertainty to the priority order of projects in a portfolio of infrastructure investments. A demonstration for the Hampton Roads region of Virginia proceeds as follows. First, we apply traditional multicriteria analysis to generate a baseline prioritization of over 93 transportation projects. Next, we identify the following scenarios: climate conditions combined with economic conditions, wear and tear, ecological conditions, and traffic-demand conditions, and climate conditions alone. Next, we adjust a multicriteria value function for each scenario. We then quantify the sensitivity of the priority order of projects to the scenarios. Last, we identify the scenarios that are disruptive to the baseline prioritization. This approach is widely applicable to strategic planning for infrastructure systems that are subject to uncertainties of emergent and future conditions.},
  Doi                      = {10.1109/TSMC.2013.2248709},
  File                     = {Published version:You-2014-p133-145.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {decision making;ecology;environmental economics;investment;resource allocation;road traffic;strategic planning;transportation;wear;Hampton roads;Virginia region;baseline prioritization;climate change influence quantification;climate change uncertainties;climate conditions;ecological conditions;economic conditions;infrastructure investment portfolio;infrastructure project priorities;multicriteria analysis;multicriteria value function;nonprobabilistic uncertainties;resource allocation;sensitivity quantification;strategic planning;structural uncertainties;traffic-demand conditions;transportation projects;wear-and-tear;Deep uncertainty;infrastructure systems;risk analysis;robust decision making;transportation planning},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6495481&isnumber=6710259},
  Zone                     = {-}
}

@InProceedings{Youdong-2008-p445-448,
  Title                    = {Multi Relational Mining in Network Intrusion Detection},
  Author                   = {Zhang Youdong},
  Booktitle                = {Fuzzy Systems and Knowledge Discovery, 2008. FSKD '08. Fifth International Conference on},
  Year                     = {2008},
  Month                    = {Oct},
  Pages                    = {445-448},
  Volume                   = {4},

  Abstract                 = {The network intrusion detection (NIDS) is faced with the question to detect many kinds of intrusion. In order to detect the complex attack, network intrusion detection system need to analysis massive data captured form different network safety equipments. So a new multi relational mining algorithm MRA2 is proposed. MRA2 depend on the association rules mining technology and the probability function dependency method which is proposed through extending the theory of function dependency. MRA2 is able to synthesize the various datalog resources to detect intrusion effectively and reappear to the complex network attack scenario.},
  Doi                      = {10.1109/FSKD.2008.505},
  File                     = {Published version:Youdong-2008-p445-448.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {data mining;probability;security of data;MRA2;association rules mining;multirelational mining;network intrusion detection;probability function dependency method;Association rules;Complex networks;Data analysis;Data mining;Face detection;Intrusion detection;Logic;Phase frequency detector;Relational databases;Safety devices;Intrusion detection;Multi Relational Mining;probability function dependency},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4666426&isnumber=4666335},
  Zone                     = {-}
}

@InProceedings{Youjun-2011-p1949-1952,
  Title                    = {Design and realization of multi-function car-carry fault diagnosis system},
  Author                   = {Yue Youjun and Li Xiang and Wang Zongqiang and Zong Qun},
  Booktitle                = {Transportation, Mechanical, and Electrical Engineering (TMEE), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {Dec},
  Pages                    = {1949-1952},

  Abstract                 = {The present study fist proposed the structure of a multi-function car-carry fault diagnosis system, and the design of vehicle status monitoring module. Secondly, this paper analyzed the design of vehicle fault diagnosis expert knowledge base, inference machine and interpreter program. The paper used fault tree analysis (FTA) to establish fault tree of automotive engine, chassis systems, body parts and electrical equipment, then simplified the fault tree to form fault diagnosis rules, and establishes car-carry fault diagnosis expert system knowledge base. In its software implementation, SQLite database is used to save and manage diagnostic rules. Expert system inference engine uses an interactive search method based adopt reasoning mechanism. The expert system uses preset text to realize explanation. Finally, this paper discusses hardware and software design of the fault diagnosis system, and demonstrates the system's human-machine interface operating results.},
  Doi                      = {10.1109/TMEE.2011.6199596},
  File                     = {Published version:Youjun-2011-p1949-1952.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {SQL;automobiles;design engineering;expert systems;fault diagnosis;fault trees;inference mechanisms;interactive systems;mechanical engineering computing;user interfaces;FTA;SQLite database;automotive engine;body parts;chassis systems;electrical equipment;expert system inference engine;fault tree analysis;inference machine;interactive search method;interpreter program;multifunction car-carry fault diagnosis system design;multifunction car-carry fault diagnosis system realization;preset text;reasoning mechanism;system human-machine interface operating results;vehicle fault diagnosis expert knowledge base;vehicle status monitoring module design;Databases;Engines;Expert systems;Fault diagnosis;Fault trees;Vehicles;Expert System;fault diagnosis;fault tree},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6199596&isnumber=6199128},
  Zone                     = {-}
}

@InProceedings{Yu-2009-p1-2,
  Title                    = {5th IEEE Workshop on Pervasive Wireless Networking (PWN 2009) Message from the workshop co-chairs},
  Author                   = {Chansu Yu and Hamdaoui, B.},
  Booktitle                = {Pervasive Computing and Communications, 2009. PerCom 2009. IEEE International Conference on},
  Year                     = {2009},
  Month                    = {March},
  Pages                    = {1-2},

  Doi                      = {10.1109/PERCOM.2009.4912867},
  File                     = {Published version:Yu-2009-p1-2.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Internet;radio networks;telecommunication computing;ubiquitous computing;Internet;home networking;pervasive computing;pervasive wireless networking;wireless LANs;wireless MANs;wireless PAN;wireless broadband;wireless connectivity;Bluetooth;Conferences;IP networks;Mobile radio mobility management;Pervasive computing;WiMAX;Wireless LAN;Wireless mesh networks;Wireless personal area networks;Wireless sensor networks},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.04},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=622912&isnumber=13557},
  Zone                     = {-}
}

@Article{Yu-2013-p714-721,
  Title                    = {An Analytical Approach to Failure Prediction for Systems Subject to General Repairs},
  Author                   = {Qiuze Yu and Huairui Guo and Haitao Liao},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Sept},
  Number                   = {3},
  Pages                    = {714-721},
  Volume                   = {62},

  Abstract                 = {The generalized renewal process (GRP) has been widely used for modeling repairable systems under general repairs. Unfortunately, most of the related work does not provide closed-form solutions for predicting the reliability metrics of such systems, such as the expected number of failures, and the expected failure intensity, at a future point in time. A technical approach reported in literature is to conduct simulations to predict the reliability metrics of interest; however, simulations can be time-consuming. To reduce computational efforts for failure prediction, we propose an analytical approach that does not rely on simulations. Our idea is to predict the system's mean residual life based on its virtual age after each repair. The predicted mean residual life is then used to determine the expected time to the next failure. To illustrate this approach, we use a log-linear failure intensity function, and provide a detailed procedure for obtaining the maximum likelihood estimates (MLE) of the model parameters. A numerical study shows that this simple yet effective approach can provide failure predictions as accurate as the simulation alternative. We then demonstrate how the proposed approach can evaluate different maintenance strategies more efficiently compared to using simulations.},
  Doi                      = {10.1109/TR.2013.2270426},
  File                     = {Published version:Yu-2013-p714-721.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {failure analysis;maintenance engineering;maximum likelihood estimation;reliability;MLE;failure prediction;general repairs;generalized renewal process;log-linear failure intensity function;maintenance strategies;maximum likelihood estimates;reliability metrics;repairable systems;system mean residual life prediction;virtual age;Analytical models;Maintenance engineering;Maximum likelihood estimation;Measurement;Numerical models;Predictive models;Reliability;Failure intensity function;General repair;maintenance strategy;virtual age},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，中国人写的，Failure Function 是指失效函数},
  Timestamp                = {2014.12.02},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6550026&isnumber=6587537},
  Zone                     = {2Q}
}

@Article{Yu-2008-p4005-4017,
  Title                    = {Peer-to-peer system-based active worm attacks: Modeling, analysis and defense },
  Author                   = {Wei Yu and Sriram Chellappan and Xun Wang and Dong Xuan},
  Journal                  = {Computer Communications },
  Year                     = {2008},
  Number                   = {17},
  Pages                    = {4005 - 4017},
  Volume                   = {31},

  Abstract                 = {Active worms continue to pose major threats to the security of today’s Internet. This is due to the ability of active worms to automatically propagate themselves and compromise hosts in the Internet. Due to the recent surge of peer-to-peer (P2P) systems with large numbers of users and rich connectivity, \{P2P\} systems can be a potential vehicle for the attacker to achieve rapid worm propagation in the Internet. In this paper, we tackle this issue by modeling and analyzing active worm propagation on top of \{P2P\} systems, and designing effective defense strategies within \{P2P\} systems to suppress worm propagation. In particular: (1) we define two P2P-based active worm attack models: an offline P2P-based hit-list attack model and an online P2P-based attack model; (2) we conduct a detailed analysis on the impacts of worm propagation on top of P2P-based systems, and study the sensitivity of worm propagation to various \{P2P\} system and attack-related parameters; (3) finally, we propose defense strategies within the \{P2P\} system to combat worms. Based on extensive numerical analysis and simulation data, we demonstrate that P2P-based active worm attacks can significantly enhance worm propagation, and important \{P2P\} related parameters (system size, topology degree, host vulnerability, etc.) have significant impacts on worm spread. We also find that our proposed defense strategies can effectively combat worms by rapidly detecting and immunizing infected hosts. },
  Doi                      = {http://dx.doi.org/10.1016/j.comcom.2008.08.008},
  File                     = {Published version:Yu-2008-p4005-4017.pdf:PDF},
  Impactfactor             = {1.079},
  ISSN                     = {0140-3664},
  Keywords                 = {\{P2P\} systems},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0140366408004362},
  Zone                     = {3Q}
}

@InProceedings{Yu-2010-p341-344,
  Title                    = {Risk Assessment of Customer Information in Telecommunication Industry},
  Author                   = {Zhengwei Yu and Yumei Wu},
  Booktitle                = {Information Science and Management Engineering (ISME), 2010 International Conference of},
  Year                     = {2010},
  Month                    = {Aug},
  Pages                    = {341-344},
  Volume                   = {2},

  Abstract                 = {A practical method of risk assessment aimed to analyze and evaluate the risks of data leakage protection related to customer information in telecommunication company is presented. This helps senior management in telecommunication industry to identify, analyze and properly treat the data leakage risks due to the complexity and diversities of business process and supportive information systems. The distribution view and flow view have been defined to describe and analyze the customer information which provides the basis for the risk assessment. A group of technical tests are proposed to find and verify the risky vulnerabilities which have been identified based on the method.},
  Doi                      = {10.1109/ISME.2010.166},
  File                     = {Yu-2010-p341-344.pdf:Yu-2010-p341-344.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {customer profiles;risk management;security of data;service industries;telecommunication industry;business process;customer information;data leakage protection;distribution view;flow view;risk assessment;risk vulnerability;senior management;supportive information systems;telecommunication industry;Communications technology;Companies;Information security;Personnel;Risk management;Testing;customer information;penetration test;risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5573907&isnumber=5573767},
  Zone                     = {-}
}

@Article{Yuan-2013-p1750-1760,
  Title                    = {Routing-Toward-Primary-User Attack and Belief Propagation-Based Defense in Cognitive Radio Networks},
  Author                   = {Zhou Yuan and Zhu Han and Sun, Y.L. and Husheng Li and Ju Bin Song},
  Journal                  = {Mobile Computing, IEEE Transactions on},
  Year                     = {2013},

  Month                    = {Sept},
  Number                   = {9},
  Pages                    = {1750-1760},
  Volume                   = {12},

  Abstract                 = {Cognitive radio (CR) networks have attracted many attentions recently, while the security issues are not fully studied yet. In this paper, we propose a new and powerful network layer attack, routing-toward-primary-user (RPU) attack in CR networks. In this attack, malicious nodes intentionally route a large amount of packets toward the primary users (PUs), aiming to cause interference to the PUs and to increase delay in the data transmission among the secondary users. In the RPU attack, it is difficult to detect the malicious nodes since the malicious nodes may claim that those nodes, to which they forward the packets, behave dishonestly and cause problems in the data transmission. To defend against this attack without introducing high complexity, we develop a defense strategy using belief propagation. First, an initial route is found from the source to the destination. Each node keeps a table recording the feedbacks from the other nodes on the route, exchanges feedback information and computes beliefs. Finally, the source node can detect the malicious nodes based on the final belief values. Simulation results show that the proposed defense strategy against the RPU attack is effective and efficient in terms of significant reduction in the delay and interference caused by the RPU attack.},
  Doi                      = {10.1109/TMC.2012.137},
  File                     = {Published version:Yuan-2013-p1750-1760.pdf:PDF},
  Impactfactor             = {2.395},
  ISSN                     = {1536-1233},
  Keywords                 = {belief networks;cognitive radio;feedback;radiofrequency interference;telecommunication computing;telecommunication network routing;telecommunication security;CR network;RPU attack;belief propagation-based defense;cognitive radio network;data transmission delay;defense strategy;feedback information;interference;malicious node detection;network layer attack;packet routing;primary user;routing-toward-primary-user attack;secondary user;security issue;source node;Belief propagation;Cognitive radio;Interference;Mobile computing;Routing;Sensors;Cognitive radio;belief propagation;routing toward primary user attack;security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6226410&isnumber=6567864},
  Zone                     = {2Q}
}

@Article{Yuan-2012-p1850-1860,
  Title                    = {Defeating Primary User Emulation Attacks Using Belief Propagation in Cognitive Radio Networks},
  Author                   = {Zhou Yuan and Niyato, D. and Husheng Li and Ju Bin Song and Zhu Han},
  Journal                  = {Selected Areas in Communications, IEEE Journal on},
  Year                     = {2012},

  Month                    = {November},
  Number                   = {10},
  Pages                    = {1850-1860},
  Volume                   = {30},

  Abstract                 = {Cognitive radio (CR) is a promising technology for future wireless spectrum allocation to improve the usage of the licensed bands. However, CR wireless networks are susceptible to various attacks and cannot offer efficient security. Primary user emulation (PUE) is one of the most serious attacks for CR networks, which can significantly increase the spectrum access failure probability. In this paper, we propose a defense strategy against the PUE attack in CR networks using belief propagation, which avoids the deployment of additional sensor networks and expensive hardware in the networks used in the existing literatures. In our proposed approach, each secondary user calculates the local function and the compatibility function, computes the messages, exchanges messages with the neighboring users, and calculates the beliefs until convergence. Then, the PUE attacker will be detected, and all the secondary users in the network will be notified in a broadcast way about the characteristics of the attacker's signal. Therefore, all SUs can avoid the PUE attacker's primary emulation signal in the future. Simulation results show that our proposed approach converges quickly, and is effective to detect the PUE attacker.},
  Doi                      = {10.1109/JSAC.2012.121102},
  File                     = {Published version:Yuan-2012-p1850-1860.pdf:PDF},
  Impactfactor             = {3.121},
  ISSN                     = {0733-8716},
  Keywords                 = {belief networks;channel allocation;cognitive radio;probability;radio spectrum management;signal detection;telecommunication security;wireless sensor networks;CR wireless network;PUE attack;belief propagation;cognitive radio network;compatibility function;defense strategy;exchange message;licensed band usage;primary user emulation;secondary user;signal detection;spectrum access failure probability;wireless spectrum allocation;Belief propagation;Cognitive radio;Convergence;Emulation;Simulation;Wireless networks;Cognitive radio;PUE attack;belief propagation;security},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6331677&isnumber=6331300},
  Zone                     = {2Q}
}

@Article{Yukalov-2014-p1155-1168,
  Title                    = {Manipulating Decision Making of Typical Agents},
  Author                   = {Yukalov, V.I. and Sornette, D.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2014},

  Month                    = {Sept},
  Number                   = {9},
  Pages                    = {1155-1168},
  Volume                   = {44},

  Abstract                 = {We investigate how the choice of decision makers can be varied under the presence of risk and uncertainty. Our analysis is based on the approach we have previously applied to individual decision makers, which we now generalize to the case of decision makers that are members of a society. The approach employs the mathematical techniques that are common in quantum theory, justifying our naming as quantum decision theory. However, we do not assume that decision makers are quantum objects. The techniques of quantum theory are needed only for defining the prospect probabilities taking into account such hidden variables as behavioral biases and other subconscious feelings. The approach describes an agent's choice as a probabilistic event occurring with a probability that is the sum of a utility factor and of an attraction factor. The attraction factor embodies subjective and unconscious dimensions in the mind of the decision maker. We show that the typical aggregate amplitude of the attraction factor is 1/4, and it can be either positive or negative depending on the relative attraction of the competing choices. The most efficient way of varying the decision makers choice is realized by influencing the attraction factor. This can be done in two ways. One method is to arrange in a special manner the payoff weights, which induces the required changes of the values of attraction factors. We show that a slight variation of the payoff weights can invert the sign of the attraction factors and reverse the decision preferences, even when the prospect utilities remain unchanged. The second method of influencing the decision makers choice is by providing information to decision makers. The methods of influencing decision making are illustrated by several experiments, whose outcomes are compared quantitatively with the predictions of our approach.},
  Doi                      = {10.1109/TSMC.2014.2314283},
  File                     = {Published version:Yukalov-2014-p1155-1168.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {decision making;decision theory;multi-agent systems;quantum theory;risk management;agent choice;attraction factor;behavioral biases;decision maker;decision making;mathematical techniques;probabilistic event;quantum decision theory;quantum theory;subconscious feelings;Aggregates;Decision making;Decision theory;Probability;Quantum mechanics;Utility theory;Vectors;Decision making under risk and uncertainty;decision theory;group consultations;information and knowledge;social interactions},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6800124&isnumber=6878502},
  Zone                     = {-}
}

@Article{Yun-2011-p1364-1369,
  Title                    = {A Probabilistic Computing Approach of Attack Graph-Based Nodes in Large-scale Network },
  Author                   = {Ye Yun and Xu Xi-shan and Qi Zhi-chang},
  Journal                  = {Procedia Environmental Sciences },
  Year                     = {2011},
  Note                     = {2011 3rd International Conference on Environmental Science and Information Application Technology \{ESIAT\} 2011 },
  Number                   = {0},
  Pages                    = {1364 - 1369},
  Volume                   = {10, Part B},

  Abstract                 = {To protect critical resources in networked environments, it is important to quantify the likelihood of potential multi- step attacks in attack graphs. Aimed at the problems that probabilistic incorrect computing caused by shared dependencies in nodes, a methodology for security risk analysis that is based on the model of attack graphs and the Common Vulnerability Scoring System (CVSS) was presented, and the problem of probabilistic incorrect computing was solved successfully by proposing the probabilistic computing approach of accurate calculation and approximate calculation, While ensuring the accuracy of the probability of attack graph-based nodes, quickly Calculated the probability of each node, which can be adapted to a large-scale network. Reasonableness and effectiveness of proposed method was verified in the simulation. Compared with the related research, our approach can be adapted to a more complex attack graph, and had good scalability. },
  Doi                      = {http://dx.doi.org/10.1016/j.proenv.2011.09.218},
  File                     = {Published version:Yun-2011-p1364-1369.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1878-0296},
  Keywords                 = {Attack Graphs},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1878029611004130},
  Zone                     = {-}
}

@Article{Yun-2011-p3-8,
  Title                    = {A Probabilistic Computing Approach of Attack Graph-Based Nodes in Large-Scale Network },
  Author                   = {Ye Yun and Xu Xi-shan and Qi Zhi-chang},
  Journal                  = {Procedia Environmental Sciences },
  Year                     = {2011},
  Note                     = {2011 3rd International Conference on Environmental Science and Information Application Technology \{ESIAT\} 2011 },
  Number                   = {0},
  Pages                    = {3 - 8},
  Volume                   = {10, Part A},

  Abstract                 = {To protect critical resources in networked environments, it is important to quantify the likelihood of potential multi- step attacks in attack graphs. Aimed at the problems that probabilistic incorrect computing caused by shared dependencies in nodes, a methodology for security risk analysis that is based on the model of attack graphs and the Common Vulnerability Scoring System (CVSS) was presented, and the problem of probabilistic incorrect computing was solved successfully by proposing the probabilistic computing approach of accurate calculation and approximate calculation, While ensuring the accuracy of the probability of attack graph-based nodes, quickly Calculated the probability of each node, which can be adapted to a large-scale network. Reasonableness and effectiveness of proposed method was verified in the simulation. Compared with the related research, our approach can be adapted to a more complex attack graph, and had good scalability. },
  Doi                      = {http://dx.doi.org/10.1016/j.proenv.2011.09.002},
  File                     = {Published version:Yun-2011-p3-8.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1878-0296},
  Keywords                 = {Attack Graphs},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1878029611001976},
  Zone                     = {-}
}

@Article{Zafra-Cabe-2011-p891-901,
  Title                    = {A Risk-Based Model Predictive Control Approach to Adaptive Interventions in Behavioral Health},
  Author                   = {Zafra-Cabeza, A. and Rivera, D.E. and Collins, L.M. and Ridao, M.A. and Camacho, E.F.},
  Journal                  = {Control Systems Technology, IEEE Transactions on},
  Year                     = {2011},

  Month                    = {July},
  Number                   = {4},
  Pages                    = {891-901},
  Volume                   = {19},

  Abstract                 = {This brief examines how control engineering and risk management techniques can be applied in the field of behavioral health through their use in the design and implementation of adaptive behavioral interventions. Adaptive interventions are gaining increasing acceptance as a means to improve prevention and treatment of chronic, relapsing disorders, such as abuse of alcohol, tobacco, and other drugs, mental illness, and obesity. A risk-based model predictive control (MPC) algorithm is developed for a hypothetical intervention inspired by Fast Track, a real-life program whose long-term goal is the prevention of conduct disorders in at-risk children. The MPC-based algorithm decides on the appropriate frequency of counselor home visits, mentoring sessions, and the availability of after-school recreation activities by relying on a model that includes identifiable risks, their costs, and the cost/benefit assessment of mitigating actions. MPC is particularly suited for the problem because of its constraint-handling capabilities, and its ability to scale to interventions involving multiple tailoring variables. By systematically accounting for risks and adapting treatment components over time, an MPC approach as described in this brief can increase intervention effectiveness and adherence while reducing waste, resulting in advantages over conventional fixed treatment. A series of simulations are conducted under varying conditions to demonstrate the effectiveness of the algorithm.},
  Doi                      = {10.1109/TCST.2010.2052256},
  File                     = {Published version:Zafra-Cabe-2011-p891-901.pdf:PDF},
  Impactfactor             = {2},
  ISSN                     = {1063-6536},
  Keywords                 = {behavioural sciences;constraint handling;cost-benefit analysis;health care;predictive control;risk management;Fast Track;MPC based algorithm;adaptive intervention;behavioral health;benefit assessment;constraint handling capability;control engineering;mental illness;relapsing disorder;risk based model predictive control approach;risk management technique;Adaptive control;Control engineering;Drugs;Mental disorders;Pediatrics;Prediction algorithms;Predictive control;Predictive models;Programmable control;Risk management;Adaptive interventions;behavioral health;predictive control;process control;risk analysis},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5499451},
  Zone                     = {3Q}
}

@InProceedings{Zaini-2011-p320-325,
  Title                    = {Contractors' strategic approaches to risk assessment techniques at project planning stage},
  Author                   = {Zaini, A.A. and Takim, R. and Endut, I.R.},
  Booktitle                = {Business, Engineering and Industrial Applications (ISBEIA), 2011 IEEE Symposium on},
  Year                     = {2011},
  Month                    = {Sept},
  Pages                    = {320-325},

  Abstract                 = {The construction industry is still plagued by poor quality, poor workmanship, poor safety and health environment, and poor practices. The current main problem in the Malaysian Construction Industry is delayed projects under the Ninth Malaysia Plan. The idea of introducing formal risk assessment among Malaysian contractors during the project planning stage is a proactive approach to achieve project objectives. Therefore, this research intends to achieve the following objectives: (1) to investigate the contractors' approaches of risk assessment techniques at the project planning stage, (2) to investigate the consequences of risk assessment techniques at the project planning stage, and (3) to determine the appropriate risk assessment techniques in dealing with the potential risk factors at the project planning stage. The research methodology applied for this research includes literature reviews and questionnaires. Findings from this research show that the application of risk assessment techniques in the Malaysian Construction Industry is still moderate. Hence, it is suggested that these contractors should try to implement formal risk assessment techniques during the project planning stage in order to achieve the project objectives.},
  Doi                      = {10.1109/ISBEIA.2011.6088829},
  File                     = {Published version:Zaini-2011-p320-325.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {construction industry;contracts;project management;risk management;strategic planning;Malaysia;construction industry;contractor strategic approaches;formal risk assessment;proactive approach;project planning stage;research methodology;Companies;Construction industry;Planning;Risk management;Training;Construction Industry;Contractors;Project Planning Stage;Risk;Risk Assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6088829&isnumber=6088773},
  Zone                     = {-}
}

@Article{Zamanali-1998-p361-364,
  Title                    = {Probabilistic-risk-assessment applications in the nuclear-power industry},
  Author                   = {Zamanali, J.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {SP361-SP364},
  Volume                   = {47},

  Abstract                 = {There has been a remarkable growth in the use of probabilistic risk assessment (PRA) methods in the US nuclear industry, both commercial and government. Use of PRA techniques has been expanded appreciably in the 1980s and 1990s as a result of the Individual Plant Examination (IPE) process. The IPE program calls for all existing nuclear utilities to use PRA methods to seek out plant vulnerabilities, and for all new applications for commercial nuclear power licenses to include full-scope PRA for design certification under 10CFR part 52. Effort must be spent to improve the consistency of PRA models, applications, evaluation approaches and processes among the nuclear utility industry. Standards are needed to guarantee proper application of PRA methods in risk-based regulation (RBR) in order to establish and maintain consistency in PRA applications. One standard application method that fits every aspect of RBR is difficult if not impossible. However, some of the PRA tasks or processes can be effectively standardized, e.g., level of detail, bounding limits on the failure rates of human recovery actions, and ways and means of data analysis. This paper discusses some historical background about the application of the PRA and identifies: appropriate efforts that the industry can spend to standardize the PRA applications in RBR; improvements in PRA techniques to achieve standardization; the benefits utilities would gain as a result of this harmony; and some of the limitations in areas which stubbornly resists standardization},
  Doi                      = {10.1109/24.740552},
  File                     = {Published version:Zamanali-1998-p361-364.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {fission reactor accidents;human factors;nuclear power stations;probability;risk management;Individual Plant Examination;US nuclear industry;commercial nuclear power licenses;common failure cause;core damage frequency;data analysis;decision making process;design certification;failure rates;human recovery actions;human reliability assessment;loss-of-coolant accident;maintenance activities;nuclear-power industry;online maintenance;operation activities;probabilistic-risk-assessment applications;risk perspective;risk-based regulation;safety requirements;standardization;Decision making;Defense industry;Government;Humans;Industrial accidents;Licenses;Nuclear facility regulation;Performance gain;Risk management;Safety},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.16},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=740552&isnumber=15995},
  Zone                     = {2Q}
}

@Article{Zamanali-1998-p361-364a,
  Title                    = {Probabilistic-risk-assessment applications in the nuclear-power industry},
  Author                   = {Zamanali, J.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {SP361-SP364},
  Volume                   = {47},

  Doi                      = {10.1109/24.740552},
  File                     = {Published version:Zamanali-1998-p361-364a.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {fission reactor accidents;human factors;nuclear power stations;probability;risk management;Individual Plant Examination;US nuclear industry;commercial nuclear power licenses;common failure cause;core damage frequency;data analysis;decision making process;design certification;failure rates;human recovery actions;human reliability assessment;loss-of-coolant accident;maintenance activities;nuclear-power industry;online maintenance;operation activities;probabilistic-risk-assessment applications;risk perspective;risk-based regulation;safety requirements;standardization;Decision making;Defense industry;Government;Humans;Industrial accidents;Licenses;Nuclear facility regulation;Performance gain;Risk management;Safety},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A，不相关},
  Timestamp                = {2014.11.18},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=740552&isnumber=15995},
  Zone                     = {2Q}
}

@Article{Zamanali-1998-p361-364b,
  Title                    = {Probabilistic-risk-assessment applications in the nuclear-power industry},
  Author                   = {Zamanali, J.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {SP361-SP364},
  Volume                   = {47},

  Abstract                 = {There has been a remarkable growth in the use of probabilistic risk assessment (PRA) methods in the US nuclear industry, both commercial and government. Use of PRA techniques has been expanded appreciably in the 1980s and 1990s as a result of the Individual Plant Examination (IPE) process. The IPE program calls for all existing nuclear utilities to use PRA methods to seek out plant vulnerabilities, and for all new applications for commercial nuclear power licenses to include full-scope PRA for design certification under 10CFR part 52. Effort must be spent to improve the consistency of PRA models, applications, evaluation approaches and processes among the nuclear utility industry. Standards are needed to guarantee proper application of PRA methods in risk-based regulation (RBR) in order to establish and maintain consistency in PRA applications. One standard application method that fits every aspect of RBR is difficult if not impossible. However, some of the PRA tasks or processes can be effectively standardized, e.g., level of detail, bounding limits on the failure rates of human recovery actions, and ways and means of data analysis. This paper discusses some historical background about the application of the PRA and identifies: appropriate efforts that the industry can spend to standardize the PRA applications in RBR; improvements in PRA techniques to achieve standardization; the benefits utilities would gain as a result of this harmony; and some of the limitations in areas which stubbornly resists standardization},
  Doi                      = {10.1109/24.740552},
  File                     = {Published version:Zamanali-1998-p361-364b.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {fission reactor accidents;human factors;nuclear power stations;probability;risk management;Individual Plant Examination;US nuclear industry;commercial nuclear power licenses;common failure cause;core damage frequency;data analysis;decision making process;design certification;failure rates;human recovery actions;human reliability assessment;loss-of-coolant accident;maintenance activities;nuclear-power industry;online maintenance;operation activities;probabilistic-risk-assessment applications;risk perspective;risk-based regulation;safety requirements;standardization;Decision making;Defense industry;Government;Humans;Industrial accidents;Licenses;Nuclear facility regulation;Performance gain;Risk management;Safety},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=740552&isnumber=15995},
  Zone                     = {2Q}
}

@Article{Zamanali-1998-p361-364c,
  Title                    = {Probabilistic-risk-assessment applications in the nuclear-power industry},
  Author                   = {Zamanali, J.},
  Journal                  = {Reliability, IEEE Transactions on},
  Year                     = {1998},

  Month                    = {Sep},
  Number                   = {3},
  Pages                    = {SP361-SP364},
  Volume                   = {47},

  Abstract                 = {There has been a remarkable growth in the use of probabilistic risk assessment (PRA) methods in the US nuclear industry, both commercial and government. Use of PRA techniques has been expanded appreciably in the 1980s and 1990s as a result of the Individual Plant Examination (IPE) process. The IPE program calls for all existing nuclear utilities to use PRA methods to seek out plant vulnerabilities, and for all new applications for commercial nuclear power licenses to include full-scope PRA for design certification under 10CFR part 52. Effort must be spent to improve the consistency of PRA models, applications, evaluation approaches and processes among the nuclear utility industry. Standards are needed to guarantee proper application of PRA methods in risk-based regulation (RBR) in order to establish and maintain consistency in PRA applications. One standard application method that fits every aspect of RBR is difficult if not impossible. However, some of the PRA tasks or processes can be effectively standardized, e.g., level of detail, bounding limits on the failure rates of human recovery actions, and ways and means of data analysis. This paper discusses some historical background about the application of the PRA and identifies: appropriate efforts that the industry can spend to standardize the PRA applications in RBR; improvements in PRA techniques to achieve standardization; the benefits utilities would gain as a result of this harmony; and some of the limitations in areas which stubbornly resists standardization},
  Doi                      = {10.1109/24.740552},
  File                     = {Published version:Zamanali-1998-p361-364c.pdf:PDF},
  Impactfactor             = {2.293},
  ISSN                     = {0018-9529},
  Keywords                 = {fission reactor accidents;human factors;nuclear power stations;probability;risk management;Individual Plant Examination;US nuclear industry;commercial nuclear power licenses;common failure cause;core damage frequency;data analysis;decision making process;design certification;failure rates;human recovery actions;human reliability assessment;loss-of-coolant accident;maintenance activities;nuclear-power industry;online maintenance;operation activities;probabilistic-risk-assessment applications;risk perspective;risk-based regulation;safety requirements;standardization;Decision making;Defense industry;Government;Humans;Industrial accidents;Licenses;Nuclear facility regulation;Performance gain;Risk management;Safety},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.17},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=740552&isnumber=15995},
  Zone                     = {2Q}
}

@Article{Zanero-2009-p70-74,
  Title                    = {Wireless Malware Propagation: A Reality Check},
  Author                   = {Zanero, S.},
  Journal                  = {Security Privacy, IEEE},
  Year                     = {2009},

  Month                    = {Sept},
  Number                   = {5},
  Pages                    = {70-74},
  Volume                   = {7},

  Abstract                 = {The paper discusses the data security on wireless communications. In recent years, several authors began to work on the concept of security attacks against wireless communication protocols- in particular, the propagation of malware through them. It was fun to design covert attack devices and evaluate the Bluetooth user population's exposure to them. However, the latest "developments" on these threats are stepping progressively away from reality and into an abstract, academic world of their own, something that might be just as fun, but that should be brought back into perspective when assessing the actual risks related to these scenarios. The author presents two distinct examples, the wireless LAN contagion and Bluetooth epidemics. The author concludes that wireless and mobile security, and, in particular, worm propagation over wireless networks, is an interesting and novel concept. It challenges and thrills, creating appealing newspaper headlines along the way. However, it must be sure to check our models against reality, and after predicting threats that failed to materialize, it must be able to understand where it went wrong.},
  Doi                      = {10.1109/MSP.2009.142},
  File                     = {Published version:Zanero-2009-p70-74.pdf:PDF},
  Impactfactor             = {0.962},
  ISSN                     = {1540-7993},
  Keywords                 = {Bluetooth;invasive software;telecommunication computing;wireless LAN;Bluetooth;data security;malware propagation;wireless LAN;wireless communications;Bluetooth;Communication system security;Computer worms;Data security;Predictive models;Wireless LAN;Wireless communication;Wireless networks;Wi-Fi;attack trends;attackers;bluetooth;malware},
  Level                    = {C},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5280138&isnumber=5280117},
  Zone                     = {4Q}
}

@InProceedings{Zhandong-2011-p998-1001,
  Title                    = {Bank-enterprise project risk assessment model based on the information entropy method},
  Author                   = {Xu Zhandong and Guotai Chi},
  Booktitle                = {Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC), 2011 2nd International Conference on},
  Year                     = {2011},
  Month                    = {Aug},
  Pages                    = {998-1001},

  Abstract                 = {Bank-enterprise project risk assessment model based on the information entropy method refer to evaluating bank-enterprise project risks by the available macro, industry and project information in the condition that commercial bank cannot master project financial information. Bank-enterprise project risk assessment has the general guidelines for all the commercial banks in condition that bank cannot master the specific financial information. The feature of paper: using the observable project risk factors including government risk R1, policy risk R2, industry risk R3, credit risk R4 and investment risk R5, we solve the problem of project risk Rp determination by determining the enterprise project risk factor weights wk according to divergent degree ratio of the single risk factor score to overall five risk factor score, which reflects the idea that the risk factor weight is bigger as the dispersion degree is greater.},
  Doi                      = {10.1109/AIMSEC.2011.6010675},
  File                     = {Published version:Zhandong-2011-p998-1001.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {banking;entropy;project management;risk management;bank-enterprise project risk assessment model;credit risk;government risk;industry risk;information entropy;investment risk;policy risk;project financial information;project risk factors;Government;Indexes;Industries;Investments;Mathematical model;Meteorology;Risk management;counterparty risk;project risk assessment;risk overlay},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6010675&isnumber=6009617},
  Zone                     = {-}
}

@InProceedings{Zhandong-2011-p998-1001a,
  Title                    = {Bank-enterprise project risk assessment model based on the information entropy method},
  Author                   = {Xu Zhandong and Guotai Chi},
  Booktitle                = {Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC), 2011 2nd International Conference on},
  Year                     = {2011},
  Month                    = {Aug},
  Pages                    = {998-1001},

  Abstract                 = {Bank-enterprise project risk assessment model based on the information entropy method refer to evaluating bank-enterprise project risks by the available macro, industry and project information in the condition that commercial bank cannot master project financial information. Bank-enterprise project risk assessment has the general guidelines for all the commercial banks in condition that bank cannot master the specific financial information. The feature of paper: using the observable project risk factors including government risk R1, policy risk R2, industry risk R3, credit risk R4 and investment risk R5, we solve the problem of project risk Rp determination by determining the enterprise project risk factor weights wk according to divergent degree ratio of the single risk factor score to overall five risk factor score, which reflects the idea that the risk factor weight is bigger as the dispersion degree is greater.},
  Doi                      = {10.1109/AIMSEC.2011.6010675},
  File                     = {Published version:Zhandong-2011-p998-1001a.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {banking;entropy;project management;risk management;bank-enterprise project risk assessment model;credit risk;government risk;industry risk;information entropy;investment risk;policy risk;project financial information;project risk factors;Government;Indexes;Industries;Investments;Mathematical model;Meteorology;Risk management;counterparty risk;project risk assessment;risk overlay},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6010675&isnumber=6009617},
  Zone                     = {-}
}

@Article{Zhang2013,
  Title                    = {Incorporation of formal safety assessment and Bayesian network in navigational risk estimation of the Yangtze River },
  Author                   = {D. Zhang and X.P. Yan and Z.L. Yang and A. Wall and J. Wang},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {93 - 105},
  Volume                   = {118},

  Abstract                 = {Abstract Formal safety assessment (FSA), as a structured and systematic risk evaluation methodology, has been increasingly and broadly used in the shipping industry around the world. Concerns have been raised as to navigational safety of the Yangtze River, China's largest and the world's busiest inland waterway. Over the last few decades, the throughput of ships in the Yangtze River has increased rapidly due to the national development of the Middle and Western parts of China. Accidents such as collisions, groundings, contacts, oil-spills and fires occur repeatedly, often causing serious consequences. In order to improve the navigational safety in the Yangtze River, this paper estimates the navigational risk of the Yangtze River using the \{FSA\} concept and a Bayesian network (BN) technique. The navigational risk model is established by considering both probability and consequences of accidents with respect to a risk matrix method, followed by a scenario analysis to demonstrate the application of the proposed model. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2013.04.006},
  File                     = {Published version:Zhang-2013-p93-105.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Maritime safety},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832013001014},
  Zone                     = {2Q}
}

@Article{Zhang-2013-p93-105,
  Title                    = {Incorporation of formal safety assessment and Bayesian network in navigational risk estimation of the Yangtze River },
  Author                   = {D. Zhang and X.P. Yan and Z.L. Yang and A. Wall and J. Wang},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {93 - 105},
  Volume                   = {118},

  Abstract                 = {Abstract Formal safety assessment (FSA), as a structured and systematic risk evaluation methodology, has been increasingly and broadly used in the shipping industry around the world. Concerns have been raised as to navigational safety of the Yangtze River, China's largest and the world's busiest inland waterway. Over the last few decades, the throughput of ships in the Yangtze River has increased rapidly due to the national development of the Middle and Western parts of China. Accidents such as collisions, groundings, contacts, oil-spills and fires occur repeatedly, often causing serious consequences. In order to improve the navigational safety in the Yangtze River, this paper estimates the navigational risk of the Yangtze River using the \{FSA\} concept and a Bayesian network (BN) technique. The navigational risk model is established by considering both probability and consequences of accidents with respect to a risk matrix method, followed by a scenario analysis to demonstrate the application of the proposed model. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2013.04.006},
  File                     = {Published version:Zhang-2013-p93-105.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Maritime safety},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832013001014},
  Zone                     = {2Q}
}

@InProceedings{Zhang-2011-p648-651,
  Title                    = {Empirical study and risk assessment on ready mixed concrete industry},
  Author                   = {Junling Zhang},
  Booktitle                = {Business Management and Electronic Information (BMEI), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {May},
  Pages                    = {648-651},
  Volume                   = {1},

  Abstract                 = {As a kind of special building material industry, the ready mixed concrete industry will confront with many risks such as the policy risk, the resource risk and the market risk and so on in the process of its investment and construction activity. The umpty risks which the concrete industry may confront with are analyzed in details first and its index system about industry risks is established based on the theory of Analytic Hierarchy Process (AHP). The index system includes three layers. They are the aim layer, the rule layer and the index layer. And then the evaluation model about investment risks is built on the basis of the former. By using an investment project about the ready mixed concrete in Tianjin as an example, the model is applied and a preferable conclusion is drawn at last.},
  Doi                      = {10.1109/ICBMEI.2011.5917019},
  File                     = {Zhang-2011-p648-651.pdf:Zhang-2011-p648-651.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {cement industry;decision making;investment;market opportunities;risk management;analytic hierarchy process;building material industry;investment;market risk;policy risk;ready mixed concrete industry;risk assessment;Concrete;Indexes;Investments;Production;index system;ready mixed concrete industry;risk;risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5917019&isnumber=5916840},
  Zone                     = {-}
}

@Article{Zhang-2002-p72-72,
  Title                    = {A Bayesian Approach for Short-Term Transmission Line Thermal Overload Risk Assessment},
  Author                   = {Zhang, J. and Pu, J. and McCalley, J. and Stern, H. and Gallus, W.},
  Journal                  = {Power Engineering Review, IEEE},
  Year                     = {2002},

  Month                    = {May},
  Number                   = {5},
  Pages                    = {72-72},
  Volume                   = {22},

  Doi                      = {10.1109/MPER.2002.4312239},
  File                     = {Published version:Zhang-2002-p72-72.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {0272-1724},
  Keywords                 = {Bayesian methods;Conductors;Power system modeling;Power system simulation;Power transmission lines;Risk management;Thermal conductivity;Thermal stresses;Transmission lines;Weather forecasting;Bayesian analysis;Markov chain Monte;Security assessment;transmission line thermal overload risk assessment},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4312239&isnumber=21570},
  Zone                     = {-}
}

@Article{Zhang-2002-p770-778,
  Title                    = {A Bayesian approach for short-term transmission line thermal overload risk assessment},
  Author                   = {Jun Zhang and Jian Pu and McCalley, J.D. and Stern, H. and Gallus, W.A., Jr.},
  Journal                  = {Power Delivery, IEEE Transactions on},
  Year                     = {2002},

  Month                    = {Jul},
  Number                   = {3},
  Pages                    = {770-778},
  Volume                   = {17},

  Doi                      = {10.1109/TPWRD.2002.1022802},
  File                     = {Published version:Zhang-2002-p770-778.pdf:PDF},
  Impactfactor             = {1.519},
  ISSN                     = {0885-8977},
  Keywords                 = {Bayes methods;Monte Carlo methods;power system security;power transmission lines;time series;Bayesian time series models;Markov Chain Monte Carlo;on-line conductor thermal overload risk assessment method;on-line decision making;power system operating conditions;predicted risk;security assessment;stressed operational environment;thermal overload risk estimation;transmission lines;weather conditions modeling;Bayesian methods;Conductors;Monte Carlo methods;Power system modeling;Power system simulation;Power transmission lines;Risk management;Thermal conductivity;Transmission lines;Weather forecasting},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1022802&isnumber=22006},
  Zone                     = {3Q}
}

@InProceedings{Zhang-2010-p5-544,
  Title                    = {Reconfigurable security protection system based on NetFPGA and embedded soft-core technology},
  Author                   = {Kai Zhang and Xiaoming Ding and Ke Xiong and Yunting Zhao},
  Booktitle                = {Computer Design and Applications (ICCDA), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {June},
  Pages                    = {V5-540-V5-544},
  Volume                   = {5},

  Abstract                 = {There are many problems in traditional network security and PC security protection systems, such as insecurity, inflexibility and so on, whether they are implemented by software or hardware. To solve the problems, this paper presents a reconfigurable security protection system on the basis of NetFPGA platform and embedded soft-core technology. This system consists of two subsystems, one is implemented on the NetFPGA for the subnet protection and the other is implemented on DE2 board to protect the terminal, in which NetFPGA is used to achieve packet filtering in hardware, immunity from ARP attacks in hardware, flow monitoring and transmitting with hardware acceleration, and DE2 board is used to realize AES/DES encryption modules in hardware. Moreover, the USB virus can be isolated effectively by the subsystem designed on DE2. To further enhance performance, security and flexibility of our system, including both the subnet protection subsystem and the terminal protection subsystem, we introduce two types of remote reconfigurable design method, by which administrator is able to reconfigure the two subsystems for both the software and the hardware logical circuits via any authorized devices. Extensive experiments show that all the functions of the blocks of the designed system are valid and the designed system is feasible.},
  Doi                      = {10.1109/ICCDA.2010.5541121},
  File                     = {Published version:Zhang-2010-p5-544.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {computer network security;computer viruses;cryptography;field programmable gate arrays;AES/DES encryption modules;ARP attacks;DE2 board;NetFPGA;PC security protection system;USB virus;embedded soft-core technology;flow monitoring;hardware acceleration;network security;packet filtering;reconfigurable security protection system;remote reconfigurable design method;subnet protection subsystem;terminal protection subsystem;Acceleration;Cryptography;Design methodology;Filtering;Hardware;Immune system;Monitoring;Protection;Security;Universal Serial Bus;DE2;NetFPGA;network security;remote reconfiguration;security protection system},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5541121&isnumber=5540818},
  Zone                     = {-}
}

@Article{Zhang-2014-p29-39,
  Title                    = {Bayesian-network-based safety risk analysis in construction projects },
  Author                   = {Limao Zhang and Xianguo Wu and Miroslaw J. Skibniewski and Jingbing Zhong and Yujie Lu},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {29 - 39},
  Volume                   = {131},

  Abstract                 = {Abstract This paper presents a systemic decision support approach for safety risk analysis under uncertainty in tunnel construction. Fuzzy Bayesian Networks (FBN) is used to investigate causal relationships between tunnel-induced damage and its influential variables based upon the risk/hazard mechanism analysis. Aiming to overcome limitations on the current probability estimation, an expert confidence indicator is proposed to ensure the reliability of the surveyed data for fuzzy probability assessment of basic risk factors. A detailed fuzzy-based inference procedure is developed, which has a capacity of implementing deductive reasoning, sensitivity analysis and abductive reasoning. The 鈥?蟽 criterion鈥?is adopted to calculate the characteristic values of a triangular fuzzy number in the probability fuzzification process, and the 伪-weighted valuation method is adopted for defuzzification. The construction safety analysis progress is extended to the entire life cycle of risk-prone events, including the pre-accident, during-construction continuous and post-accident control. A typical hazard concerning the tunnel leakage in the construction of Wuhan Yangtze Metro Tunnel in China is presented as a case study, in order to verify the applicability of the proposed approach. The results demonstrate the feasibility of the proposed approach and its application potential. A comparison of advantages and disadvantages between \{FBN\} and fuzzy fault tree analysis (FFTA) as risk analysis tools is also conducted. The proposed approach can be used to provide guidelines for safety analysis and management in construction projects, and thus increase the likelihood of a successful project in a complex environment. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2014.06.006},
  File                     = {Published version:Zhang-2014-p29-39.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Construction safety},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.24},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832014001288},
  Zone                     = {2Q}
}

@Article{Zhang-2014-p103-112,
  Title                    = {A mixture Weibull proportional hazard model for mechanical system failure prediction utilising lifetime and monitoring data },
  Author                   = {Qing Zhang and Cheng Hua and Guanghua Xu},
  Journal                  = {Mechanical Systems and Signal Processing },
  Year                     = {2014},
  Number                   = {1–2},
  Pages                    = {103 - 112},
  Volume                   = {43},

  Abstract                 = {Abstract As mechanical systems increase in complexity, it is becoming more and more common to observe multiple failure modes. The system failure can be regarded as the result of interaction and competition between different failure modes. It is therefore necessary to combine multiple failure modes when analysing the failure of an overall system. In this paper, a mixture Weibull proportional hazard model (MWPHM) is proposed to predict the failure of a mechanical system with multiple failure modes. The mixed model parameters are estimated by combining historical lifetime and monitoring data of all failure modes. In addition, the system failure probability density is obtained by proportionally mixing the failure probability density of multiple failure modes. Monitoring data are input into the \{MWPHM\} to estimate the system reliability and predict the system failure time. A simulated sample set is used to verify the ability of the \{MWPHM\} to model multiple failure modes. Finally, the \{MWPHM\} and the traditional Weibull proportional hazard model (WPHM) are applied to a high-pressure water descaling pump, which has two failure modes: sealing ring wear and thrust bearing damage. Results show that the \{MWPHM\} is greatly superior in system failure prediction to the WPHM. },
  Doi                      = {http://dx.doi.org/10.1016/j.ymssp.2013.10.013},
  File                     = {Published version:Zhang-2014-p103-112.pdf:PDF},
  Impactfactor             = {1.913},
  ISSN                     = {0888-3270},
  Keywords                 = {Failure prediction},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.30},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0888327013005165},
  Zone                     = {2Q}
}

@InProceedings{Zhang-2010-p1-5,
  Title                    = {The design of information security protection framework to support Smart Grid},
  Author                   = {Tao Zhang and Weimin Lin and Yufei Wang and Song Deng and Congcong Shi and Lu Chen},
  Booktitle                = {Power System Technology (POWERCON), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {Oct},
  Pages                    = {1-5},

  Abstract                 = {With the unified and strong smart grid construction by State Grid Corporation of China, information security protection architecture is facing new challenges. The characteristics of smart grid such as informationization, automation, and interaction, have enhanced the two-way interaction between grid and consumers. The design method of information security protection architecture in U.S. Smart Grid and new information security protection requirements of China Smart Gird and new information security risks were proposed and analyzed. An information security protection model and overall information security protection strategy were proposed considering the characteristics of China Smart Grid and the new information security protection requirements. Based on this model, the information security protection framework was designed to support the information security protection in smart grid. The proposed framework consists of four parts: security governance, security management, security maintenance, security technology. Finally, a methodology for the implementation of information security protection framework was given, which guides the business systems in every aspect of smart grid to implement information security protection works from the points of technology, management, and operation.},
  Doi                      = {10.1109/POWERCON.2010.5666681},
  File                     = {Published version:Zhang-2010-p1-5.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {power system management;power system protection;power system security;smart power grids;information security protection;security governance;security maintenance;security management;security technology;support smart grid;Cryptography;Fires;Information security;Inspection;Lead;Optimization;Implementation Methodology;Information Security;Protection Model;Smart Grid},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.12},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5666681&isnumber=5666013},
  Zone                     = {-}
}

@Article{Zhang-2013-p79-85,
  Title                    = {Mechanism analysis and risk assessment of escalation scenario in chemical industry zones },
  Author                   = {Xin-mei Zhang and Chen Chen},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2013},
  Number                   = {1–2},
  Pages                    = {79 - 85},
  Volume                   = {91},

  Abstract                 = {Many major hazard installations (MHIs) are located in chemical industry zones and escalation effect may be triggered when the fire or explosion occurs on a MHI. To investigate the mechanism of the accident escalation, a systematic quantitative assessment methodology is proposed by the considering the feature and uncertainty of the escalation scenario. The main accident energy carriers of the escalation are heat radiation, overpressure of blast and fragments. The escalation probability, joint influence of the three energy carriers and risk characterization of the accident scenarios are carried out. By the new methodology, the escalation scenario in chemical industry zones can be analyzed and the risk escalation morphology is demonstrated by the simulation software. The visualized risk cloud figure gives a supplementary way to prevent the escalation scenario in chemical industry zones planning.},
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2012.02.003},
  File                     = {Published version:Zhang-2013-p79-85.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Escalation scenario},
  Level                    = {B},
  Owner                    = {Qiqi},
  Review                   = {B，讲的是化学工业区域扩张的风险评估
MHI， major hazard installations，重大危险源},
  Timestamp                = {2014.11.12},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957582012000213},
  Zone                     = {3Q}
}

@Article{Zhang-2013-p79-85a,
  Title                    = {Mechanism analysis and risk assessment of escalation scenario in chemical industry zones },
  Author                   = {Xin-mei Zhang and Chen Chen},
  Journal                  = {Process Safety and Environmental Protection },
  Year                     = {2013},
  Number                   = {1–2},
  Pages                    = {79 - 85},
  Volume                   = {91},

  Abstract                 = {Many major hazard installations (MHIs) are located in chemical industry zones and escalation effect may be triggered when the fire or explosion occurs on a MHI. To investigate the mechanism of the accident escalation, a systematic quantitative assessment methodology is proposed by the considering the feature and uncertainty of the escalation scenario. The main accident energy carriers of the escalation are heat radiation, overpressure of blast and fragments. The escalation probability, joint influence of the three energy carriers and risk characterization of the accident scenarios are carried out. By the new methodology, the escalation scenario in chemical industry zones can be analyzed and the risk escalation morphology is demonstrated by the simulation software. The visualized risk cloud figure gives a supplementary way to prevent the escalation scenario in chemical industry zones planning. },
  Doi                      = {http://dx.doi.org/10.1016/j.psep.2012.02.003},
  File                     = {Published version:Zhang-2013-p79-85a.pdf:PDF},
  Impactfactor             = {1.495},
  ISSN                     = {0957-5820},
  Keywords                 = {Escalation scenario},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957582012000213},
  Zone                     = {3Q}
}

@Article{Zhang-2001-p312-323,
  Title                    = {Derived fuzzy knowledge model for estimating the depth of anesthesia},
  Author                   = {Xu-Sheng Zhang and Roy, Rob J.},
  Journal                  = {Biomedical Engineering, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {March},
  Number                   = {3},
  Pages                    = {312-323},
  Volume                   = {48},

  Doi                      = {10.1109/10.914794},
  File                     = {Published version:Zhang-2001-p312-323.pdf:PDF},
  Impactfactor             = {2.348},
  ISSN                     = {0018-9294},
  Keywords                 = {adaptive estimation;adaptive signal processing;electroencephalography;entropy;fuzzy logic;medical signal processing;patient monitoring;physiological models;sleep;surgery;EEG;anesthesia depth estimation;approximate entropy;asleep state;awake state;derived fuzzy knowledge model;fuzzy if-then rules;generalization ability;halothane;isoflurane;propofol;spectral entropy;Adaptive systems;Anesthesia;Anesthetic drugs;Brain modeling;Direction of arrival estimation;Dogs;Entropy;Fuzzy neural networks;Fuzzy systems;Monitoring;Algorithms;Anesthesia;Animals;Computer Simulation;Dogs;Electroencephalography;Fuzzy Logic;Halothane;Isoflurane;Models, Biological;Monitoring, Physiologic;Movement;Neural Networks (Computer);Nonlinear Dynamics;Online Systems;Predictive Value of Tests;Propofol;Reproducibility of Results;Signal Processing, Computer-Assisted},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=914794&isnumber=19757},
  Zone                     = {3Q}
}

@InProceedings{Zhang-2008-p25-30,
  Title                    = {Cooperation Forensic Computing Research},
  Author                   = {Youdong Zhang},
  Booktitle                = {Knowledge Discovery and Data Mining, 2008. WKDD 2008. First International Workshop on},
  Year                     = {2008},
  Month                    = {Jan},
  Pages                    = {25-30},

  Abstract                 = {The network forensic computing is faced with the question of the complex network intrusion analyses. So a new concept of cooperation forensic computing is defined. Through to extend the theory of function dependency, a new method called probability function dependency relationships is proposed. Combined it with the Bayesian network and K2 algorithm, the network forensic computing algorithm called CFA is proposed. For the complex network attack, CFA is able to synthesize the various forensic data resource to reappearance the crime scenario intuitionally and realize the network forensic analysis effectively.},
  Doi                      = {10.1109/WKDD.2008.128},
  File                     = {Published version:Zhang-2008-p25-30.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Bayes methods;probability;security of data;telecommunication security;Bayesian network;K2 algorithm;cooperation forensic computing;network forensic computing;network intrusion analysis;probability function dependency relationships;Algorithm design and analysis;Bayesian methods;Complex networks;Computer networks;Data engineering;Data mining;Data preprocessing;Forensics;Knowledge engineering;Network synthesis},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4470343&isnumber=4470329},
  Zone                     = {-}
}

@Article{Zhang-2014-p161-168,
  Title                    = {Development of a mitigation strategy against the cascading propagation of risk in R\&amp;D network },
  Author                   = {Yanlu Zhang and Naiding Yang},
  Journal                  = {Safety Science },
  Year                     = {2014},
  Number                   = {0},
  Pages                    = {161 - 168},
  Volume                   = {68},

  Abstract                 = {Abstract Considering severe consequences caused by cascading propagation of risk to R&amp;D network, it is essential to develop a mitigation strategy against it for keeping network safety. Firstly, we propose the generation algorithm of R&amp;D network based on two rules of preferential attachment. Then we develop a mitigation strategy on restoring failed firms against cascading propagation of risk, where three types of restoration methods (i.e. random restoration, high-degree restoration and high-capacity restoration) and restoration mechanism are described. Finally, we explore the effects of mitigation strategy under different values some critical parameters through numerical simulation. The simulation results show that all three restoration methods can generally enhance the robustness of R&amp;D network, which is increasingly improved with the increasing homogeneity of firms’ capacities distribution. High-capacity restoration is the most efficient one on mitigating cascading propagation of risk for any degree of capacity distribution, any proportion of restored firms and any kind of attack. But the gap between high-capacity restoration and any other restoration in the efficiency of mitigating cascading propagation of risk is increasingly less with the increasing proportion of restored firms. For any given restoration method, random attack has the least impact on the robustness of R&amp;D network, whereas the other attacks have almost the similar impacts. This research work will provide a useful theoretical basis on building the optimal risk-mitigation strategy to keep the safety of R&amp;D network in the real world. },
  Doi                      = {http://dx.doi.org/10.1016/j.ssci.2014.04.006},
  File                     = {Published version:Zhang-2014-p161-168.pdf:PDF},
  Impactfactor             = {1.359},
  ISSN                     = {0925-7535},
  Keywords                 = {R&amp;D network safety},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0925753514000915},
  Zone                     = {3Q}
}

@Article{Zhang-2013-p1-8,
  Title                    = {Research on robustness of R\&amp;D network under cascading propagation of risk with gray attack information },
  Author                   = {Yanlu Zhang and Naiding Yang},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2013},
  Number                   = {0},
  Pages                    = {1 - 8},
  Volume                   = {117},

  Abstract                 = {Abstract Facing the cascading propagation phenomenon of risk in R&amp;D network and the imprecision of attack information, this paper builds the cascading propagation model of risk with gray attack information. In this model, gray attack information described by node degree is measured by negative and positive deviations, and the critical threshold of resisting risk is also proposed as a new indicator of robustness of R&amp;D network. Then the paper analyzes the robustness of R&amp;D network under cascading propagation of risk with gray attack information through numerical simulation. The results show that R&amp;D network has the strongest robustness under random attack, but has the weakest one under intentional attack; robustness of R&amp;D network increases with the increase of deviation from attack information, which becomes increasingly significant when all enterprises' capacities distribution is heterogeneous; robustness of R&amp;D network under one attack decreases with the increasing heterogeneity of all enterprises' capacities distribution; robustness of R&amp;D network is more sensitive to the negative deviation than to the positive deviation from attack information. This research work will provide a theoretical basis for preventing and controlling cascading propagation in R&amp;D network in the future. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2013.03.009},
  File                     = {Published version:Zhang-2013-p1-8.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {R&amp;D network},
  Level                    = {A},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832013000793},
  Zone                     = {2Q}
}

@Article{Zhang-2013-p14-18,
  Title                    = {The influence of pH on thermal fatigue crack propagation behavior of WC–8Co cemented carbide },
  Author                   = {Zhongjian Zhang and Tao Xu and Wen Peng and Ding Chen and Yong Jiang and Yuanyuan Shi},
  Journal                  = {International Journal of Refractory Metals and Hard Materials },
  Year                     = {2013},
  Note                     = {Special Issue on 2012 Zhuzhou International Conference on Cemented Carbides },
  Number                   = {0},
  Pages                    = {14 - 18},
  Volume                   = {40},

  Abstract                 = {Abstract The notch quench method was used to study the influence of pH values on thermal fatigue crack propagation behavior of WC–8Co cemented carbide. The pH values of the aqueous solutions were varied between 5.2, 7.4 and 8.8. The results show that the corrosive environment has a strong influence on the thermal fatigue crack propagation behavior of the materials for testing. Acidic and alkali environment can reduce the thermal fatigue crack initial life and increase the crack propagation rate. Thermal attack results in a significant loss of hardness and the effect is even visible in an acidic environment. In all cases, only \{WO3\} and CoWO4 phases are present in the oxides. The mode of crack propagation requires the simultaneous actions of thermal attack and corrosion. },
  Doi                      = {http://dx.doi.org/10.1016/j.ijrmhm.2013.01.019},
  File                     = {Published version:Zhang-2013-p14-18.pdf:PDF},
  Impactfactor             = {1.858},
  ISSN                     = {0263-4368},
  Keywords                 = {Cemented carbide},
  Level                    = {B},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.19},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S026343681300036X},
  Zone                     = {3Q}
}

@InProceedings{Zhao-2013-p249-254,
  Title                    = {Study of fault diagnosis method based on fuzzy Bayesian network and application in CTCS-3 train control system},
  Author                   = {Jingjing Zhao and Wei Zheng},
  Booktitle                = {Intelligent Rail Transportation (ICIRT), 2013 IEEE International Conference on},
  Year                     = {2013},
  Month                    = {Aug},
  Pages                    = {249-254},

  Abstract                 = {The fault diagnosis approach based-on Bayesian network is frequently used in fault diagnosis field, has better completeness and explanation facility, but it has some disadvantages due to lack of the quantitative data information, and it is difficult to applied in complicated system, China train control system lever-3 (CTCS-3) is a complicated system with high security requirement, so a Bayesian fuzzy inference nets real-time internal fault diagnostic system for CTCS-3 train control system is proposed. The membership functions and symptom-fault mapping relationship for CTCS-3 fault diagnosis system are obtained from pre-measured experimental data as well as experts' diagnostic experience/knowledge to distinguish the effect of true fault from various factors. The cores dangerous of train control system are supervision and protect against exceedance of safe speed distance and driver exceeds safe speed distance, according the two safety links, first of all, set up fault tree; and then convert to Bayesian network, finally the fuzzy Bayesian network diagnosis arithmetic of fault diagnosis system with accuracy is designed and presented. The validity and effectiveness of the proposed approach is witnessed clearly from the testing results obtained. In the last part of the paper, the fault diagnosis system of CTCS-3 is established by using the fuzzy inference algorithm.},
  Doi                      = {10.1109/ICIRT.2013.6696302},
  File                     = {Published version:Zhao-2013-p249-254.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Bayes methods;fault diagnosis;fault trees;fuzzy reasoning;mechanical engineering computing;rail traffic control;railway safety;real-time systems;Bayesian fuzzy inference nets;CTCS-3 fault diagnosis system;CTCS-3 train control system;China train control system lever-3;expert diagnostic experience;expert diagnostic knowledge;fault tree;fuzzy Bayesian network diagnosis arithmetic;membership functions;premeasured experimental data;real-time internal fault diagnostic system;safe speed distance;security requirement;symptom-fault mapping relationship;Bayes methods;Control systems;Fault diagnosis;Fault trees;Inference algorithms;Safety;Security;Bayesian network;CTCS-3 fault diagnosis system;fault tree;fuzzy inference;membership function},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6696302&isnumber=6696257},
  Zone                     = {-}
}

@InProceedings{Zhao-2011-p573-578,
  Title                    = {An event-chain risk assessment model based on definition evolution in safety criterions},
  Author                   = {Nuo Zhao and Tingdi Zhao},
  Booktitle                = {Reliability, Maintainability and Safety (ICRMS), 2011 9th International Conference on},
  Year                     = {2011},
  Month                    = {June},
  Pages                    = {573-578},

  Abstract                 = {Risk assessment is the core work in system safety. However, when system safety originated, there remains controversial in conception of hazard, risk, and mishap. This could be well reflected in criterions. As we know, in system safety field, a series of MIL-STD-882 versions are the most widely used criterions. And the standards of ANSI/GEIA-STD-0010 series have been paid more attention recent years. Definitions of core concepts in system safety have been changing and evolving in different versions of MIL-STD-882(X) and ANSI/GEIA-STD-0010. This controversy is the nature problem of risk analysis. And the core of this controversy, especially in dynamic operational process assessment, is fixing the range of risk definition and formalizing the risk modeling language. Therefore, a fixed range and unified modeling language is required in system safety, which makes it possible to help safety engineers predominate the risk level and provide risk control measures. This paper purposes to tap the characters of concepts and category of risk assessment, then to provide a formal model of risk assessment containing both the static condition: hazard and the mishap dynamic process based on event-chain.},
  Doi                      = {10.1109/ICRMS.2011.5979333},
  File                     = {Published version:Zhao-2011-p573-578.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Unified Modeling Language;risk management;safety;ANSI/GEIA-STD-0010 series;MIL-STD-882 versions;definition evolution;dynamic operational process assessment;event-chain risk assessment model;formal model;mishap dynamic process;risk analysis;risk definition;risk modeling language;safety criterions;system safety;unified modeling language;Computational modeling;Hazards;Humans;Mathematical model;Military standards;Risk management;definition evolution;event-chain risk assessment model;risk assessment;safety criterions;system safety},
  Level                    = {-},
  Owner                    = {Qiqi},
  Review                   = {~，事件链},
  Timestamp                = {2015.01.14},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5979333&isnumber=5979226},
  Zone                     = {-}
}

@InProceedings{Zhao-2011-p573-578a,
  Title                    = {An event-chain risk assessment model based on definition evolution in safety criterions},
  Author                   = {Nuo Zhao and Tingdi Zhao},
  Booktitle                = {Reliability, Maintainability and Safety (ICRMS), 2011 9th International Conference on},
  Year                     = {2011},
  Month                    = {June},
  Pages                    = {573-578},

  Abstract                 = {Risk assessment is the core work in system safety. However, when system safety originated, there remains controversial in conception of hazard, risk, and mishap. This could be well reflected in criterions. As we know, in system safety field, a series of MIL-STD-882 versions are the most widely used criterions. And the standards of ANSI/GEIA-STD-0010 series have been paid more attention recent years. Definitions of core concepts in system safety have been changing and evolving in different versions of MIL-STD-882(X) and ANSI/GEIA-STD-0010. This controversy is the nature problem of risk analysis. And the core of this controversy, especially in dynamic operational process assessment, is fixing the range of risk definition and formalizing the risk modeling language. Therefore, a fixed range and unified modeling language is required in system safety, which makes it possible to help safety engineers predominate the risk level and provide risk control measures. This paper purposes to tap the characters of concepts and category of risk assessment, then to provide a formal model of risk assessment containing both the static condition: hazard and the mishap dynamic process based on event-chain.},
  Doi                      = {10.1109/ICRMS.2011.5979333},
  File                     = {Published version:Zhao-2011-p573-578a.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Unified Modeling Language;risk management;safety;ANSI/GEIA-STD-0010 series;MIL-STD-882 versions;definition evolution;dynamic operational process assessment;event-chain risk assessment model;formal model;mishap dynamic process;risk analysis;risk definition;risk modeling language;safety criterions;system safety;unified modeling language;Computational modeling;Hazards;Humans;Mathematical model;Military standards;Risk management;definition evolution;event-chain risk assessment model;risk assessment;safety criterions;system safety},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5979333&isnumber=5979226},
  Zone                     = {-}
}

@InProceedings{Zhao-2014-p1-6,
  Title                    = {A synthetic risk assessment model based on AHP},
  Author                   = {Yuan Zhao and Jian Jiao and Tingdi Zhao},
  Booktitle                = {Reliability and Maintainability Symposium (RAMS), 2014 Annual},
  Year                     = {2014},
  Month                    = {Jan},
  Pages                    = {1-6},

  Abstract                 = {As an important part of system safety, risk assessment is an evaluation process for identified hazards. The corresponding risks should be evaluated to support decision-making. The risk, including the severity and probability of the potential accident, should be evaluated using an integrated index to facilitate risk ranking. In this study, the risk assessment is executed though the analytic hierarchy process (AHP) theory which allows synthesizing the severity and probability in evaluating a hazard. Using the AHP theory, the impact weights of the severity and probability can be determined by a pair-wise comparison matrix during which scales are used to describe their relative contributions to the risk. The severity and probability grades are defined to evaluate the severity and probability. The corresponding trapezoidal fuzzy numbers of these grades are also introduced. Finally, the risk of hazard is calculated using the impact weights of severity and probability. Comparing with the traditional risk assessment, the proposed risk assessment model considers the different impact weights of severity and probability to risk level and provides a more reasonable risk assessment result. An example of risk assessment in aviation industry is introduced and the comparison of the results between the proposed model and the traditional risk assessment is also made to demonstrate the engineering practicability and effectiveness of the new model in the risk assessment process.},
  Doi                      = {10.1109/RAMS.2014.6798469},
  File                     = {Published version:Zhao-2014-p1-6.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {accident prevention;analytic hierarchy process;fuzzy set theory;occupational safety;risk management;AHP;analytic hierarchy processing;aviation industry;hazard evaluation;hazards identification;potential accident probability;potential accident severity;probability weights;risk ranking;severity weights;synthetic risk assessment model;system safety;trapezoidal fuzzy numbers;Educational institutions;Hazards;Mathematical model;Probability;Random access memory;Risk management;analytic hierarchy process;risk;risk assessment;trapezoidal fuzzy number},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6798469&isnumber=6798433},
  Zone                     = {-}
}

@InProceedings{Zhao-2014-p1-6a,
  Title                    = {A synthetic risk assessment model based on AHP},
  Author                   = {Yuan Zhao and Jian Jiao and Tingdi Zhao},
  Booktitle                = {Reliability and Maintainability Symposium (RAMS), 2014 Annual},
  Year                     = {2014},
  Month                    = {Jan},
  Pages                    = {1-6},

  Abstract                 = {As an important part of system safety, risk assessment is an evaluation process for identified hazards. The corresponding risks should be evaluated to support decision-making. The risk, including the severity and probability of the potential accident, should be evaluated using an integrated index to facilitate risk ranking. In this study, the risk assessment is executed though the analytic hierarchy process (AHP) theory which allows synthesizing the severity and probability in evaluating a hazard. Using the AHP theory, the impact weights of the severity and probability can be determined by a pair-wise comparison matrix during which scales are used to describe their relative contributions to the risk. The severity and probability grades are defined to evaluate the severity and probability. The corresponding trapezoidal fuzzy numbers of these grades are also introduced. Finally, the risk of hazard is calculated using the impact weights of severity and probability. Comparing with the traditional risk assessment, the proposed risk assessment model considers the different impact weights of severity and probability to risk level and provides a more reasonable risk assessment result. An example of risk assessment in aviation industry is introduced and the comparison of the results between the proposed model and the traditional risk assessment is also made to demonstrate the engineering practicability and effectiveness of the new model in the risk assessment process.},
  Doi                      = {10.1109/RAMS.2014.6798469},
  File                     = {Published version:Zhao-2014-p1-6a.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {accident prevention;analytic hierarchy process;fuzzy set theory;occupational safety;risk management;AHP;analytic hierarchy processing;aviation industry;hazard evaluation;hazards identification;potential accident probability;potential accident severity;probability weights;risk ranking;severity weights;synthetic risk assessment model;system safety;trapezoidal fuzzy numbers;Educational institutions;Hazards;Mathematical model;Probability;Random access memory;Risk management;analytic hierarchy process;risk;risk assessment;trapezoidal fuzzy number},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.03.15},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6798469&isnumber=6798433},
  Zone                     = {-}
}

@InProceedings{Zhaoyang-2011-p1-4,
  Title                    = {Failure detection of dual-redundancy BLDC motor based on wavelet transform},
  Author                   = {Fu Zhaoyang and Liu Jinglin},
  Booktitle                = {Electrical Machines and Systems (ICEMS), 2011 International Conference on},
  Year                     = {2011},
  Month                    = {Aug},
  Pages                    = {1-4},

  Abstract                 = {In order to improve the reliability of the system, a dual-redundancy high-voltage brushless DC motor is designed. According to the structural features of dual-redundancy brushless DC motor, the mathematical model is built up. Methods of motor fault detection are studied. The fault signal is analysized by fourier transform. For the deficiency of Fourier transform, a fault detection using wavelet transform method is proposed. The current is determined to the fault detection signal based on the motor fault tree. The coif 5 is selected as the wavelet basis function. Through the analysis of motor failures, the characteristics of the winding open circuit, a phase with Hall for high and low are obtained by the coif 5 wavelet function. The fault feature vectors are obtained by the layer 2 decomposition coefficients.},
  Doi                      = {10.1109/ICEMS.2011.6073909},
  File                     = {Published version:Zhaoyang-2011-p1-4.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {Fourier transforms;brushless DC motors;fault diagnosis;fault trees;machine windings;vectors;wavelet transforms;Fourier transform;coif5 wavelet function;dual-redundancy high-voltage BLDC motor;dual-redundancy high-voltage brushless DC motor;fault feature vector;motor failure detection;motor fault detection signal analysis;motor fault tree;system reliability improvement;wavelet basis function;wavelet transform method;winding open circuit;Brushless DC motors;Circuit faults;Permanent magnet motors;Torque;Windings},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.19},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6073909&isnumber=6073288},
  Zone                     = {-}
}

@InProceedings{Zheng-2010-p202-206,
  Title                    = {Insurance IT outsourcing risk assessment modeling and empirical study},
  Author                   = {Qin Zheng and Huang Na},
  Booktitle                = {Information and Financial Engineering (ICIFE), 2010 2nd IEEE International Conference on},
  Year                     = {2010},
  Month                    = {Sept},
  Pages                    = {202-206},

  Abstract                 = {Insurance is one of the highest investments by the VIPs, which constitute the main source of profits in the financial industry. With the rapid growth of the insurance informatization, IT outsourcing service is growing up increasingly in the insurance informatization construction of position. In order to obtain better risk assessment, combing with the mechanism research of insurance information technology (IT) outsourcing, this paper develop a Framework Model of insurance IT outsourcing risk assessment based on the Control Objectives for Information and related Technology (COBIT) framework structure and the traditional model of IT outsourcing risk assessment. The Interview and Delphi Methods is to prove the high integration and Acquisition of available of the risk factors of the framework model, the Risk Matrix and Borda Rank Method is to prove the adaptability of the risk category and risk dimension. Finally, the result indicates that the structure of risk category, risk dimension and risk factor can do effective assessment for the insurance IT outsourcing risk.},
  Doi                      = {10.1109/ICIFE.2010.5609282},
  File                     = {Published version:Zheng-2010-p202-206.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {information technology;insurance;investment;outsourcing;risk management;Borda Rank method;COBIT framework structure;Control Objectives for Information and related Technology;Delphi methods;Risk Matrix method;financial industry;information technology;insurance IT outsourcing;insurance informatization construction;investments;risk assessment modeling;Biological system modeling;Contracts;Industries;Insurance;Outsourcing;Risk management;IT outsourcing;insurance industry;risk assessment;risk matrix},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.12.13},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5609282&isnumber=5609243},
  Zone                     = {-}
}

@InProceedings{Zheng-2009-p1-6,
  Title                    = {A Study of Network Throughput Gain in Optical-Wireless (FiWi) Networks Subject to Peer-to-Peer Communications},
  Author                   = {Zeyu Zheng and Jianping Wang and Jin Wang},
  Booktitle                = {Communications, 2009. ICC '09. IEEE International Conference on},
  Year                     = {2009},
  Month                    = {June},
  Pages                    = {1-6},

  Doi                      = {10.1109/ICC.2009.5198676},
  File                     = {Published version:Zheng-2009-p1-6.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1938-1883},
  Keywords                 = {Internet;optical fibre networks;peer-to-peer computing;radio access networks;FiWi network;Internet access;PON subnetwork;access network architecture;network throughput gain;optical-wireless networks;passive optical networks;peer-to-peer communications;wireless mesh networks;wireless subnetwork;wireless-optical-wireless communication mode;Bandwidth;IP networks;Interference;Optical fiber networks;Optical network units;Passive optical networks;Peer to peer computing;Telecommunication traffic;Throughput;Wireless communication},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.04},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=622912&isnumber=13557},
  Zone                     = {-}
}

@Article{Zhou-2015-p1-1,
  Title                    = {Design and Analysis of Multimodel-Based Anomaly Intrusion Detection Systems in Industrial Process Automation},
  Author                   = {Zhou, C. and Huang, S. and Xiong, N. and Yang, S.-H. and Li, H. and Qin, Y. and Li, X.},
  Journal                  = {Systems, Man, and Cybernetics: Systems, IEEE Transactions on},
  Year                     = {2015},
  Number                   = {99},
  Pages                    = {1-1},
  Volume                   = {PP},

  Abstract                 = {Industrial process automation is undergoing an increased use of information communication technologies due to high flexibility interoperability and easy administration. But it also induces new security risks to existing and future systems. Intrusion detection is a key technology for security protection. However, traditional intrusion detection systems for the IT domain are not entirely suitable for industrial process automation. In this paper, multiple models are constructed by comprehensively analyzing the multidomain knowledge of field control layers in industrial process automation, with consideration of two aspects: physics and information. And then, a novel multimodel-based anomaly intrusion detection system with embedded intelligence and resilient coordination for the field control system in industrial process automation is designed. In the system, an anomaly detection based on multimodel is proposed, and the corresponding intelligent detection algorithms are designed. Furthermore, to overcome the disadvantages of anomaly detection, a classifier based on an intelligent hidden Markov model, is designed to differentiate the actual attacks from faults. Finally, based on a combination simulation platform using optimized performance network engineering tool, the detection accuracy and the real-time performance of the proposed intrusion detection system are analyzed in detail. Experimental results clearly demonstrate that the proposed system has good performance in terms of high precision and good real-time capability.},
  Doi                      = {10.1109/TSMC.2015.2415763},
  File                     = {Published version:Zhou-2015-p1-1.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {2168-2216},
  Keywords                 = {Automation;Control systems;Hidden Markov models;Intrusion detection;Process control;Protocols;Anomaly intrusion detection;hidden Markov model (HMM);industrial process automation;multimodel;security},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.05.08},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7081762&isnumber=6376248},
  Zone                     = {-}
}

@InProceedings{Zhou-2009-p4270-4274,
  Title                    = {Dynamic Risk Assessment for Excavation Engineering Based on Human Factors},
  Author                   = {Jian Zhou and Qunyan Tang and Zhang, Jiao},
  Booktitle                = {Information Science and Engineering (ICISE), 2009 1st International Conference on},
  Year                     = {2009},
  Month                    = {Dec},
  Pages                    = {4270-4274},

  Abstract                 = {The excavation of foundation pits is usually confronted with many potential risks during the long construction time. Risks often come from varies of working procedures. Many factors may cause the procedures failing, but human errors show the greatest probability to cause accidents. Luke Model is a valid method of quantitative analysis for human factors. Through quantitative analysis, get the failure probability of excavation engineering caused by human factors. Based on these probabilities, use method of entropy weight measurement to calculate risks degree and coefficient of entropy weight of each human factor. Finally control the risks of each working procedure according to these two indexes and eventually get the aim of reducing risks maximally.},
  Doi                      = {10.1109/ICISE.2009.523},
  File                     = {Published version:Zhou-2009-p4270-4274.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {civil engineering;entropy;failure analysis;foundations;human factors;probability;risk management;Luke model;construction time;dynamic risk assessment;entropy weight measurement;excavation engineering;failure probability;foundation pit excavation;human factors;quantitative analysis;Accidents;Conference management;Engineering management;Entropy;Failure analysis;Human factors;Mathematical model;Probability;Risk analysis;Risk management},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.11.28},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5455234&isnumber=5454428},
  Zone                     = {-}
}

@InProceedings{Zhou-2010-p3213-3218,
  Title                    = {Impact of smart metering on energy efficiency},
  Author                   = {Long Zhou and Fang-Yuan Xu and Ying-Nan Ma},
  Booktitle                = {Machine Learning and Cybernetics (ICMLC), 2010 International Conference on},
  Year                     = {2010},
  Month                    = {July},
  Pages                    = {3213-3218},
  Volume                   = {6},

  Doi                      = {10.1109/ICMLC.2010.5580715},
  File                     = {Published version:Zhou-2010-p3213-3218.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {metering;smart power grids;UK;cyber security;energy efficiency;governmental fiscal stimuli;smart grid technology;smart metering;Companies;Electricity;Government;Investments;Pricing;Real time systems;Smart grids;Intelligence;Smart grid;Smart metering},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5580715&isnumber=5580670},
  Zone                     = {-}
}

@Article{Zhu-2007-p1733-1755,
  Title                    = {A framework to integrate software behavior into dynamic probabilistic risk assessment },
  Author                   = {Dongfeng Zhu and Ali Mosleh and Carol Smidts},
  Journal                  = {Reliability Engineering \& System Safety },
  Year                     = {2007},
  Note                     = {Special Issue on \{ESREL\} 2005 },
  Number                   = {12},
  Pages                    = {1733 - 1755},
  Volume                   = {92},

  Abstract                 = {Software plays an increasingly important role in modern safety-critical systems. Although, research has been done to integrate software into the classical probabilistic risk assessment (PRA) framework, current \{PRA\} practice overwhelmingly neglects the contribution of software to system risk. Dynamic probabilistic risk assessment (DPRA) is considered to be the next generation of \{PRA\} techniques. \{DPRA\} is a set of methods and techniques in which simulation models that represent the behavior of the elements of a system are exercised in order to identify risks and vulnerabilities of the system. The fact remains, however, that modeling software for use in the \{DPRA\} framework is also quite complex and very little has been done to address the question directly and comprehensively. This paper develops a methodology to integrate software contributions in the \{DPRA\} environment. The framework includes a software representation, and an approach to incorporate the software representation into the \{DPRA\} environment SimPRA. The software representation is based on multi-level objects and the paper also proposes a framework to simulate the multi-level objects in the simulation-based \{DPRA\} environment. This is a new methodology to address the state explosion problem in the \{DPRA\} environment. This study is the first systematic effort to integrate software risk contributions into \{DPRA\} environments. },
  Doi                      = {http://dx.doi.org/10.1016/j.ress.2006.09.024},
  File                     = {Published version:Zhu-2007-p1733-1755.pdf:PDF},
  Impactfactor             = {1.901},
  ISSN                     = {0951-8320},
  Keywords                 = {Probabilistic risk assessment},
  Level                    = {A},
  Owner                    = {Qiqi},
  Review                   = {A},
  Timestamp                = {2014.11.25},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0951832006002171},
  Zone                     = {2Q}
}

@Article{Zong-fa-2006-p-,
  Title                    = {Ministry of Education, Changping District, Beijing 102206, China); A Method to Analyze Power System Cascading Failure Based on Fault Tree and Fuzzy Reasoning [J]},
  Author                   = {Zong-fa, ZHOU and Xin, AI and Hui-qiong, DENG and Hao, LIU},
  Journal                  = {Power System Technology},
  Year                     = {2006},
  Volume                   = {8},

  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.01.21},
  Zone                     = {-}
}

@InProceedings{Zuccato-2011-p521-526,
  Title                    = {Service Security Requirement Profiles for Telecom: How Software Engineers May Tackle Security},
  Author                   = {Zuccato, A and Daniels, N. and Jampathom, C.},
  Booktitle                = {Availability, Reliability and Security (ARES), 2011 Sixth International Conference on},
  Year                     = {2011},
  Month                    = {Aug},
  Pages                    = {521-526},

  Doi                      = {10.1109/ARES.2011.81},
  File                     = {Published version:Zuccato-2011-p521-526.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {formal specification;security of data;telecommunication security;business oriented security requirement profiles;security requirement engineering;service security requirement profiles;software engineers;telecom;Availability;Information security;Privacy;Resilience;Risk analysis;Holistic Security Requirement;Security Requirement Profile;Service Security Requirement},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6045971&isnumber=6045921},
  Zone                     = {-}
}

@InProceedings{Zuccato-2008-p1139-1147,
  Title                    = {Security Requirement Engineering at a Telecom Provider},
  Author                   = {Zuccato, A and Endersz, V. and Daniels, N.},
  Booktitle                = {Availability, Reliability and Security, 2008. ARES 08. Third International Conference on},
  Year                     = {2008},
  Month                    = {March},
  Pages                    = {1139-1147},

  __markedentry            = {[Qiqi:]},
  Doi                      = {10.1109/ARES.2008.14},
  File                     = {Published version:Zuccato-2008-p1139-1147.pdf:PDF},
  Impactfactor             = {-},
  Keywords                 = {formal specification;telecommunication security;telecommunication services;SKYDD covering information;checklists;information classification;reference tables;security requirement engineering;telecom business information security;telecom provider;Availability;Design methodology;Information security;Lead time reduction;Mathematical model;Protection;Reliability engineering;Risk analysis;Risk management;Telecommunications},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4529472&isnumber=4529303},
  Zone                     = {-}
}

@PhdThesis{刘燕辉-2012-p-,
  Title                    = {基于贝叶斯网络的攻击图分析},
  Author                   = {刘燕辉},
  School                   = {华东理工大学},
  Year                     = {2012},

  Abstract                 = {随着网络安全威胁的增加,网络威胁行为表现出了复杂性、多样性和不确定性等特点,原来的网络安全检测工具只能检测到单个主机存在的安全威胁,但是网络脆弱性必须考虑到在整个网络中的存在的多步骤、多主机的攻击场景。目前普遍的方法就是对于给定的网络配置建立攻击图。攻击图可以用来表示攻击者在试图攻击网络时,是否可以从初始状态经过连续的一系列攻击并利用已经取得的权限作为跳板再次发起攻击最终到达目标状态。一个攻击图包含一系列的攻击路径,每一个攻击路径都描绘了一个攻击场景。随着攻击场景的增加,整个网络的安全性就会下降。因此有必要对一个给定的网络进行定量分析。

贝叶斯网络是目前对不确定性知识的表达和推理的一种最有效的工具。本文在贝叶斯网络的基础上对攻击图进行整体安全分析,然后结合前人对网络安全评估研究的基础上,采用CVE漏洞库中得分级评分标准作为一个有力的证据,使得其辅助贝叶斯网络的CBF(条件概率表)形成贝叶斯网络从而进一步对攻击图进行量化分析。利用入侵检测系统发出的告警作为贝叶斯网络推理证据,通过推理计算攻击图中的每条路径的概率值进行比较,从而可以快速准确的找到攻击者试图发起进攻的路径,预测其攻击的最终目标。

入侵检测系统的引入是本文的主要特点,利用入侵检测系统对网路中的非法入侵行为发出实时告警的主要功能,以贝叶斯网络推理实验为基础,增加了实时证据引入,使得网络安全状态的更新得到了较大的改进,同时贝叶斯网络推理算法的引入,使得网络安全的分析结果更加有说服力,与其他方法相比,能更加方便快速的帮助网络管理员对网络中存在的安全威胁进行及时的修复,从而大大增强了网络的安全和稳定。},
  Impactfactor             = {-},
  Keywords                 = {攻击图;贝叶斯网络;入侵检测系统;条件概率表},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.29},
  Zone                     = {-}
}

@Article{张少俊-2010-p2376-2386,
  Title                    = {贝叶斯推理在攻击图节点置信度计算中的应用},
  Author                   = {张少俊 and 李建华 and 宋珊珊 and 李斓 and 陈秀真},
  Journal                  = {软件学报},
  Year                     = {2010},
  Number                   = {09},
  Pages                    = {2376--2386},

  Abstract                 = {网络攻击图是根据观测到的攻击证据推测网络安全状态的理想模板.现有的攻击图节点置信度计算方法或在模型通用性、计算复杂度方面存在一定不足,或又过多依靠经验公式进行推理而缺乏严密的数学理论支撑.为此,提出一种基于贝叶斯推理的攻击图节点置信度计算方法.方法对似然加权法进行了改进,以支持攻击证据之间的时间偏序关系.实验结果表明,该方法能够有效提高节点置信度的计算准确性,且具有线性计算复杂度,适合于处理大规模攻击图节点置信度的实时计算问题.},
  Impactfactor             = {-},
  Keywords                 = {网络安全;攻击图;置信度;贝叶斯推理;似然加权},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.29},
  Zone                     = {-}
}

@Article{王玉敏-2012-p60-65,
  Title                    = {工业控制系统的常见攻击},
  Author                   = {王玉敏},
  Journal                  = {中国仪器仪表},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {60--65},

  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.10.18},
  Zone                     = {-}
}

@PhdThesis{赵华-2013-p-,
  Title                    = {工业控制系统异常检测算法研究},
  Author                   = {赵华},
  School                   = {冶金自动化研究设计院},
  Year                     = {2013},

  __markedentry            = {[Qiqi:]},
  Abstract                 = {工业控制系统(Industrial Control Systems,ICS)被广泛应用在国家关键基础设施中,是石化、冶金、交通等运行的重要环节。但是随着计算机技术与ICS的深度融合,使得原本相对孤立封闭的工业控制系统变得开放,面临日趋严重的信息安全问题。针对此问题,人们一直在寻求解决ICS信息安全问题的方法和技术,以确保关键基础设施安全运行。近几年,国内外的一些学者均提出应用入侵检测技术检测针对ICS的攻击。

在阅读大量国内外参考文献的基础上,本文对ICS信息安全现状和入侵检测系统进行了归纳总结,并对异常检测技术及算法进行了概括介绍。针对以往一些检测方法在ICS网络安全防护应用中存在的缺点,从工业控制系统的特点出发,本文提出一种基于被控对象模型的非参数CUSUM异常检测方法。利用ICS输入决定输出的特性,建立ICS的数学模型预测系统的输出,一旦控制系统的传感器遭受攻击,实际输出信号将发生改变。计算工业控制模型的预测输出与传感器测量信号的差值,形成基于时间的统计序列,采用改进的非参数CUSUM算法,实现入侵检测并报警。

仿真检测实验证明,该方法具有良好的实时性和低误报率。还对监测ICS中的误操作有一定帮助。},
  Impactfactor             = {-},
  Keywords                 = {工业控制系统;信息安全;异常检测;非参数CUSUM算法},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.08.29},
  Zone                     = {-}
}

@Standard{IEC62443-1-1,
  Title                    = {Industrial communication networks – Network and system security --- Part 1-1: Terminology, concepts and models},
  Institution              = {International Electrotechnical Commission},
  Organization             = {International Electrotechnical Commission},

  File                     = {IEC62443-1-1.pdf:IEC62443-1-1.pdf:PDF},
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.02.03},
  Zone                     = {-}
}

@Book{,
  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.19},
  Zone                     = {-}
}

@Article{-2014-p1-33,
  Title                    = {IEEE Draft Standard for Cyber Security Requirements for Substation Automation, Protection and Control Systems},
  Journal                  = {IEEE PC37.240/D14, June 2014},
  Year                     = {2014},

  Month                    = {July},
  Pages                    = {1-33},

  Impactfactor             = {-},
  Keywords                 = {Access control;Authentication;Computer security;Encryption;IEEE standards;Network security;Remote access;Substations;critical infrastructure protection;cyber security;electronic access;encryption;password management;remote access;substations},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.05},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6851841&isnumber=6851840},
  Zone                     = {-}
}

@Book{,
  Title                    = {Guidelines for Initiating Events and Independent Protection Layers in Layer of Protection Analysis},
  Publisher                = {Center for Chemical Process Safety},
  Year                     = {2014},
  Month                    = {December},

  Impactfactor             = {-},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2015.04.28},
  Zone                     = {-}
}

@InProceedings{-2005-p-,
  Title                    = {2005 IEEE Wireless Communications and Networking Conference (IEEE Cat. No.05TH8794)},
  Booktitle                = {Wireless Communications and Networking Conference, 2005 IEEE},
  Year                     = {2005},
  Month                    = {March},
  Pages                    = {-},
  Volume                   = {2},

  Doi                      = {10.1109/WCNC.2005.1424580},
  File                     = {Published version:-2005-p-.pdf:PDF},
  Impactfactor             = {-},
  ISSN                     = {1525-3511},
  Keywords                 = {3G mobile communication;4G mobile communication;MIMO systems;OFDM modulation;access protocols;ad hoc networks;cellular radio;code division multiple access;diversity
reception;encoding;indoor radio;internetworking;iterative methods;metropolitan area networks;mobility management (mobile radio);multicast communication;multimedia
communication;multiuser channels;personal communication networks;quality of service;radio receivers;resource allocation;satellite communication;scheduling;security of
data;signal processing;spread spectrum communication;telecommunication congestion control;telecommunication network planning;telecommunication network routing;telecommunication
security;telecommunication traffic;transceivers;transport protocols;ultra wideband technology;wireless LAN;wireless sensor networks;3G networks;4G wireless networks
concepts;802.11 MAC;CDMA link design;IP mobility;IPv6 mobility;MANET architecture;MANET connectivity;MANET routing;MIMO coding;MIMO transceiver design;MIMO-OFDM signal
processing;TCP over wireless;UWB;WLAN mobility;call admission control;cellular network engineering;cellular network performance;cellular-WLAN interworking;coding
principles;coexistence;communication systems performance analysis;communication theory;communications signal processing;cross-layer design;diversity;energy
efficiency;experimental evaluations;handoff management;indoor services;iterative multiuser receivers;key management;location management;location tracking;mobile ad-hoc
networks;mobility tracking;multicast routing;multimedia;multiuser DS-CDMA;multiuser communications;paging;power efficient broadcasting;quality of service;radio resource
management;satellite networks;scheduling;sensor networks;service accountability;service provisioning;spectrum utilization;traffic engineering;voice over IP;wireless MAC
analysis;wireless MAC protocols;wireless channel models;wireless circuits;wireless metropolitan area networks;wireless network optimization;wireless network performance;wireless
network security;wireless security;wireless transmission enhancements},
  Level                    = {-},
  Owner                    = {Qiqi},
  Timestamp                = {2014.09.04},
  Url                      = {http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1424580&isnumber=30729},
  Zone                     = {-}
}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:中文\;1\;刘燕辉-2012-p-\;王玉敏-2012-p60-65\;赵华-2013-p-\;;
1 ExplicitGroup:其他\;0\;Cheng-2014-p1372-1381\;Fajardo-2013-p260-272\;G
ordon-2006-p1-1\;Volkanovsk-2009-p1116-1127\;Zong-fa-2006-p-\;;
1 ExplicitGroup:信息安全\;0\;-2014-p1-33\;Abercrombi-2011-p1-10\;Adamov-20
11-p169-172\;Ahmed-2013-p871-878\;Al-Shorafa-2013-p666-673\;Atmaja-201
1-p1-6\;Beckers-2013-p465-474\;Cheminod-2012-p247-265\;Dandurand-2013-
p1-16\;Du-2009-p141-149\;Ericsson-2010-p1501-1507\;Farkhani-2006-p2778
-2783\;Fletcher-2011-p106-113\;Gaska-2013-p1-19\;Haddad-2011-p1-6\;Hal
ey-2008-p133-153\;He-2012-p1-6\;Islam-2011-p70-75\;Jerman-Bla-1990-p36
1-365\;Jiang-2010-p355-360\;Kassou-2012-p7-11\;Kornecki-2013-p1393-139
9\;Lai-2013-p366-371\;Lin-2003-p371-372\;Linda-2011-p26-32\;Mellado-20
06-p8-8\;Menzel-2009-p41-48\;Mougouei-2012-p111-116\;Okubo-2012-p367-3
72\;Pallotti-2011-p1-4\;Peng-2013-p442-447\;Ramjan-2012-p298-302\;Roy-
2004-p509-512\;Rugge-2012-p1-10\;Rysavy-2013-p1435-1440\;Ryu-2008-p653
-658\;Salini-2012-p1-6\;Soler-2008-p282-289\;Thuraising-2009-p2-3\;Ven
kataSwa-2011-p1-6\;Wagner-2012-p1-4\;Warschofsk-2010-p43-50\;Weiss-200
8-p169-172\;Yahya-2013-p190-194\;Yang-2011-p1-7\;Zhou-2010-p3213-3218\
;Zuccato-2008-p1139-1147\;Zuccato-2011-p521-526\;;
2 ExplicitGroup:Cyber Security Requirement\;0\;Al-Shorafa-2013-p666-67
3\;Beckers-2013-p465-474\;Du-2009-p141-149\;Farkhani-2006-p2778-2783\;
Haley-2008-p133-153\;Islam-2011-p70-75\;Jerman-Bla-1990-p361-365\;Kass
ou-2012-p7-11\;Lai-2013-p366-371\;Lin-2003-p371-372\;Lin-2003-p371-372
a\;Markose-2008-p75-81a\;Mellado-2006-p8-8\;Menzel-2009-p41-48\;Mougou
ei-2012-p111-116\;Okubo-2012-p367-372\;Roy-2004-p509-512\;Salini-2012-
p1-6\;Salini-2012-p1785-1797\;Soler-2008-p282-289\;Soler-2008-p282-289
a\;VenkataSwa-2011-p1-6\;Warschofsk-2010-p43-50\;Weiss-2008-p169-172\;
Xu-2006-p79-82\;Yahya-2013-p190-194\;Zuccato-2008-p1139-1147\;Zuccato-
2011-p521-526\;;
2 ExplicitGroup:Risk Assessment\;0\;;
3 ExplicitGroup:Function Dependency\;0\;Abadeer-2003-p17-22\;Ambrose-2
014-p267-273\;Degang-2013-p1-1\;Dullaert-2010-p1-4\;Guo-2011-p862-867\
;Gupta-2008-p965-968\;Hamadani-2007-p1191-1195\;Hofmann-1990-p84-87\;H
ousel-1979-p194-208\;Kameyama-1977-p1297-1302\;Kikuchi-2001-p4-2293\;K
izony-2009-p211-211\;Lang-2010-p1-4\;Ma-2010-p2-66\;Malik-2010-p950-95
5\;Nie-2008-p1-4\;Paluri-2005-p6561-6566\;Sendur-2002-p2744-2756\;Shua
ngchen-2009-p431-434\;Vasyliuk-2011-p222-223\;Vogelsang-2012-p8-14\;Xu
-2007-p532-534\;Youdong-2008-p445-448\;Zhang-2008-p25-30\;;
2 ExplicitGroup:Domino\;0\;Abdolhamid-2010-p416-426\;Abdolhamid-2011-p
575-593\;;
2 ExplicitGroup:轨迹预测\;0\;Vasquez-2004-p4-3936\;;
1 ExplicitGroup:工业控制系统风险评估\;0\;-2014-p319-334\;Aneziris-2010-p321-329\
;Aqlan-2014-p39-48\;Aziz-2014-p77-84\;Baird-2006-p309-318\;Baron-1999-
p87-100\;Basole-2014-p109-120\;Bernatik-2004-p271-278\;Beroggi-1994-p1
450-1457\;Billinton-2004-p1759-1765\;Bolognesi-2014-p345-355\;Chelton-
1991-p269-288\;Dai-2001-p616-623\;Davies-2002-p203-230\;Dimopoulos-201
2-p2219-2226\;Floyd-2008-p25-31\;Goel-2000-p381-386\;Huang-2012-p1056-
1066\;Hudepohl-1996-p56-60\;Kull-2008-p409-419\;Kvam-1998-p30-34\;Lang
-2000-p279-284\;Liu-2012-p1067-1078\;Liu-2013-p23-36\;Lo-2003-p573-581
\;McCaffery-2007-p24-31\;McCalley-1999-p1400-1406\;Mendiluce-2014-p34-
41\;Moore-2013-p1685-1689\;Niskanen-2012-p535-543\;Pinto-2014-p57-76\;
Pujol-2013-p283-296\;Qinqin-2014-p251-260\;Ribe-2012-p15-20\;Sauer-199
2-p242-249\;Singh-2014-p1-15\;Sivulka-2009-p174-185\;Syrus-2001-p339-3
50\;Syrus-2001-p351-358\;Topuz-2011-p393-403\;Tsai-2011-p158-171\;Worr
all-2012-p32-44\;Zamanali-1998-p361-364\;Zhang-2013-p79-85\;;
2 ExplicitGroup:功能失效\;0\;Bai-2008-p550-555\;Betensky-2003-p7-8\;Chen-2
007-p78-85\;Drumheller-1999-p627-636\;Ferreyra-2005-p494-500\;Heussen-
2012-p1-6\;Hibey-1983-p621-624\;Hirata-2012-p1340-1343\;Imaizumi-2004-
p2-959\;Ishikawa-2008-p333-337\;Jiang-2012-p747-751\;Laukonen-1994-p18
4-189\;Li-2010-p120-125\;Liu-2002-p2-954\;Mabrouk-2014-p1148-1151\;Nav
arro-1996-p685-690\;Petakos-2000-p163-164\;Shore-1997-p116-121\;Sobh-1
996-p33-36\;Toyota-1999-p886-891\;Vucovich-2007-p405-412\;Wang-2012-p1
-4\;Williams-2009-p1-28\;Xiaoqian-2013-p102-107\;;
1 ExplicitGroup:概率风险评估\;0\;Baudrit-2006-p593-608\;BenMahmou-2011-p1-9\
;Bogumil-1982-p24-28\;Chou-1998-p1009-1013\;Dunglinson-1983-p150-163\;
Durant-1988-p138-143\;Fenton-2014-p21-26\;Henneaux-2013-p4722-4731\;Hs
iao-2008-p311-321\;Huang-2012-p1485-1493\;Huh-2011-p210-218\;Kelly-199
0-p1001-1021\;Ladbury-2011-p3004-3010\;Ladbury-2013-p4457-4463\;Ladbur
y-2013-p4464-4469\;Laugier-2011-p4-19\;Li-2001-p84-88\;Nava-2004-p657-
662\;Stewart-1991-p1000-1007\;Wilson-2000-p127-130\;Zamanali-1998-p361
-364a\;;
1 ExplicitGroup:语言表达\;0\;Ahmed-2008-p226-240\;Basu-1993-p177-189\;Boyc
e-2007-p386-397\;Chou-1994-p1566-1575\;Clancey-1989-p9-23\;Deng-1990-p
295-310\;Devabhaktu-2003-p1822-1833\;Dolk-1984-p619-628\;Fernandez--20
04-p164-171\;Freedman-1991-p347-358\;Harp-1995-p727-737\;Hong-2003-p52
8-541\;Jones-1995-p1025-1038\;Keller-1991-p82-87\;Lanza-2014-p319-325\
;Liu-2013-p115-127\;Luo-2014-p86-102\;Machias-1990-p701-708\;Manivanna
n-1994-p534-548\;Na-2014-p499-501\;Rudinskiy-2007-p68-73\;Sacha-2014-p
1604-1613\;Sakti-2007-p1199-1211\;Sirounian-1995-p82-105\;Wang-1997-p2
333-2343\;Zhang-2001-p312-323\;;
1 ExplicitGroup:故障树\;0\;Bian-2009-p4452-4457\;Carrasco-2009-p207-220\;
Chaoyang-2010-p959-962\;Cheshmikha-2014-p1-6\;Cox-1982-p465-468\;Ding-
2013-p4752-4756\;Du-1973-p235-240\;El-Hadidy-2013-p1-5\;Fronczak-1997-
p259-259\;Gang-2011-p157-160\;Hu-2010-p1-5\;Janan-1985-p329-337\;Khosh
gofta-1996-p244-251\;Koren-1977-p1154-1158\;Merle-2011-p1-6\;Modarres-
1984-p325-328\;Schneeweis-1997-p201-207\;Schneeweis-2001-p161-166\;Ste
cher-1986-p51-58\;Wang-2007-p1051-1054\;Wang-2009-p1-4\;Yi-2009-p264-2
68\;Youjun-2011-p1949-1952\;Zhao-2013-p249-254\;Zhaoyang-2011-p1-4\;;
1 ExplicitGroup:Security Risk Assessment\;0\;Amin-2013-p19-24\;Briggs-
2008-p19-24\;Chen-2005-p823-834\;Chen-2005-p823-834a\;Dai-2001-p616-62
3a\;Dai-2001-p616-623b\;Dissanayak-2011-p1302-1308a\;Duan-2014-p53-58\
;Ericsson-2009-p1174-1181\;Feng-2011-p4332-4340\;Fu-2011-p124-128\;GUA
N-2013-p60-63\;He-2008-p249-253\;Hole-2009-p34-41\;Kirschen-2007-p527-
533\;Kirschen-2007-p527-533a\;Lijian-2010-p3-604\;Liu-2011-p869-875\;L
iu-2011-p876-881\;Liu-2012-p600-609\;Lo-2012-p247-257\;Moore-2013-p168
5-1689a\;Ni-2003-p1165-1172\;Poolsappas-2012-p61-74a\;Shamala-2013-p45
-52\;Shi-2011-p272-277\;Tamasi-2011-p892-899\;Tamasi-2011-p892-899a\;T
sai-2011-p801-816a\;Wan-2000-p1247-1254\;Xiao-2009-p78-85\;Yang-2014-p
72-90\;Yang-2014-p72-90a\;You-2011-p2062-2074\;;
1 ExplicitGroup:Dynamic Risk Assessment\;0\;Abimbola-2014-p74-85\;Ahme
d-2013-p777-785\;Ammar-2001-p171-183a\;Baron-1999-p87-100b\;Bi-2012-p1
112-1118\;Bontempi-2004-p533-542a\;Dissanayak-2011-p1302-1308b\;Fang-2
005-p201-207a\;Gain-2015-p120-131\;Ghazel-2014-p1123-1133a\;Goseva-Pop
-2003-p946-960a\;Henneaux-2013-p4722-4731b\;Hsieh-2014-p268-276\;Hu-20
13-p522-533\;Huh-2011-p210-218b\;Kalantarni-2009-p600-606\;Kalantarni-
2010-p191-199\;Kim-2014-p124-133\;Laugier-2011-p4-19b\;Li-2014-p59-65\
;Linda-2011-p194-200a\;Loncomilla-2014-p406-415a\;Mandelli-2013-p146-1
60\;Paltrinier-2014-p669-679\;Poolsappas-2012-p61-74b\;Rey-2014-p109-1
19\;Roy-2014-p-\;Stroeve-2013-p133-149\;Tsai-2011-p801-816b\;Vezzaro-2
014-p292-303a\;Vierow-2014-p320-328\;Xu-2011-p314-322a\;Yacoub-2002-p5
29-547a\;Zhu-2007-p1733-1755\;;
1 ExplicitGroup:Simulation\;0\;Aitken-2010-p1-6\;Ammar-2001-p171-183\;
Baron-1999-p87-100a\;Beroggi-2000-p32-44\;Bogumil-1982-p24-28a\;Bontem
pi-2004-p533-542\;Cheng-2008-p532-538\;Cheng-2012-p1106-1109\;Dissanay
ak-2011-p1302-1308\;Ertle-2012-p1-5\;Fang-2005-p201-207\;Fu-2010-p154-
159\;Gasteiger-2012-p2178-2187\;Ghazel-2014-p1123-1133\;Glaber-2014-p2
506-2515\;Goseva-Pop-2003-p946-960\;Guang-Bin-2012-p1-4\;Gueven-2011-p
663-670\;Henneaux-2013-p4722-4731a\;Hou-2013-p1001-1004\;Huh-2011-p210
-218a\;Jin-2012-p65-73\;Kapela-2004-p1915-1922\;Kuo-2011-p643-646\;Lau
gier-2011-p4-19a\;LeitedaS-1999-p1-205\;Li-2008-p455-460\;Linda-2011-p
194-200\;Liu-2010-p198-209\;Liu-2013-p75-79\;Loncomilla-2014-p406-415\
;Poolsappas-2012-p61-74\;Qi-2010-p1-4\;Song-2013-p1-6\;Tsai-2011-p801-
816\;Vega-2013-p735-738\;Wang-2006-p46-59\;Wang-2008-p1-6\;Wardzinski-
2008-p1-4\;Wei-2010-p1-5\;Wesley-2002-p7-3389\;Worrall-2012-p32-44a\;X
u-2004-p214-219\;Xu-2010-p1-7\;Xu-2011-p314-322\;Yacoub-2000-p210-221\
;Yacoub-2002-p529-547\;Zhou-2009-p4270-4274\;;
1 ExplicitGroup:Intrusion Evidence\;0\;-2013-p465-470\;Dun-2014-p191-2
01\;Gal-2013-p59-80\;Hubballi-2014-p1-17\;Kim-2014-p119-134\;Liu-2015-
p180-197\;Pearson-2012-p1026-1031\;Santos-2013-p33-41\;Soellner-2014-p
84-96\;Sun-2014-p83-95\;Tombros-2014-p-\;Villemant-2014-p247-277\;Wang
-2015-p53-70\;;
1 ExplicitGroup:Intrusion Detect\;0\;Ahmed-2011-p1738-1749\;Altwaijry-
2012-p1-6\;Chen-2014-p5972-5983\;Davis-2011-p353-375\;Elbasiony-2013-p
753-762\;Fahrner-2012-p262-271\;Gowrison-2013-p921-927\;Herrero-2013-p
250-261\;Mukherjee-2012-p119-128\;Patel-2013-p25-41\;Pereira-2012-p122
6-1234\;Pinzon-2013-p15-31\;Rouached-2012-p968-975\;Sengupta-2013-p161
-168\;Shahaboddin-1\;Shin-2013-p315-322\;Wang-2006-p93-100\;Wu-2011-p5
48-555\;Yang-2007-p479-484\;Yang-2011-p538-546\;;
1 ExplicitGroup:Failure Prediction\;0\;-2014-p860-864\;Bala-2015-p980-
989\;Baldoni-2014-p-\;Bandara-2014-p421-435\;Bielikova-2014-p48-56\;Ch
en-2013-p180-195\;Dimic-2014-p955-960\;Filipe-2014-p-\;Gandhi-2014-p-\
;Hassine-2014-p15-24\;Hindley-2014-p10-19\;Hoffmann-2014-p201-205\;Jar
din-2014-p-\;Kawamura-2014-p143-152\;Khanmohamm-2015-p1782-1785\;Kutyl
owsk-2015-p41-48\;Levy-2014-p437-439\;Li-2014-p42-53\;Li-2014-p63-72\;
Lv-2014-p71-79\;Maleki-2014-p36-58\;Mattiasson-2014-p175-191\;Park-201
4-p123-136\;Xu-2014-p59-67\;Zhang-2014-p103-112\;;
1 ExplicitGroup:Function Reliability\;0\;Ben-Haim-2008-p5-12\;Bowerman
-1990-p158-161\;Bowerman-1990-p162-166\;Canfield-1970-p13-16\;Chan-196
8-p199-201\;Chee-2014-p7026-7034\;Chung-1971-p85-87\;Ferreyra-2005-p49
4-500b\;Fujii-2006-p3310-3313\;H-2000-p2117-2132\;Hajek-2002-p828-839\
;Han-2014-p6819-6843\;Haroutunia-2000-p2690-2697\;Harutyunya-2004-p276
8-2773\;Hwang-1979-p316-319\;Kemerer-1992-p1011-1024\;Kini-1982-p752-7
71\;Kleyle-1978-p153-160\;Lin-2002-p41-46\;Quigley-2005-p604-611\;Smei
tink-1990-p71-75\;Yeh-2009-p374-382\;Yeh-2009-p476-484\;Yeh-2011-p404-
414\;Yeh-2012-p702-709\;Yoon-1999-p1067-1071\;;
1 ExplicitGroup:其他\;0\;;
1 ExplicitGroup:Case Study\;0\;Vezzaro-2014-p1707-1716\;Vezzaro-2014-p
292-303b\;;
1 ExplicitGroup:Function Failure\;0\;Alam-2002-p42-48\;Betensky-2003-p
7-8a\;Ch-2006-p189-198\;Chen-2007-p78-85a\;Chen-2009-p275-283\;Drumhel
ler-1999-p627-636a\;Ferreyra-2005-p494-500a\;Hibey-1983-p621-624a\;Kaw
ada-2005-p1341-1353\;Launer-1993-p71-80\;Miranda-1999-p265-267\;Navarr
o-1996-p685-690a\;Petakos-2000-p163-164a\;Pham-2007-p454-458\;Pierce-1
982-p1452-1458\;Scheuer-1969-p30-30\;Shore-1997-p116-121a\;Takaoka-197
1-p536-542\;Taylor-1962-p13-17\;Toniutti-2010-p3074-3083\;Yamamoto-199
2-p28-35\;Yu-2013-p714-721\;;
1 ExplicitGroup:Industry Risk Assessment\;0\;-2011-p428-433\;Ando-2000
-p358-363\;Aneziris-2010-p321-329a\;Aqlan-2014-p39-48a\;Bartlett-2002-
p344-349\;Basharat-2013-p176-181\;Berges-2014-p1887-1890a\;Bernatik-20
04-p271-278a\;Carin-2008-p20-26\;Chan-2009-p2405-2414\;Chan-2011-p492-
500\;Cheminod-2013-p277-293\;Davies-2002-p203-230a\;Dhillon-1988-p199-
208\;Dunglinson-1983-p150-163a\;Guo-2011-p5783-5786\;Hamoud-2003-p1587
-1593\;Hearn-2001-p730-734\;Hearty-2009-p124-137\;Huang-2008-p1354-135
9\;Huang-2012-p1056-1066a\;Joksimovic-1982-p959-969\;KarimiAzar-2011-p
9105-9111b\;Kleindorfe-2012-p85-96\;Li-2013-p413-418\;Lin-2013-p379-39
2\;Linda-2011-p194-200b\;Liu-2011-p393-397b\;Liu-2012-p1067-1078a\;Lo-
2003-p573-581a\;Lorin-2014-p50-57\;Marhavilas-2008-p596-603\;McCalley-
1999-p1400-1406a\;Moore-2013-p1685-1689b\;Murphy-1989-p28-31\;Niskanen
-2012-p535-543a\;Pinto-2011-p616-624b\;Pinto-2014-p57-76a\;Pourali-201
4-p2883-2890\;Pujol-2013-p283-296a\;Rachidi-2008-p2489-2496\;Ramprasad
-2010-p213-218\;Reinhold-2015-p282-291\;Ribe-2012-p15-20a\;Sauer-1992-
p242-249a\;Shi-2010-p132-136\;Singh-2014-p1-15a\;Sivulka-2009-p174-185
a\;Song-2009-p167-170\;Song-2009-p213-216\;Sun-2010-p2021-2025\;Syrus-
2001-p339-350a\;Syrus-2001-p351-358a\;Topuz-2011-p393-403a\;Tsai-2010-
p470-481\;Tsai-2010-p485-490\;Tsai-2011-p158-171a\;Vandenhove-2015-p14
-23\;Visser-2008-p1371-1379\;Vogel-2009-p882-889\;Wang-2003-p551-554\;
Wu-2010-p3552-3555\;Wu-2011-p2516-2519\;Xiangwei-2008-p1-4\;Yan-2011-p
1757-1760\;Yifeng-2012-p1-6\;Yu-2010-p341-344\;Zaini-2011-p320-325\;Za
manali-1998-p361-364b\;Zamanali-1998-p361-364c\;Zhandong-2011-p998-100
1\;Zhang-2011-p648-651\;Zhang-2013-p79-85a\;Zhao-2014-p1-6\;Zheng-2010
-p202-206\;;
1 ExplicitGroup:Industial Security\;0\;Alcaraz-2013-p1091-1104\;Bertol
otti-2007-p387-397\;Cheminod-2013-p277-293a\;Das-2013-p1966-1977\;Dera
khshan-2013-p315-325\;Dzung-2005-p1152-1177\;Galloway-2013-p860-880\;G
arber-2012-p17-19\;Genge-2012-p1146-1161\;Hare-2010-p128-139\;Islam-20
12-p1243-1256\;Jaatun-2014-p872-876\;Jiang-2014-p-\;Kharlamov-2014-p65
7-668\;Kim-2011-p496-506\;KumarSoma-2014-p1391-1412\;Kurubetti-2014-p8
6-94\;Leith-2013-p982-993\;Leszczyna-2015-p81-89\;Lipow-1987-p355-361\
;McLean-1990-p9-16\;Piggin-2014-p70-74\;Queiroz-2011-p589-597\;Rahimi-
2012-p3-13\;Ramos-2007-p605-610\;Rastrello-2013-p974-981\;Reniers-2008
-p336-343\;Reniers-2014-p94-101\;Sh-1994-p12-13\;Shin-2010-p744-757\;S
ilva-2014-p67-68\;Sinha-2012-p37-41\;Suleiman-2014-p-\;Theoharido-2007
-p64-67\;Valenzano-2014-p6-17\;Vollmer-2014-p1337-1347a\;Walker-1977-p
494-497\;Yampolskiy-2012-p48-58\;stouffer2011guide\;;
1 ExplicitGroup:Risk based Control\;0\;Aiyou-2014-p204-212\;Diaz-Lopez
-2014-p-\;Dongliang-2011-p1086-1093\;Iino-2006-p914-919\;Luo-2011-p144
8-1453\;Matsuura-2012-p1972-1984\;Santos-2014-p1-9\;Shaikh-2012-p447-4
64\;Wang-2013-p2194-2200\;Whist\;Yihua-2012-p1-6\;Yingzhuo-2010-p3822-
3827\;Zafra-Cabe-2011-p891-901\;;
1 ExplicitGroup:Bayes\;0\;Amundson-2014-p320-325\;Bobbio-2001-p249-260
\;Bouejla-2014-p222-230\;Buritica-2015-p233-241\;Frigault-2008-p698-70
3\;Haenninen-2014-p305-312\;Holm-2015-p304-318\;Kordy-2014-p1-38\;Liu-
2006-p2914-2918\;Liu-2015-p1917-1926\;Mukashema-2014-p331-340\;Njah-20
15-p404-416\;Perkusich-2015-p437-450\;Ramirez-2015-p119-136\;Samet-201
3-p3657-3667\;Shin-2015-p208-217\;Shin-2015-p208-217a\;Sommestad-2009-
p1-10\;Wang-2006-p2917-2933\;Wu-2011-p3433-3438\;Wu-2011-p3433-3438a\;
Wu-2012-p730-731\;Xie-2010-p211-220\;Yun-2011-p1364-1369\;Yun-2011-p3-
8\;;
2 ExplicitGroup:PPTC\;0\;Huang-1996-p225-263\;;
1 ExplicitGroup:Unknown Attack\;0\;Ahmadineja-2011-p2221-2240\;;
1 ExplicitGroup:Event Tree Risk Assessment\;0\;Auguston-2005-p2-1875\;
Burnap-2014-p608-613\;Chung-2014-p410-413\;Duqueroie-2013-p334-341\;Fa
nliang-2013-p1-4\;Garcez-2014-p624-632\;Guang-Bin-2012-p1-4a\;Hole-201
0-p21-27\;Kenarangui-1991-p120-124\;Lee-2015-p40-47\;Liankun-2014-p524
-527\;Ma-2014-p642-646\;Neri-2008-p397-415\;Powell-2010-p1-9\;Rutt-200
6-p61-70\;Sauciuc-2014-p937-941\;Stroeve-2013-p133-149a\;Vijaya-2010-p
226-229\;Wahlstrom-2014-p3132-3137\;Xu-2004-p214-219a\;Zhao-2011-p573-
578\;;
1 ExplicitGroup:Event Chain\;0\;;
1 ExplicitGroup:论文引用\;0\;Alberts-2003-p-\;Barbosa-2013-p150-158\;Carde
nas-2011-p355-366\;Chen-2005-p823-834\;Chen-2005-p823-834a\;Chen-2010-
p2-3\;Clinton-2013-p-\;Dai-2001-p616-623a\;Dai-2001-p616-623b\;Feinste
in-2003-p1-314\;Feng-2011-p4332-4340a\;Fovino-2012-p3943-3950\;Hamoud-
2003-p2-764\;Heckerman-1995-p49-57\;Jonkman-2003-p1-30\;Kaplan-1981-p1
1-27\;Karabacak-2005-p147-159\;Kirschen-2007-p527-533\;Kirschen-2007-p
527-533a\;Langner-2011-p49-51\;Lee-2002-p5-22\;Mili-2004-p38-63\;Mille
r-2012-p51-56\;Office-1999-p-\;Oman-2000-p-\;Ralston-2007-p583-594\;Ra
usand-2013-p-\;Schneier-1999-p21-29\;Shi-2011-p272-277a\;Slay-2008-p73
-82\;Stouffer-2011-p82-800\;Tamasi-2011-p892-899\;Tamasi-2011-p892-899
a\;Team-2013-p-\;Teixeira-2010-p5991-5998\;Wei-2010-p15-22\;Yang-2014-
p72-90\;Yang-2014-p72-90a\;;
2 ExplicitGroup:Model Based Risk Assessment\;0\;Aagedal-2002-p51-62\;A
hmed-2011-p1-10\;Aqlan-2014-p39-48b\;Bouwknegt-2015-p50-58\;Fan-2011-p
621-625\;Fei-2014-p616-619\;Gran-2004-p311-324\;Haimes-1999-p315-315\;
Houmb-2002-p79-91\;Izvercian-2014-p598-605\;Lagani-2015-p-\;Lavasani-2
015-p75-88\;Loutchkina-2014-p342-352\;Mei-2009-p814-823\;Meng-2012-p14
54-1464\;Paul-2014-p165-181\;Petrovic-2014-p8157-8164\;Pinto-2014-p57-
76b\;Purb-2014-p21-29\;Quan-2010-p3566-3569\;Tianshui-2014-p233-238\;X
iaolin-2008-p1057-1061\;Yan-2009-p317-320\;Zhandong-2011-p998-1001a\;Z
hao-2011-p573-578a\;Zhao-2014-p1-6a\;;
2 ExplicitGroup:Bayesian Network Simulation\;0\;Alipour-2012-p612-617\
;Frigault-2008-p698-703a\;Gribaudo-2015-p91-111\;Kordy-2014-p1-38b\;Ok
e-2007-p1-6\;Poolsappas-2012-p61-74c\;Rmayti-2014-p7-12\;Shin-2015-p20
8-217b\;Wu-2012-p730-731a\;;
2 ExplicitGroup:IEEE SMC\;0\;Asian-2014-p1139-1154\;Cardin-2015-p1-1\;
Chan-2014-p1-1\;Codetta-Ra-2015-p13-24\;Codetta-Ra-2015-p13-24a\;Deng-
2014-p209-219\;Ding-2015-p1-1\;Fanti-2015-p485-495a\;Filippini-2015-p1
57-169\;Kujawski-2013-p979-987a\;Li-2014-p422-434\;Loutchkina-2014-p34
2-352c\;Marvel-2015-p260-275a\;Mo-2014-p757-769\;Noori-2013-p229-234\;
Ross-2014-p1-1\;Samejima-2015-p725-733\;Shen-2013-p266-276\;Sun-2013-p
801-813\;Wu-2014-p1077-1087\;Wu-2014-p1624-1632a\;You-2014-p133-145\;Y
ukalov-2014-p1155-1168\;Zhou-2015-p1-1\;;
1 ExplicitGroup:Attack Propagate\;0\;Almaraz-2012-p119-131\;Arnault-20
05-p1374-1383\;Ayday-2012-p375-386\;Beretta-2015-p252-264\;Chen-2013-p
1387-1394\;Chen-2015-p22-35\;De-2009-p413-425\;Eriksson-2006-p370-379\
;Feng-2015-p87-99\;Furdek-2010-p1000-1009\;Gaeta-2014-p1482-1494\;Huan
g-2008-p2064-2074\;Jackson-2012-p930-943\;Jirattigal-2011-p249-258\;Ka
ryotis-2007-p2397-2410\;Khattab-2006-p1152-1164\;Khouzani-2012-p1347-1
360\;Kordy-2014-p1-38a\;Levy-2005-p63-65\;Li-2011-p3878-3885\;Lin-2009
-p59-69\;Liu-2007-p2-17\;Lu-2010-p1483-1493\;Ma-2013-p457-466\;Mollege
n-1966-p414-419\;Nait-Abdes-2008-p127-133\;Oliner-1969-p812-826\;Penna
-2012-p1806-1822\;Reniers-2014-p583-589\;Romeis-2014-p1-10\;Seo-2013-p
366-385\;Sheen-2013-p527-536\;Skorin-Kap-2012-p418-429\;Vosough-2014-p
4722-4725\;Wang-2014-p155-163\;Wang-2015-p897-904\;Wu-2014-p33-42\;Xia
openg-2014-p7-12\;Xiong-2001-p282-293\;Yampolskiy-2014-p-\;Yu-2008-p40
05-4017\;Yuan-2012-p1850-1860\;Yuan-2013-p1750-1760\;Zanero-2009-p70-7
4\;Zhang-2013-p1-8\;Zhang-2013-p14-18\;Zhang-2014-p161-168\;;
1 ExplicitGroup:Event Tree Bayesian\;0\;Jun-1999-p623-628\;Khakzad-201
1-p925-932\;Khakzad-2013-p46-53\;Schetinin-2013-p5466-5476\;Yang-2013-
p333-342\;;
1 ExplicitGroup:Criterion\;0\;IEC62443-1-1\;;
1 ExplicitGroup:Introduction Background\;0\;Ammar-2001-p171-183a\;Diss
anayak-2011-p1302-1308b\;Fang-2005-p201-207a\;Linda-2011-p194-200a\;Ts
ai-2011-p801-816b\;Yacoub-2002-p529-547a\;;
1 ExplicitGroup:Security Protection\;0\;Wang-2012-p1-5\;Yanliang-2010-
p1-6\;Zhang-2010-p1-5\;Zhang-2010-p5-544\;;
1 ExplicitGroup:Dynamic Protection\;0\;;
1 ExplicitGroup:Beyesian Network\;0\;Bonafede-2007-p22-28\;Fenton-2014
-p21-26a\;Goulding-2012-p4933-4940\;Gregoriade-2013-p28-43\;Greiner-20
13-p4-11\;Ie拧mantas-2014-p85-90\;Kalantarni-2009-p600-606a\;Kelly-2009
-p628-643\;Leu-2013-p122-133\;Leu2013\;Loutchkina-2014-p342-352a\;Lout
chkina2014\;Poolsappas-2012-p61-74d\;Poolsappasit2012\;Vellaithur-2015
-p566-575\;Vellaithurai2015\;Villacourt-1994-p458-462\;Villacourt1994\
;Zhang-2002-p72-72\;Zhang-2002-p770-778\;Zhang-2013-p93-105\;Zhang-201
4-p29-39\;Zhang2013\;;
1 ExplicitGroup:Test\;0\;;
2 ExplicitGroup:Test\;0\;Leu-2013-p122-133\;Leu2013\;Loutchkina-2014-p
342-352a\;Loutchkina2014\;Poolsappas-2012-p61-74d\;Poolsappasit2012\;V
ellaithur-2015-p566-575\;Vellaithurai2015\;Villacourt-1994-p458-462\;V
illacourt1994\;Zhang-2013-p93-105\;Zhang2013\;;
}

